India (Hindi: Bhārat), officially the Republic of India (Hindi: Bhārat Gaṇarājya), is a country in South Asia. It is the second-most populous country, the seventh-largest country by land area, and the most populous democracy in the world. Bounded by the Indian Ocean on the south, the Arabian Sea on the southwest, and the Bay of Bengal on the southeast, it shares land borders with Pakistan to the west; China, Nepal, and Bhutan to the north; and Bangladesh and Myanmar to the east. In the Indian Ocean, India is in the vicinity of Sri Lanka and the Maldives; its Andaman and Nicobar Islands share a maritime border with Thailand and Indonesia. Modern humans arrived on the Indian subcontinent from Africa no later than 55,000 years ago. Their long occupation, initially in varying forms of isolation as hunter-gatherers, has made the region highly diverse, second only to Africa in human genetic diversity. Settled life emerged on the subcontinent in the western margins of the Indus river basin 9,000 years ago, evolving gradually into the Indus Valley Civilisation of the third millennium BCE. By 1200 BCE, an archaic form of Sanskrit, an Indo-European language, had diffused into India from the northwest, unfolding as the language of the Rigveda, and recording the dawning of Hinduism in India. The Dravidian languages of India were supplanted in the northern and western regions. By 400 BCE, stratification and exclusion by caste had emerged within Hinduism, and Buddhism and Jainism had arisen, proclaiming social orders unlinked to heredity. Early political consolidations gave rise to the loose-knit Maurya and Gupta Empires based in the Ganges Basin. Their collective era was suffused with wide-ranging creativity, but also marked by the declining status of women, and the incorporation of untouchability into an organised system of belief. In South India, the Middle kingdoms exported Dravidian-languages scripts and religious cultures to the kingdoms of Southeast Asia.In the early medieval era, Christianity, Islam, Judaism, and Zoroastrianism put down roots on India's southern and western coasts. Muslim armies from Central Asia intermittently overran India's northern plains, eventually establishing the Delhi Sultanate, and drawing northern India into the cosmopolitan networks of medieval Islam. In the 15th century, the Vijayanagara Empire created a long-lasting composite Hindu culture in south India. In the Punjab, Sikhism emerged, rejecting institutionalised religion. The Mughal Empire, in 1526, ushered in two centuries of relative peace, leaving a legacy of luminous architecture. Gradually expanding rule of the British East India Company followed, turning India into a colonial economy, but also consolidating its sovereignty. British Crown rule began in 1858. The rights promised to Indians were granted slowly, but technological changes were introduced, and ideas of education, modernity and the public life took root. A pioneering and influential nationalist movement emerged, which was noted for nonviolent resistance and became the major factor in ending British rule. In 1947 the British Indian Empire was partitioned into two independent dominions, a Hindu-majority Dominion of India and a Muslim-majority Dominion of Pakistan, amid large-scale loss of life and an unprecedented migration.India has been a secular federal republic since 1950, governed in a democratic parliamentary system. It is a pluralistic, multilingual and multi-ethnic society. India's population grew from 361 million in 1951 to 1,211 million in 2011. During the same time, its nominal per capita income increased from US$64 annually to US$1,498, and its literacy rate from 16.6% to 74%. From being a comparatively destitute country in 1951, India has become a fast-growing major economy, a hub for information technology services, with an expanding middle class. It has a space programme which includes several planned or completed extraterrestrial missions. Indian movies, music, and spiritual teachings play an increasing role in global culture. India has substantially reduced its rate of poverty, though at the cost of increasing economic inequality. India is a nuclear-weapon state, which ranks high in military expenditure. It has disputes over Kashmir with its neighbours, Pakistan and China, unresolved since the mid-20th century. Among the socio-economic challenges India faces are gender inequality, child malnutrition, and rising levels of air pollution. India's land is megadiverse, with four biodiversity hotspots. Its forest cover comprises 21.4% of its area. India's wildlife, which has traditionally been viewed with tolerance in India's culture, is supported among these forests, and elsewhere, in protected habitats.
According to the Oxford English Dictionary (third edition 2009), the name "India" is derived from the Classical Latin India, a reference to South Asia and an uncertain region to its east; and in turn derived successively from: Hellenistic Greek India ( Ἰνδία); ancient Greek Indos ( Ἰνδός); Old Persian Hindush, an eastern province of the Achaemenid empire; and ultimately its cognate, the Sanskrit Sindhu, or "river," specifically the Indus river and, by implication, its well-settled southern basin. The ancient Greeks referred to the Indians as Indoi (Ἰνδοί), which translates as "The people of the Indus".The term Bharat (Bhārat; pronounced [ˈbʱaːɾət] (listen)), mentioned in both Indian epic poetry and the Constitution of India, is used in its variations by many Indian languages. A modern rendering of the historical name Bharatavarsha, which applied originally to a region of the Gangetic Valley, Bharat gained increased currency from the mid-19th century as a native name for India.Hindustan ([ɦɪndʊˈstaːn] (listen)) is a Middle Persian name for India, introduced during the Mughal Empire and used widely since. Its meaning has varied, referring to a region encompassing present-day northern India and Pakistan or to India in its near entirety.
By 55,000 years ago, the first modern humans, or Homo sapiens, had arrived on the Indian subcontinent from Africa, where they had earlier evolved. The earliest known modern human remains in South Asia date to about 30,000 years ago. After 6500 BCE, evidence for domestication of food crops and animals, construction of permanent structures, and storage of agricultural surplus appeared in Mehrgarh and other sites in what is now Balochistan. These gradually developed into the Indus Valley Civilisation, the first urban culture in South Asia, which flourished during 2500–1900 BCE in what is now Pakistan and western India. Centred around cities such as Mohenjo-daro, Harappa, Dholavira, and Kalibangan, and relying on varied forms of subsistence, the civilisation engaged robustly in crafts production and wide-ranging trade.During the period 2000–500 BCE, many regions of the subcontinent transitioned from the Chalcolithic cultures to the Iron Age ones. The Vedas, the oldest scriptures associated with Hinduism, were composed during this period, and historians have analysed these to posit a Vedic culture in the Punjab region and the upper Gangetic Plain. Most historians also consider this period to have encompassed several waves of Indo-Aryan migration into the subcontinent from the north-west. The caste system, which created a hierarchy of priests, warriors, and free peasants, but which excluded indigenous peoples by labelling their occupations impure, arose during this period. On the Deccan Plateau, archaeological evidence from this period suggests the existence of a chiefdom stage of political organisation. In South India, a progression to sedentary life is indicated by the large number of megalithic monuments dating from this period, as well as by nearby traces of agriculture, irrigation tanks, and craft traditions. In the late Vedic period, around the 6th century BCE, the small states and chiefdoms of the Ganges Plain and the north-western regions had consolidated into 16 major oligarchies and monarchies that were known as the mahajanapadas. The emerging urbanisation gave rise to non-Vedic religious movements, two of which became independent religions. Jainism came into prominence during the life of its exemplar, Mahavira. Buddhism, based on the teachings of Gautama Buddha, attracted followers from all social classes excepting the middle class; chronicling the life of the Buddha was central to the beginnings of recorded history in India. In an age of increasing urban wealth, both religions held up renunciation as an ideal, and both established long-lasting monastic traditions. Politically, by the 3rd century BCE, the kingdom of Magadha had annexed or reduced other states to emerge as the Mauryan Empire. The empire was once thought to have controlled most of the subcontinent except the far south, but its core regions are now thought to have been separated by large autonomous areas. The Mauryan kings are known as much for their empire-building and determined management of public life as for Ashoka's renunciation of militarism and far-flung advocacy of the Buddhist dhamma.The Sangam literature of the Tamil language reveals that, between 200 BCE and 200 CE, the southern peninsula was ruled by the Cheras, the Cholas, and the Pandyas, dynasties that traded extensively with the Roman Empire and with West and South-East Asia. In North India, Hinduism asserted patriarchal control within the family, leading to increased subordination of women. By the 4th and 5th centuries, the Gupta Empire had created a complex system of administration and taxation in the greater Ganges Plain; this system became a model for later Indian kingdoms. Under the Guptas, a renewed Hinduism based on devotion, rather than the management of ritual, began to assert itself. This renewal was reflected in a flowering of sculpture and architecture, which found patrons among an urban elite. Classical Sanskrit literature flowered as well, and Indian science, astronomy, medicine, and mathematics made significant advances.
The Indian early medieval age, 600 CE to 1200 CE, is defined by regional kingdoms and cultural diversity. When Harsha of Kannauj, who ruled much of the Indo-Gangetic Plain from 606 to 647 CE, attempted to expand southwards, he was defeated by the Chalukya ruler of the Deccan. When his successor attempted to expand eastwards, he was defeated by the Pala king of Bengal. When the Chalukyas attempted to expand southwards, they were defeated by the Pallavas from farther south, who in turn were opposed by the Pandyas and the Cholas from still farther south. No ruler of this period was able to create an empire and consistently control lands much beyond his core region. During this time, pastoral peoples, whose land had been cleared to make way for the growing agricultural economy, were accommodated within caste society, as were new non-traditional ruling classes. The caste system consequently began to show regional differences.In the 6th and 7th centuries, the first devotional hymns were created in the Tamil language. They were imitated all over India and led to both the resurgence of Hinduism and the development of all modern languages of the subcontinent. Indian royalty, big and small, and the temples they patronised drew citizens in great numbers to the capital cities, which became economic hubs as well. Temple towns of various sizes began to appear everywhere as India underwent another urbanisation. By the 8th and 9th centuries, the effects were felt in South-East Asia, as South Indian culture and political systems were exported to lands that became part of modern-day Myanmar, Thailand, Laos, Cambodia, Vietnam, Philippines, Malaysia, and Java. Indian merchants, scholars, and sometimes armies were involved in this transmission; South-East Asians took the initiative as well, with many sojourning in Indian seminaries and translating Buddhist and Hindu texts into their languages. After the 10th century, Muslim Central Asian nomadic clans, using swift-horse cavalry and raising vast armies united by ethnicity and religion, repeatedly overran South Asia's north-western plains, leading eventually to the establishment of the Islamic Delhi Sultanate in 1206. The sultanate was to control much of North India and to make many forays into South India. Although at first disruptive for the Indian elites, the sultanate largely left its vast non-Muslim subject population to its own laws and customs. By repeatedly repulsing Mongol raiders in the 13th century, the sultanate saved India from the devastation visited on West and Central Asia, setting the scene for centuries of migration of fleeing soldiers, learned men, mystics, traders, artists, and artisans from that region into the subcontinent, thereby creating a syncretic Indo-Islamic culture in the north. The sultanate's raiding and weakening of the regional kingdoms of South India paved the way for the indigenous Vijayanagara Empire. Embracing a strong Shaivite tradition and building upon the military technology of the sultanate, the empire came to control much of peninsular India, and was to influence South Indian society for long afterwards.
In the early 16th century, northern India, then under mainly Muslim rulers, fell again to the superior mobility and firepower of a new generation of Central Asian warriors. The resulting Mughal Empire did not stamp out the local societies it came to rule. Instead, it balanced and pacified them through new administrative practices and diverse and inclusive ruling elites, leading to more systematic, centralised, and uniform rule. Eschewing tribal bonds and Islamic identity, especially under Akbar, the Mughals united their far-flung realms through loyalty, expressed through a Persianised culture, to an emperor who had near-divine status. The Mughal state's economic policies, deriving most revenues from agriculture and mandating that taxes be paid in the well-regulated silver currency, caused peasants and artisans to enter larger markets. The relative peace maintained by the empire during much of the 17th century was a factor in India's economic expansion, resulting in greater patronage of painting, literary forms, textiles, and architecture. Newly coherent social groups in northern and western India, such as the Marathas, the Rajputs, and the Sikhs, gained military and governing ambitions during Mughal rule, which, through collaboration or adversity, gave them both recognition and military experience. Expanding commerce during Mughal rule gave rise to new Indian commercial and political elites along the coasts of southern and eastern India. As the empire disintegrated, many among these elites were able to seek and control their own affairs. By the early 18th century, with the lines between commercial and political dominance being increasingly blurred, a number of European trading companies, including the English East India Company, had established coastal outposts. The East India Company's control of the seas, greater resources, and more advanced military training and technology led it to increasingly flex its military muscle and caused it to become attractive to a portion of the Indian elite; these factors were crucial in allowing the company to gain control over the Bengal region by 1765 and sideline the other European companies. Its further access to the riches of Bengal and the subsequent increased strength and size of its army enabled it to annexe or subdue most of India by the 1820s. India was then no longer exporting manufactured goods as it long had, but was instead supplying the British Empire with raw materials. Many historians consider this to be the onset of India's colonial period. By this time, with its economic power severely curtailed by the British parliament and having effectively been made an arm of British administration, the company began more consciously to enter non-economic arenas like education, social reform, and culture.
Historians consider India's modern age to have begun sometime between 1848 and 1885. The appointment in 1848 of Lord Dalhousie as Governor General of the East India Company set the stage for changes essential to a modern state. These included the consolidation and demarcation of sovereignty, the surveillance of the population, and the education of citizens. Technological changes—among them, railways, canals, and the telegraph—were introduced not long after their introduction in Europe. However, disaffection with the company also grew during this time and set off the Indian Rebellion of 1857. Fed by diverse resentments and perceptions, including invasive British-style social reforms, harsh land taxes, and summary treatment of some rich landowners and princes, the rebellion rocked many regions of northern and central India and shook the foundations of Company rule. Although the rebellion was suppressed by 1858, it led to the dissolution of the East India Company and the direct administration of India by the British government. Proclaiming a unitary state and a gradual but limited British-style parliamentary system, the new rulers also protected princes and landed gentry as a feudal safeguard against future unrest. In the decades following, public life gradually emerged all over India, leading eventually to the founding of the Indian National Congress in 1885.The rush of technology and the commercialisation of agriculture in the second half of the 19th century was marked by economic setbacks and many small farmers became dependent on the whims of far-away markets. There was an increase in the number of large-scale famines, and, despite the risks of infrastructure development borne by Indian taxpayers, little industrial employment was generated for Indians. There were also salutary effects: commercial cropping, especially in the newly canalled Punjab, led to increased food production for internal consumption. The railway network provided critical famine relief, notably reduced the cost of moving goods, and helped nascent Indian-owned industry. After World War I, in which approximately one million Indians served, a new period began. It was marked by British reforms but also repressive legislation, by more strident Indian calls for self-rule, and by the beginnings of a nonviolent movement of non-co-operation, of which Mohandas Karamchand Gandhi would become the leader and enduring symbol. During the 1930s, slow legislative reform was enacted by the British; the Indian National Congress won victories in the resulting elections. The next decade was beset with crises: Indian participation in World War II, the Congress's final push for non-co-operation, and an upsurge of Muslim nationalism. All were capped by the advent of independence in 1947, but tempered by the partition of India into two states: India and Pakistan.Vital to India's self-image as an independent nation was its constitution, completed in 1950, which put in place a secular and democratic republic. It has remained a democracy with civil liberties, an active Supreme Court, and a largely independent press. Economic liberalisation, which began in the 1990s, has created a large urban middle class, transformed India into one of the world's fastest-growing economies, and increased its geopolitical clout. Indian movies, music, and spiritual teachings play an increasing role in global culture. Yet, India is also shaped by seemingly unyielding poverty, both rural and urban; by religious and caste-related violence; by Maoist-inspired Naxalite insurgencies; and by separatism in Jammu and Kashmir and in Northeast India. It has unresolved territorial disputes with China and with Pakistan. India's sustained democratic freedoms are unique among the world's newer nations; however, in spite of its recent economic successes, freedom from want for its disadvantaged population remains a goal yet to be achieved.
India accounts for the bulk of the Indian subcontinent, lying atop the Indian tectonic plate, a part of the Indo-Australian Plate. India's defining geological processes began 75 million years ago when the Indian Plate, then part of the southern supercontinent Gondwana, began a north-eastward drift caused by seafloor spreading to its south-west, and later, south and south-east. Simultaneously, the vast Tethyan oceanic crust, to its northeast, began to subduct under the Eurasian Plate. These dual processes, driven by convection in the Earth's mantle, both created the Indian Ocean and caused the Indian continental crust eventually to under-thrust Eurasia and to uplift the Himalayas. Immediately south of the emerging Himalayas, plate movement created a vast trough that rapidly filled with river-borne sediment and now constitutes the Indo-Gangetic Plain. Cut off from the plain by the ancient Aravalli Range lies the Thar Desert.The original Indian Plate survives as peninsular India, the oldest and geologically most stable part of India. It extends as far north as the Satpura and Vindhya ranges in central India. These parallel chains run from the Arabian Sea coast in Gujarat in the west to the coal-rich Chota Nagpur Plateau in Jharkhand in the east. To the south, the remaining peninsular landmass, the Deccan Plateau, is flanked on the west and east by coastal ranges known as the Western and Eastern Ghats; the plateau contains the country's oldest rock formations, some over one billion years old. Constituted in such fashion, India lies to the north of the equator between 6° 44′ and 35° 30′ north latitude and 68° 7′ and 97° 25′ east longitude.India's coastline measures 7,517 kilometres (4,700 mi) in length; of this distance, 5,423 kilometres (3,400 mi) belong to peninsular India and 2,094 kilometres (1,300 mi) to the Andaman, Nicobar, and Lakshadweep island chains. According to the Indian naval hydrographic charts, the mainland coastline consists of the following: 43% sandy beaches; 11% rocky shores, including cliffs; and 46% mudflats or marshy shores. Major Himalayan-origin rivers that substantially flow through India include the Ganges and the Brahmaputra, both of which drain into the Bay of Bengal. Important tributaries of the Ganges include the Yamuna and the Kosi; the latter's extremely low gradient, caused by long-term silt deposition, leads to severe floods and course changes. Major peninsular rivers, whose steeper gradients prevent their waters from flooding, include the Godavari, the Mahanadi, the Kaveri, and the Krishna, which also drain into the Bay of Bengal; and the Narmada and the Tapti, which drain into the Arabian Sea. Coastal features include the marshy Rann of Kutch of western India and the alluvial Sundarbans delta of eastern India; the latter is shared with Bangladesh. India has two archipelagos: the Lakshadweep, coral atolls off India's south-western coast; and the Andaman and Nicobar Islands, a volcanic chain in the Andaman Sea.The Indian climate is strongly influenced by the Himalayas and the Thar Desert, both of which drive the economically and culturally pivotal summer and winter monsoons. The Himalayas prevent cold Central Asian katabatic winds from blowing in, keeping the bulk of the Indian subcontinent warmer than most locations at similar latitudes. The Thar Desert plays a crucial role in attracting the moisture-laden south-west summer monsoon winds that, between June and October, provide the majority of India's rainfall. Four major climatic groupings predominate in India: tropical wet, tropical dry, subtropical humid, and montane.
India is a megadiverse country, a term employed for 17 countries which display high biological diversity and contain many species exclusively indigenous, or endemic, to them. India is a habitat for 8.6% of all mammal species, 13.7% of bird species, 7.9% of reptile species, 6% of amphibian species, 12.2% of fish species, and 6.0% of all flowering plant species. Fully a third of Indian plant species are endemic. India also contains four of the world's 34 biodiversity hotspots, or regions that display significant habitat loss in the presence of high endemism.India's forest cover is 701,673 km2 (270,917 sq mi), which is 21.35% of the country's total land area. It can be subdivided further into broad categories of canopy density, or the proportion of the area of a forest covered by its tree canopy. Very dense forest, whose canopy density is greater than 70%, occupies 2.61% of India's land area. It predominates in the tropical moist forest of the Andaman Islands, the Western Ghats, and Northeast India. Moderately dense forest, whose canopy density is between 40% and 70%, occupies 9.59% of India's land area. It predominates in the temperate coniferous forest of the Himalayas, the moist deciduous sal forest of eastern India, and the dry deciduous teak forest of central and southern India. Open forest, whose canopy density is between 10% and 40%, occupies 9.14% of India's land area, and predominates in the babul-dominated thorn forest of the central Deccan Plateau and the western Gangetic plain.Among the Indian subcontinent's notable indigenous trees are the astringent Azadirachta indica, or neem, which is widely used in rural Indian herbal medicine, and the luxuriant Ficus religiosa, or peepul, which is displayed on the ancient seals of Mohenjo-daro, and under which the Buddha is recorded in the Pali canon to have sought enlightenment,Many Indian species have descended from those of Gondwana, the southern supercontinent from which India separated more than 100 million years ago. India's subsequent collision with Eurasia set off a mass exchange of species. However, volcanism and climatic changes later caused the extinction of many endemic Indian forms. Still later, mammals entered India from Asia through two zoogeographical passes flanking the Himalayas. This had the effect of lowering endemism among India's mammals, which stands at 12.6%, contrasting with 45.8% among reptiles and 55.8% among amphibians. Notable endemics are the vulnerable hooded leaf monkey and the threatened Beddom's toad of the Western Ghats. India contains 172 IUCN-designated threatened animal species, or 2.9% of endangered forms. These include the endangered Bengal tiger and the Ganges river dolphin. Critically endangered species include: the gharial, a crocodilian; the great Indian bustard; and the Indian white-rumped vulture, which has become nearly extinct by having ingested the carrion of diclofenac-treated cattle. The pervasive and ecologically devastating human encroachment of recent decades has critically endangered Indian wildlife. In response, the system of national parks and protected areas, first established in 1935, was expanded substantially. In 1972, India enacted the Wildlife Protection Act and Project Tiger to safeguard crucial wilderness; the Forest Conservation Act was enacted in 1980 and amendments added in 1988. India hosts more than five hundred wildlife sanctuaries and thirteen biosphere reserves, four of which are part of the World Network of Biosphere Reserves; twenty-five wetlands are registered under the Ramsar Convention.
India is the world's most populous democracy. A parliamentary republic with a multi-party system, it has eight recognised national parties, including the Indian National Congress and the Bharatiya Janata Party (BJP), and more than 40 regional parties. The Congress is considered centre-left in Indian political culture, and the BJP right-wing. For most of the period between 1950—when India first became a republic—and the late 1980s, the Congress held a majority in the parliament. Since then, however, it has increasingly shared the political stage with the BJP, as well as with powerful regional parties which have often forced the creation of multi-party coalition governments at the centre.In the Republic of India's first three general elections, in 1951, 1957, and 1962, the Jawaharlal Nehru-led Congress won easy victories. On Nehru's death in 1964, Lal Bahadur Shastri briefly became prime minister; he was succeeded, after his own unexpected death in 1966, by Nehru's daughter Indira Gandhi, who went on to lead the Congress to election victories in 1967 and 1971. Following public discontent with the state of emergency she declared in 1975, the Congress was voted out of power in 1977; the then-new Janata Party, which had opposed the emergency, was voted in. Its government lasted just over two years. Voted back into power in 1980, the Congress saw a change in leadership in 1984, when Indira Gandhi was assassinated; she was succeeded by her son Rajiv Gandhi, who won an easy victory in the general elections later that year. The Congress was voted out again in 1989 when a National Front coalition, led by the newly formed Janata Dal in alliance with the Left Front, won the elections; that government too proved relatively short-lived, lasting just under two years. Elections were held again in 1991; no party won an absolute majority. The Congress, as the largest single party, was able to form a minority government led by P. V. Narasimha Rao. A two-year period of political turmoil followed the general election of 1996. Several short-lived alliances shared power at the centre. The BJP formed a government briefly in 1996; it was followed by two comparatively long-lasting United Front coalitions, which depended on external support. In 1998, the BJP was able to form a successful coalition, the National Democratic Alliance (NDA). Led by Atal Bihari Vajpayee, the NDA became the first non-Congress, coalition government to complete a five-year term. Again in the 2004 Indian general elections, no party won an absolute majority, but the Congress emerged as the largest single party, forming another successful coalition: the United Progressive Alliance (UPA). It had the support of left-leaning parties and MPs who opposed the BJP. The UPA returned to power in the 2009 general election with increased numbers, and it no longer required external support from India's communist parties. That year, Manmohan Singh became the first prime minister since Jawaharlal Nehru in 1957 and 1962 to be re-elected to a consecutive five-year term. In the 2014 general election, the BJP became the first political party since 1984 to win a majority and govern without the support of other parties. The incumbent prime minister is Narendra Modi, a former chief minister of Gujarat. On 20 July 2017, Ram Nath Kovind was elected India's 14th president and took the oath of office on 25 July 2017.
India is a federation with a parliamentary system governed under the Constitution of India—the country's supreme legal document. It is a constitutional republic and representative democracy, in which "majority rule is tempered by minority rights protected by law". Federalism in India defines the power distribution between the union and the states. The Constitution of India, which came into effect on 26 January 1950, originally stated India to be a "sovereign, democratic republic;" this characterisation was amended in 1971 to "a sovereign, socialist, secular, democratic republic". India's form of government, traditionally described as "quasi-federal" with a strong centre and weak states, has grown increasingly federal since the late 1990s as a result of political, economic, and social changes. The Government of India comprises three branches: Executive: The President of India is the ceremonial head of state, who is elected indirectly for a five-year term by an electoral college comprising members of national and state legislatures. The Prime Minister of India is the head of government and exercises most executive power. Appointed by the president, the prime minister is by convention supported by the party or political alliance having a majority of seats in the lower house of parliament. The executive of the Indian government consists of the president, the vice president, and the Union Council of Ministers—with the cabinet being its executive committee—headed by the prime minister. Any minister holding a portfolio must be a member of one of the houses of parliament. In the Indian parliamentary system, the executive is subordinate to the legislature; the prime minister and their council are directly responsible to the lower house of the parliament. Civil servants act as permanent executives and all decisions of the executive are implemented by them. Legislature: The legislature of India is the bicameral parliament. Operating under a Westminster-style parliamentary system, it comprises an upper house called the Rajya Sabha (Council of States) and a lower house called the Lok Sabha (House of the People). The Rajya Sabha is a permanent body of 245 members who serve staggered six-year terms. Most are elected indirectly by the state and union territorial legislatures in numbers proportional to their state's share of the national population. All but two of the Lok Sabha's 545 members are elected directly by popular vote; they represent single-member constituencies for five-year terms. The remaining two members are nominated by the president from among the Anglo-Indian community, in case the president decides they are not adequately represented. Judiciary: India has a three-tier unitary independent judiciary comprising the supreme court, headed by the Chief Justice of India, 25 high courts, and a large number of trial courts. The supreme court has original jurisdiction over cases involving fundamental rights and over disputes between states and the centre and has appellate jurisdiction over the high courts. It has the power to both strike down union or state laws which contravene the constitution, and invalidate any government action it deems unconstitutional.
India is a federal union comprising 28 states and 8 union territories (listed below as 1–28 and A–H, respectively). All states, as well as the union territories of Jammu and Kashmir, Puducherry and the National Capital Territory of Delhi, have elected legislatures and governments following the Westminster system of governance. The remaining five union territories are directly ruled by the central government through appointed administrators. In 1956, under the States Reorganisation Act, states were reorganised on a linguistic basis. There are over a quarter of a million local government bodies at city, town, block, district and village levels.
In the 1950s, India strongly supported decolonisation in Africa and Asia and played a leading role in the Non-Aligned Movement. After initially cordial relations with neighbouring China, India went to war with China in 1962, and was widely thought to have been humiliated. India has had tense relations with neighbouring Pakistan; the two nations have gone to war four times: in 1947, 1965, 1971, and 1999. Three of these wars were fought over the disputed territory of Kashmir, while the fourth, the 1971 war, followed from India's support for the independence of Bangladesh. In the late 1980s, the Indian military twice intervened abroad at the invitation of the host country: a peace-keeping operation in Sri Lanka between 1987 and 1990; and an armed intervention to prevent a 1988 coup d'état attempt in the Maldives. After the 1965 war with Pakistan, India began to pursue close military and economic ties with the Soviet Union; by the late 1960s, the Soviet Union was its largest arms supplier.Aside from ongoing its special relationship with Russia, India has wide-ranging defence relations with Israel and France. In recent years, it has played key roles in the South Asian Association for Regional Cooperation and the World Trade Organization. The nation has provided 100,000 military and police personnel to serve in 35 UN peacekeeping operations across four continents. It participates in the East Asia Summit, the G8+5, and other multilateral forums. India has close economic ties with South America, Asia, and Africa; it pursues a "Look East" policy that seeks to strengthen partnerships with the ASEAN nations, Japan, and South Korea that revolve around many issues, but especially those involving economic investment and regional security. China's nuclear test of 1964, as well as its repeated threats to intervene in support of Pakistan in the 1965 war, convinced India to develop nuclear weapons. India conducted its first nuclear weapons test in 1974 and carried out additional underground testing in 1998. Despite criticism and military sanctions, India has signed neither the Comprehensive Nuclear-Test-Ban Treaty nor the Nuclear Non-Proliferation Treaty, considering both to be flawed and discriminatory. India maintains a "no first use" nuclear policy and is developing a nuclear triad capability as a part of its "Minimum Credible Deterrence" doctrine. It is developing a ballistic missile defence shield and, a fifth-generation fighter jet. Other indigenous military projects involve the design and implementation of Vikrant-class aircraft carriers and Arihant-class nuclear submarines.Since the end of the Cold War, India has increased its economic, strategic, and military co-operation with the United States and the European Union. In 2008, a civilian nuclear agreement was signed between India and the United States. Although India possessed nuclear weapons at the time and was not a party to the Nuclear Non-Proliferation Treaty, it received waivers from the International Atomic Energy Agency and the Nuclear Suppliers Group, ending earlier restrictions on India's nuclear technology and commerce. As a consequence, India became the sixth de facto nuclear weapons state. India subsequently signed co-operation agreements involving civilian nuclear energy with Russia, France, the United Kingdom, and Canada. The President of India is the supreme commander of the nation's armed forces; with 1.395 million active troops, they compose the world's second-largest military. It comprises the Indian Army, the Indian Navy, the Indian Air Force, and the Indian Coast Guard. The official Indian defence budget for 2011 was US$36.03 billion, or 1.83% of GDP. For the fiscal year spanning 2012–2013, US$40.44 billion was budgeted. According to a 2008 Stockholm International Peace Research Institute (SIPRI) report, India's annual military expenditure in terms of purchasing power stood at US$72.7 billion. In 2011, the annual defence budget increased by 11.6%, although this does not include funds that reach the military through other branches of government. As of 2012, India is the world's largest arms importer; between 2007 and 2011, it accounted for 10% of funds spent on international arms purchases. Much of the military expenditure was focused on defence against Pakistan and countering growing Chinese influence in the Indian Ocean. In May 2017, the Indian Space Research Organisation launched the South Asia Satellite, a gift from India to its neighbouring SAARC countries. In October 2018, India signed a US$5.43 billion (over ₹400 billion) agreement with Russia to procure four S-400 Triumf surface-to-air missile defence systems, Russia's most advanced long-range missile defence system.
According to the International Monetary Fund (IMF), the Indian economy in 2019 was nominally worth $2.9 trillion; it is the fifth-largest economy by market exchange rates, and is around $11 trillion, the third-largest by purchasing power parity, or PPP. With its average annual GDP growth rate of 5.8% over the past two decades, and reaching 6.1% during 2011–2012, India is one of the world's fastest-growing economies. However, the country ranks 139th in the world in nominal GDP per capita and 118th in GDP per capita at PPP. Until 1991, all Indian governments followed protectionist policies that were influenced by socialist economics. Widespread state intervention and regulation largely walled the economy off from the outside world. An acute balance of payments crisis in 1991 forced the nation to liberalise its economy; since then it has moved slowly towards a free-market system by emphasising both foreign trade and direct investment inflows. India has been a member of WTO since 1 January 1995.The 513.7-million-worker Indian labour force is the world's second-largest, as of 2016. The service sector makes up 55.6% of GDP, the industrial sector 26.3% and the agricultural sector 18.1%. India's foreign exchange remittances of US$70 billion in 2014, the largest in the world, were contributed to its economy by 25 million Indians working in foreign countries. Major agricultural products include: rice, wheat, oilseed, cotton, jute, tea, sugarcane, and potatoes. Major industries include: textiles, telecommunications, chemicals, pharmaceuticals, biotechnology, food processing, steel, transport equipment, cement, mining, petroleum, machinery, and software. In 2006, the share of external trade in India's GDP stood at 24%, up from 6% in 1985. In 2008, India's share of world trade was 1.68%; In 2011, India was the world's tenth-largest importer and the nineteenth-largest exporter. Major exports include: petroleum products, textile goods, jewellery, software, engineering goods, chemicals, and manufactured leather goods. Major imports include: crude oil, machinery, gems, fertiliser, and chemicals. Between 2001 and 2011, the contribution of petrochemical and engineering goods to total exports grew from 14% to 42%. India was the world's second largest textile exporter after China in the 2013 calendar year.Averaging an economic growth rate of 7.5% for several years prior to 2007, India has more than doubled its hourly wage rates during the first decade of the 21st century. Some 431 million Indians have left poverty since 1985; India's middle classes are projected to number around 580 million by 2030. Though ranking 51st in global competitiveness, as of 2010, India ranks 17th in financial market sophistication, 24th in the banking sector, 44th in business sophistication, and 39th in innovation, ahead of several advanced economies. With seven of the world's top 15 information technology outsourcing companies based in India, as of 2009, the country is viewed as the second-most favourable outsourcing destination after the United States. India's consumer market, the world's eleventh-largest, is expected to become fifth-largest by 2030.Driven by growth, India's nominal GDP per capita increased steadily from US$329 in 1991, when economic liberalisation began, to US$1,265 in 2010, to an estimated US$1,723 in 2016. It is expected to grow to US$2,358 by 2020. However, it has remained lower than those of other Asian developing countries like Indonesia, Malaysia, Philippines, Sri Lanka, and Thailand, and is expected to remain so in the near future. Its GDP per capita is higher than Bangladesh, Pakistan, Nepal, Afghanistan and others. According to a 2011 PricewaterhouseCoopers (PwC) report, India's GDP at purchasing power parity could overtake that of the United States by 2045. During the next four decades, Indian GDP is expected to grow at an annualised average of 8%, making it potentially the world's fastest-growing major economy until 2050. The report highlights key growth factors: a young and rapidly growing working-age population; growth in the manufacturing sector because of rising education and engineering skill levels; and sustained growth of the consumer market driven by a rapidly growing middle-class. The World Bank cautions that, for India to achieve its economic potential, it must continue to focus on public sector reform, transport infrastructure, agricultural and rural development, removal of labour regulations, education, energy security, and public health and nutrition.According to the Worldwide Cost of Living Report 2017 released by the Economist Intelligence Unit (EIU) which was created by comparing more than 400 individual prices across 160 products and services, four of the cheapest cities were in India: Bangalore (3rd), Mumbai (5th), Chennai (5th) and New Delhi (8th).
India's telecommunication industry, the world's fastest-growing, added 227 million subscribers during the period 2010–2011, and after the third quarter of 2017, India surpassed the US to become the second largest smartphone market in the world after China.The Indian automotive industry, the world's second-fastest growing, increased domestic sales by 26% during 2009–2010, and exports by 36% during 2008–2009. India's capacity to generate electrical power is 300 gigawatts, of which 42 gigawatts is renewable. At the end of 2011, the Indian IT industry employed 2.8 million professionals, generated revenues close to US$100 billion equalling 7.5% of Indian GDP, and contributed 26% of India's merchandise exports.The pharmaceutical industry in India is among the significant emerging markets for the global pharmaceutical industry. The Indian pharmaceutical market is expected to reach $48.5 billion by 2020. India's R & D spending constitutes 60% of the biopharmaceutical industry. India is among the top 12 biotech destinations in the world. The Indian biotech industry grew by 15.1% in 2012–2013, increasing its revenues from ₹204.4 billion (Indian rupees) to ₹235.24 billion (US$3.94 billion at June 2013 exchange rates).
Despite economic growth during recent decades, India continues to face socio-economic challenges. In 2006, India contained the largest number of people living below the World Bank's international poverty line of US$1.25 per day. The proportion decreased from 60% in 1981 to 42% in 2005. Under the World Bank's later revised poverty line, it was 21% in 2011. 30.7% of India's children under the age of five are underweight. According to a Food and Agriculture Organization report in 2015, 15% of the population is undernourished. The Mid-Day Meal Scheme attempts to lower these rates.According to a 2016 Walk Free Foundation report there were an estimated 18.3 million people in India, or 1.4% of the population, living in the forms of modern slavery, such as bonded labour, child labour, human trafficking, and forced begging, among others. According to the 2011 census, there were 10.1 million child labourers in the country, a decline of 2.6 million from 12.6 million in 2001.Since 1991, economic inequality between India's states has consistently grown: the per-capita net state domestic product of the richest states in 2007 was 3.2 times that of the poorest. Corruption in India is perceived to have decreased. According to the Corruption Perceptions Index, India ranked 78th out of 180 countries in 2018 with a score of 41 out of 100, an improvement from 85th in 2014.
Indian cultural history spans more than 4,500 years. During the Vedic period (c. 1700 – c. 500 BCE), the foundations of Hindu philosophy, mythology, theology and literature were laid, and many beliefs and practices which still exist today, such as dhárma, kárma, yóga, and mokṣa, were established. India is notable for its religious diversity, with Hinduism, Buddhism, Sikhism, Islam, Christianity, and Jainism among the nation's major religions. The predominant religion, Hinduism, has been shaped by various historical schools of thought, including those of the Upanishads, the Yoga Sutras, the Bhakti movement, and by Buddhist philosophy.
Much of Indian architecture, including the Taj Mahal, other works of Mughal architecture, and South Indian architecture, blends ancient local traditions with imported styles. Vernacular architecture is also regional in its flavours. Vastu shastra, literally "science of construction" or "architecture" and ascribed to Mamuni Mayan, explores how the laws of nature affect human dwellings; it employs precise geometry and directional alignments to reflect perceived cosmic constructs. As applied in Hindu temple architecture, it is influenced by the Shilpa Shastras, a series of foundational texts whose basic mythological form is the Vastu-Purusha mandala, a square that embodied the "absolute". The Taj Mahal, built in Agra between 1631 and 1648 by orders of Emperor Shah Jahan in memory of his wife, has been described in the UNESCO World Heritage List as "the jewel of Muslim art in India and one of the universally admired masterpieces of the world's heritage". Indo-Saracenic Revival architecture, developed by the British in the late 19th century, drew on Indo-Islamic architecture.The earliest literature in India, composed between 1500 BCE and 1200 CE, was in the Sanskrit language. Major works of Sanskrit literature include the Rigveda (c. 1500 BCE – 1200 BCE), the epics: Mahābhārata (c. 400 BCE – 400 CE) and the Ramayana (c. 300 BCE and later); Abhijñānaśākuntalam (The Recognition of Śakuntalā, and other dramas of Kālidāsa (c. 5th century CE) and Mahākāvya poetry. In Tamil literature, the Sangam literature (c. 600 BCE – 300 BCE) consisting of 2,381 poems, composed by 473 poets, is the earliest work. From the 14th to the 18th centuries, India's literary traditions went through a period of drastic change because of the emergence of devotional poets like Kabīr, Tulsīdās, and Guru Nānak. This period was characterised by a varied and wide spectrum of thought and expression; as a consequence, medieval Indian literary works differed significantly from classical traditions. In the 19th century, Indian writers took a new interest in social questions and psychological descriptions. In the 20th century, Indian literature was influenced by the works of the Bengali poet and novelist Rabindranath Tagore, who was a recipient of the Nobel Prize in Literature.
Indian music ranges over various traditions and regional styles. Classical music encompasses two genres and their various folk offshoots: the northern Hindustani and southern Carnatic schools. Regionalised popular forms include filmi and folk music; the syncretic tradition of the bauls is a well-known form of the latter. Indian dance also features diverse folk and classical forms. Among the better-known folk dances are: the bhangra of Punjab, the bihu of Assam, the Jhumair and chhau of Jharkhand, Odisha and West Bengal, garba and dandiya of Gujarat, ghoomar of Rajasthan, and the lavani of Maharashtra. Eight dance forms, many with narrative forms and mythological elements, have been accorded classical dance status by India's National Academy of Music, Dance, and Drama. These are: bharatanatyam of the state of Tamil Nadu, kathak of Uttar Pradesh, kathakali and mohiniyattam of Kerala, kuchipudi of Andhra Pradesh, manipuri of Manipur, odissi of Odisha, and the sattriya of Assam. Theatre in India melds music, dance, and improvised or written dialogue. Often based on Hindu mythology, but also borrowing from medieval romances or social and political events, Indian theatre includes: the bhavai of Gujarat, the jatra of West Bengal, the nautanki and ramlila of North India, tamasha of Maharashtra, burrakatha of Andhra Pradesh, terukkuttu of Tamil Nadu, and the yakshagana of Karnataka. India has a theatre training institute the National School of Drama (NSD) that is situated at New Delhi It is an autonomous organisation under the Ministry of Culture, Government of India. The Indian film industry produces the world's most-watched cinema. Established regional cinematic traditions exist in the Assamese, Bengali, Bhojpuri, Hindi, Kannada, Malayalam, Punjabi, Gujarati, Marathi, Odia, Tamil, and Telugu languages. The Hindi language film industry (Bollywood) is the largest sector representing 43% of box office revenue, followed by the South Indian Telugu and Tamil film industries which represent 36% combined.Television broadcasting began in India in 1959 as a state-run medium of communication and expanded slowly for more than two decades. The state monopoly on television broadcast ended in the 1990s. Since then, satellite channels have increasingly shaped the popular culture of Indian society. Today, television is the most penetrative media in India; industry estimates indicate that as of 2012 there are over 554 million TV consumers, 462 million with satellite or cable connections compared to other forms of mass media such as the press (350 million), radio (156 million) or internet (37 million).
Traditional Indian society is sometimes defined by social hierarchy. The Indian caste system embodies much of the social stratification and many of the social restrictions found in the Indian subcontinent. Social classes are defined by thousands of endogamous hereditary groups, often termed as jātis, or "castes". India declared untouchability to be illegal in 1947 and has since enacted other anti-discriminatory laws and social welfare initiatives. At the workplace in urban India, and in international or leading Indian companies, caste-related identification has pretty much lost its importance.Family values are important in the Indian tradition, and multi-generational patriarchal joint families have been the norm in India, though nuclear families are becoming common in urban areas. An overwhelming majority of Indians, with their consent, have their marriages arranged by their parents or other family elders. Marriage is thought to be for life, and the divorce rate is extremely low, with less than one in a thousand marriages ending in divorce. Child marriages are common, especially in rural areas; many women wed before reaching 18, which is their legal marriageable age. Female infanticide in India, and lately female foeticide, have created skewed gender ratios; the number of missing women in the country quadrupled from 15 million to 63 million in the 50-year period ending in 2014, faster than the population growth during the same period, and constituting 20 percent of India's female electorate. Accord to an Indian government study, an additional 21 million girls are unwanted and do not receive adequate care. Despite a government ban on sex-selective foeticide, the practice remains commonplace in India, the result of a preference for boys in a patriarchal society. The payment of dowry, although illegal, remains widespread across class lines. Deaths resulting from dowry, mostly from bride burning, are on the rise, despite stringent anti-dowry laws.Many Indian festivals are religious in origin. The best known include: Diwali, Ganesh Chaturthi, Thai Pongal, Holi, Durga Puja, Eid ul-Fitr, Bakr-Id, Christmas, and Vaisakhi.
The most widely worn traditional dress in India, for both women and men, from ancient times until the advent of modern times, was draped. For women it eventually took the form of a sari, a single long piece of cloth, famously six yards long, and of width spanning the lower body. The sari is tied around the waist and knotted at one end, wrapped around the lower body, and then over the shoulder. In its more modern form, it has been used to cover the head, and sometimes the face, as a veil. It has been combined with an underskirt, or Indian petticoat, and tucked in the waist band for more secure fastening, It is also commonly worn with an Indian blouse, or choli, which serves as the primary upper-body garment, the sari's end, passing over the shoulder, now serving to obscure the upper body's contours, and to cover the midriff.For men, a similar but shorter length of cloth, the dhoti, has served as a lower-body garment. It too is tied around the waist and wrapped. In south India, it is usually wrapped around the lower body, the upper end tucked in the waistband, the lower left free. In addition, in northern India, it is also wrapped once around each leg before being brought up through the legs to be tucked in at the back. Other forms of traditional apparel that involve no stitching or tailoring are the chaddar (a shawl worn by both sexes to cover the upper body during colder weather, or a large veil worn by women for framing the head, or covering it) and the pagri (a turban or a scarf worn around the head as a part of a tradition, or to keep off the sun or the cold). Until the beginning of the first millennium CE, the ordinary dress of people in India was entirely unstitched. The arrival of the Kushans from Central Asia, circa 48 CE, popularised cut and sewn garments in the style of Central Asian favoured by the elite in northern India. However, it was not until Muslim rule was established, first with the Delhi sultanate and then the Mughal Empire, that the range of stitched clothes in India grew and their use became significantly more widespread. Among the various garments gradually establishing themselves in northern India during medieval and early-modern times and now commonly worn are: the shalwars and pyjamas both forms of trousers, as well as the tunics kurta and kameez. In southern India, however, the traditional draped garments were to see much longer continuous use.Shalwars are atypically wide at the waist but narrow to a cuffed bottom. They are held up by a drawstring or elastic belt, which causes them to become pleated around the waist. The pants can be wide and baggy, or they can be cut quite narrow, on the bias, in which case they are called churidars. The kameez is a long shirt or tunic. The side seams are left open below the waist-line,), which gives the wearer greater freedom of movement. The kameez is usually cut straight and flat; older kameez use traditional cuts; modern kameez are more likely to have European-inspired set-in sleeves. The kameez may have a European-style collar, a Mandarin-collar, or it may be collarless; in the latter case, its design as a women's garment is similar to a kurta. At first worn by Muslim women, the use of shalwar kameez gradually spread, making them a regional style, especially in the Punjab region.A kurta, which traces its roots to Central Asian nomadic tunics, has evolved stylistically in India as a garment for everyday wear as well as for formal occasions. It is traditionally made of cotton or silk; it is worn plain or with embroidered decoration, such as chikan; and it can be loose or tight in the torso, typically falling either just above or somewhere below the wearer's knees. The sleeves of a traditional kurta fall to the wrist without narrowing, the ends hemmed but not cuffed; the kurta can be worn by both men and women; it is traditionally collarless, though standing collars are increasingly popular; and it can be worn over ordinary pyjamas, loose shalwars, churidars, or less traditionally over jeans.In the last 50 years, fashions have changed a great deal in India. Increasingly, in urban settings in northern India, the sari is no longer the apparel of everyday wear, transformed instead into one for formal occasions. The traditional shalwar kameez is rarely worn by younger women, who favour churidars or jeans. The kurtas worn by young men usually fall to the shins and are seldom plain. In white-collar office settings, ubiquitous air conditioning allows men to wear sports jackets year-round. For weddings and formal occasions, men in the middle- and upper classes often wear bandgala, or short Nehru jackets, with pants, with the groom and his groomsmen sporting sherwanis and churidars. The dhoti, the once universal garment of Hindu India, the wearing of which in the homespun and handwoven form of khadi allowed Gandhi to bring Indian nationalism to the millions, is seldom seen in the cities, reduced now, with brocaded border, to the liturgical vestments of Hindu priests.
Indian cuisine consists of a wide variety of regional and traditional cuisines. Given the range of diversity in soil type, climate, culture, ethnic groups, and occupations, these cuisines vary substantially from each other, using locally available spices, herbs, vegetables, and fruit. Indian foodways have been influenced by religion, in particular Hindu cultural choices and traditions. They have been also shaped by Islamic rule, particularly that of the Mughals, by the arrival of the Portuguese on India's southwestern shores, and by British rule. These three influences are reflected, respectively, in the dishes of pilaf and biryani; the vindaloo; and the tiffin and the Railway mutton curry. Earlier, the Columbian exchange had brought the potato, the tomato, maize, peanuts, cashew nuts, pineapples, guavas, and most notably, chilli peppers, to India. Each became staples of use. In turn, the spice trade between India and Europe was a catalyst for Europe's Age of Discovery.The cereals grown in India, their choice, times, and regions of planting, correspond strongly to the timing of India's monsoons, and the variation across regions in their associated rainfall. In general, the broad division of cereal zones in India, as determined by their dependence on rain, was firmly in place before the arrival of artificial irrigation. Rice, which requires a lot of water, has been grown traditionally in regions of high rainfall in the northeast and the western coast, wheat in regions of moderate rainfall, like India's northern plains, and millet in regions of low rainfall, such as on the Deccan Plateau and in Rajasthan.The foundation of a typical Indian meal is a cereal cooked in plain fashion, and complemented with flavourful savoury dishes. The latter includes lentils, pulses and vegetables spiced commonly with ginger and garlic, but also more discerningly with a combination of spices that may include coriander, cumin, turmeric, cinnamon, cardamon and others as informed by culinary conventions. In an actual meal, this mental representation takes the form of a platter, or thali, with a central place for the cooked cereal, peripheral ones, often in small bowls, for the flavourful accompaniments, and the simultaneous, rather than piecemeal, ingestion of the two in each act of eating, whether by actual mixing—for example of rice and lentils—or in the folding of one—such as bread—around the other, such as cooked vegetables. A notable feature of Indian food is the existence of a number of distinctive vegetarian cuisines, each a feature of the geographical and cultural histories of its adherents. The appearance of ahimsa, or the avoidance of violence toward all forms of life in many religious orders early in Indian history, especially Upanishadic Hinduism, Buddhism and Jainism, is thought to have been a notable factor in the prevalence of vegetarianism among a segment of India's Hindu population, especially in southern India, Gujarat, and the Hindi-speaking belt of north-central India, as well as among Jains. Among these groups, strong discomfort is felt at thoughts of eating meat, and contributes to the low proportional consumption of meat to overall diet in India. Unlike China, which has increased its per capita meat consumption substantially in its years of increased economic growth, in India the strong dietary traditions have contributed to dairy, rather than meat, becoming the preferred form of animal protein consumption accompanying higher economic growth.In the last millennium, the most significant import of cooking techniques into India occurred during the Mughal Empire. The cultivation of rice had spread much earlier from India to Central and West Asia; however, it was during Mughal rule that dishes, such as the pilaf, developed in the interim during the Abbasid caliphate, and cooking techniques such as the marinating of meat in yogurt, spread into northern India from regions to its northwest. To the simple yogurt marinade of Persia, onions, garlic, almonds, and spices began to be added in India. Rice grown to the southwest of the Mughal capital, Agra, which had become famous in the Islamic world for its fine grain, was partially cooked and layered alternately with the sauteed meat, the pot sealed tightly, and slow cooked according to another Persian cooking technique, to produce what has today become the Indian biryani, a feature of festive dining in many parts of India. In food served in restaurants in urban north India, and internationally, the diversity of Indian food has been partially concealed by the dominance of Punjabi cuisine. This was caused in large part by an entrepreneurial response among people from the Punjab region who had been displaced by the 1947 partition of India, and had arrived in India as refugees. The identification of Indian cuisine with the tandoori chicken—cooked in the tandoor oven, which had traditionally been used for baking bread in the rural Punjab and the Delhi region, especially among Muslims, but which is originally from Central Asia—dates to this period.
In India, several traditional indigenous sports remain fairly popular, such as kabaddi, kho kho, pehlwani and gilli-danda. Some of the earliest forms of Asian martial arts, such as kalarippayattu, musti yuddha, silambam, and marma adi, originated in India. Chess, commonly held to have originated in India as chaturaṅga, is regaining widespread popularity with the rise in the number of Indian grandmasters. Pachisi, from which parcheesi derives, was played on a giant marble court by Akbar.The improved results garnered by the Indian Davis Cup team and other Indian tennis players in the early 2010s have made tennis increasingly popular in the country. India has a comparatively strong presence in shooting sports, and has won several medals at the Olympics, the World Shooting Championships, and the Commonwealth Games. Other sports in which Indians have succeeded internationally include badminton (Saina Nehwal and P V Sindhu are two of the top-ranked female badminton players in the world), boxing, and wrestling. Football is popular in West Bengal, Goa, Tamil Nadu, Kerala, and the north-eastern states. Cricket is the most popular sport in India. Major domestic competitions include the Indian Premier League, which is the most-watched cricket league in the world and ranks sixth among all sports leagues.India has hosted or co-hosted several international sporting events: the 1951 and 1982 Asian Games; the 1987, 1996, and 2011 Cricket World Cup tournaments; the 2003 Afro-Asian Games; the 2006 ICC Champions Trophy; the 2010 Hockey World Cup; the 2010 Commonwealth Games; and the 2017 FIFA U-17 World Cup. Major international sporting events held annually in India include the Chennai Open, the Mumbai Marathon, the Delhi Half Marathon, and the Indian Masters. The first Formula 1 Indian Grand Prix featured in late 2011 but has been discontinued from the F1 season calendar since 2014. India has traditionally been the dominant country at the South Asian Games. An example of this dominance is the basketball competition where the Indian team won three out of four tournaments to date.
Outline of India
Overview Etymology History Geography Biodiversity Politics Foreign relations and military Economy Demographics Culture
Government Official website of Government of India Government of India Web DirectoryGeneral information "India". The World Factbook. Central Intelligence Agency. India at Curlie India from UCB Libraries GovPubs India from the BBC News Indian State district block village website Wikimedia Atlas of India Geographic data related to India at OpenStreetMap Key Development Forecasts for India from International Futures
The following table lists estimates for the population of India (including what are now Pakistan and Bangladesh) from prehistory up until 1820. It includes estimates and growth rates according to five different economic historians, along with interpolated estimates and overall aggregate averages derived from their estimates. The population grew from the South Asian Stone Age in 10,000 BC to the Maurya Empire in 200 BC at a steadily increasing growth rate, before population growth slowed down in the classical era up to 500 AD, and then became largely stagnant during the early medieval era up to 1000 AD. The population growth rate then increased in the late medieval era (during the Delhi Sultanate) from 1000 to 1500.India's population growth rate under the Mughal Empire (16th–18th centuries) was higher than during any previous period in Indian history. Under the Mughal Empire, India experienced an unprecedented economic and demographic upsurge, due to Mughal agrarian reforms that intensified agricultural production, proto-industrialization that established India as the most important centre of manufacturing in international trade, and a relatively high degree of urbanisation for its time; 15% of the population lived in urban centres, higher than the percentage of the population in 19th-century British India and contemporary Europe up until the 19th century.Under the reign of Akbar (reigned 1556–1605) in 1600, the Mughal Empire's urban population was up to 17 million people, larger than the urban population in Europe. By 1700, Mughal India had an urban population of 23 million people, larger than British India's urban population of 22.3 million in 1871. Nizamuddin Ahmad (1551–1621) reported that, under Akbar's reign, Mughal India had 120 large cities and 3,200 townships. A number of cities in India had a population between a quarter-million and half-million people, with larger cities including Agra (in Agra Subah) with up to 800,000 people and Dhaka (in Bengal Subah) with over 1 million people. Mughal India also had a large number of villages, with 455,698 villages by the time of Aurangzeb (reigned 1658–1707).In the early 18th century, the average life expectancy in Mughal India was 35 years. In comparison, the average life expectancy for several European nations in the 18th century were 34 years in early modern England, up to 30 years in France, and about 25 years in Prussia.
The total fertility rate is the number of children born per woman. It is based on fairly good data for the entire years. Sources: Our World In Data and Gapminder Foundation. Life expectancy from 1881 to 1950 The population of India under the British Raj (including what are now Pakistan and Bangladesh) according to censuses: Studies of India's population since 1881 have focused on such topics as total population, birth and death rates, growth rates, geographic distribution, literacy, the rural and urban divide, cities of a million, and the three cities with populations over eight million: Delhi, Greater Mumbai (Bombay), and Kolkata (Calcutta).Mortality rates fell in the period 1920–45, primarily due to biological immunisation. Other factors included rising incomes, better living conditions, improved nutrition, a safer and cleaner environment, and better official health policies and medical care.
The table below summarises India's demographics (excluding the Mao-Maram, Paomata and Purul subdivisions of Senapati District of Manipur state due to cancellation of census results) according to religion at the 2011 census in per cent. The data is "unadjusted" (without excluding Assam and Jammu and Kashmir); the 1981 census was not conducted in Assam and the 1991 census was not conducted in Jammu and Kashmir. Missing citing/reference for "Changes in religious demagraphics over time" table below. Characteristics of religious groups
The table below represents the infant mortality rate trends in India, based on sex, over the last 15 years. In the urban areas of India, average male infant mortality rates are slightly higher than average female infant mortality rates. Some activists believe India's 2011 census shows a serious decline in the number of girls under the age of seven – activists posit that eight million female fetuses may have been aborted between 2001 and 2011. These claims are controversial. Scientists who study human sex ratios and demographic trends suggest that birth sex ratio between 1.08 and 1.12 can be because of natural factors, such as the age of mother at baby's birth, age of father at baby's birth, number of babies per couple, economic stress, endocrinological factors, etc. The 2011 census birth sex ratio in India, of 917 girls to 1000 boys, is similar to 870–930 girls to 1000 boys birth sex ratios observed in Japanese, Chinese, Cuban, Filipino and Hawaiian ethnic groups in the United States between 1940 and 2005. They are also similar to birth sex ratios below 900 girls to 1000 boys observed in mothers of different age groups and gestation periods in the United States.
41.03% of the Indians speak Hindi while the rest speak Assamese, Bengali, Gujarati, Maithili, Kannada, Malayalam, Marathi, Odia, Punjabi, Tamil, Telugu, Urdu and a variety of other languages. There are a total of 122 languages and 234 mother tongues. The 22 languages are Languages specified in the Eighth Schedule of Indian Constitution and 100 non-specified languages. The table immediately below excludes Mao-Maram, Paomata and Purul subdivisions of Senapati District of Manipur state due to cancellation of census results.
Source: UN World Population Prospects
From the Demographic Health Survey:
Caste and community statistics as recorded from "Socially and Educationally Backward Classes Commission" (SEBC) or Mandal Commission of 1979. This was completed in 1983.There has not yet been a proper consensus on contemporary figures.The following data is from the Mandal report:
India is projected to overtake China as the world's most populous nation by 2027. Note that these projections make assumptions about future fertility and death rates which may not turn out to be correct in the event. Fertility rates also vary from region to region, with some higher than the national average and some lower of China.
In millions
The national Census of India does not recognise racial or ethnic groups within India, but recognises many of the tribal groups as Scheduled Castes and Tribes (see list of Scheduled Tribes in India). According to a 2009 study published by Reich et al., the modern Indian population is composed of two genetically divergent and heterogeneous populations which mixed in ancient times (about 1,200–3,500 BP), known as Ancestral North Indians (ANI) and Ancestral South Indians (ASI). ASI corresponds to the Dravidian-speaking population of southern India, whereas ANI corresponds to the Indo-Aryan-speaking population of northern India. 700,000 people from the United States of any race live in India. Between 300,000 and 1 million Anglo-Indians live in India.For a list of ethnic groups in the Republic of India (as well as neighbouring countries) see ethnic groups of the Indian subcontinent.
Numerous genomic studies have been conducted in the last 15 years to seek insights into India's demographic and cultural diversity. These studies paint a complex and conflicting picture. In a 2003 study, Basu, Majumder et al. have concluded on the basis of results obtained from mtDNA, Y-chromosome and autosomal markers that "(1) there is an underlying unity of female lineages in India, indicating that the initial number of female settlers may have been small; (2) the tribal and the caste populations are highly differentiated; (3) the Austroasiatic tribals are the earliest settlers in India, providing support to one anthropological hypothesis while refuting some others; (4) a major wave of humans entered India through the northeast; (5) the Tibeto-Burman tribals share considerable genetic commonalities with the Austroasiatic tribals, supporting the hypothesis that they may have shared a common habitat in southern China, but the two groups of tribals can be differentiated on the basis of Y-chromosomal haplotypes; (6) the Dravidian speaking populations were possibly widespread throughout India but are regulated to South India now ; (7) formation of populations by fission that resulted in founder and drift effects have left their imprints on the genetic structures of contemporary populations; (8) the upper castes show closer genetic affinities with Central Asian populations, although those of southern India are more distant than those of northern India; (9) historical gene flow into India has contributed to a considerable obliteration of genetic histories of contemporary populations so that there is at present no clear congruence of genetic and geographical or sociocultural affinities." In a later 2010 review article, Majumder affirms some of these conclusions, introduces and revises some other. The ongoing studies, concludes Majumder, suggest India has served as the major early corridor for geographical dispersal of modern humans from out-of-Africa. The archaeological and genetic traces of the earliest settlers in India has not provided any conclusive evidence. The tribal populations of India are older than the non-tribal populations. The autosomal differentiation and genetic diversity within India's caste populations at 0.04 is significantly lower than 0.14 for continental populations and 0.09 for 31 world population sets studied by Watkins et al., suggesting that while tribal populations were differentiated, the differentiation effects within India's caste population was less than previously thought. Majumder also concludes that recent studies suggest India has been a major contributor to the gene pool of southeast Asia. Another study covering a large sample of Indian populations allowed Watkins et al. to examine eight Indian caste groups and four endogamous south Indian tribal populations. The Indian castes data show low between-group differences, while the tribal Indian groups show relatively high between-group differentiation. This suggests that people between Indian castes were not reproductively isolated, while Indian tribal populations experienced reproductive isolation and drift. Furthermore, the genetic fixation index data shows historical genetic differentiation and segregation between Indian castes population is much smaller than those found in east Asia, Africa and other continental populations; while being similar to the genetic differentiation and segregation observed in European populations. In 2006, Sahoo et al. reported their analysis of genomic data on 936 Y-chromosomes representing 32 tribal and 45 caste groups from different regions of India. These scientists find that the haplogroup frequency distribution across the country, between different caste groups, was found to be predominantly driven by geographical, rather than cultural determinants. They conclude there is clear evidence for both large-scale immigration into ancient India of Sino-Tibetan speakers and language change of former Austroasiatic speakers, in the northeast Indian region. The genome studies conducted up until 2010 have been on relatively small population sets. Many are from just one southeastern state of Andhra Pradesh (including Telangana, which was part of the state until June 2014). Thus, any conclusions on demographic history of India must be interpreted with caution. A larger national genome study with demographic growth and sex ratio balances may offer further insights on the extent of genetic differentiation and segregation in India over the millenniums.
2011 census of India National Commission on Population
List of most populous cities in India List of most populous metropolitan areas in India List of million-plus urban agglomerations in India List of states and union territories of India by population
Census of India; government site with detailed data from 2001 census Population of India as per Census India 2011 Census of India map generator; generates maps based on 2001 census figures Demographic data for India; provides sources of demographic data for India 2001 maps; provides maps of social, economic and demographic data of India in 2001 Population of India 2011 map; distribution of population amongst states and union territories India's Demographic Outlook: Implications and Trends United Nations "World Population Prospects": Country Profile – India Aggregated demographic statistics from Indian and global data sources Demographic statistics for India – online on Bluenomics India comparing with China population projection graph Based on data from database of UN Population Division
The economy of India is characterised as a developing market economy. It is the world's sixth-largest economy by nominal GDP and the third-largest by purchasing power parity (PPP). According to the IMF, on a per capita income basis, India ranked 142nd by GDP (nominal) and 124th by GDP (PPP) in 2020. From independence in 1947 until 1991, successive governments promoted protectionist economic policies with extensive state intervention and regulation which is characterised as Dirigism. The end of the Cold War and an acute balance of payments crisis in 1991 led to the adoption of a broad program of economic liberalisation. Since the start of the 21st century, annual average GDP growth has been 6% to 7%, and from 2014 to 2018, India was the world's fastest growing major economy, surpassing China. Historically, India was the largest economy in the world for most of the two millennia from the 1st until 19th century.The long-term growth perspective of the Indian economy remains positive due to its young population and corresponding low dependency ratio, healthy savings and investment rates, and is increasing integration into the global economy. The economy slowed in 2017, due to shocks of "demonetisation" in 2016 and introduction of Goods and Services Tax in 2017. Nearly 60% of India's GDP is driven by domestic private consumption and continues to remain the world's sixth-largest consumer market. Apart from private consumption, India's GDP is also fueled by government spending, investment, and exports. In 2018, India was the world's tenth-largest importer and the nineteenth-largest exporter. India has been a member of World Trade Organization since 1 January 1995. It ranks 63rd on Ease of doing business index and 68th on Global Competitiveness Report. With 520 million workers, the Indian labour force is the world's second-largest as of 2019. India has one of the world's highest number of billionaires and extreme income inequality. Since India has a vast informal economy, barely 2% of Indians pay income taxes. During the 2008 global financial crisis the economy faced mild slowdown, India undertook stimulus measures (both fiscal and monetary) to boost growth and generate demand; in subsequent years economic growth revived. According to 2017 PricewaterhouseCoopers (PwC) report, India's GDP at purchasing power parity could overtake that of the United States by 2050. According to World Bank, to achieve sustainable economic development India must focus on public sector reform, infrastructure, agricultural and rural development, removal of land and labour regulations, financial inclusion, spur private investment and exports, education and public health.In 2019, India's ten largest trading partners were USA, China, UAE, Saudi Arabia, Hong Kong, Iraq, Singapore, Germany, South Korea and Switzerland. In 2018–19, the foreign direct investment (FDI) in India was $64.4 billion with service sector, computer, and telecom industry remains leading sectors for FDI inflows. India has free trade agreements with several nations, including ASEAN, SAFTA, Mercosur, South Korea, Japan and few others which are in effect or under negotiating stage. The service sector makes up 55.6% of GDP and remains the fastest growing sector, while the industrial sector and the agricultural sector employs a majority of the labor force. The Bombay Stock Exchange and National Stock Exchange are one of the world's largest stock exchanges by market capitalization. India is the world's sixth-largest manufacturer, representing 3% of global manufacturing output and employs over 57 million people. Nearly 66% of India's population is rural whose primary source of livelihood is agriculture, and contributes about 50% of India's GDP. It has the world's fifth-largest foreign-exchange reserves worth ₹38,832.21 billion (US$540 billion). India has a high national debt with 68% of GDP, while its fiscal deficit remained at 3.4% of GDP. However, as per 2019 CAG report, the actual fiscal deficit is 5.85% of GDP. India's government-owned banks faced mounting bad debt, resulting in low credit growth, simultaneously the NBFC sector has been engulfed in a liquidity crisis. India faces high unemployment, rising income inequality, and major slump in aggregate demand. In recent years, independent economists and financial institutions have accused the government of fudging various economic data, especially GDP growth.India ranks second globally in food and agricultural production, while agricultural exports were $38.5 billion. The construction and real estate sector is the second largest employer after agriculture, and a vital sector to gauge economic activity. The Indian textiles industry is estimated at $150 billion and contributes 7% of industrial output and 2% of India's GDP while employs over 45 million people directly. The Indian IT industry is a major exporter of IT services with $180 billion in revenue and employs over four million people. India's telecommunication industry is the world's second largest by number of mobile phone, smartphone, and internet users. It is the world's tenth-largest oil producer and the third-largest oil consumer. The Indian automobile industry is the world's fourth largest by production. It has $672 billion worth of retail market which contributes over 10% of India's GDP and has one of world's fastest growing e-commerce markets. India has the world's fourth-largest natural resources, with mining sector contributes 11% of the country's industrial GDP and 2.5% of total GDP. It is also the world's second-largest coal producer, the second-largest cement producer, the second-largest steel producer, and the third-largest electricity producer.
For a continuous duration of nearly 1700 years from the year 1 AD, India was the top most economy constituting 35 to 40% of world GDP. The combination of protectionist, import-substitution, Fabian socialism, and social democratic-inspired policies governed India for sometime after the end of British rule. The economy was then characterised as Dirigism, It had extensive regulation, protectionism, public ownership of large monopolies, pervasive corruption and slow growth. Since 1991, continuing economic liberalisation has moved the country towards a market-based economy. By 2008, India had established itself as one of the world's faster-growing economies.
The citizens of the Indus Valley Civilisation, a permanent settlement that flourished between 2800 BC and 1800 BC, practised agriculture, domesticated animals, used uniform weights and measures, made tools and weapons, and traded with other cities. Evidence of well-planned streets, a drainage system and water supply reveals their knowledge of urban planning, which included the first-known urban sanitation systems and the existence of a form of municipal government.
Maritime trade was carried out extensively between South India and Southeast and West Asia from early times until around the fourteenth century AD. Both the Malabar and Coromandel Coasts were the sites of important trading centres from as early as the first century BC, used for import and export as well as transit points between the Mediterranean region and southeast Asia. Over time, traders organised themselves into associations which received state patronage. Historians Tapan Raychaudhuri and Irfan Habib claim this state patronage for overseas trade came to an end by the thirteenth century AD, when it was largely taken over by the local Parsi, Jewish, Syrian Christian and Muslim communities, initially on the Malabar and subsequently on the Coromandel coast.
Other scholars suggest trading from India to West Asia and Eastern Europe was active between the 14th and 18th centuries. During this period, Indian traders settled in Surakhani, a suburb of greater Baku, Azerbaijan. These traders built a Hindu temple, which suggests commerce was active and prosperous for Indians by the 17th century.Further north, the Saurashtra and Bengal coasts played an important role in maritime trade, and the Gangetic plains and the Indus valley housed several centres of river-borne commerce. Most overland trade was carried out via the Khyber Pass connecting the Punjab region with Afghanistan and onward to the Middle East and Central Asia. Although many kingdoms and rulers issued coins, barter was prevalent. Villages paid a portion of their agricultural produce as revenue to the rulers, while their craftsmen received a part of the crops at harvest time for their services.
The Indian economy was large and prosperous under the Mughal Empire, up until the 18th century. Sean Harkin estimates China and India may have accounted for 60 to 70 percent of world GDP in the 17th century. The Mughal economy functioned on an elaborate system of coined currency, land revenue and trade. Gold, silver and copper coins were issued by the royal mints which functioned on the basis of free coinage. The political stability and uniform revenue policy resulting from a centralised administration under the Mughals, coupled with a well-developed internal trade network, ensured that India–before the arrival of the British–was to a large extent economically unified, despite having a traditional agrarian economy characterised by a predominance of subsistence agriculture, with 64% of the workforce in the primary sector (including agriculture), but with 36% of the workforce also in the secondary and tertiary sectors, higher than in Europe, where 65–90% of its workforce were in agriculture in 1700 and 65–75% were in agriculture in 1750. Agricultural production increased under Mughal agrarian reforms, with Indian agriculture being advanced compared to Europe at the time, such as the widespread use of the seed drill among Indian peasants before its adoption in European agriculture, and higher per-capita agricultural output and standards of consumption. The Mughal Empire had a thriving industrial manufacturing economy, with India producing about 25% of the world's industrial output up until 1750, making it the most important manufacturing center in international trade. Manufactured goods and cash crops from the Mughal Empire were sold throughout the world. Key industries included textiles, shipbuilding, and steel, and processed exports included cotton textiles, yarns, thread, silk, jute products, metalware, and foods such as sugar, oils and butter. Cities and towns boomed under the Mughal Empire, which had a relatively high degree of urbanization for its time, with 15% of its population living in urban centres, higher than the percentage of the urban population in contemporary Europe at the time and higher than that of British India in the 19th century.In early modern Europe, there was significant demand for products from Mughal India, particularly cotton textiles, as well as goods such as spices, peppers, indigo, silks, and saltpeter (for use in munitions). European fashion, for example, became increasingly dependent on Mughal Indian textiles and silks. From the late 17th century to the early 18th century, Mughal India accounted for 95% of British imports from Asia, and the Bengal Subah province alone accounted for 40% of Dutch imports from Asia. In contrast, there was very little demand for European goods in Mughal India, which was largely self-sufficient. Indian goods, especially those from Bengal, were also exported in large quantities to other Asian markets, such as Indonesia and Japan. At the time, Mughal Bengal was the most important center of cotton textile production.In the early 18th century, the Mughal Empire declined, as it lost western, central and parts of south and north India to the Maratha Empire, which integrated and continued to administer those regions. The decline of the Mughal Empire led to decreased agricultural productivity, which in turn negatively affected the textile industry. The subcontinent's dominant economic power in the post-Mughal era was the Bengal Subah in the east., which continued to maintain thriving textile industries and relatively high real wages. However, the former was devastated by the Maratha invasions of Bengal and then British colonization in the mid-18th century. After the loss at the Third Battle of Panipat, the Maratha Empire disintegrated into several confederate states, and the resulting political instability and armed conflict severely affected economic life in several parts of the country – although this was mitigated by localised prosperity in the new provincial kingdoms. By the late eighteenth century, the British East India Company had entered the Indian political theatre and established its dominance over other European powers. This marked a determinative shift in India's trade, and a less-powerful impact on the rest of the economy.
There is no doubt that our grievances against the British Empire had a sound basis. As the painstaking statistical work of the Cambridge historian Angus Maddison has shown, India's share of world income collapsed from 22.6% in 1700, almost equal to Europe's share of 23.3% at that time, to as low as 3.8% in 1952. Indeed, at the beginning of the 20th century, "the brightest jewel in the British Crown" was the poorest country in the world in terms of per capita income. From the beginning of the 19th century, the British East India Company's gradual expansion and consolidation of power brought a major change in taxation and agricultural policies, which tended to promote commercialisation of agriculture with a focus on trade, resulting in decreased production of food crops, mass impoverishment and destitution of farmers, and in the short term, led to numerous famines. The economic policies of the British Raj caused a severe decline in the handicrafts and handloom sectors, due to reduced demand and dipping employment. After the removal of international restrictions by the Charter of 1813, Indian trade expanded substantially with steady growth. The result was a significant transfer of capital from India to England, which, due to the colonial policies of the British, led to a massive drain of revenue rather than any systematic effort at modernisation of the domestic economy. Under British rule, India's share of the world economy declined from 24.4% in 1700 down to 4.2% in 1950. India's GDP (PPP) per capita was stagnant during the Mughal Empire and began to decline prior to the onset of British rule. India's share of global industrial output declined from 25% in 1750 down to 2% in 1900. At the same time, the United Kingdom's share of the world economy rose from 2.9% in 1700 up to 9% in 1870. The British East India Company, following their conquest of Bengal in 1757, had forced open the large Indian market to British goods, which could be sold in India without tariffs or duties, compared to local Indian producers who were heavily taxed, while in Britain protectionist policies such as bans and high tariffs were implemented to restrict Indian textiles from being sold there, whereas raw cotton was imported from India without tariffs to British factories which manufactured textiles from Indian cotton and sold them back to the Indian market. British economic policies gave them a monopoly over India's large market and cotton resources. India served as both a significant supplier of raw goods to British manufacturers and a large captive market for British manufactured goods.British territorial expansion in India throughout the 19th century created an institutional environment that, on paper, guaranteed property rights among the colonisers, encouraged free trade, and created a single currency with fixed exchange rates, standardised weights and measures and capital markets within the company-held territories. It also established a system of railways and telegraphs, a civil service that aimed to be free from political interference, a common-law and an adversarial legal system. This coincided with major changes in the world economy – industrialisation, and significant growth in production and trade. However, at the end of colonial rule, India inherited an economy that was one of the poorest in the developing world, with industrial development stalled, agriculture unable to feed a rapidly growing population, a largely illiterate and unskilled labour force, and extremely inadequate infrastructure.The 1872 census revealed that 91.3% of the population of the region constituting present-day India resided in villages. This was a decline from the earlier Mughal era, when 85% of the population resided in villages and 15% in urban centers under Akbar's reign in 1600. Urbanisation generally remained sluggish in British India until the 1920s, due to the lack of industrialisation and absence of adequate transportation. Subsequently, the policy of discriminating protection (where certain important industries were given financial protection by the state), coupled with the Second World War, saw the development and dispersal of industries, encouraging rural–urban migration, and in particular the large port cities of Bombay, Calcutta and Madras grew rapidly. Despite this, only one-sixth of India's population lived in cities by 1951.The impact of British rule on India's economy is a controversial topic. Leaders of the Indian independence movement and economic historians have blamed colonial rule for the dismal state of India's economy in its aftermath and argued that financial strength required for industrial development in Britain was derived from the wealth taken from India. At the same time, right-wing historians have countered that India's low economic performance was due to various sectors being in a state of growth and decline due to changes brought in by colonialism and a world that was moving towards industrialisation and economic integration.Several economic historians have argued that real wage decline occurred in the early 19th century, or possibly beginning in the very late 18th century, largely as a result of British imperialism. Economic historian Prasannan Parthasarathi presented earnings data which showed real wages and living standards in 18th century Bengal and Mysore being higher than in Britain, which in turn had the highest living standards in Europe. Mysore's average per-capita income was five times higher than subsistence level, i.e. five times higher than $400 (1990 international dollars), or $2,000 per capita. In comparison, the highest national per-capita incomes in 1820 were $1,838 for the Netherlands and $1,706 for Britain. It has also been argued that India went through a period of deindustrialization in the latter half of the 18th century as an indirect outcome of the collapse of the Mughal Empire.
Indian economic policy after independence was influenced by the colonial experience, which was seen as exploitative by Indian leaders exposed to British social democracy and the planned economy of the Soviet Union. Domestic policy tended towards protectionism, with a strong emphasis on import substitution industrialisation, economic interventionism, a large government-run public sector, business regulation, and central planning, while trade and foreign investment policies were relatively liberal. Five-Year Plans of India resembled central planning in the Soviet Union. Steel, mining, machine tools, telecommunications, insurance, and power plants, among other industries, were effectively nationalised in the mid-1950s. The Indian economy of this period is characterised as Dirigism. Never talk to me about profit, Jeh, it is a dirty word. Jawaharlal Nehru, the first prime minister of India, along with the statistician Prasanta Chandra Mahalanobis, formulated and oversaw economic policy during the initial years of the country's independence. They expected favourable outcomes from their strategy, involving the rapid development of heavy industry by both public and private sectors, and based on direct and indirect state intervention, rather than the more extreme Soviet-style central command system. The policy of concentrating simultaneously on capital- and technology-intensive heavy industry and subsidising manual, low-skill cottage industries was criticised by economist Milton Friedman, who thought it would waste capital and labour, and retard the development of small manufacturers. I cannot decide how much to borrow, what shares to issue, at what price, what wages and bonus to pay, and what dividend to give. I even need the government's permission for the salary I pay to a senior executive. Since 1965, the use of high-yielding varieties of seeds, increased fertilisers and improved irrigation facilities collectively contributed to the Green Revolution in India, which improved the condition of agriculture by increasing crop productivity, improving crop patterns and strengthening forward and backward linkages between agriculture and industry. However, it has also been criticised as an unsustainable effort, resulting in the growth of capitalistic farming, ignoring institutional reforms and widening income disparities.In 1984, Rajiv Gandhi promised economic liberalization, he made V. P. Singh the finance minister, who tried to reduce tax-evasion and tax-receipts rose due to this crackdown although taxes were lowered. This process lost its momentum during later tenure of Mr. Gandhi as his government was marred by scandals.
The collapse of the Soviet Union, which was India's major trading partner, and the Gulf War, which caused a spike in oil prices, resulted in a major balance-of-payments crisis for India, which found itself facing the prospect of defaulting on its loans. India asked for a $1.8 billion bailout loan from the International Monetary Fund (IMF), which in return demanded de-regulation.In response, the Narasimha Rao government, including Finance Minister Manmohan Singh, initiated economic reforms in 1991. The reforms did away with the Licence Raj, reduced tariffs and interest rates and ended many public monopolies, allowing automatic approval of foreign direct investment in many sectors. Since then, the overall thrust of liberalisation has remained the same, although no government has tried to take on powerful lobbies such as trade unions and farmers, on contentious issues such as reforming labour laws and reducing agricultural subsidies. By the turn of the 21st century, India had progressed towards a free-market economy, with a substantial reduction in state control of the economy and increased financial liberalisation. This has been accompanied by increases in life expectancy, literacy rates, and food security, although urban residents have benefited more than rural residents. While the credit rating of India was hit by its nuclear weapons tests in 1998, it has since been raised to investment level in 2003 by Standard & Poor's (S&P) and Moody's. India experienced high growth rates, averaging 9% from 2003 to 2007. Growth then moderated in 2008 due to the global financial crisis. In 2003, Goldman Sachs predicted that India's GDP in current prices would overtake France and Italy by 2020, Germany, UK and Russia by 2025 and Japan by 2035, making it the third-largest economy of the world, behind the US and China. India is often seen by most economists as a rising economic superpower which will play a major role in the 21st-century global economy.Starting in 2012, India entered a period of reduced growth, which slowed to 5.6%. Other economic problems also became apparent: a plunging Indian rupee, a persistent high current account deficit and slow industrial growth. India started recovery in 2013–14 when the GDP growth rate accelerated to 6.4% from the previous year's 5.5%. The acceleration continued through 2014–15 and 2015–16 with growth rates of 7.5% and 8.0% respectively. For the first time since 1990, India grew faster than China which registered 6.9% growth in 2015. However the growth rate subsequently decelerated, to 7.1% and 6.6% in 2016–17 and 2017–18 respectively, partly because of the disruptive effects of 2016 Indian banknote demonetisation and the Goods and Services Tax (India).India is ranked 63rd out of 190 countries in the World Bank's 2020 ease of doing business index, up 14 points from the last year's 100 and up 37 points in just two years. In terms of dealing with construction permits and enforcing contracts, it is ranked among the 10 worst in the world, while it has a relatively favourable ranking when it comes to protecting minority investors or getting credit. The strong efforts taken by the Department of Industrial Policy and Promotion (DIPP) to boost ease of doing business rankings at the state level is said to impact the overall rankings of India.
During the COVID-19 pandemic, numerous rating agencies downgraded India's GDP predictions for FY21 to negative figures, signalling a recession in India, the most severe since 1979. According to a Dun & Bradstreet report, the country is likely to suffer a recession in the third quarter of FY2020 as a result of the over 2-month long nation-wide lockdown imposed to curb the spread of COVID-19.
The following table shows the main economic indicators in 1980–2018. Inflation under 5% is in green.
Historically, India has classified and tracked its economy and GDP in three sectors: agriculture, industry, and services. Agriculture includes crops, horticulture, milk and animal husbandry, aquaculture, fishing, sericulture, aviculture, forestry, and related activities. Industry includes various manufacturing sub-sectors. India's definition of services sector includes its construction, retail, software, IT, communications, hospitality, infrastructure operations, education, healthcare, banking and insurance, and many other economic activities.
Agriculture and allied sectors like forestry, logging and fishing accounted for 17% of the GDP, the sector employed 49% of its total workforce in 2014. Agriculture accounted for 23% of GDP, and employed 59% of the country's total workforce in 2016. As the Indian economy has diversified and grown, agriculture's contribution to GDP has steadily declined from 1951 to 2011, yet it is still the country's largest employment source and a significant piece of its overall socio-economic development. Crop-yield-per-unit-area of all crops has grown since 1950, due to the special emphasis placed on agriculture in the five-year plans and steady improvements in irrigation, technology, application of modern agricultural practices and provision of agricultural credit and subsidies since the Green Revolution in India. However, international comparisons reveal the average yield in India is generally 30% to 50% of the highest average yield in the world. The states of Uttar Pradesh, Punjab, Haryana, Madhya Pradesh, Andhra Pradesh, Telangana, Bihar, West Bengal, Gujarat and Maharashtra are key contributors to Indian agriculture. India receives an average annual rainfall of 1,208 millimetres (47.6 in) and a total annual precipitation of 4000 billion cubic metres, with the total utilisable water resources, including surface and groundwater, amounting to 1123 billion cubic metres. 546,820 square kilometres (211,130 sq mi) of the land area, or about 39% of the total cultivated area, is irrigated. India's inland water resources and marine resources provide employment to nearly six million people in the fisheries sector. In 2010, India had the world's sixth-largest fishing industry. India is the largest producer of milk, jute and pulses, and has the world's second-largest cattle population with 170 million animals in 2011. It is the second-largest producer of rice, wheat, sugarcane, cotton and groundnuts, as well as the second-largest fruit and vegetable producer, accounting for 10.9% and 8.6% of the world fruit and vegetable production, respectively. India is also the second-largest producer and the largest consumer of silk, producing 77,000 tons in 2005. India is the largest exporter of cashew kernels and cashew nut shell liquid (CNSL). Foreign exchange earned by the country through the export of cashew kernels during 2011–12 reached ₹43.9 billion (equivalent to ₹67 billion or US$930 million in 2019) based on statistics from the Cashew Export Promotion Council of India (CEPCI). 131,000 tonnes of kernels were exported during 2011–12. There are about 600 cashew processing units in Kollam, Kerala. India's foodgrain production remained stagnant at approximately 252 million tonnes (MT) during both the 2015–16 and 2014–15 crop years (July–June). India exports several agriculture products, such as Basmati rice, wheat, cereals, spices, fresh fruits, dry fruits, buffalo beef meat, cotton, tea, coffee and other cash crops particularly to the Middle East, Southeast and East Asian countries. About 10 percent of its export earnings come from this trade. At around 1,530,000 square kilometres (590,000 sq mi), India has the second-largest amount of arable land, after the US, with 52% of total land under cultivation. Although the total land area of the country is only slightly more than one-third of China or the US, India's arable land is marginally smaller than that of the US, and marginally larger than that of China. However, agricultural output lags far behind its potential. The low productivity in India is a result of several factors. According to the World Bank, India's large agricultural subsidies are distorting what farmers grow and hampering productivity-enhancing investment. Over-regulation of agriculture has increased costs, price risks and uncertainty, and governmental intervention in labour, land, and credit are hurting the market. Infrastructure such as rural roads, electricity, ports, food storage, retail markets and services remain inadequate. The average size of land holdings is very small, with 70% of holdings being less than one hectare (2.5 acres) in size. Irrigation facilities are inadequate, as revealed by the fact that only 46% of the total cultivable land was irrigated as of 2016, resulting in farmers still being dependent on rainfall, specifically the monsoon season, which is often inconsistent and unevenly distributed across the country. In an effort to bring an additional 20,000,000 hectares (49,000,000 acres) of land under irrigation, various schemes have been attempted, including the Accelerated Irrigation Benefit Programme (AIBP) which was provided ₹800 billion (equivalent to ₹930 billion or US$13 billion in 2019) in the union budget. Farming incomes are also hampered by lack of food storage and distribution infrastructure; a third of India's agricultural production is lost from spoilage.
Industry accounts for 26% of GDP and employs 22% of the total workforce. According to the World Bank, India's industrial manufacturing GDP output in 2015 was 6th largest in the world on current US dollar basis ($559 billion), and 9th largest on inflation-adjusted constant 2005 US dollar basis ($197.1 billion). The industrial sector underwent significant changes due to the 1991 economic reforms, which removed import restrictions, brought in foreign competition, led to the privatisation of certain government-owned public-sector industries, liberalised the foreign direct investment (FDI) regime, improved infrastructure and led to an expansion in the production of fast-moving consumer goods. Post-liberalisation, the Indian private sector was faced with increasing domestic and foreign competition, including the threat of cheaper Chinese imports. It has since handled the change by squeezing costs, revamping management, and relying on cheap labour and new technology. However, this has also reduced employment generation, even among smaller manufacturers who previously relied on labour-intensive processes.
With strength of over 1.3 million active personnel, India has the third-largest military force and the largest volunteer army. The total budget sanctioned for the Indian military for the financial year 2019–20 was ₹3.01 trillion (US$42 billion). Defence spending is expected to rise to US$62 billion by 2022.
Primary energy consumption of India is the third-largest after China and the US with 5.3% global share in the year 2015. Coal and crude oil together account for 85% of the primary energy consumption of India. India's oil reserves meet 25% of the country's domestic oil demand. As of April 2015, India's total proven crude oil reserves are 763.476 million metric tons, while gas reserves stood at 1,490 billion cubic metres (53 trillion cubic feet). Oil and natural gas fields are located offshore at Bombay High, Krishna Godavari Basin and the Cauvery Delta, and onshore mainly in the states of Assam, Gujarat and Rajasthan. India is the fourth-largest consumer of oil and net oil imports were nearly ₹8,200 billion (US$110 billion) in 2014–15, which had an adverse effect on the country's current account deficit. The petroleum industry in India mostly consists of public sector companies such as Oil and Natural Gas Corporation (ONGC), Hindustan Petroleum Corporation Limited (HPCL), Bharat Petroleum Corporation Limited (BPCL) and Indian Oil Corporation Limited (IOCL). There are some major private Indian companies in the oil sector such as Reliance Industries Limited (RIL) which operates the world's largest oil refining complex.India became the world's third-largest producer of electricity in 2013 with a 4.8% global share in electricity generation, surpassing Japan and Russia. By the end of calendar year 2015, India had an electricity surplus with many power stations idling for want of demand. The utility electricity sector had an installed capacity of 303 GW as of May 2016 of which thermal power contributed 69.8%, hydroelectricity 15.2%, other sources of renewable energy 13.0%, and nuclear power 2.1%. India meets most of its domestic electricity demand through its 106 billion tonnes of proven coal reserves. India is also rich in certain alternative sources of energy with significant future potential such as solar, wind and biofuels (jatropha, sugarcane). India's dwindling uranium reserves stagnated the growth of nuclear energy in the country for many years. Recent discoveries in the Tummalapalle belt may be among the top 20 natural uranium reserves worldwide, and an estimated reserve of 846,477 metric tons (933,081 short tons) of thorium – about 25% of world's reserves – are expected to fuel the country's ambitious nuclear energy program in the long-run. The Indo-US nuclear deal has also paved the way for India to import uranium from other countries.
Engineering is the largest sub-sector of India's industrial sector, by GDP, and the third-largest by exports. It includes transport equipment, machine tools, capital goods, transformers, switchgear, furnaces, and cast and forged parts for turbines, automobiles, and railways. The industry employs about four million workers. On a value-added basis, India's engineering subsector exported $67 billion worth of engineering goods in the 2013–14 fiscal year, and served part of the domestic demand for engineering goods.The engineering industry of India includes its growing car, motorcycle and scooters industry, and productivity machinery such as tractors. India manufactured and assembled about 18 million passenger and utility vehicles in 2011, of which 2.3 million were exported. India is the largest producer and the largest market for tractors, accounting for 29% of global tractor production in 2013. India is the 12th-largest producer and 7th-largest consumer of machine tools.The automotive manufacturing industry contributed $79 billion (4% of GDP) and employed 6.76 million people (2% of the workforce) in 2016.
India is one of the largest centres for polishing diamonds and gems and manufacturing jewellery; it is also one of the two largest consumers of gold. After crude oil and petroleum products, the export and import of gold, precious metals, precious stones, gems and jewellery accounts for the largest portion of India's global trade. The industry contributes about 7% of India's GDP, employs millions, and is a major source of its foreign-exchange earnings. The gems and jewellery industry created $60 billion in economic output on value-added basis in 2017, and is projected to grow to $110 billion by 2022.The gems and jewellery industry has been economically active in India for several thousand years. Until the 18th century, India was the only major reliable source of diamonds. Now, South Africa and Australia are the major sources of diamonds and precious metals, but along with Antwerp, New York City, and Ramat Gan, Indian cities such as Surat and Mumbai are the hubs of world's jewellery polishing, cutting, precision finishing, supply and trade. Unlike other centres, the gems and jewellery industry in India is primarily artisan-driven; the sector is manual, highly fragmented, and almost entirely served by family-owned operations. The particular strength of this sub-sector is in precision cutting, polishing and processing small diamonds (below one carat). India is also a hub for processing of larger diamonds, pearls, and other precious stones. Statistically, 11 out of 12 diamonds set in any jewellery in the world are cut and polished in India. It is also a major hub of gold and other precious-metal-based jewellery. Domestic demand for gold and jewellery products is another driver of India's GDP.
Petroleum products and chemicals are a major contributor to India's industrial GDP, and together they contribute over 34% of its export earnings. India hosts many oil refinery and petrochemical operations, including the world's largest refinery complex in Jamnagar that processes 1.24 million barrels of crude per day. By volume, the Indian chemical industry was the third-largest producer in Asia, and contributed 5% of the country's GDP. India is one of the five-largest producers of agrochemicals, polymers and plastics, dyes and various organic and inorganic chemicals. Despite being a large producer and exporter, India is a net importer of chemicals due to domestic demands.The chemical industry contributed $163 billion to the economy in FY18 and is expected to reach $300–400 billion by 2025. The industry employed 17.33 million people (4% of the workforce) in 2016.
The Indian pharmaceutical industry has grown in recent years to become a major manufacturer of health care products for the world. India holds a 20% market share in the global supply of generics by volume. The Indian pharmaceutical sector also supplies over 62% of the global demand for various vaccines. India's pharmaceutical exports stood at $17.27 billion in 2017–18 and are expected to reach $20 billion by 2020. The industry grew from $6 billion in 2005 to $36.7 billion in 2016, a compound annual growth rate (CAGR) of 17.46%. It is expected to grow at a CAGR of 15.92% to reach $55 billion in 2020. India is expected to become the sixth-largest pharmaceutical market in the world by 2020. It is one of the fastest-growing industrial sub-sectors and a significant contributor to India's export earnings. The state of Gujarat has become a hub for the manufacture and export of pharmaceuticals and active pharmaceutical ingredients (APIs).
The textile and apparel market in India was estimated to be $108.5 billion in 2015. It is expected to reach a size of $226 billion by 2023. The industry employees over 35 million people. By value, the textile industry accounts for 7% of India's industrial, 2% of GDP and 15% of the country's export earnings. India exported $39.2 billion worth of textiles in the 2017–18 fiscal year.India's textile industry has transformed in recent years from a declining sector to a rapidly developing one. After freeing the industry in 2004–2005 from a number of limitations, primarily financial, the government permitted massive investment inflows, both domestic and foreign. From 2004 to 2008, total investment into the textile sector increased by 27 billion dollars. Ludhiana produces 90% of woollens in India and is known as the Manchester of India. Tirupur has gained universal recognition as the leading source of hosiery, knitted garments, casual wear, and sportswear. Expanding textile centres such as Ichalkaranji enjoy one of the highest per-capita incomes in the country. India's cotton farms, fibre and textile industry provides employment to 45 million people in India, including some child labour (1%). The sector is estimated to employ around 400,000 children under the age of 18.
The services sector has the largest share of India's GDP, accounting for 57% in 2012, up from 15% in 1950. It is the seventh-largest services sector by nominal GDP, and third largest when purchasing power is taken into account. The services sector provides employment to 27% of the workforce. Information technology and business process outsourcing are among the fastest-growing sectors, having a cumulative growth rate of revenue 33.6% between fiscal years 1997–98 and 2002–03, and contributing to 25% of the country's total exports in 2007–08.
In March 1953, the Indian Parliament passed the Air Corporations Act to streamline and nationalise the then existing privately owned eight domestic airlines into Indian Airlines for domestic services and the Tata group-owned Air India for international services. The International Airports Authority of India (IAAI) was constituted in 1972 while the National Airports Authority was constituted in 1986. The Bureau of Civil Aviation Security was established in 1987 following the crash of Air India Flight 182.
The government de-regularised the civil aviation sector in 1991 when the government allowed private airlines to operate charter and non-scheduled services under the 'Air Taxi' Scheme until 1994, when the Air Corporation Act was repealed and private airlines could now operate scheduled services. Private airlines including Jet Airways, Air Sahara, Modiluft, Damania Airways and NEPC Airlines commenced domestic operations during this period.The aviation industry experienced a rapid transformation following deregulation. Several low-cost carriers entered the Indian market in 2004–05. Major new entrants included Air Deccan, Air Sahara, Kingfisher Airlines, SpiceJet, GoAir, Paramount Airways and IndiGo. Kingfisher Airlines became the first Indian air carrier on 15 June 2005 to order Airbus A380 aircraft worth US$3 billion. However, Indian aviation would struggle due to an economic slowdown and rising fuel and operation costs. This led to consolidation, buyouts and discontinuations. In 2007, Air Sahara and Air Deccan were acquired by Jet Airways and Kingfisher Airlines respectively. Paramount Airways ceased operations in 2010 and Kingfisher shut down in 2012. Etihad Airways agreed to acquire a 24% stake in Jet Airways in 2013. AirAsia India, a low-cost carrier operating as a joint venture between Air Asia and Tata Sons launched in 2014. As of 2013–14, only IndiGo and GoAir were generating profits. The average domestic passenger air fare dropped by 70% between 2005 and 2017, after adjusting for inflation.
The financial services industry contributed $809 billion (37% of GDP) and employed 14.17 million people (3% of the workforce) in 2016, and the banking sector contributed $407 billion (19% of GDP) and employed 5.5 million people (1% of the workforce) in 2016. The Indian money market is classified into the organised sector, comprising private, public and foreign-owned commercial banks and cooperative banks, together known as 'scheduled banks'; and the unorganised sector, which includes individual or family-owned indigenous bankers or money lenders and non-banking financial companies. The unorganised sector and microcredit are preferred over traditional banks in rural and sub-urban areas, especially for non-productive purposes such as short-term loans for ceremonies.Prime Minister Indira Gandhi nationalised 14 banks in 1969, followed by six others in 1980, and made it mandatory for banks to provide 40% of their net credit to priority sectors including agriculture, small-scale industry, retail trade and small business, to ensure that the banks fulfilled their social and developmental goals. Since then, the number of bank branches has increased from 8,260 in 1969 to 72,170 in 2007 and the population covered by a branch decreased from 63,800 to 15,000 during the same period. The total bank deposits increased from ₹59.1 billion (equivalent to ₹2.3 trillion or US$32 billion in 2019) in 1970–71 to ₹38,309 billion (equivalent to ₹78 trillion or US$1.1 trillion in 2019) in 2008–09. Despite an increase of rural branches – from 1,860 or 22% of the total in 1969 to 30,590 or 42% in 2007 – only 32,270 of 500,000 villages are served by a scheduled bank.India's gross domestic savings in 2006–07 as a percentage of GDP stood at a high 32.8%. More than half of personal savings are invested in physical assets such as land, houses, cattle, and gold. The government-owned public-sector banks hold over 75% of total assets of the banking industry, with the private and foreign banks holding 18.2% and 6.5% respectively. Since liberalisation, the government has approved significant banking reforms. While some of these relate to nationalised banks – such as reforms encouraging mergers, reducing government interference and increasing profitability and competitiveness – other reforms have opened the banking and insurance sectors to private and foreign companies.
According to the report of The National Association of Software and Services Companies (NASSCOM), India has a presence of around 400 companies in the fintech space, with an investment of about $420 million in 2015. The NASSCOM report also estimated the fintech software and services market to grow 1.7 times by 2020, making it worth $8 billion. The Indian fintech landscape is segmented as follows – 34% in payment processing, followed by 32% in banking and 12% in the trading, public and private markets.
The information technology (IT) industry in India consists of two major components: IT Services and business process outsourcing (BPO). The sector has increased its contribution to India's GDP from 1.2% in 1998 to 7.5% in 2012. According to NASSCOM, the sector aggregated revenues of US$147 billion in 2015, where export revenue stood at US$99 billion and domestic at US$48 billion, growing by over 13%.The growth in the IT sector is attributed to increased specialisation, and an availability of a large pool of low-cost, highly skilled, fluent English-speaking workers – matched by increased demand from foreign consumers interested in India's service exports, or looking to outsource their operations. The share of the Indian IT industry in the country's GDP increased from 4.8% in 2005–06 to 7% in 2008. In 2009, seven Indian firms were listed among the top 15 technology outsourcing companies in the world.The business process outsourcing services in the outsourcing industry in India caters mainly to Western operations of multinational corporations. As of 2012, around 2.8 million people work in the outsourcing sector. Annual revenues are around $11 billion, around 1% of GDP. Around 2.5 million people graduate in India every year. Wages are rising by 10–15 percent as a result of skill shortages.
India became the tenth-largest insurance market in the world in 2013, rising from 15th in 2011. At a total market size of US$66.4 billion in 2013, it remains small compared to world's major economies, and the Indian insurance market accounted for just 2% of the world's insurance business in 2017. India's life and non-life insurance industry collected ₹6.10 trillion (US$86 billion) in total gross insurance premiums in 2018. Life insurance accounts for 75.41% of the insurance market and the rest is general insurance. Of the 52 insurance companies in India, 24 are active in life-insurance business.Specialised insurers Export Credit Guarantee Corporation and Agriculture Insurance Company (AIC) offer credit guarantee and crop insurance. It has introduced several innovative products such as weather insurance and insurance related to specific crops. The premium underwritten by the non-life insurers during 2010–11 was ₹425 billion (equivalent to ₹700 billion or US$9.9 billion in 2019) against ₹346 billion (equivalent to ₹620 billion or US$8.8 billion in 2019) in 2009–10. The growth was satisfactory, particularly given across-the-broad cuts in the tariff rates. The private insurers underwrote premiums of ₹174 billion (equivalent to ₹290 billion or US$4.0 billion in 2019) against ₹140 billion (equivalent to ₹250 billion or US$3.5 billion in 2019) in 2009–10. The Indian insurance business had been under-developed with low levels of insurance penetration.
The retail industry, excluding wholesale, contributed $482 billion (22% of GDP) and employed 249.94 million people (57% of the workforce) in 2016. The industry is the second largest employer in India, after agriculture. The Indian retail market is estimated to be US$600 billion and one of the top-five retail markets in the world by economic value. India has one of the fastest-growing retail markets in the world, and is projected to reach $1.3 trillion by 2020. The e-commerce retail market in India was valued at $32.7 billion in 2018, and is expected to reach $71.9 billion by 2022.India's retail industry mostly consists of local mom-and-pop stores, owner-manned shops and street vendors. Retail supermarkets are expanding, with a market share of 4% in 2008. In 2012, the government permitted 51% FDI in multi-brand retail and 100% FDI in single-brand retail. However, a lack of back-end warehouse infrastructure and state-level permits and red tape continue to limit growth of organised retail. Compliance with over thirty regulations such as "signboard licences" and "anti-hoarding measures" must be made before a store can open for business. There are taxes for moving goods from state to state, and even within states. According to The Wall Street Journal, the lack of infrastructure and efficient retail networks cause a third of India's agriculture produce to be lost from spoilage.
The World Travel & Tourism Council calculated that tourism generated ₹15.24 trillion (US$210 billion) or 9.4% of the nation's GDP in 2017 and supported 41.622 million jobs, 8% of its total employment. The sector is predicted to grow at an annual rate of 6.9% to ₹32.05 trillion (US$450 billion) by 2028 (9.9% of GDP). Over 10 million foreign tourists arrived in India in 2017 compared to 8.89 million in 2016, recording a growth of 15.6%. India earned $21.07 billion in foreign exchange from tourism receipts in 2015. International tourism to India has seen a steady growth from 2.37 million arrivals in 1997 to 8.03 million arrivals in 2015. The United States is the largest source of international tourists to India, while European Union nations and Japan are other major sources of international tourists. Less than 10% of international tourists visit the Taj Mahal, with the majority visiting other cultural, thematic and holiday circuits. Over 12 million Indian citizens take international trips each year for tourism, while domestic tourism within India adds about 740 million Indian travellers.India has a fast-growing medical tourism sector of its health care economy, offering low-cost health services and long-term care. In October 2015, the medical tourism sector was estimated to be worth US$3 billion. It is projected to grow to $7–8 billion by 2020. In 2014, 184,298 foreign patients traveled to India to seek medical treatment.
An ASSOCHAM-PwC joint study projected that the Indian media and entertainment industry would grow from a size of $30.364 billion in 2017 to $52.683 billion by 2022, recording a CAGR of 11.7%. The study also predicted that television, cinema and over-the-top services would account for nearly half of the overall industry growth during the period.
India's healthcare sector is expected to grow at a CAGR of 29% between 2015 and 2020, to reach US$280 billion, buoyed by rising incomes, greater health awareness, increased precedence of lifestyle diseases, and improved access to health insurance.The ayurveda industry in India recorded a market size of $4.4 billion in 2018. The Confederation of Indian Industry estimates that the industry will grow at a CAGR 16% until 2025. Nearly 75% of the market comprises over-the-counter personal care and beauty products, while ayurvedic well-being or ayurvedic tourism services accounted for 15% of the market.
The telecommunication sector generated ₹2.20 trillion (US$31 billion) in revenue in 2014–15, accounting for 1.94% of total GDP. India is the second-largest market in the world by number of telephone users (both fixed and mobile phones) with 1.053 billion subscribers as of 31 August 2016. It has one of the lowest call-tariffs in the world, due to fierce competition among telecom operators. India has the world's third-largest Internet user-base. As of 31 March 2016, there were 342.65 million Internet subscribers in the country.Industry estimates indicate that there are over 554 million TV consumers in India as of 2012. India is the largest direct-to-home (DTH) television market in the world by number of subscribers. As of May 2016, there were 84.80 million DTH subscribers in the country.
Mining contributed $63 billion (3% of GDP) and employed 20.14 million people (5% of the workforce) in 2016. India's mining industry was the fourth-largest producer of minerals in the world by volume, and eighth-largest producer by value in 2009. In 2013, it mined and processed 89 minerals, of which four were fuel, three were atomic energy minerals, and 80 non-fuel. The government-owned public sector accounted for 68% of mineral production by volume in 2011–12.Nearly 50% of India's mining industry, by output value, is concentrated in eight states: Odisha, Rajasthan, Chhattisgarh, Andhra Pradesh, Telangana, Jharkhand, Madhya Pradesh and Karnataka. Another 25% of the output by value comes from offshore oil and gas resources. India operated about 3,000 mines in 2010, half of which were coal, limestone and iron ore. On output-value basis, India was one of the five largest producers of mica, chromite, coal, lignite, iron ore, bauxite, barite, zinc and manganese; while being one of the ten largest global producers of many other minerals. India was the fourth-largest producer of steel in 2013, and the seventh-largest producer of aluminium.India's mineral resources are vast. However, its mining industry has declined – contributing 2.3% of its GDP in 2010 compared to 3% in 2000, and employed 2.9 million people – a decreasing percentage of its total labour. India is a net importer of many minerals including coal. India's mining sector decline is because of complex permit, regulatory and administrative procedures, inadequate infrastructure, shortage of capital resources, and slow adoption of environmentally sustainable technologies.
Until the liberalisation of 1991, India was largely and intentionally isolated from world markets, to protect its economy and to achieve self-reliance. Foreign trade was subject to import tariffs, export taxes and quantitative restrictions, while foreign direct investment (FDI) was restricted by upper-limit equity participation, restrictions on technology transfer, export obligations and government approvals; these approvals were needed for nearly 60% of new FDI in the industrial sector. The restrictions ensured that FDI averaged only around $200 million annually between 1985 and 1991; a large percentage of the capital flows consisted of foreign aid, commercial borrowing and deposits of non-resident Indians. India's exports were stagnant for the first 15 years after independence, due to general neglect of trade policy by the government of that period; imports in the same period, with early industrialisation, consisted predominantly of machinery, raw materials and consumer goods. Since liberalisation, the value of India's international trade has increased sharply, with the contribution of total trade in goods and services to the GDP rising from 16% in 1990–91 to 47% in 2009–10. Foreign trade accounted for 48.8% of India's GDP in 2015. Globally, India accounts for 1.44% of exports and 2.12% of imports for merchandise trade and 3.34% of exports and 3.31% of imports for commercial services trade. India's major trading partners are the European Union, China, the United States and the United Arab Emirates. In 2006–07, major export commodities included engineering goods, petroleum products, chemicals and pharmaceuticals, gems and jewellery, textiles and garments, agricultural products, iron ore and other minerals. Major import commodities included crude oil and related products, machinery, electronic goods, gold and silver. In November 2010, exports increased 22.3% year-on-year to ₹851 billion (equivalent to ₹1.5 trillion or US$22 billion in 2019), while imports were up 7.5% at ₹1,251 billion (equivalent to ₹2.3 trillion or US$32 billion in 2019). The trade deficit for the same month dropped from ₹469 billion (equivalent to ₹950 billion or US$13 billion in 2019) in 2009 to ₹401 billion (equivalent to ₹720 billion or US$10 billion in 2019) in 2010.India is a founding-member of General Agreement on Tariffs and Trade (GATT) and its successor, the WTO. While participating actively in its general council meetings, India has been crucial in voicing the concerns of the developing world. For instance, India has continued its opposition to the inclusion of labour, environmental issues and other non-tariff barriers to trade in WTO policies.India secured 43rd place in competitiveness index.
Since independence, India's balance of payments on its current account has been negative. Since economic liberalisation in the 1990s, precipitated by a balance-of-payment crisis, India's exports rose consistently, covering 80.3% of its imports in 2002–03, up from 66.2% in 1990–91. However, the global economic slump followed by a general deceleration in world trade saw the exports as a percentage of imports drop to 61.4% in 2008–09. India's growing oil import bill is seen as the main driver behind the large current account deficit, which rose to $118.7 billion, or 11.11% of GDP, in 2008–09. Between January and October 2010, India imported $82.1 billion worth of crude oil. The Indian economy has run a trade deficit every year from 2002 to 2012, with a merchandise trade deficit of US$189 billion in 2011–12. Its trade with China has the largest deficit, about $31 billion in 2013.India's reliance on external assistance and concessional debt has decreased since liberalisation of the economy, and the debt service ratio decreased from 35.3% in 1990–91 to 4.4% in 2008–09. In India, external commercial borrowings (ECBs), or commercial loans from non-resident lenders, are being permitted by the government for providing an additional source of funds to Indian corporates. The Ministry of Finance monitors and regulates them through ECB policy guidelines issued by the Reserve Bank of India (RBI) under the Foreign Exchange Management Act of 1999. India's foreign exchange reserves have steadily risen from $5.8 billion in March 1991 to ₹38,832.21 billion (US$540 billion) in July 2020. In 2012, the United Kingdom announced an end to all financial aid to India, citing the growth and robustness of Indian economy.India's current account deficit reached an all-time high in 2013. India has historically funded its current account deficit through borrowings by companies in the overseas markets or remittances by non-resident Indians and portfolio inflows. From April 2016 to January 2017, RBI data showed that, for the first time since 1991, India was funding its deficit through foreign direct investment inflows. The Economic Times noted that the development was "a sign of rising confidence among long-term investors in Prime Minister Narendra Modi's ability to strengthen the country's economic foundation for sustained growth".
As the third-largest economy in the world in PPP terms, India has attracted foreign direct investment (FDI). During the year 2011, FDI inflow into India stood at $36.5 billion, 51.1% higher than the 2010 figure of $24.15 billion. India has strengths in telecommunication, information technology and other significant areas such as auto components, chemicals, apparels, pharmaceuticals, and jewellery. Despite a surge in foreign investments, rigid FDI policies were a significant hindrance. Over time, India has adopted a number of FDI reforms. India has a large pool of skilled managerial and technical expertise. The size of the middle-class population stands at 300 million and represents a growing consumer market.India liberalised its FDI policy in 2005, allowing up to a 100% FDI stake in ventures. Industrial policy reforms have substantially reduced industrial licensing requirements, removed restrictions on expansion and facilitated easy access to foreign technology and investment. The upward growth curve of the real-estate sector owes some credit to a booming economy and liberalised FDI regime. In March 2005, the government amended the rules to allow 100% FDI in the construction sector, including built-up infrastructure and construction development projects comprising housing, commercial premises, hospitals, educational institutions, recreational facilities, and city- and regional-level infrastructure. Between 2012 and 2014, India extended these reforms to defence, telecom, oil, retail, aviation, and other sectors.From 2000 to 2010, the country attracted $178 billion as FDI. The inordinately high investment from Mauritius is due to routing of international funds through the country given significant tax advantages – double taxation is avoided due to a tax treaty between India and Mauritius, and Mauritius is a capital gains tax haven, effectively creating a zero-taxation FDI channel. FDI accounted for 2.1% of India's GDP in 2015.As the government has eased 87 foreign investment direct rules across 21 sectors in the last three years, FDI inflows hit $60.1 billion between 2016 and 2017 in India.
Since 2000, Indian companies have expanded overseas, investing FDI and creating jobs outside India. From 2006 to 2010, FDI by Indian companies outside India amounted to 1.34 per cent of its GDP. Indian companies have deployed FDI and started operations in the United States, Europe and Africa. The Indian company Tata is the United Kingdom's largest manufacturer and private-sector employer.
The Indian rupee (₹) is the only legal tender in India, and is also accepted as legal tender in neighbouring Nepal and Bhutan, both of which peg their currency to that of the Indian rupee. The rupee is divided into 100 paise. The highest-denomination banknote is the ₹2,000 note; the lowest-denomination coin in circulation is the 50 paise coin. Since 30 June 2011, all denominations below 50 paise have ceased to be legal currency. India's monetary system is managed by the Reserve Bank of India (RBI), the country's central bank. Established on 1 April 1935 and nationalised in 1949, the RBI serves as the nation's monetary authority, regulator and supervisor of the monetary system, banker to the government, custodian of foreign exchange reserves, and as an issuer of currency. It is governed by a central board of directors, headed by a governor who is appointed by the Government of India. The benchmark interest rates are set by the Monetary Policy Committee. The rupee was linked to the British pound from 1927 to 1946, and then to the US dollar until 1975 through a fixed exchange rate. It was devalued in September 1975 and the system of fixed par rate was replaced with a basket of four major international currencies: the British pound, the US dollar, the Japanese yen and the Deutsche Mark. In 1991, after the collapse of its largest trading partner, the Soviet Union, India faced the major foreign exchange crisis and the rupee was devalued by around 19% in two stages on 1 and 2 July. In 1992, a Liberalized Exchange Rate Mechanism (LERMS) was introduced. Under LERMS, exporters had to surrender 40 percent of their foreign exchange earnings to the RBI at the RBI-determined exchange rate; the remaining 60% could be converted at the market-determined exchange rate. In 1994, the rupee was convertible on the current account, with some capital controls.After the sharp devaluation in 1991 and transition to current account convertibility in 1994, the value of the rupee has been largely determined by market forces. The rupee has been fairly stable during the decade 2000–2010. In October 2018, rupee touched an all-time low 74.90 to the US dollar.
In May 2014, the World Bank reviewed and proposed revisions to its poverty calculation methodology of 2005 and purchasing-power-parity basis for measuring poverty. According to the revised methodology, the world had 872.3 million people below the new poverty line, of which 179.6 million lived in India. With 17.5% of the total world's population, India had a 20.6% share of world's poorest in 2013. According to a 2005–2006 survey, India had about 61 million children under the age of 5 who were chronically malnourished. A 2011 UNICEF report stated that between 1990 and 2010, India achieved a 45 percent reduction in under age 5 mortality rates, and now ranks 46th of 188 countries on this metric.Since the early 1960s, successive governments have implemented various schemes to alleviate poverty, under central planning, that have met with partial success. In 2005, the government enacted the Mahatma Gandhi National Rural Employment Guarantee Act (MGNREGA), guaranteeing 100 days of minimum wage employment to every rural household in all the districts of India. In 2011, it was widely criticised and beset with controversy for corrupt officials, deficit financing as the source of funds, poor quality of infrastructure built under the programme, and unintended destructive effects. Other studies suggest that the programme has helped reduce rural poverty in some cases. Yet other studies report that India's economic growth has been the driver of sustainable employment and poverty reduction, though a sizeable population remains in poverty. India lifted 271 million people out of poverty between 2006 and 2016, recording the fastest reductions in the multidimensional poverty index values during the period with strong improvements in areas such as assets, cooking fuel, sanitation and nutrition.On the 2019 Global Hunger Index India ranked 102nd (out of 117 countries), being categorized as 'serious' in severity.
Agricultural and allied sectors accounted for about 52.1% of the total workforce in 2009–10. While agriculture employment has fallen over time in percentage of labour employed, services which includes construction and infrastructure have seen a steady growth accounting for 20.3% of employment in 2012–13. Of the total workforce, 7% is in the organised sector, two-thirds of which are in the government-controlled public sector. About 51.2% of the workforce in India is self-employed. According to a 2005–06 survey, there is a gender gap in employment and salaries. In rural areas, both men and women are primarily self-employed, mostly in agriculture. In urban areas, salaried work was the largest source of employment for both men and women in 2006.Unemployment in India is characterised by chronic (disguised) unemployment. Government schemes that target eradication of both poverty and unemployment – which in recent decades has sent millions of poor and unskilled people into urban areas in search of livelihoods – attempt to solve the problem by providing financial assistance for starting businesses, honing skills, setting up public sector enterprises, reservations in governments, etc. The decline in organised employment, due to the decreased role of the public sector after liberalisation, has further underlined the need for focusing on better education and created political pressure for further reforms. India's labour regulations are heavy, even by developing country standards, and analysts have urged the government to abolish or modify them to make the environment more conducive for employment generation. The 11th five-year plan has also identified the need for a congenial environment to be created for employment generation, by reducing the number of permissions and other bureaucratic clearances required. Inequalities and inadequacies in the education system have been identified as an obstacle, which prevents the benefits of increased employment opportunities from reaching all sectors of society.Child labour in India is a complex problem that is rooted in poverty. Since the 1990s, the government has implemented a variety of programs to eliminate child labour. These have included setting up schools, launching free school lunch programs, creating special investigation cells, etc. Author Sonalde Desai stated that recent studies on child labour in India have found some pockets of industries in which children are employed, but overall, relatively few Indian children are employed. Child labour below the age of 10 is now rare. In the 10–14 age group, the latest surveys find only 2% of children working for wage, while another 9% work within their home or rural farms assisting their parents in times of high work demand such as sowing and harvesting of crops.India has the largest diaspora around the world, an estimated 16 million people, many of whom work overseas and remit funds back to their families. The Middle East region is the largest source of employment of expat Indians. The crude oil production and infrastructure industry of Saudi Arabia employs over 2 million expat Indians. Cities such as Dubai and Abu Dhabi in United Arab Emirates have employed another 2 million Indians during the construction boom in recent decades. In 2009–10, remittances from Indian migrants overseas stood at ₹2,500 billion (equivalent to ₹4.5 trillion or US$63 billion in 2019), the highest in the world, but their share in FDI remained low at around 1%.
Corruption has been a pervasive problem in India. A 2005 study by Transparency International (TI) found that more than half of those surveyed had first-hand experience of paying a bribe or peddling influence to get a job done in a public office in the previous year. A follow-up study in 2008 found this rate to be 40 percent. In 2011, TI ranked India at 95th place amongst 183 countries in perceived levels of public sector corruption. By 2016, India saw a reduction in corruption and its ranking improved to 79th place.In 1996, red tape, bureaucracy and the Licence Raj were suggested as a cause for the institutionalised corruption and inefficiency. More recent reports suggest the causes of corruption include excessive regulations and approval requirements, mandated spending programs, monopoly of certain goods and service providers by government-controlled institutions, bureaucracy with discretionary powers, and lack of transparent laws and processes. Computerisation of services, various central and state vigilance commissions, and the 2005 Right to Information Act – which requires government officials to furnish information requested by citizens or face punitive action – have considerably reduced corruption and opened avenues to redress grievances.In 2011, the Indian government concluded that most spending fails to reach its intended recipients, as the large and inefficient bureaucracy consumes budgets. India's absence rates are among the worst in the world; one study found that 25% of public sector teachers and 40% of government-owned public-sector medical workers could not be found at the workplace. Similarly, there are many issues facing Indian scientists, with demands for transparency, a meritocratic system, and an overhaul of the bureaucratic agencies that oversee science and technology.India has an underground economy, with a 2006 report alleging that India topped the worldwide list for black money with almost $1,456 billion stashed in Swiss banks. This would amount to 13 times the country's total external debt. These allegations have been denied by the Swiss Banking Association. James Nason, the Head of International Communications for the Swiss Banking Association, suggested "The (black money) figures were rapidly picked up in the Indian media and in Indian opposition circles, and circulated as gospel truth. However, this story was a complete fabrication. The Swiss Bankers Association never published such a report. Anyone claiming to have such figures (for India) should be forced to identify their source and explain the methodology used to produce them." A recent step taken by Prime Minister Modi, on 8 November 2016, involved the demonetization of all 500 and 1000 rupee bank notes (replaced by new 500 and 2000 rupee notes) to return black money into the economy.
India has made progress increasing the primary education attendance rate and expanding literacy to approximately three-fourths of the population. India's literacy rate had grown from 52.2% in 1991 to 74.04% in 2011. The right to education at elementary level has been made one of the fundamental rights under the eighty-sixth Amendment of 2002, and legislation has been enacted to further the objective of providing free education to all children. However, the literacy rate of 74% is lower than the worldwide average and the country suffers from a high drop-out rate. Literacy rates and educational opportunities vary by region, gender, urban and rural areas, and among different social groups.
Poverty rates in India's poorest states are three to four times higher than those in the more advanced states. While India's average annual per capita income was $1,410 in 2011 – placing it among the poorest of the world's middle-income countries – it was just $436 in Uttar Pradesh (which has more people than Brazil) and only $294 in Bihar, one of India's poorest states. A critical problem facing India's economy is the sharp and growing regional variations among India's different states and territories in terms of poverty, availability of infrastructure and socio-economic development. Six low-income states – Assam, Chhattisgarh, Nagaland, Madhya Pradesh, Odisha and Uttar Pradesh – are home to more than one-third of India's population. Severe disparities exist among states in terms of income, literacy rates, life expectancy and living conditions.The five-year plans, especially in the pre-liberalisation era, attempted to reduce regional disparities by encouraging industrial development in the interior regions and distributing industries across states. The results have been discouraging as these measures increased inefficiency and hampered effective industrial growth. The more advanced states have been better placed to benefit from liberalisation, with well-developed infrastructure and an educated and skilled workforce, which attract the manufacturing and service sectors. Governments of less-advanced states have tried to reduce disparities by offering tax holidays and cheap land, and focused on sectors like tourism which can develop faster than other sectors. India's income Gini coefficient is 33.9, according to the United Nations Development Program (UNDP), indicating overall income distribution to be more uniform than East Asia, Latin America and Africa. The Global Wealth Migration Review 2019 report, published by New World Wealth, estimated that 48% of India's total wealth was held by high-net worth individuals.There is a continuing debate on whether India's economic expansion has been pro-poor or anti-poor. Studies suggest that economic growth has been pro-poor and has reduced poverty in India.
The development of Indian security markets began with the launch of the Bombay Stock Exchange (BSE) in July 1875 and Ahmedabad Stock exchange in 1894. Since then, 22 other exchanges have traded in Indian cities. In 2014, India's stock exchange market became the 10th largest in the world by market capitalisation, just above those of South Korea and Australia. India's two major stock exchanges, BSE and National Stock Exchange of India, had a market capitalisation of US$1.71 trillion and US$1.68 trillion as of February 2015, according to World Federation of Exchanges.The initial public offering (IPO) market in India has been small compared to NYSE and NASDAQ, raising US$300 million in 2013 and US$1.4 billion in 2012. Ernst & Young stated that the low IPO activity reflects market conditions, slow government approval processes and complex regulations. Before 2013, Indian companies were not allowed to list their securities internationally without first completing an IPO in India. In 2013, these security laws were reformed and Indian companies can now choose where they want to list first: overseas, domestically, or both concurrently. Further, security laws have been revised to ease overseas listings of already-listed companies, to increase liquidity for private equity and international investors in Indian companies.
Economic Advisory Council Economic development in India List of megaprojects in India Make in India – a government program to encourage manufacturing in India NITI Aayog Startup IndiaEvents: Late-2000s recession Oil price increases since 2003 Demonetization Economic impact of the COVID-19 pandemic in IndiaLists: List of companies of India List of largest companies in India List of the largest trading partners of India Trade unions in India Natural resources of India
Ministry of Finance Ministry of Commerce and Industry Ministry of Statistics and Programme Implementation India profile at the CIA World Factbook India profile at The World Bank India – OECD
The Government of India (ISO: Bhārat Sarkār), often abbreviated as GoI, is the union government created by the constitution of India as the legislative, executive and judicial authority of the union of twenty eight states and eight union territories of a constitutionally democratic republic. The seat of the Government is located in New Delhi, the capital of India.
Modelled after the Westminster system for governing the state, the Union government is mainly composed of the executive, the legislature, and the judiciary, in which all powers are vested by the constitution in the prime minister, parliament and the supreme court. The president of India is the head of state and the commander-in-chief of the Indian Armed Forces whilst the elected prime minister acts as the head of the executive, and is responsible for running the Union government. The parliament is bicameral in nature, with the Lok Sabha being the lower house, and the Rajya Sabha the upper house. The judiciary systematically contains an apex supreme court, 24 high courts, and several district courts, all inferior to the supreme court.The basic civil and criminal laws governing the citizens of India are set down in major parliamentary legislation, such as the civil procedure code, the penal code, and the criminal procedure code. Similar to the Union government, individual State governments each consist of executive, legislative and judiciary. The legal system as applicable to the Union and individual State governments is based on the English Common and Statutory Law. The full name of the country is the Republic of India. India and Bharat are equally official short names for the Republic of India in the Constitution, and both names appears on legal banknotes, in treaties and in legal cases. The terms "Union government", "Central government" and "Bhārat Sarkār" are often used officially and unofficially to refer to the Government of India. The term New Delhi is commonly used as a metonym for the Union government, as the seat of the government is in New Delhi.
The powers of the legislature in India are exercised by the Parliament, a bicameral legislature consisting of the Rajya Sabha and the Lok Sabha. Of the two houses of parliament, the Rajya Sabha is considered to be the upper house or the Council of States and consists of members appointed by the president and elected by the state and territorial legislatures. The Lok Sabha is considered the lower house or the House of the people.The parliament does not have complete control and sovereignty, as its laws are subject to judicial review by the Supreme Court. However, it does exercise some control over the executive. The members of the cabinet, including the prime minister, are either chosen from parliament or elected thereto within six months of assuming office. The cabinet as a whole is responsible to the Lok Sabha. The Lok Sabha is a temporary house and can be dissolved only when the party in power loses the support of the majority of the house. The Rajya Sabha is a permanent house and can never be dissolved. The members of the Rajya Sabha are elected for a six-year term.
The executive of government is the one that has sole authority and responsibility for the daily administration of the state bureaucracy. The division of power into separate branches of government is central to the republican idea of the separation of powers.
The executive power is vested mainly in the President of India, as per Article 53(1) of the constitution. The president has all constitutional powers and exercises them directly or through subordinate officers as per the aforesaid Article 53(1). The president is to act in accordance with aid and advice tendered by the Prime Minister of India, who leads the Council of Ministers of the Republic of India as described in Article 74 of the Constitution of India. The council of ministers remains in power during the 'pleasure' of the president. However, in practice, the council of ministers must retain the support of the Lok Sabha. If a president were to dismiss the council of ministers on his or her own initiative, it might trigger a constitutional crisis. Thus, in practice, the Council of Ministers cannot be dismissed as long as it holds the support of a majority in the Lok Sabha. The President is responsible for appointing many high officials in India. These high officials include the governors of the 29 states; the chief justice; other judges of the supreme court and high courts on the advice of other judges; the attorney general; the comptroller and auditor general; the chief election commissioner and other election commissioners; the chairman and members of the Union Public Service Commission; the officers of the All India Services (IAS, IFoS and IPS) and Central Civil Services in group 'A'; and the ambassadors and high commissioners to other countries on the recommendations of the cabinet.The President, as the head of state, also receives the credentials of ambassadors from other countries, whilst the prime minister, as head of government, receives credentials of high commissioners from other members of the Commonwealth, in line with historical tradition. The President is the de jure commander-in-chief of the Indian Armed Forces.The President of India can grant a pardon to or reduce the sentence of a convicted person for one time, particularly in cases involving punishment of death. The decisions involving pardoning and other rights by the president are independent of the opinion of the prime minister or the Lok Sabha majority. In most other cases, however, the president exercises his or her executive powers on the advice of the prime minister. Presently, the President of India is Ram Nath Kovind.
The vice president is the second highest constitutional position in India after the president. The vice president represents the nation in the absence of the president and takes charge as acting president in the incident of resignation impeachment or removal of the president. The vice president also has the legislative function of acting as the chairman of the Rajya Sabha. The vice president is elected indirectly by members of an electoral college consisting of the members of both the houses of the parliament in accordance with the system of proportional representation by means of the single transferable vote and the voting is by secret ballot conducted by the election commission.
The Prime Minister of India, as addressed in the Constitution of India, is the chief of the government, chief adviser to the president, head of the council of ministers and the leader of the majority party in the parliament. The prime minister leads the executive of the Government of India. The prime minister is the senior member of cabinet in the executive of government in a parliamentary system. The prime minister selects and can dismiss other members of the cabinet; allocates posts to members within the Government; is the presiding member and chairman of the cabinet and is responsible for bringing a proposal of legislation. The resignation or death of the prime minister dissolves the cabinet. The prime minister is appointed by the president to assist the latter in the administration of the affairs of the executive.
The Cabinet of India includes the prime minister and cabinet ministers. Each minister must be a member of one of the houses of the parliament. The cabinet is headed by the prime minister, and is advised by the cabinet secretary, who also acts as the head of the Indian Administrative Service and other civil services. Other ministers are either as union cabinet ministers, who are heads of the various ministries; or ministers of state, who are junior members who report directly to one of the cabinet ministers, often overseeing a specific aspect of government; or ministers of state (independent charges), who do not report to a cabinet minister. As per article 88 of the constitution, every minister shall have the right to speak in, and to take part in the proceedings of, either house, any joint sitting of the houses, and any committee of parliament of which he may be named a member, but shall not be entitled to a vote in the house where he is not a member.
A secretary to the Government of India, a civil servant, generally an Indian Administrative Service (IAS) officer, is the administrative head of the ministry or department, and is the principal adviser to the minister on all matters of policy and administration within the ministry/department. Secretaries to the Government of India rank 23rd on Indian order of precedence. Secretaries at the higher level are assisted by one or many additional secretaries, who are further assisted by joint secretaries. At the middle they are assisted by directors/deputy secretaries and under secretaries. At the lower level, there are section officers, assistant section officers, upper division clerks, lower division clerks and other secretarial staff.
The Civil Services of India are the civil services and the permanent bureaucracy of India. The executive decisions are implemented by the Indian civil servants. In the parliamentary democracy of India, the ultimate responsibility for running the administration rests with the elected representatives of the people which are the ministers. These ministers are accountable to the legislatures which are also elected by the people on the basis of universal adult suffrage. The ministers are indirectly responsible to the people themselves. But the handful of ministers are not expected to deal personally with the various problems of modern administration. Thus the ministers lay down the policy and it is for the civil servants to enforce it.
The cabinet secretary (IAST: Maṃtrimaṇḍala Saciva) is the top-most executive official and senior-most civil servant of the Government of India. The cabinet secretary is the ex-officio head of the Civil Services Board, the Cabinet Secretariat, the Indian Administrative Service (IAS) and head of all civil services under the rules of business of the government. The cabinet secretary is generally the senior-most officer of the Indian Administrative Service. The cabinet secretary ranks 11th on the Indian order of precedence. The cabinet secretary is under the direct charge of the prime minister.
India's independent union judicial system began under the British, and its concepts and procedures resemble those of Anglo-Saxon countries. The Supreme Court of India consists of the chief justice and 30 associate justices, all appointed by the president on the advice of the Chief Justice of India. The jury trials were abolished in India in the early 1960s, after the famous case KM Nanavati v. State of Maharashtra, for reasons of being vulnerable to media and public pressure, as well as to being misled. Unlike its United States counterpart, the Indian justice system consists of a unitary system at both state and union level. The judiciary consists of the Supreme Court of India, high courts at the state level, and district courts and Sessions Courts at the district level.
The Supreme Court of India is situated in New Delhi, the capital region of India. The supreme court is the highest judicial forum and final court of appeal under the Constitution of India, the highest constitutional court, with the power of constitutional review. Consisting of the Chief Justice of India and 30 sanctioned other judges, it has extensive powers in the form of original, appellate and advisory jurisdictions.As the final court of appeal of the country, it takes up appeals primarily against verdicts of the high courts of various states of the Union and other courts and tribunals. It safeguards fundamental rights of citizens and settles disputes between various governments in the country. As an advisory court, it hears matters which may specifically be referred to it under the constitution by the president. It also may take cognisance of matters on its own (or 'suo moto'), without anyone drawing its attention to them. The law declared by the supreme court becomes binding on all courts within India and also by the union and state governments. Per Article 142, it is the duty of the president to enforce the decrees of the supreme court. In addition, Article 32 of the constitution gives an extensive original jurisdiction to the supreme court in regard to enforcing fundamental rights. It is empowered to issue directions, orders or writs, including writs in the nature of habeas corpus, mandamus, prohibition, quo warranto and certiorari to enforce them. The supreme court has been conferred with power to direct transfer of any civil or criminal case from one state high court to another state high court, or from a court subordinate to another state high court and the supreme court. Although the proceedings in the supreme court arise out of the judgment or orders made by the subordinate courts, of late the supreme court has started entertaining matters in which interest of the public at large is involved. This may be done by any individual or group of persons either by filing a writ petition at the filing counter of the court, or by addressing a letter to the Chief Justice of India, highlighting the question of public importance for redress. These are known as public interest litigations.
India has a quasi-federal form of government, called "union" or "central" government, with elected officials at the union, state and local levels. At the national level, the head of government, the prime minister, is appointed by the president of India from the party or coalition that has the majority of seats in the Lok Sabha. The members of the Lok Sabha are directly elected for a term of five years by universal adult suffrage through a first-past-the-post voting system. Members of the Rajya Sabha, which represents the states, are elected by the members of State legislative assemblies by proportional representation, except for 12 members who are nominated by the president. India is currently the largest democracy in the world, with around 900 million eligible voters, as of 2019.
State governments in India are the governments ruling states of India and the chief minister heads the state government. Power is divided between union government and state governments. State government's legislature is bicameral in 5 states and unicameral in the rest. Lower house is elected with 5 years term, while in upper house 1/3 of the total members in the house gets elected every 2 years with 6-year term. Local government function at the basic level. It is the third level of government apart from union and state governments. It consists of panchayats in rural areas and municipalities in urban areas. They are elected directly or indirectly by the people.
India has a three-tier tax structure, wherein the constitution empowers the union government to levy income tax, tax on capital transactions (wealth tax, inheritance tax), sales tax, service tax, customs and excise duties and the state governments to levy sales tax on intrastate sale of goods, tax on entertainment and professions, excise duties on manufacture of alcohol, stamp duties on transfer of property and collect land revenue (levy on land owned). The local governments are empowered by the state government to levy property tax and charge users for public utilities like water supply, sewage etc. More than half of the revenues of the union and state governments come from taxes, of which 3/4 come from direct taxes. More than a quarter of the union government's tax revenues is shared with the state governments.The tax reforms, initiated in 1991, have sought to rationalise the tax structure and increase compliance by taking steps in the following directions: Reducing the rates of individual and corporate income taxes, excises, customs and making it more progressive Reducing exemptions and concessions Simplification of laws and procedures Introduction of permanent account number (PAN) to track monetary transactions 21 of the 29 states introduced value added tax (VAT) on 1 April 2005 to replace the complex and multiple sales tax systemThe non-tax revenues of the central government come from fiscal services, interest receipts, public sector dividends, etc., while the non-tax revenues of the States are grants from the central government, interest receipts, dividends and income from general, economic and social services.Inter-state share in the union tax pool is decided by the recommendations of the Finance Commission to the president. Total tax receipts of Centre and State amount to approximately 18% of national GDP. This compares to a figure of 37–45% in the OECD.
The Finance minister of India usually presents the annual union budget in the parliament on the last working day of February. However, for the F.Y. 2017–18, this tradition had been changed. Now budget will be presented on the 1st day of February. The budget has to be passed by the Lok Sabha before it can come into effect on 1 April, the start of India's fiscal year. The Union budget is preceded by an economic survey which outlines the broad direction of the budget and the economic performance of the country for the outgoing financial yearIndia's non-development revenue expenditure had increased nearly five-fold in 2003–04 since 1990–91 and more than tenfold since 1985–1986. Interest payments are the single largest item of expenditure and accounted for more than 40% of the total non-development expenditure in the 2003–04 budget. Defense expenditure increased fourfold during the same period and has been increasing because of India's desire to project its military prowess beyond South Asia. In 2007, India's defence spending stood at US$26.5 billion.
Foreign relations of India National Portal of India National Social-media Portal Parliamentary democracy List of Government of India agencies Union Government ministries of India
Official Portal of the Indian Government
An ocean is a body of water that composes much of a planet's hydrosphere. On Earth, an ocean is one of the major conventional divisions of the World Ocean. These are, in descending order by area, the Pacific, Atlantic, Indian, Southern (Antarctic), and Arctic Oceans. The phrases "the ocean" or "the sea" used without specification refer to the interconnected body of salt water covering the majority of the Earth's surface. As a general term, "the ocean" is mostly interchangeable with "the sea" in American English, but not in British English. Strictly speaking, a sea is a body of water (generally a division of the world ocean) partly or fully enclosed by land.Saline seawater covers approximately 361,000,000 km2 (139,000,000 sq mi) and is customarily divided into several principal oceans and smaller seas, with the ocean covering approximately 71% of Earth's surface and 90% of the Earth's biosphere. The ocean contains 97% of Earth's water, and oceanographers have stated that less than 20% of the World Ocean has been mapped. The total volume is approximately 1.35 billion cubic kilometers (320 million cu mi) with an average depth of nearly 3,700 meters (12,100 ft).As the world ocean is the principal component of Earth's hydrosphere, it is integral to life, forms part of the carbon cycle, and influences climate and weather patterns. The World Ocean is the habitat of 230,000 known species, but because much of it is unexplored, the number of species that exist in the ocean is much larger, possibly over two million. The origin of Earth's oceans is unknown; oceans are thought to have formed in the Hadean eon and may have been the cause for the emergence of life. Extraterrestrial oceans may be composed of water or other elements and compounds. The only confirmed large stable bodies of extraterrestrial surface liquids are the lakes of Titan, although there is evidence for the existence of oceans elsewhere in the Solar System. Early in their geologic histories, Mars and Venus are theorized to have had large water oceans. The Mars ocean hypothesis suggests that nearly a third of the surface of Mars was once covered by water, and a runaway greenhouse effect may have boiled away the global ocean of Venus. Compounds such as salts and ammonia dissolved in water lower its freezing point so that water might exist in large quantities in extraterrestrial environments as brine or convecting ice. Unconfirmed oceans are speculated beneath the surface of many dwarf planets and natural satellites; notably, the ocean of the moon Europa is estimated to have over twice the water volume of Earth. The Solar System's giant planets are also thought to have liquid atmospheric layers of yet to be confirmed compositions. Oceans may also exist on exoplanets and exomoons, including surface oceans of liquid water within a circumstellar habitable zone. Ocean planets are a hypothetical type of planet with a surface completely covered with liquid.
The word ocean comes from the figure in classical antiquity, Oceanus (; Greek: Ὠκεανός Ōkeanós, pronounced [ɔːkeanós]), the elder of the Titans in classical Greek mythology, believed by the ancient Greeks and Romans to be the divine personification of the sea, an enormous river encircling the world. The concept of Ōkeanós has an Indo-European connection. Greek Ōkeanós has been compared to the Vedic epithet ā-śáyāna-, predicated of the dragon Vṛtra-, who captured the cows/rivers. Related to this notion, the Okeanos is represented with a dragon-tail on some early Greek vases.
Though generally described as several separate oceans, the global, interconnected body of salt water is sometimes referred to as the World Ocean or global ocean. The concept of a continuous body of water with relatively free interchange among its parts is of fundamental importance to oceanography.The major oceanic divisions – listed below in descending order of area and volume – are defined in part by the continents, various archipelagos, and other criteria. Oceans are fringed by smaller, adjoining bodies of water such as seas, gulfs, bays, bights, and straits.
The mid-ocean ridges of the world are connected and form a single global mid-oceanic ridge system that is part of every ocean and the longest mountain range in the world. The continuous mountain range is 65,000 km (40,000 mi) long (several times longer than the Andes, the longest continental mountain range).
Oceanographers divide the ocean into different vertical zones defined by physical and biological conditions. The pelagic zone includes all open ocean regions, and can be divided into further regions categorized by depth and light abundance. The photic zone includes the oceans from the surface to a depth of 200 m; it is the region where photosynthesis can occur and is, therefore, the most biodiverse. Because plants require photosynthesis, life found deeper than the photic zone must either rely on material sinking from above (see marine snow) or find another energy source. Hydrothermal vents are the primary source of energy in what is known as the aphotic zone (depths exceeding 200 m). The pelagic part of the photic zone is known as the epipelagic. The pelagic part of the aphotic zone can be further divided into vertical regions according to temperature. The mesopelagic is the uppermost region. Its lowermost boundary is at a thermocline of 12 °C (54 °F), which, in the tropics generally lies at 700–1,000 meters (2,300–3,300 ft). Next is the bathypelagic lying between 10 and 4 °C (50 and 39 °F), typically between 700–1,000 meters (2,300–3,300 ft) and 2,000–4,000 meters (6,600–13,100 ft), lying along the top of the abyssal plain is the abyssopelagic, whose lower boundary lies at about 6,000 meters (20,000 ft). The last zone includes the deep oceanic trench, and is known as the hadalpelagic. This lies between 6,000–11,000 meters (20,000–36,000 ft) and is the deepest oceanic zone. The benthic zones are aphotic and correspond to the three deepest zones of the deep-sea. The bathyal zone covers the continental slope down to about 4,000 meters (13,000 ft). The abyssal zone covers the abyssal plains between 4,000 and 6,000 m. Lastly, the hadal zone corresponds to the hadalpelagic zone, which is found in oceanic trenches. The pelagic zone can be further subdivided into two subregions: the neritic zone and the oceanic zone. The neritic zone encompasses the water mass directly above the continental shelves whereas the oceanic zone includes all the completely open water. In contrast, the littoral zone covers the region between low and high tide and represents the transitional area between marine and terrestrial conditions. It is also known as the intertidal zone because it is the area where tide level affects the conditions of the region. If a zone undergoes dramatic changes in temperature with depth, it contains a thermocline. The tropical thermocline is typically deeper than the thermocline at higher latitudes. Polar waters, which receive relatively little solar energy, are not stratified by temperature and generally lack a thermocline because surface water at polar latitudes are nearly as cold as water at greater depths. Below the thermocline, water is very cold, ranging from −1 °C to 3 °C. Because this deep and cold layer contains the bulk of ocean water, the average temperature of the world ocean is 3.9 °C. If a zone undergoes dramatic changes in salinity with depth, it contains a halocline. If a zone undergoes a strong, vertical chemistry gradient with depth, it contains a chemocline. The halocline often coincides with the thermocline, and the combination produces a pronounced pycnocline.
Oceanic maritime currents have different origins. Tidal currents are in phase with the tide, hence are quasiperiodic; they may form various knots in certain places, most notably around headlands. Non-periodic currents have for origin the waves, wind and different densities. The wind and waves create surface currents (designated as “drift currents”). These currents can decompose in one quasi-permanent current (which varies within the hourly scale) and one movement of Stokes drift under the effect of rapid waves movement (at the echelon of a couple of seconds).). The quasi-permanent current is accelerated by the breaking of waves, and in a lesser governing effect, by the friction of the wind on the surface.This acceleration of the current takes place in the direction of waves and dominant wind. Accordingly, when the sea depth increases, the rotation of the earth changes the direction of currents in proportion with the increase of depth, while friction lowers their speed. At a certain sea depth, the current changes direction and is seen inverted in the opposite direction with current speed becoming null: known as the Ekman spiral. The influence of these currents is mainly experienced at the mixed layer of the ocean surface, often from 400 to 800 meters of maximum depth. These currents can considerably alter, change and are dependent on the various yearly seasons. If the mixed layer is less thick (10 to 20 meters), the quasi-permanent current at the surface adopts an extreme oblique direction in relation to the direction of the wind, becoming virtually homogeneous, until the Thermocline.In the deep however, maritime currents are caused by the temperature gradients and the salinity between water density masses. In littoral zones, breaking waves are so intense and the depth measurement so low, that maritime currents reach often 1 to 2 knots.
Ocean currents greatly affect Earth's climate by transferring heat from the tropics to the polar regions. Transferring warm or cold air and precipitation to coastal regions, winds may carry them inland. Surface heat and freshwater fluxes create global density gradients that drive the thermohaline circulation part of large-scale ocean circulation. It plays an important role in supplying heat to the polar regions, and thus in sea ice regulation. Changes in the thermohaline circulation are thought to have significant impacts on Earth's energy budget. In so far as the thermohaline circulation governs the rate at which deep waters reach the surface, it may also significantly influence atmospheric carbon dioxide concentrations. For a discussion of the possibilities of changes to the thermohaline circulation under global warming, see shutdown of thermohaline circulation. The Antarctic Circumpolar Current encircles that continent, influencing the area's climate and connecting currents in several oceans. One of the most dramatic forms of weather occurs over the oceans: tropical cyclones (also called "typhoons" and "hurricanes" depending upon where the system forms).
The ocean has a significant effect on the biosphere. Oceanic evaporation, as a phase of the water cycle, is the source of most rainfall, and ocean temperatures determine climate and wind patterns that affect life on land. Life within the ocean evolved 3 billion years prior to life on land. Both the depth and the distance from shore strongly influence the biodiversity of the plants and animals present in each region.As it is thought that life evolved in the ocean, the diversity of life is immense, including: Bacteria : ubiquitous single-celled prokaryotes found throughout the world Archaea : prokaryotes distinct from bacteria, that inhabit many environments of the ocean, as well as many extreme environments Algae : algae is a "catch-all" term to include many photosynthetic, single-celled eukaryotes, such as green algae, diatoms, and dinoflagellates, but also multicellular algae, such as some red algae (including organisms like Pyropia, which is the source of the edible nori seaweed), and brown algae (including organisms like kelp). Plants : including sea grasses, or mangroves Fungi : many marine fungi with diverse roles are found in oceanic environments Animals : most animal phyla have species that inhabit the ocean, including many that are only found in marine environments such as sponges, Cnidaria (such as corals and jellyfish), comb jellies, Brachiopods, and Echinoderms (such as sea urchins and sea stars). Many other familiar animal groups primarily live in the ocean, including cephalopods (includes octopus and squid), crustaceans (includes lobsters, crabs, and shrimp), fish, sharks, cetaceans (includes whales, dolphins, and porpoises).In addition, many land animals have adapted to living a major part of their life on the oceans. For instance, seabirds are a diverse group of birds that have adapted to a life mainly on the oceans. They feed on marine animals and spend most of their lifetime on water, many only going on land for breeding. Other birds that have adapted to oceans as their living space are penguins, seagulls and pelicans. Seven species of turtles, the sea turtles, also spend most of their time in the oceans.
A zone of rapid salinity increase with depth is called a halocline. The temperature of maximum density of seawater decreases as its salt content increases. Freezing temperature of water decreases with salinity, and boiling temperature of water increases with salinity. Typical seawater freezes at around −2 °C at atmospheric pressure. If precipitation exceeds evaporation, as is the case in polar and temperate regions, salinity will be lower. If evaporation exceeds precipitation, as is the case in tropical regions, salinity will be higher. Thus, oceanic waters in polar regions have lower salinity content than oceanic waters in temperate and tropical regions.Salinity can be calculated using the chlorinity, which is a measure of the total mass of halogen ions (includes fluorine, chlorine, bromine, and iodine) in seawater. By international agreement, the following formula is used to determine salinity: Salinity (in ‰) = 1.80655 × Chlorinity (in ‰)The average chlorinity is about 19.2‰, and, thus, the average salinity is around 34.7‰
The motions of the ocean surface, known as undulations or waves, are the partial and alternate rising and falling of the ocean surface. The series of mechanical waves that propagate along the interface between water and air is called swell.
Although Earth is the only known planet with large stable bodies of liquid water on its surface and the only one in the Solar System, other celestial bodies are thought to have large oceans. In June 2020, NASA scientists reported that it's likely that exoplanets with oceans may be common in the Milky Way galaxy, based on mathematical modeling studies.
The gas giants, Jupiter and Saturn, are thought to lack surfaces and instead have a stratum of liquid hydrogen; however their planetary geology is not well understood. The possibility of the ice giants Uranus and Neptune having hot, highly compressed, supercritical water under their thick atmospheres has been hypothesised. Although their composition is still not fully understood, a 2006 study by Wiktorowicz and Ingersall ruled out the possibility of such a water "ocean" existing on Neptune, though some studies have suggested that exotic oceans of liquid diamond are possible.The Mars ocean hypothesis suggests that nearly a third of the surface of Mars was once covered by water, though the water on Mars is no longer oceanic (much of it residing in the ice caps). The possibility continues to be studied along with reasons for their apparent disappearance. Astronomers now think that Venus may have had liquid water and perhaps oceans for over 2 billion years.
A global layer of liquid water thick enough to decouple the crust from the mantle is thought to be present on the natural satellites Titan, Europa, Enceladus and, with less certainty, Callisto, Ganymede and Triton. A magma ocean is thought to be present on Io. Geysers have been found on Saturn's moon Enceladus, possibly originating from an ocean about 10 kilometers (6.2 mi) beneath the surface ice shell. Other icy moons may also have internal oceans, or may once have had internal oceans that have now frozen.Large bodies of liquid hydrocarbons are thought to be present on the surface of Titan, although they are not large enough to be considered oceans and are sometimes referred to as lakes or seas. The Cassini–Huygens space mission initially discovered only what appeared to be dry lakebeds and empty river channels, suggesting that Titan had lost what surface liquids it might have had. Later flybys of Titan provided radar and infrared images that showed a series of hydrocarbon lakes in the colder polar regions. Titan is thought to have a subsurface liquid-water ocean under the ice in addition to the hydrocarbon mix that forms atop its outer crust.
Ceres appears to be differentiated into a rocky core and icy mantle and may harbour a liquid-water ocean under its surface.Not enough is known of the larger trans-Neptunian objects to determine whether they are differentiated bodies capable of supporting oceans, although models of radioactive decay suggest that Pluto, Eris, Sedna, and Orcus have oceans beneath solid icy crusts approximately 100 to 180 km thick. In June 2020, astronomers reported evidence that the dwarf planet Pluto may have had a subsurface ocean, and consequently may have been habitable, when it was first formed.
Some planets and natural satellites outside the Solar System are likely to have oceans, including possible water ocean planets similar to Earth in the habitable zone or "liquid-water belt". The detection of oceans, even through the spectroscopy method, however is likely extremely difficult and inconclusive. Theoretical models have been used to predict with high probability that GJ 1214 b, detected by transit, is composed of exotic form of ice VII, making up 75% of its mass, making it an ocean planet. Other possible candidates are merely speculated based on their mass and position in the habitable zone include planet though little is actually known of their composition. Some scientists speculate Kepler-22b may be an "ocean-like" planet. Models have been proposed for Gliese 581 d that could include surface oceans. Gliese 436 b is speculated to have an ocean of "hot ice". Exomoons orbiting planets, particularly gas giants within their parent star's habitable zone may theoretically have surface oceans. Terrestrial planets will acquire water during their accretion, some of which will be buried in the magma ocean but most of it will go into a steam atmosphere, and when the atmosphere cools it will collapse on to the surface forming an ocean. There will also be outgassing of water from the mantle as the magma solidifies—this will happen even for planets with a low percentage of their mass composed of water, so "super-Earth exoplanets may be expected to commonly produce water oceans within tens to hundreds of millions of years of their last major accretionary impact."
Oceans, seas, lakes and other bodies of liquids can be composed of liquids other than water, for example the hydrocarbon lakes on Titan. The possibility of seas of nitrogen on Triton was also considered but ruled out. There is evidence that the icy surfaces of the moons Ganymede, Callisto, Europa, Titan and Enceladus are shells floating on oceans of very dense liquid water or water–ammonia. Earth is often called the ocean planet because it is 70% covered in water. Extrasolar terrestrial planets that are extremely close to their parent star will be tidally locked and so one half of the planet will be a magma ocean. It is also possible that terrestrial planets had magma oceans at some point during their formation as a result of giant impacts. Hot Neptunes close to their star could lose their atmospheres via hydrodynamic escape, leaving behind their cores with various liquids on the surface. Where there are suitable temperatures and pressures, volatile chemicals that might exist as liquids in abundant quantities on planets include ammonia, argon, carbon disulfide, ethane, hydrazine, hydrogen, hydrogen cyanide, hydrogen sulfide, methane, neon, nitrogen, nitric oxide, phosphine, silane, sulfuric acid, and water.Supercritical fluids, although not liquids, do share various properties with liquids. Underneath the thick atmospheres of the planets Uranus and Neptune, it is expected that these planets are composed of oceans of hot high-density fluid mixtures of water, ammonia and other volatiles. The gaseous outer layers of Jupiter and Saturn transition smoothly into oceans of supercritical hydrogen. The atmosphere of Venus is 96.5% carbon dioxide, which is a supercritical fluid at its surface.
Oceans at Curlie Smithsonian Ocean Portal NOAA – National Oceanic and Atmospheric Administration – Ocean Ocean :: Science Daily Ocean-bearing Planets: Looking For Extraterrestrial Life In All The Right Places Titan Likely To Have Huge Underground Ocean | Mind Blowing Science Origins of the oceans and continents". UN Atlas of the Oceans.
In psychological trait theory, the Big Five personality traits, also known as the five-factor model (FFM) and the OCEAN model, is a suggested taxonomy, or grouping, for personality traits, developed from the 1980s onwards. When factor analysis (a statistical technique) is applied to personality survey data, it reveals semantic associations: some words used to describe aspects of personality are often applied to the same person. For example, someone described as conscientious is more likely to be described as "always prepared" rather than "messy". These associations suggest five broad dimensions used in common language to describe the human personality and psyche.The theory identifies five factors: openness to experience (inventive/curious vs. consistent/cautious) conscientiousness (efficient/organized vs. extravagant/careless) extraversion (outgoing/energetic vs. solitary/reserved) agreeableness (friendly/compassionate vs. challenging/callous) neuroticism (sensitive/nervous vs. resilient/confident)The five factors are abbreviated in the acronyms OCEAN or CANOE. Beneath each proposed global factor, there are a number of correlated and more specific primary factors. For example, extraversion is typically associated with qualities such as gregariousness, assertiveness, excitement-seeking, warmth, activity, and positive emotions.Family life and upbringing will affect these traits. Twin studies and other research have shown that about half of the variation between individuals results from their genetic inheritance and half from their environment. Researchers have found conscientiousness, extraversion, openness to experience, and neuroticism to be relatively stable from childhood through adulthood.
The Big Five personality traits was the model to comprehend the relationship between personality and academic behaviors. This model was defined by several independent sets of researchers who used factor analysis of verbal descriptors of human behavior. These researchers began by studying relationships between a large number of verbal descriptors related to personality traits. They reduced the lists of these descriptors by 5–10 fold and then used factor analysis to group the remaining traits (using data mostly based upon people's estimations, in self-report questionnaire and peer ratings) in order to find the underlying factors of personality.The initial model was advanced by Ernest Tupes and Raymond Christal in 1961, but failed to reach an academic audience until the 1980s. In 1990, J.M. Digman advanced his five-factor model of personality, which Lewis Goldberg extended to the highest level of organization. These five overarching domains have been found to contain and subsume most known personality traits and are assumed to represent the basic structure behind all personality traits.At least four sets of researchers have worked independently within lexical hypothesis in personality theory for decades on this problem and have identified generally the same five factors: Tupes and Christal were first, followed by Goldberg at the Oregon Research Institute, Cattell at the University of Illinois, and Costa and McCrae. These four sets of researchers used somewhat different methods in finding the five traits, and thus each set of five factors has somewhat different names and definitions. However, all have been found to be highly inter-correlated and factor-analytically aligned. Studies indicate that the Big Five traits are not nearly as powerful in predicting and explaining actual behavior as are the more numerous facets or primary traits.Each of the Big Five personality traits contains two separate, but correlated, aspects reflecting a level of personality below the broad domains but above the many facet scales that are also part of the Big Five. The aspects are labeled as follows: Volatility and Withdrawal for Neuroticism; Enthusiasm and Assertiveness for Extraversion; Intellect and Openness for Openness to Experience; Industriousness and Orderliness for Conscientiousness; and Compassion and Politeness for Agreeableness. People who do not exhibit a clear predisposition to a single factor in each dimension above are considered adaptable, moderate and reasonable, yet they can also be perceived as unprincipled, inscrutable and calculating.
Openness to experience is a general appreciation for art, emotion, adventure, unusual ideas, imagination, curiosity, and variety of experience. People who are open to experience are intellectually curious, open to emotion, sensitive to beauty and willing to try new things. They tend to be, when compared to closed people, more creative and more aware of their feelings. They are also more likely to hold unconventional beliefs. High openness can be perceived as unpredictability or lack of focus, and more likely to engage in risky behavior or drug-taking. Moreover, individuals with high openness are said to pursue self-actualization specifically by seeking out intense, euphoric experiences. Conversely, those with low openness seek to gain fulfillment through perseverance and are characterized as pragmatic and data-driven—sometimes even perceived to be dogmatic and closed-minded. Some disagreement remains about how to interpret and contextualize the openness factor.
Conscientiousness is a tendency to display self-discipline, act dutifully, and strive for achievement against measures or outside expectations. It is related to the way in which people control, regulate, and direct their impulses. High conscientiousness is often perceived as being stubborn and focused. Low conscientiousness is associated with flexibility and spontaneity, but can also appear as sloppiness and lack of reliability. High scores on conscientiousness indicate a preference for planned rather than spontaneous behavior. The average level of conscientiousness rises among young adults and then declines among older adults.
Extraversion is characterized by breadth of activities (as opposed to depth), surgency from external activity/situations, and energy creation from external means. The trait is marked by pronounced engagement with the external world. Extraverts enjoy interacting with people, and are often perceived as full of energy. They tend to be enthusiastic, action-oriented individuals. They possess high group visibility, like to talk, and assert themselves. Extraverted people may appear more dominant in social settings, as opposed to introverted people in this setting.Introverts have lower social engagement and energy levels than extraverts. They tend to seem quiet, low-key, deliberate, and less involved in the social world. Their lack of social involvement should not be interpreted as shyness or depression; instead they are more independent of their social world than extraverts. Introverts need less stimulation, and more time alone than extraverts. This does not mean that they are unfriendly or antisocial; rather, they are reserved in social situations.Generally, people are a combination of extraversion and introversion, with personality psychologist Hans Eysenck suggesting a model by which individual neurological differences produce these traits.
The agreeableness trait reflects individual differences in general concern for social harmony. Agreeable individuals value getting along with others. They are generally considerate, kind, generous, trusting and trustworthy, helpful, and willing to compromise their interests with others. Agreeable people also have an optimistic view of human nature. Disagreeable individuals place self-interest above getting along with others. They are generally unconcerned with others' well-being, and are less likely to extend themselves for other people. Sometimes their skepticism about others' motives causes them to be suspicious, unfriendly, and uncooperative. Low agreeableness personalities are often competitive or challenging people, which can be seen as argumentative or untrustworthy.Because agreeableness is a social trait, research has shown that one's agreeableness positively correlates with the quality of relationships with one's team members. Agreeableness also positively predicts transformational leadership skills. In a study conducted among 169 participants in leadership positions in a variety of professions, individuals were asked to take a personality test and have two evaluations completed by directly supervised subordinates. Leaders with high levels of agreeableness were more likely to be considered transformational rather than transactional. Although the relationship was not strong (r=0.32, β=0.28, p<0.01), it was the strongest of the Big Five traits. However, the same study showed no predictive power of leadership effectiveness as evaluated by the leader's direct supervisor.Conversely, agreeableness has been found to be negatively related to transactional leadership in the military. A study of Asian military units showed leaders with a high level of agreeableness to be more likely to receive a low rating for transformational leadership skills. Therefore, with further research, organizations may be able to determine an individual's potential for performance based on their personality traits. For instance, in their journal article "Which Personality Attributes Are Most Important in the Workplace?" Paul Sackett and Philip Walmsley claim that conscientiousness and agreeableness are “important to success across many different jobs."
Neuroticism is the tendency to experience negative emotions, such as anger, anxiety, or depression. It is sometimes called emotional instability, or is reversed and referred to as emotional stability. According to Hans Eysenck's (1967) theory of personality, neuroticism is interlinked with low tolerance for stress or aversive stimuli. Neuroticism is a classic temperament trait that has been studied in temperament research for decades, before it was adapted by the FFM. Those who score high in neuroticism are emotionally reactive and vulnerable to stress. They are more likely to interpret ordinary situations as threatening. They can perceive minor frustrations as hopelessly difficult. They also tend to be flippant in the way they express emotions. Their negative emotional reactions tend to persist for unusually long periods of time, which means they are often in a bad mood. For instance, neuroticism is connected to a pessimistic approach toward work, to certainty that work impedes personal relationships, and to higher levels of anxiety from the pressures at work. Furthermore, those who score high on neuroticism may display more skin-conductance reactivity than those who score low on neuroticism. These problems in emotional regulation can diminish the ability of a person scoring high on neuroticism to think clearly, make decisions, and cope effectively with stress. Lacking contentment in one's life achievements can correlate with high neuroticism scores and increase one's likelihood of falling into clinical depression. Moreover, individuals high in neuroticism tend to experience more negative life events, but neuroticism also changes in response to positive and negative life experiences. Also, individuals with higher levels of neuroticism tend to have worse psychological well being.At the other end of the scale, individuals who score low in neuroticism are less easily upset and are less emotionally reactive. They tend to be calm, emotionally stable, and free from persistent negative feelings. Freedom from negative feelings does not mean that low-scorers experience a lot of positive feelings.Neuroticism is similar but not identical to being neurotic in the Freudian sense (i.e., neurosis.) Some psychologists prefer to call neuroticism by the term emotional instability to differentiate it from the term neurotic in a career test.
Historically preceding The Big Five personality traits (B5) or the Five Factors Model (FFM), was Hippocrates's four types of temperament— sanguine, phlegmatic, choleric, and melancholic. The sanguine type is most closely related to emotional stability and extraversion, the phlegmatic type is also stable but introverted, the choleric type is unstable and extraverted, and the melancholic type is unstable and introverted.In 1884, Sir Francis Galton was the first person who is known to have investigated the hypothesis that it is possible to derive a comprehensive taxonomy of human personality traits by sampling language: the lexical hypothesis.In 1936, Gordon Allport and S. Odbert put Sir Francis Galton's hypothesis into practice by extracting 4,504 adjectives which they believed were descriptive of observable and relatively permanent traits from the dictionaries at that time. In 1940, Raymond Cattell retained the adjectives, and eliminated synonyms to reduce the total to 171. He constructed a self-report instrument for the clusters of personality traits he found from the adjectives, which he called the Sixteen Personality Factor Questionnaire. In 1949, the first systematic multivariate research of personality was conducted by Joy P. Guilford. Guilford analyzed ten factors of personality, which he measured by the Guilford-Zimmerman Temperament Survey. These scales included general activity (energy vs inactivity); restraint (seriousness vs impulsiveness); ascendance (social boldness vs submissiveness); sociability (social interest vs shyness); emotional stability (evenness vs fluctuation of mood); objectivity (thick-skinned vs hypersensitive); friendliness (agreeableness vs belligerence); thoughtfulness (reflective vs disconnected), personal relations (tolerance vs hypercritical); masculinity (hard-boiled vs sympathetic). These overlapping scales were later further analyzed by Guilford et al., and condense into three dimensions: social activity (general activity, ascendence, sociability), introversion-extraversion (restraint, thoughtfulness), and emotional health (emotional stability, objectivity, friendliness, personal relations).Based on a subset of only 20 of the 36 dimensions that Cattell had originally discovered, Ernest Tupes and Raymond Christal (1961) claimed to have found just five broad factors which they labeled: "surgency", "agreeableness", "dependability", "emotional stability", and "culture". Warren Norman subsequently relabeled "dependability" as "conscientiousness".
During the late 1960s to 1970s, the changing zeitgeist made publication of personality research difficult. In his 1968 book Personality and Assessment, Walter Mischel asserted that personality instruments could not predict behavior with a correlation of more than 0.3. Social psychologists like Mischel argued that attitudes and behavior were not stable, but varied with the situation. Predicting behavior from personality instruments was claimed to be impossible.
The paradigm shift back to acceptance of the five-factor model came in the early 1980s. In a 1980 symposium in Honolulu, four prominent researchers, Lewis Goldberg, Naomi Takemoto-Chock, Andrew Comrey, and John M. Digman, reviewed the available personality instruments of the day. This event was followed by widespread acceptance of the five-factor model among personality researchers during the 1980s.By 1983, experiments had demonstrated that the predictions of personality models correlated better with real-life behavior under stressful emotional conditions, as opposed to typical survey administration under neutral emotional conditions.Peter Saville and his team included the five-factor "Pentagon" model with the original OPQ in 1984. Pentagon was closely followed by the NEO five-factor personality inventory, published by Costa and McCrae in 1985. However, the methodology employed in constructing the NEO instrument has been subject to critical scrutiny (see section below).Emerging methodologies increasing confirmed personality theories during the 1980s. Though generally failing to predict single instances of behavior, researchers found that they could predict patterns of behavior by aggregating large numbers of observations. As a result, correlations between personality and behavior increased substantially, and it was clear that "personality" did in fact exist.Personality and social psychologists now generally agree that both personal and situational variables are needed to account for human behavior. Trait theories amassed favorable evidence, and there was a resurgence of interest in this area. In the 1980s, Lewis Goldberg started his own lexical project, again emphasizing five broad factors which he later labeled the "Big Five". Colin G. DeYoung et al. (2016) tested how these 25 facets could be integrated with the 10-factor structure of traits within the Big Five. The developers mainly researched the Big Five model and how the five broad factors are compatible with the 25 scales of the Personality Inventory (PID-5) for the DSM-5. DeYoung et al. considers the PID-5 to measure facet-level traits. Because the Big Five factors are broader than the 25 scales of the PID-5, there is disagreement in personality psychology relating to the number of factors within the Big Five. According to DeYoung et al. (2016), "the number of valid facets might be limited only by the number of traits that can be shown to have discriminant validity."The FFM-associated test was used by Cambridge Analytica, and was part of the "psychographic profiling" controversy during the 2016 US presidential election.
There of course are factors that influence a personality and these are called the determinants of personality. These factors determine the traits which a person develops in the course of development from a child.
There are debates between researchers of temperament and researchers of personality as to whether or not biologically-based differences define a concept of temperament or a part of personality. The presence of such differences in pre-cultural individuals (such as animals or young infants) suggests that they belong to temperament since personality is a socio-cultural concept. For this reason developmental psychologists generally interpret individual differences in children as an expression of temperament rather than personality. Some researchers argue that temperaments and personality traits are age-specific manifestations of virtually the same latent qualities. Some believe that early childhood temperaments may become adolescent and adult personality traits as individuals' basic genetic characteristics actively, reactively, and passively interact with their changing environments.Researchers of adult temperament point out that, similarly to sex, age and mental illness, temperament is based on biochemical systems whereas personality is a product of socialization of an individual possessing these four types of features. Temperament interacts with social-cultural factors, but still cannot be controlled or easily changed by these factors. Therefore, it is suggested that temperament should be kept as an independent concept for further studies and not be conflated with personality. Moreover, temperament refers to dynamical features of behavior (energetic, tempo, sensitivity and emotionality-related), whereas personality is to be considered a psycho-social construct comprising the content characteristics of human behavior (such as values, attitudes, habits, preferences, personal history, self-image). Temperament researchers point out that the lack of attention to extant temperament research by the developers of the Big Five model lead to an overlap between its dimensions and dimensions described in multiple temperament models much earlier. For example, neuroticism reflects the traditional temperament dimension of emotionality, extraversion the temperament dimension of "energy" or "activity", and openness to experience the temperament dimension of sensation-seeking.
Genetically informative research, including twin studies, suggest that heritability and environmental factors both influence all five factors to the same degree. Among four recent twin studies, the mean percentage for heritability was calculated for each personality and it was concluded that heritability influenced the five factors broadly. The self-report measures were as follows: openness to experience was estimated to have a 57% genetic influence, extraversion 54%, conscientiousness 49%, neuroticism 48%, and agreeableness 42%.
The Big Five personality traits have been assessed in some non-human species but methodology is debatable. In one series of studies, human ratings of chimpanzees using the Hominoid Personality Questionnaire, revealed factors of extraversion, conscientiousness and agreeableness – as well as an additional factor of dominance – across hundreds of chimpanzees in zoological parks, a large naturalistic sanctuary, and a research laboratory. Neuroticism and openness factors were found in an original zoo sample, but were not replicated in a new zoo sample or in other settings (perhaps reflecting the design of the CPQ). A study review found that markers for the three dimensions extraversion, neuroticism, and agreeableness were found most consistently across different species, followed by openness; only chimpanzees showed markers for conscientious behavior.
Research on the Big Five, and personality in general, has focused primarily on individual differences in adulthood, rather than in childhood and adolescence, and often include temperament traits. Recently, there has been growing recognition of the need to study child and adolescent personality trait development in order to understand how traits develop and change throughout the lifespan.Recent studies have begun to explore the developmental origins and trajectories of the Big Five among children and adolescents, especially those that relate to temperament. Many researchers have sought to distinguish between personality and temperament. Temperament often refers to early behavioral and affective characteristics that are thought to be driven primarily by genes. Models of temperament often include four trait dimensions: surgency/ sociability, negative emotionality, persistence/effortful control, and activity level. Some of these differences in temperament are evident at, if not before, birth. For example, both parents and researchers recognize that some newborn infants are peaceful and easily soothed while others are comparatively fussy and hard to calm. Unlike temperament, however, many researchers view the development of personality as gradually occurring throughout childhood. Contrary to some researchers who question whether children have stable personality traits, Big Five or otherwise, most researchers contend that there are significant psychological differences between children that are associated with relatively stable, distinct, and salient behavior patterns.The structure, manifestations, and development of the Big Five in childhood and adolescence have been studied using a variety of methods, including parent- and teacher-ratings, preadolescent and adolescent self- and peer-ratings, and observations of parent-child interactions. Results from these studies support the relative stability of personality traits across the human lifespan, at least from preschool age through adulthood. More specifically, research suggests that four of the Big Five –namely Extraversion, Neuroticism, Conscientiousness, and Agreeableness- reliably describe personality differences in childhood, adolescence, and adulthood. However, some evidence suggests that Openness may not be a fundamental, stable part of childhood personality. Although some researchers have found that Openness in children and adolescents relates to attributes such as creativity, curiosity, imagination, and intellect, many researchers have failed to find distinct individual differences in Openness in childhood and early adolescence. Potentially, Openness may (a) manifest in unique, currently unknown ways in childhood or (b) may only manifest as children develop socially and cognitively. Other studies have found evidence for all of the Big Five traits in childhood and adolescence as well as two other child-specific traits: Irritability and Activity. Despite these specific differences, the majority of findings suggest that personality traits –particularly Extraversion, Neuroticism, Conscientiousness, and Agreeableness- are evident in childhood and adolescence and are associated with distinct social-emotional patterns of behavior that are largely consistent with adult manifestations of those same personality traits. Some researchers have proposed the youth personality trait is best described by six trait dimensions: neuroticism, extraversion, openness to experience, agreeableness, conscientiousness, and activity. Despite some preliminary evidence for this “Little Six” model, research in this area has been delayed by a lack of available measures. Previous research has found evidence that most adults become more agreeable, conscientious, and less neurotic as they age. This has been referred to as the maturation effect. Many researchers have sought to investigate how trends in adult personality development compare to trends in youth personality development. Two main population-level indices have been important in this area of research: rank-order consistency and mean-level consistency. Rank-order consistency indicates the relative placement of individuals within a group. Mean-level consistency indicates whether groups increase or decrease on certain traits throughout the lifetime.Findings from these studies indicate that, consistent with adult personality trends, youth personality becomes increasingly more stable in terms of rank-order throughout childhood. Unlike adult personality research, which indicates that people become agreeable, conscientious, and emotionally stable with age, some findings in youth personality research have indicated that mean-levels of agreeableness, conscientiousness, and openness to experience decline from late childhood to late adolescence. The disruption hypothesis, which proposes that biological, social, and psychological changes experienced during youth result in temporary dips in maturity, has been proposed to explain these findings.
In Big Five studies, extraversion has been associated with surgency. Children with high Extraversion are energetic, talkative, social, and dominant with children and adults; whereas, children with low Extraversion tend to be quiet, calm, inhibited, and submissive to other children and adults. Individual differences in Extraversion first manifest in infancy as varying levels of positive emotionality. These differences in turn predict social and physical activity during later childhood and may represent, or be associated with, the behavioral activation system. In children, Extraversion/Positive Emotionality includes four sub-traits: three traits that are similar to the previously described traits of temperament – activity, sociability, shyness, and the trait of dominance. Activity: Similarly to findings in temperament research, children with high activity tend to have high energy levels and more intense and frequent motor activity compared to their peers. Salient differences in activity reliably manifest in infancy, persist through adolescence, and fade as motor activity decreases in adulthood or potentially develops into talkativeness. Dominance: Children with high dominance tend to influence the behavior of others, particularly their peers, to obtain desirable rewards or outcomes. Such children are generally skilled at organizing activities and games and deceiving others by controlling their nonverbal behavior. Shyness: Children with high shyness are generally socially withdrawn, nervous, and inhibited around strangers. In time, such children may become fearful even around "known others", especially if their peers reject them. Similar pattern was described in temperament longitudinal studies of shyness Sociability: Children with high sociability generally prefer to be with others rather than alone. During middle childhood, the distinction between low sociability and high shyness becomes more pronounced, particularly as children gain greater control over how and where they spend their time.
Many studies of longitudinal data, which correlate people's test scores over time, and cross-sectional data, which compare personality levels across different age groups, show a high degree of stability in personality traits during adulthood, especially Neuroticism trait that is often regarded as a temperament trait similarly to longitudinal research in temperament for the same traits. It is shown that the personality stabilizes for working-age individuals within about four years after starting working. There is also little evidence that adverse life events can have any significant impact on the personality of individuals. More recent research and meta-analyses of previous studies, however, indicate that change occurs in all five traits at various points in the lifespan. The new research shows evidence for a maturation effect. On average, levels of agreeableness and conscientiousness typically increase with time, whereas extraversion, neuroticism, and openness tend to decrease. Research has also demonstrated that changes in Big Five personality traits depend on the individual's current stage of development. For example, levels of agreeableness and conscientiousness demonstrate a negative trend during childhood and early adolescence before trending upwards during late adolescence and into adulthood. In addition to these group effects, there are individual differences: different people demonstrate unique patterns of change at all stages of life.In addition, some research (Fleeson, 2001) suggests that the Big Five should not be conceived of as dichotomies (such as extraversion vs. introversion) but as continua. Each individual has the capacity to move along each dimension as circumstances (social or temporal) change. He is or she is therefore not simply on one end of each trait dichotomy but is a blend of both, exhibiting some characteristics more often than others:Research regarding personality with growing age has suggested that as individuals enter their elder years (79–86), those with lower IQ see a raise in extraversion, but a decline in conscientiousness and physical well being.Research by Cobb-Clark and Schurer indicates that personality traits are generally stable among adult workers. The research done on personality also mirrors previous results on locus of control.
While personality is mostly stable in adulthood, some diseases can alter personality. Gradual impairment of memory is the hallmark feature of Alzheimer's disease, but changes in personality also commonly occur. A review of personality change in Alzheimer's disease found a characteristic pattern of personality change in patients with Alzheimer's disease: a large decrease in Conscientiousness of two to three standard deviations, a decrease in Extraversion of one to two standard deviations, a reduction in Agreeableness of less than one standard deviation, and an increase in Neuroticism of between one and two standard deviations.
Cross-cultural research has shown some patterns of gender differences on responses to the NEO-PI-R and the Big Five Inventory. For example, women consistently report higher Neuroticism, Agreeableness, warmth (an extraversion facet) and openness to feelings, and men often report higher assertiveness (a facet of extraversion) and openness to ideas as assessed by the NEO-PI-R.A study of gender differences in 55 nations using the Big Five Inventory found that women tended to be somewhat higher than men in neuroticism, extraversion, agreeableness, and conscientiousness. The difference in neuroticism was the most prominent and consistent, with significant differences found in 49 of the 55 nations surveyed. Gender differences in personality traits are largest in prosperous, healthy, and more gender-egalitarian cultures. A plausible explanation for this is that acts by women in individualistic, egalitarian countries are more likely to be attributed to their personality, rather than being attributed to ascribed gender roles within collectivist, traditional countries. Differences in the magnitude of sex differences between more or less developed world regions were due to differences between men, not women, in these respective regions. That is, men in highly developed world regions were less neurotic, extraverted, conscientious and agreeable compared to men in less developed world regions. Women, on the other hand tended not to differ in personality traits across regions. The authors of this study speculated that resource-poor environments (that is, countries with low levels of development) may inhibit the development of gender differences, whereas resource-rich environments facilitate them. This may be because males require more resources than females in order to reach their full developmental potential. The authors also argued that due to different evolutionary pressures, men may have evolved to be more risk taking and socially dominant, whereas women evolved to be more cautious and nurturing. Ancient hunter-gatherer societies may have been more egalitarian than later agriculturally oriented societies. Hence, the development of gender inequalities may have acted to constrain the development of gender differences in personality that originally evolved in hunter-gatherer societies. As modern societies have become more egalitarian, again, it may be that innate sex differences are no longer constrained and hence manifest more fully than in less-developed cultures. Currently, this hypothesis remains untested, as gender differences in modern societies have not been compared with those in hunter-gatherer societies.
Frank Sulloway argues that firstborns are more conscientious, more socially dominant, less agreeable, and less open to new ideas compared to laterborns. Large-scale studies using random samples and self-report personality tests, however, have found milder effects than Sulloway claimed, or no significant effects of birth order on personality. A study using the Project Talent data, which is a large-scale representative survey of American high-schoolers, with 272,003 eligible targets, found statistically significant but very small effects (the average absolute correlation between birth order and personality was .02) of birth order on personality, such that first borns were slightly more conscientious, dominant, and agreeable, while also being less neurotic and less sociable. Parental SES and participant gender had much larger correlations with personality. In 2002, the journal of psychology posted a Big Five Personality Trait Difference; Researchers explored relationship between the five factor model and the Universal-Diverse Orientation (UDO) in counselor trainees. (Thompson, R., Brossart, D., and Mivielle, A., 2002) UDO is known as one social attitude that produces a strong awareness and/or acceptance towards the similarities and differences amongst individuals. (Miville, M., Romas, J., Johnson, J., and Lon, R. 2002) The study has shown the counselor trainees that are more open to the idea of creative expression (a facet of Openness to Experience, Openness to Aesthetics) amongst individuals are more likely to work with a diverse group of clients, and feel comfortable in their role.
The Big Five have been pursued in a variety of languages and cultures, such as German, Chinese, and Indian. For example, Thompson has claimed to find the Big Five structure across several cultures using an international English language scale. Cheung, van de Vijver, and Leong (2011) suggest, however, that the Openness factor is particularly unsupported in Asian countries and that a different fifth factor is identified.Recent work has found relationships between Geert Hofstede's cultural factors, Individualism, Power Distance, Masculinity, and Uncertainty Avoidance, with the average Big Five scores in a country. For instance, the degree to which a country values individualism correlates with its average extraversion, whereas people living in cultures which are accepting of large inequalities in their power structures tend to score somewhat higher on conscientiousness. Personality differences around the world might even have contributed to the emergence of different political systems. A recent study has found that countries' average personality trait levels are correlated with their political systems: countries with higher average trait Openness tended to have more democratic institutions, an association that held even after factoring out other relevant influences such as economic development.Attempts to replicate the Big Five in other countries with local dictionaries have succeeded in some countries but not in others. Apparently, for instance, Hungarians do not appear to have a single agreeableness factor. Other researchers have found evidence for agreeableness but not for other factors. It is important to recognize that individual differences in traits are relevant in a specific cultural context, and that the traits do not have their effects outside of that context.
As of 2002, there were over fifty published studies relating the FFM to personality disorders. Since that time, quite a number of additional studies have expanded on this research base and provided further empirical support for understanding the DSM personality disorders in terms of the FFM domains.In her review of the personality disorder literature published in 2007, Lee Anna Clark asserted that "the five-factor model of personality is widely accepted as representing the higher-order structure of both normal and abnormal personality traits". However, other researchers disagree that this model is widely accepted (see the section Critique below) and suggest that it simply replicates early temperament research. Noticeably, FFM publications never compare their findings to temperament models even though temperament and mental disorders (especially personality disorders) are thought to be based on the same neurotransmitter imbalances, just to varying degrees.The five-factor model was claimed to significantly predict all ten personality disorder symptoms and outperform the Minnesota Multiphasic Personality Inventory (MMPI) in the prediction of borderline, avoidant, and dependent personality disorder symptoms. However, most predictions related to an increase in Neuroticism and a decrease in Agreeableness, and therefore did not differentiate between the disorders very well.
Five major models have been posed to explain the nature of the relationship between personality and mental illness. There is currently no single "best model", as each of them has received at least some empirical support. It is also important to note that these models are not mutually exclusive – more than one may be operating for a particular individual and various mental disorders may be explained by different models. The Vulnerability/Risk Model: According to this model, personality contributes to the onset or etiology of various common mental disorders. In other words, pre-existing personality traits either cause the development of CMDs directly or enhance the impact of causal risk factors. There is strong support for neuroticism being a robust vulnerability factor. The Pathoplasty Model: This model proposes that premorbid personality traits impact the expression, course, severity, and/or treatment response of a mental disorder. An example of this relationship would be a heightened likelihood of committing suicide for a depressed individual who also has low levels of constraint. The Common Cause Model: According to the common cause model, personality traits are predictive of CMDs because personality and psychopathology have shared genetic and environmental determinants which result in non-causal associations between the two constructs. The Spectrum Model: This model proposes that associations between personality and psychopathology are found because these two constructs both occupy a single domain or spectrum and psychopathology is simply a display of the extremes of normal personality function. Support for this model is provided by an issue of criterion overlap. For instance, two of the primary facet scales of neuroticism in the NEO-PI-R are "depression" and "anxiety". Thus the fact that diagnostic criteria for depression, anxiety, and neuroticism assess the same content increases the correlations between these domains. The Scar Model: According to the scar model, episodes of a mental disorder 'scar' an individual's personality, changing it in significant ways from premorbid functioning. An example of a scar effect would be a decrease in openness to experience following an episode of PTSD.
To examine how the Big Five personality traits are related to subjective health outcomes (positive and negative mood, physical symptoms, and general health concern) and objective health conditions (chronic illness, serious illness, and physical injuries), a study, conducted by Jasna Hudek-Knezevic and Igor Kardum, from a sample of 822 healthy volunteers (438 women and 384 men). As a result, out of the Big Five personality traits, neuroticism was found most related to worse subjective health outcomes and optimistic control to better subjective health outcomes. When relating to objective health conditions, connections drawn were presented weak, except for neuroticism significantly predicted chronic illness, whereas optimistic control was more closely related to physical injuries caused by accident.Being highly conscientious may add as much as five years to one's life. The Big Five personality traits also predict positive health outcomes. In an elderly Japanese sample, conscientiousness, extraversion, and openness were related to lower risk of mortality.Higher conscientiousness is associated with lower obesity risk. In already obese individuals, higher conscientiousness is associated with a higher likelihood of becoming non-obese over a 5-year period.
Personality plays an important role in academic achievement. A study of 308 undergraduates who completed the Five Factor Inventory Processes and reported their GPA suggested that conscientiousness and agreeableness have a positive relationship with all types of learning styles (synthesis-analysis, methodical study, fact retention, and elaborative processing), whereas neuroticism shows an inverse relationship. Moreover, extraversion and openness were proportional to elaborative processing. The Big Five personality traits accounted for 14% of the variance in GPA, suggesting that personality traits make some contributions to academic performance. Furthermore, reflective learning styles (synthesis-analysis and elaborative processing) were able to mediate the relationship between openness and GPA. These results indicate that intellectual curiosity significantly enhances academic performance if students combine their scholarly interest with thoughtful information processing.A recent study of Israeli high-school students found that those in the gifted program systematically scored higher on openness and lower on neuroticism than those not in the gifted program. While not a measure of the Big Five, gifted students also reported less state anxiety than students not in the gifted program. Specific Big Five personality traits predict learning styles in addition to academic success. GPA and exam performance are both predicted by conscientiousness neuroticism is negatively related to academic success openness predicts utilizing synthesis-analysis and elaborative-processing learning styles neuroticism negatively correlates with learning styles in general openness and extraversion both predict all four learning styles.Studies conducted on college students have concluded that hope, which is linked to agreeableness, has a positive effect on psychological well being. Individuals high in neurotic tendencies are less likely to display hopeful tendencies and are negatively associated with well-being. Personality can sometimes be flexible and measuring the big five personality for individuals as they enter certain stages of life may predict their educational identity. Recent studies have suggested the likelihood of an individual's personality affecting their educational identity.
Learning styles have been described as "enduring ways of thinking and processing information".In 2008, the Association for Psychological Science (APS) commissioned a report whose conclusion indicates that no significant evidence exists to make the conclusion that learning-style assessments should be included in the education system. Thus it is premature, at best, to conclude that the evidence linking the Big Five to "learning styles", or "learning styles" to learning itself, is valid. However, the APS also suggested in their report that all existing learning styles have not been exhausted and that there could exist learning styles that have the potential to be worthy of being included in educational practices. There are studies that conclude that personality and thinking styles may be intertwined in ways that link thinking styles to the Big Five personality traits. There is no general consensus on the number or specifications of particular learning styles, but there have been many different proposals. As one example, Schmeck, Ribich, and Ramanaiah (1997) defined four types of learning styles: synthesis analysis methodical study fact retention elaborative processingWhen all four facets are implicated within the classroom, they will each likely improve academic achievement. This model asserts that students develop either agentic/shallow processing or reflective/deep processing. Deep processors are more often than not found to be more conscientious, intellectually open, and extraverted when compared to shallow processors. Deep processing is associated with appropriate study methods (methodical study) and a stronger ability to analyze information (synthesis analysis), whereas shallow processors prefer structured fact retention learning styles and are better suited for elaborative processing. The main functions of these four specific learning styles are as follows: Openness has been linked to learning styles that often lead to academic success and higher grades like synthesis analysis and methodical study. Because conscientiousness and openness have been shown to predict all four learning styles, it suggests that individuals who possess characteristics like discipline, determination, and curiosity are more likely to engage in all of the above learning styles.According to the research carried out by Komarraju, Karau, Schmeck & Avdic (2011), conscientiousness and agreeableness are positively related with all four learning styles, whereas neuroticism was negatively related with those four. Furthermore, extraversion and openness were only positively related to elaborative processing, and openness itself correlated with higher academic achievement.In addition, a previous study by Mikael Jensen has shown relationships between The Big Five personality traits, learning, and academic achievement. According to psychologist Jensen, all personality traits, except neuroticism, are associated with learning goals and motivation. Openness and conscientiousness influence individuals to learn to a high degree unrecognized, while extraversion and agreeableness have similar effects. Conscientiousness and neuroticism also influence individuals to perform well in front of others for a sense of credit and reward, while agreeableness forces individuals to avoid this strategy of learning. As a result of Jensen's study, it is likely that individuals who score high on the agreeableness trait will learn just to perform well in front of others.Besides openness, all Big Five personality traits helped predict the educational identity of students. Based on these findings, scientists are beginning to see that there might be a large influence of the Big Five traits on academic motivation that then leads to predicting a student's academic performance.Some authors suggested that Big Five personality traits combined with learning styles can help predict some variations in the academic performance and the academic motivation of an individual which can then influence their academic achievements. This may be seen because individual differences in personality represent stable approaches to information processing. For instance, conscientiousness has consistently emerged as a stable predictor of success in exam performance, largely because conscientious students experience fewer study delays. The reason conscientiousness shows a positive association with the four learning styles is because students with high levels of conscientiousness develop focused learning strategies and appear to be more disciplined and achievement-oriented.
It is believed that the Big Five traits are predictors of future performance outcomes. Job outcome measures include job and training proficiency and personnel data. However, research demonstrating such prediction has been criticized, in part because of the apparently low correlation coefficients characterizing the relationship between personality and job performance. In a 2007 article co-authored by six current or former editors of psychological journals, Dr. Kevin Murphy, Professor of Psychology at Pennsylvania State University and Editor of the Journal of Applied Psychology (1996–2002), states: The problem with personality tests is ... that the validity of personality measures as predictors of job performance is often disappointingly low. The argument for using personality tests to predict performance does not strike me as convincing in the first place. Such criticisms were put forward by Walter Mischel, whose publication caused a two-decades' long crisis in personality psychometrics. However, later work demonstrated (1) that the correlations obtained by psychometric personality researchers were actually very respectable by comparative standards, and (2) that the economic value of even incremental increases in prediction accuracy was exceptionally large, given the vast difference in performance by those who occupy complex job positions.There have been studies that link national innovation to openness to experience and conscientiousness. Those who express these traits have showed leadership and beneficial ideas towards the country of origin.Some businesses, organizations, and interviewers assess individuals based on the Big Five personality traits. Research has suggested that individuals who are considered leaders typically exhibit lower amounts of neurotic traits, maintain higher levels of openness (envisioning success), balanced levels of conscientiousness (well-organized), and balanced levels of extraversion (outgoing, but not excessive). Further studies have linked professional burnout to neuroticism, and extraversion to enduring positive work experience. When it comes to making money, research has suggested that those who are high in agreeableness (especially men) are not as successful in accumulating income.Some research suggests that vocational outcomes are correlated to Big Five personality traits. Conscientiousness predicts job performance in general. Conscientiousness is considered as top-ranked in overall job performance, research further categorized the Big 5 behaviors into 3 perspectives: task performance, organizational citizenship behavior, and counterproductive work behavior. Task performance is the set of activity that a worker is hired to complete, and results showed that Extraversion ranked second after the Conscientiousness, with Emotional Stability tied with Agreeableness ranked third. For organizational citizenship behavior, relatively less tied to the specific task core but benefits an organization by contributing to its social and psychological environment, Agreeableness and Emotional Stability ranked second and third. Lastly, Agreeableness tied with Conscientiousness as top ranked for Counterproductive work behavior, which refers to intentional behavior that is counter to the legitimate interests of the organization or its members.In addition, research has demonstrated that agreeableness is negatively related to salary. Those high in agreeableness make less, on average, than those low in the same trait. Neuroticism is also negatively related to salary while conscientiousness and extraversion are positive predictors of salary. Occupational self-efficacy has also been shown to be positively correlated with conscientiousness and negatively correlated with neuroticism. Significant predictors of career-advancement goals are: extraversion, conscientiousness, and agreeableness. Some research has also suggested that the Conscientiousness of a supervisor is positively associated with an employee's perception of abusive supervision. While others have suggested that those with low agreeableness and high neuroticism are traits more related to abusive supervision.A 2019 study of Canadian adults found conscientiousness to be positively associated with wages, while agreeableness, extraversion, and neuroticism were negatively associated with wages. In the United States, by contrast, no negative correlation between extraversion and wages has been found. Also, the magnitudes found for agreeableness and conscientiousness in this study were higher for women than for men (i.e. there was a higher negative penalty for greater agreeableness in women, as well as a higher positive reward for greater conscientiousness).Research designed to investigate the individual effects of Big Five personality traits on work performance via worker completed surveys and supervisor ratings of work performance has implicated individual traits in several different work roles performances. A "work role" is defined as the responsibilities an individual has while they are working. Nine work roles have been identified, which can be classified in three broader categories: proficiency (the ability of a worker to effectively perform their work duties), adaptivity (a workers ability to change working strategies in response to changing work environments), and proactivity (extent to which a worker will spontaneously put forth effort to change the work environment). These three categories of behavior can then be directed towards three different levels: either the individual, team, or organizational level leading to the nine different work role performance possibilities. Openness is positively related to proactivity at the individual and the organizational levels and is negatively related to team and organizational proficiency. These effects were found to be completely independent of one another. Agreeableness is negatively related to individual task proactivity. Extraversion is negatively related to individual task proficiency. Conscientiousness is positively related to all forms of work role performance. Neuroticism is negatively related to all forms of work role performance.Two theories have been integrated in an attempt to account for these differences in work role performance. Trait activation theory posits that within a person trait levels predict future behavior, that trait levels differ between people, and that work-related cues activate traits which leads to work relevant behaviors. Role theory suggests that role senders provide cues to elicit desired behaviors. In this context, role senders (i.e.: supervisors, managers, et cetera) provide workers with cues for expected behaviors, which in turn activates personality traits and work relevant behaviors. In essence, expectations of the role sender lead to different behavioral outcomes depending on the trait levels of individual workers and because people differ in trait levels, responses to these cues will not be universal.
The Big Five model of personality was used for attempts to predict satisfaction in romantic relationships, relationship quality in dating, engaged, and married couples.Dating couples Self-reported relationship quality is negatively related to partner-reported neuroticism and positively related to both self and partner-reported conscientiousnessEngaged couples Self-reported relationship quality was higher among those high in partner-reported openness, agreeableness and conscientiousness. Self-reported relationship quality was higher among those high in self-reported extraversion and agreeableness. Self-reported relationship quality is negatively related to both self and partner-reported neuroticism Observers rated the relationship quality higher if the participating partner's self-reported extraversion was highMarried couples High self-reported neuroticism, extraversion, and agreeableness are related to high levels of self-reported relationship quality Partner-reported agreeableness is related to observed relationship quality.These reports are, however, rare and not conclusive.
The Big Five Personality Model also has applications in the study of political psychology. Studies have been finding links between the big five personality traits and political identification. It has been found by several studies that individuals who score high in Conscientiousness are more likely to possess a right-wing political identification. On the opposite end of the spectrum, a strong correlation was identified between high scores in Openness to Experience and a left-leaning ideology. While the traits of agreeableness, extraversion, and neuroticism have not been consistently linked to either conservative or liberal ideology, with studies producing mixed results, such traits are promising when analyzing the strength of an individual's party identification. However, correlations between the Big Five and political beliefs, while present, tend to be small, with one study finding correlations ranged from 0.14 to 0.24.
The predictive effects of the Big Five personality traits relate mostly to social functioning and rules-driven behavior and are not very specific for prediction of particular aspects of behavior. For example, it was noted that high neuroticism precedes the development of all common mental disorders and is not attributed with personality by all temperament researchers. Further evidence is required to fully uncover the nature and differences between personality traits, temperament and life outcomes. Social and contextual parameters also play a role in outcomes and the interaction between the two is not yet fully understood.
Several measures of the Big Five exist: International Personality Item Pool (IPIP) NEO-PI-R The Ten-Item Personality Inventory (TIPI) and the Five Item Personality Inventory (FIPI) are very abbreviated rating forms of the Big Five personality traits. Self-descriptive sentence questionnaires Lexical questionnaires Self-report questionnaires Relative-scored Big 5 measureThe most frequently used measures of the Big Five comprise either items that are self-descriptive sentences or, in the case of lexical measures, items that are single adjectives. Due to the length of sentence-based and some lexical measures, short forms have been developed and validated for use in applied research settings where questionnaire space and respondent time are limited, such as the 40-item balanced International English Big-Five Mini-Markers or a very brief (10 item) measure of the Big Five domains. Research has suggested that some methodologies in administering personality tests are inadequate in length and provide insufficient detail to truly evaluate personality. Usually, longer, more detailed questions will give a more accurate portrayal of personality. The five factor structure has been replicated in peer reports. However, many of the substantive findings rely on self-reports. Much of the evidence on the measures of the Big 5 relies on self-report questionnaires, which makes self-report bias and falsification of responses difficult to deal with and account for. It has been argued that the Big Five tests do not create an accurate personality profile because the responses given on these tests are not true in all cases. For example, questionnaires are answered by potential employees who might choose answers that paint them in the best light.Research suggests that a relative-scored Big Five measure in which respondents had to make repeated choices between equally desirable personality descriptors may be a potential alternative to traditional Big Five measures in accurately assessing personality traits, especially when lying or biased responding is present. When compared with a traditional Big Five measure for its ability to predict GPA and creative achievement under both normal and "fake good"-bias response conditions, the relative-scored measure significantly and consistently predicted these outcomes under both conditions; however, the Likert questionnaire lost its predictive ability in the faking condition. Thus, the relative-scored measure proved to be less affected by biased responding than the Likert measure of the Big Five. Andrew H. Schwartz analyzed 700 million words, phrases, and topic instances collected from the Facebook messages of 75,000 volunteers, who also took standard personality tests, and found striking variations in language with personality, gender, and age.
The proposed Big Five model has been subjected to considerable critical scrutiny in a number of published studies. One prominent critic of the model has been Jack Block at the University of California, Berkeley. In response to Block, the model was defended in a paper published by Costa and McCrae. This was followed by a number of published critical replies from Block.It has been argued that there are limitations to the scope of the Big Five model as an explanatory or predictive theory. It has also been argued that measures of the Big Five account for only 56% of the normal personality trait sphere alone (not even considering the abnormal personality trait sphere). Also, the static Big Five is not theory-driven, it is merely a statistically-driven investigation of certain descriptors that tend to cluster together often based on less than optimal factor analytic procedures. Measures of the Big Five constructs appear to show some consistency in interviews, self-descriptions and observations, and this static five-factor structure seems to be found across a wide range of participants of different ages and cultures. However, while genotypic temperament trait dimensions might appear across different cultures, the phenotypic expression of personality traits differs profoundly across different cultures as a function of the different socio-cultural conditioning and experiential learning that takes place within different cultural settings.Moreover, the fact that the Big Five model was based on lexical hypothesis, (i.e. on the verbal descriptors of individual differences) indicated strong methodological flaws in this model, especially related to its main factors, Extraversion and Neuroticism. First, there is a natural pro-social bias of language in people's verbal evaluations. After all, language is an invention of group dynamics that was developed to facilitate socialization, the exchange of information and to synchronize group activity. This social function of language therefore creates a sociability bias in verbal descriptors of human behavior: there are more words related to social than physical or even mental aspects of behavior. The sheer number of such descriptors will cause them to group into a largest factor in any language, and such grouping has nothing to do with the way that core systems of individual differences are set up. Second, there is also a negativity bias in emotionality (i.e. most emotions have negative affectivity), and there are more words in language to describe negative rather than positive emotions. Such asymmetry in emotional valence creates another bias in language. Experiments using the lexical hypothesis approach indeed demonstrated that the use of lexical material skews the resulting dimensionality according to a sociability bias of language and a negativity bias of emotionality, grouping all evaluations around these two dimensions. This means that the two largest dimensions in the Big Five model might be just an artifact of the lexical approach that this model employed.
One common criticism is that the Big Five does not explain all of human personality. Some psychologists have dissented from the model precisely because they feel it neglects other domains of personality, such as religiosity, manipulativeness/machiavellianism, honesty, sexiness/seductiveness, thriftiness, conservativeness, masculinity/femininity, snobbishness/egotism, sense of humour, and risk-taking/thrill-seeking. Dan P. McAdams has called the Big Five a "psychology of the stranger", because they refer to traits that are relatively easy to observe in a stranger; other aspects of personality that are more privately held or more context-dependent are excluded from the Big Five.In many studies, the five factors are not fully orthogonal to one another; that is, the five factors are not independent. Orthogonality is viewed as desirable by some researchers because it minimizes redundancy between the dimensions. This is particularly important when the goal of a study is to provide a comprehensive description of personality with as few variables as possible.
Factor analysis, the statistical method used to identify the dimensional structure of observed variables, lacks a universally recognized basis for choosing among solutions with different numbers of factors. A five factor solution depends on some degree of interpretation by the analyst. A larger number of factors may underlie these five factors. This has led to disputes about the "true" number of factors. Big Five proponents have responded that although other solutions may be viable in a single dataset, only the five factor structure consistently replicates across different studies.Moreover, the factor analysis that this model is based on is a linear method incapable of capturing nonlinear, feedback and contingent relationships between core systems of individual differences.
A frequent criticism is that the Big Five is not based on any underlying theory; it is merely an empirical finding that certain descriptors cluster together under factor analysis. Although this does not mean that these five factors do not exist, the underlying causes behind them are unknown. Jack Block's final published work before his death in January 2010 drew together his lifetime perspective on the five-factor model.He summarized his critique of the model in terms of: the atheoretical nature of the five-factors. their "cloudy" measurement. the model's inappropriateness for studying early childhood. the use of factor analysis as the exclusive paradigm for conceptualizing personality. the continuing non-consensual understandings of the five-factors. the existence of unrecognized but successful efforts to specify aspects of character not subsumed by the five-factors.He went on to suggest that repeatedly observed higher order factors hierarchically above the proclaimed Big Five personality traits may promise deeper biological understanding of the origins and implications of these superfactors.
It has been noted that even though early lexical studies in the English language indicated five large groups of personality traits, more recent, and more comprehensive, cross-language studies have provided evidence for six large groups rather than five. These six groups forms the basis of the HEXACO model of personality structure. Based on these findings it has been suggested that the Big Five system should be replaced by HEXACO, or revised to better align with lexical evidence.
Big Five personality traits and culture Core self-evaluations Dark triad DISC assessment Facet Genomics of personality traits Goal orientation HEXACO model of personality structure List of U.S. states ranked per five-factor model personality trait Moral foundations theory Myers–Briggs Type Indicator Personality psychology Szondi test Trait theory
The Atlantic Ocean is the second-largest of the world's oceans, with an area of about 106,460,000 km2 (41,100,000 sq mi). It covers approximately 20 percent of Earth's surface and about 29 percent of its water surface area. It separates the "Old World" from the "New World". The Atlantic Ocean occupies an elongated, S-shaped basin extending longitudinally between Europe and Africa to the east, and the Americas to the west. As one component of the interconnected World Ocean, it is connected in the north to the Arctic Ocean, to the Pacific Ocean in the southwest, the Indian Ocean in the southeast, and the Southern Ocean in the south (other definitions describe the Atlantic as extending southward to Antarctica). The Equatorial Counter Current subdivides it into the North(ern) Atlantic Ocean and the South(ern) Atlantic Ocean at about 8°N.Scientific explorations of the Atlantic include the Challenger expedition, the German Meteor expedition, Columbia University's Lamont-Doherty Earth Observatory and the United States Navy Hydrographic Office.
The oldest known mentions of an "Atlantic" sea come from Stesichorus around mid-sixth century BC (Sch. A. R. 1. 211): Atlantikôi pelágei (Greek: Ἀτλαντικῷ πελάγει; English: 'the Atlantic sea'; etym. 'Sea of Atlantis') and in The Histories of Herodotus around 450 BC (Hdt. 1.202.4): Atlantis thalassa (Greek: Ἀτλαντὶς θάλασσα; English: 'Sea of Atlantis' or 'the Atlantis sea') where the name refers to "the sea beyond the pillars of Heracles" which is said to be part of the sea that surrounds all land. In these uses, the name refers to Atlas, the Titan in Greek mythology, who supported the heavens and who later appeared as a frontispiece in Medieval maps and also lent his name to modern atlases. On the other hand, to early Greek sailors and in Ancient Greek mythological literature such as the Iliad and the Odyssey, this all-encompassing ocean was instead known as Oceanus, the gigantic river that encircled the world; in contrast to the enclosed seas well known to the Greeks: the Mediterranean and the Black Sea. In contrast, the term "Atlantic" originally referred specifically to the Atlas Mountains in Morocco and the sea off the Strait of Gibraltar and the North African coast. The Greek word thalassa has been reused by scientists for the huge Panthalassa ocean that surrounded the supercontinent Pangaea hundreds of millions of years ago. The term "Aethiopian Ocean", derived from Ancient Ethiopia, was applied to the Southern Atlantic as late as the mid-19th century. During the Age of Discovery, the Atlantic was also known to English cartographers as the Great Western Ocean.The term The Pond is often used by British and American speakers in context to the Atlantic Ocean, as a form of meiosis, or sarcastic understatement. The term dates to as early as 1640, first appearing in print in pamphlet released during the reign of Charles I, and reproduced in 1869 in Nehemiah Wallington's Historical Notices of Events Occurring Chiefly in The Reign of Charles I, where "great Pond" is used in reference to the Atlantic Ocean by Francis Windebank, Charles I's Secretary of State.
The MAR divides the Atlantic longitudinally into two-halves, in each of which a series of basins are delimited by secondary, transverse ridges. The MAR reaches above 2,000 m (6,600 ft) along most of its length, but is interrupted by larger transform faults at two places: the Romanche Trench near the Equator and the Gibbs Fracture Zone at 53°N. The MAR is a barrier for bottom water, but at these two transform faults deep water currents can pass from one side to the other.The MAR rises 2–3 km (1.2–1.9 mi) above the surrounding ocean floor and its rift valley is the divergent boundary between the North American and Eurasian plates in the North Atlantic and the South American and African plates in the South Atlantic. The MAR produces basaltic volcanoes in Eyjafjallajökull, Iceland, and pillow lava on the ocean floor. The depth of water at the apex of the ridge is less than 2,700 m (1,500 fathoms; 8,900 ft) in most places, while the bottom of the ridge is three times as deep.The MAR is intersected by two perpendicular ridges: the Azores–Gibraltar Transform Fault, the boundary between the Nubian and Eurasian plates, intersects the MAR at the Azores Triple Junction, on either side of the Azores microplate, near the 40°N. A much vaguer, nameless boundary, between the North American and South American plates, intersects the MAR near or just north of the Fifteen-Twenty Fracture Zone, approximately at 16°N.In the 1870s, the Challenger expedition discovered parts of what is now known as the Mid-Atlantic Ridge, or: An elevated ridge rising to an average height of about 1,900 fathoms [3,500 m; 11,400 ft] below the surface traverses the basins of the North and South Atlantic in a meridianal direction from Cape Farewell, probably its far south at least as Gough Island, following roughly the outlines of the coasts of the Old and the New Worlds. The remainder of the ridge was discovered in the 1920s by the German Meteor expedition using echo-sounding equipment. The exploration of the MAR in the 1950s led to the general acceptance of seafloor spreading and plate tectonics.Most of the MAR runs under water but where it reaches the surfaces it has produced volcanic islands. While nine of these have collectively been nominated a World Heritage Site for their geological value, four of them are considered of "Outstanding Universal Value" based on their cultural and natural criteria: Þingvellir, Iceland; Landscape of the Pico Island Vineyard Culture, Portugal; Gough and Inaccessible Islands, United Kingdom; and Brazilian Atlantic Islands: Fernando de Noronha and Atol das Rocas Reserves, Brazil.
Continental shelves in the Atlantic are wide off Newfoundland, southernmost South America, and north-eastern Europe. In the western Atlantic carbonate platforms dominate large areas, for example, the Blake Plateau and Bermuda Rise. The Atlantic is surrounded by passive margins except at a few locations where active margins form deep trenches: the Puerto Rico Trench (8,376 m or 27,480 ft maximum depth) in the western Atlantic and South Sandwich Trench (8,264 m or 27,113 ft) in the South Atlantic. There are numerous submarine canyons off north-eastern North America, western Europe, and north-western Africa. Some of these canyons extend along the continental rises and farther into the abyssal plains as deep-sea channels.In 1922 a historic moment in cartography and oceanography occurred. The USS Stewart used a Navy Sonic Depth Finder to draw a continuous map across the bed of the Atlantic. This involved little guesswork because the idea of sonar is straight forward with pulses being sent from the vessel, which bounce off the ocean floor, then return to the vessel. The deep ocean floor is thought to be fairly flat with occasional deeps, abyssal plains, trenches, seamounts, basins, plateaus, canyons, and some guyots. Various shelves along the margins of the continents constitute about 11% of the bottom topography with few deep channels cut across the continental rise. The mean depth between 60°N and 60°S is 3,730 m (12,240 ft), or close to the average for the global ocean, with a modal depth between 4,000 and 5,000 m (13,000 and 16,000 ft).In the South Atlantic the Walvis Ridge and Rio Grande Rise form barriers to ocean currents. The Laurentian Abyss is found off the eastern coast of Canada.
On average, the Atlantic is the saltiest major ocean; surface water salinity in the open ocean ranges from 33 to 37 parts per thousand (3.3–3.7%) by mass and varies with latitude and season. Evaporation, precipitation, river inflow and sea ice melting influence surface salinity values. Although the lowest salinity values are just north of the equator (because of heavy tropical rainfall), in general, the lowest values are in the high latitudes and along coasts where large rivers enter. Maximum salinity values occur at about 25° north and south, in subtropical regions with low rainfall and high evaporation.The high surface salinity in the Atlantic, on which the Atlantic thermohaline circulation is dependent, is maintained by two processes: the Agulhas Leakage/Rings, which brings salty Indian Ocean waters into the South Atlantic, and the "Atmospheric Bridge", which evaporates subtropical Atlantic waters and exports it to the Pacific.
The Atlantic Ocean consists of four major, upper water masses with distinct temperature and salinity. The Atlantic Subarctic Upper Water in the northernmost North Atlantic is the source for Subarctic Intermediate Water and North Atlantic Intermediate Water. North Atlantic Central Water can be divided into the Eastern and Western North Atlantic central Water since the western part is strongly affected by the Gulf Stream and therefore the upper layer is closer to underlying fresher subpolar intermediate water. The eastern water is saltier because of its proximity to Mediterranean Water. North Atlantic Central Water flows into South Atlantic Central Water at 15°N.There are five intermediate waters: four low-salinity waters formed at subpolar latitudes and one high-salinity formed through evaporation. Arctic Intermediate Water, flows from north to become the source for North Atlantic Deep Water south of the Greenland-Scotland sill. These two intermediate waters have different salinity in the western and eastern basins. The wide range of salinities in the North Atlantic is caused by the asymmetry of the northern subtropical gyre and the large number of contributions from a wide range of sources: Labrador Sea, Norwegian-Greenland Sea, Mediterranean, and South Atlantic Intermediate Water.The North Atlantic Deep Water (NADW) is a complex of four water masses, two that form by deep convection in the open ocean — Classical and Upper Labrador Sea Water — and two that form from the inflow of dense water across the Greenland-Iceland-Scotland sill — Denmark Strait and Iceland-Scotland Overflow Water. Along its path across Earth the composition of the NADW is affected by other water masses, especially Antarctic Bottom Water and Mediterranean Overflow Water. The NADW is fed by a flow of warm shallow water into the northern North Atlantic which is responsible for the anomalous warm climate in Europe. Changes in the formation of NADW have been linked to global climate changes in the past. Since man-made substances were introduced into the environment, the path of the NADW can be traced throughout its course by measuring tritium and radiocarbon from nuclear weapon tests in the 1960s and CFCs.
The clockwise warm-water North Atlantic Gyre occupies the northern Atlantic, and the counter-clockwise warm-water South Atlantic Gyre appears in the southern Atlantic.In the North Atlantic, surface circulation is dominated by three inter-connected currents: the Gulf Stream which flows north-east from the North American coast at Cape Hatteras; the North Atlantic Current, a branch of the Gulf Stream which flows northward from the Grand Banks; and the Subpolar Front, an extension of the North Atlantic Current, a wide, vaguely defined region separating the subtropical gyre from the subpolar gyre. This system of currents transport warm water into the North Atlantic, without which temperatures in the North Atlantic and Europe would plunge dramatically. North of the North Atlantic Gyre, the cyclonic North Atlantic Subpolar Gyre plays a key role in climate variability. It is governed by ocean currents from marginal seas and regional topography, rather than being steered by wind, both in the deep ocean and at sea level. The subpolar gyre forms an important part of the global thermohaline circulation. Its eastern portion includes eddying branches of the North Atlantic Current which transport warm, saline waters from the subtropics to the north-eastern Atlantic. There this water is cooled during winter and forms return currents that merge along the eastern continental slope of Greenland where they form an intense (40–50 Sv) current which flows around the continental margins of the Labrador Sea. A third of this water becomes part of the deep portion of the North Atlantic Deep Water (NADW). The NADW, in its turn, feeds the meridional overturning circulation (MOC), the northward heat transport of which is threatened by anthropogenic climate change. Large variations in the subpolar gyre on a decade-century scale, associated with the North Atlantic oscillation, are especially pronounced in Labrador Sea Water, the upper layers of the MOC.The South Atlantic is dominated by the anti-cyclonic southern subtropical gyre. The South Atlantic Central Water originates in this gyre, while Antarctic Intermediate Water originates in the upper layers of the circumpolar region, near the Drake Passage and the Falkland Islands. Both these currents receive some contribution from the Indian Ocean. On the African east coast the small cyclonic Angola Gyre lies embedded in the large subtropical gyre. The southern subtropical gyre is partly masked by a wind-induced Ekman layer. The residence time of the gyre is 4.4–8.5 years. North Atlantic Deep Water flows southward below the thermocline of the subtropical gyre.
The Sargasso Sea in the western North Atlantic can be defined as the area where two species of Sargassum (S. fluitans and natans) float, an area 4,000 km (2,500 mi) wide and encircled by the Gulf Stream, North Atlantic Drift, and North Equatorial Current. This population of seaweed probably originated from Tertiary ancestors on the European shores of the former Tethys Ocean and has, if so, maintained itself by vegetative growth, floating in the ocean for millions of years. Other species endemic to the Sargasso Sea include the sargassum fish, a predator with algae-like appendages which hovers motionless among the Sargassum. Fossils of similar fishes have been found in fossil bays of the former Tethys Ocean, in what is now the Carpathian region, that were similar to the Sargasso Sea. It is possible that the population in the Sargasso Sea migrated to the Atlantic as the Tethys closed at the end of the Miocene around 17 Ma. The origin of the Sargasso fauna and flora remained enigmatic for centuries. The fossils found in the Carpathians in the mid-20th century, often called the "quasi-Sargasso assemblage", finally showed that this assemblage originated in the Carpathian Basin from where it migrated over Sicily to the Central Atlantic where it evolved into modern species of the Sargasso Sea.The location of the spawning ground for European eels remained unknown for decades. In the early 19th century it was discovered that the southern Sargasso Sea is the spawning ground for both the European and American eel and that the former migrate more than 5,000 km (3,100 mi) and the latter 2,000 km (1,200 mi). Ocean currents such as the Gulf Stream transport eel larvae from the Sargasso Sea to foraging areas in North America, Europe, and Northern Africa. Recent but disputed research suggests that eels possibly use Earth's magnetic field to navigate through the ocean both as larvae and as adults.
Climate is influenced by the temperatures of the surface waters and water currents as well as winds. Because of the ocean's great capacity to store and release heat, maritime climates are more moderate and have less extreme seasonal variations than inland climates. Precipitation can be approximated from coastal weather data and air temperature from water temperatures.The oceans are the major source of the atmospheric moisture that is obtained through evaporation. Climatic zones vary with latitude; the warmest zones stretch across the Atlantic north of the equator. The coldest zones are in high latitudes, with the coldest regions corresponding to the areas covered by sea ice. Ocean currents influence the climate by transporting warm and cold waters to other regions. The winds that are cooled or warmed when blowing over these currents influence adjacent land areas.The Gulf Stream and its northern extension towards Europe, the North Atlantic Drift is thought to have at least some influence on climate. For example, the Gulf Stream helps moderate winter temperatures along the coastline of southeastern North America, keeping it warmer in winter along the coast than inland areas. The Gulf Stream also keeps extreme temperatures from occurring on the Florida Peninsula. In the higher latitudes, the North Atlantic Drift, warms the atmosphere over the oceans, keeping the British Isles and north-western Europe mild and cloudy, and not severely cold in winter like other locations at the same high latitude. The cold water currents contribute to heavy fog off the coast of eastern Canada (the Grand Banks of Newfoundland area) and Africa's north-western coast. In general, winds transport moisture and air over land areas.
Icebergs are common from early February to the end of July across the shipping lanes near the Grand Banks of Newfoundland. The ice season is longer in the polar regions, but there is little shipping in those areas.Hurricanes are a hazard in the western parts of the North Atlantic during the summer and autumn. Due to a consistently strong wind shear and a weak Intertropical Convergence Zone, South Atlantic tropical cyclones are rare.
The Atlantic Ocean is underlain mostly by dense mafic oceanic crust made up of basalt and gabbro and overlain by fine clay, silt and siliceous ooze on the abyssal plain. The continental margins and continental shelf mark lower density, but greater thickness felsic continental rock that often much older than that of the seafloor. The oldest oceanic crust in the Atlantic is up to 145 million years and situated off the west coast of Africa and east coast of North America, or on either side of the South Atlantic.In many places, the continental shelf and continental slope are covered in thick sedimentary layers. For instance, on the North American side of the ocean, large carbonate deposits formed in warm shallow waters such as Florida and the Bahamas, while coarse river outwash sands and silt are common in shallow shelf areas like the Georges Bank. Coarse sand, boulders, and rocks were transported into some areas, such as off the coast of Nova Scotia or the Gulf of Maine during the Pleistocene ice ages.
The break-up of Pangaea began in the Central Atlantic, between North America and Northwest Africa, where rift basins opened during the Late Triassic and Early Jurassic. This period also saw the first stages of the uplift of the Atlas Mountains. The exact timing is controversial with estimates ranging from 200 to 170 Ma.The opening of the Atlantic Ocean coincided with the initial break-up of the supercontinent Pangaea, both of which were initiated by the eruption of the Central Atlantic Magmatic Province (CAMP), one of the most extensive and voluminous large igneous provinces in Earth's history associated with the Triassic–Jurassic extinction event, one of Earth's major extinction events. Theoliitic dikes, sills, and lava flows from the CAMP eruption at 200 Ma have been found in West Africa, eastern North America, and northern South America. The extent of the volcanism has been estimated to 4.5×106 km2 (1.7×106 sq mi) of which 2.5×106 km2 (9.7×105 sq mi) covered what is now northern and central Brazil.The formation of the Central American Isthmus closed the Central American Seaway at the end of the Pliocene 2.8 Ma ago. The formation of the isthmus resulted in the migration and extinction of many land-living animals, known as the Great American Interchange, but the closure of the seaway resulted in a "Great American Schism" as it affected ocean currents, salinity, and temperatures in both the Atlantic and Pacific. Marine organisms on both sides of the isthmus became isolated and either diverged or went extinct.
Geologically, the Northern Atlantic is the area delimited to the south by two conjugate margins, Newfoundland and Iberia, and to the north by the Arctic Eurasian Basin. The opening of the Northern Atlantic closely followed the margins of its predecessor, the Iapetus Ocean, and spread from the Central Atlantic in six stages: Iberia–Newfoundland, Porcupine–North America, Eurasia–Greenland, Eurasia–North America. Active and inactive spreading systems in this area are marked by the interaction with the Iceland hotspot.Seafloor spreading led to the extension of the crust and formations of troughs and sedimentary basins. The Rockall Trough opened between 105 and 84 million years ago although along the rift failed along with one leading into the Bay of Biscay. Spreading began opening the Labrador Sea around 61 million years ago, continuing until 36 million years ago. Geologists distinguish two magmatic phases. One from 62 to 58 million years ago predates the separation of Greenland from northern Europe while the second from 56 to 52 million years ago happened as the separation occurred. Iceland began to form 62 million years ago due to a particularly concentrated mantle plume. Large quantities of basalt erupted at this time period are found on Baffin Island, Greenland, the Faroe Islands, and Scotland, with ash falls in Western Europe acting as a stratigraphic marker. The opening of the North Atlantic caused significant uplift of continental crust along the coast. For instance, in spite of 7 km thick basalt, Gunnbjorn Field in East Greenland is the highest point on the island, elevated enough that it exposes older Mesozoic sedimentary rocks at its base, similar to old lava fields above sedimentary rocks in the uplifted Hebrides of western Scotland.
West Gondwana (South America and Africa) broke up in the Early Cretaceous to form the South Atlantic. The apparent fit between the coastlines of the two continents was noted on the first maps that included the South Atlantic and it was also the subject of the first computer-assisted plate tectonic reconstructions in 1965. This magnificent fit, however, has since then proven problematic and later reconstructions have introduced various deformation zones along the shorelines to accommodate the northward-propagating break-up. Intra-continental rifts and deformations have also been introduced to subdivide both continental plates into sub-plates.Geologically the South Atlantic can be divided into four segments: Equatorial segment, from 10°N to the Romanche Fracture Zone (RFZ);; Central segment, from RFZ to Florianopolis Fracture Zone (FFZ, north of Walvis Ridge and Rio Grande Rise); Southern segment, from FFZ to the Agulhas-Falkland Fracture Zone (AFFZ); and Falkland segment, south of AFFZ.In the southern segment the Early Cretaceous (133–130 Ma) intensive magmatism of the Paraná–Etendeka Large Igneous Province produced by the Tristan hotspot resulted in an estimated volume of 1.5×106 to 2.0×106 km3 (3.6×105 to 4.8×105 cu mi). It covered an area of 1.2×106 to 1.6×106 km2 (4.6×105 to 6.2×105 sq mi) in Brazil, Paraguay, and Uruguay and 0.8×105 km2 (3.1×104 sq mi) in Africa. Dyke swarms in Brazil, Angola, eastern Paraguay, and Namibia, however, suggest the LIP originally covered a much larger area and also indicate failed rifts in all these areas. Associated offshore basaltic flows reach as far south as the Falkland Islands and South Africa. Traces of magmatism in both offshore and onshore basins in the central and southern segments have been dated to 147–49 Ma with two peaks between 143 and 121 Ma and 90–60 Ma.In the Falkland segment rifting began with dextral movements between the Patagonia and Colorado sub-plates between the Early Jurassic (190 Ma) and the Early Cretaceous (126.7 Ma). Around 150 Ma sea-floor spreading propagated northward into the southern segment. No later than 130 Ma rifting had reached the Walvis Ridge–Rio Grande Rise.In the central segment rifting started to break Africa in two by opening the Benue Trough around 118 Ma. Rifting in the central segment, however, coincided with the Cretaceous Normal Superchron (also known as the Cretaceous quiet period), a 40 Ma period without magnetic reversals, which makes it difficult to date sea-floor spreading in this segment.The equatorial segment is the last phase of the break-up, but, because it is located on the Equator, magnetic anomalies cannot be used for dating. Various estimates date the propagation of sea-floor spreading in this segment to the period 120–96 Ma. This final stage, nevertheless, coincided with or resulted in the end of continental extension in Africa.About 50 Ma the opening of the Drake Passage resulted from a change in the motions and separation rate of the South American and Antarctic plates. First small ocean basins opened and a shallow gateway appeared during the Middle Eocene. 34–30 Ma a deeper seaway developed, followed by an Eocene–Oligocene climatic deterioration and the growth of the Antarctic ice sheet.
An embryonic subduction margin is potentially developing west of Gibraltar. The Gibraltar Arc in the western Mediterranean is migrating westward into the Central Atlantic where it joins the converging African and Eurasian plates. Together these three tectonic forces are slowly developing into a new subduction system in the eastern Atlantic Basin. Meanwhile, the Scotia Arc and Caribbean Plate in the western Atlantic Basin are eastward-propagating subduction systems that might, together with the Gibraltar system, represent the beginning of the closure of the Atlantic Ocean and the final stage of the Atlantic Wilson cycle.
Humans evolved in Africa; first by diverging from other apes around 7 mya; then developing stone tools around 2.6 mya; to finally evolve as modern humans around 100 kya. The earliest evidence for the complex behavior associated with this behavioral modernity has been found in the Greater Cape Floristic Region (GCFR) along the coast of South Africa. During the latest glacial stages, the now-submerged plains of the Agulhas Bank were exposed above sea level, extending the South African coastline farther south by hundreds of kilometers. A small population of modern humans — probably fewer than a thousand reproducing individuals — survived glacial maxima by exploring the high diversity offered by these Palaeo-Agulhas plains. The GCFR is delimited to the north by the Cape Fold Belt and the limited space south of it resulted in the development of social networks out of which complex Stone Age technologies emerged. Human history thus begins on the coasts of South Africa where the Atlantic Benguela Upwelling and Indian Ocean Agulhas Current meet to produce an intertidal zone on which shellfish, fur seal, fish and sea birds provided the necessary protein sources. The African origin of this modern behaviour is evidenced by 70,000 years-old engravings from Blombos Cave, South Africa.
Mitochondrial DNA (mtDNA) studies indicate that 80–60,000 years ago a major demographic expansion within Africa, derived from a single, small population, coincided with the emergence of behavioral complexity and the rapid MIS 5–4 environmental changes. This group of people not only expanded over the whole of Africa, but also started to disperse out of Africa into Asia, Europe, and Australasia around 65,000 years ago and quickly replaced the archaic humans in these regions. During the Last Glacial Maximum (LGM) 20,000 years ago humans had to abandon their initial settlements along the European North Atlantic coast and retreat to the Mediterranean. Following rapid climate changes at the end of the LGM this region was repopulated by Magdalenian culture. Other hunter-gatherers followed in waves interrupted by large-scale hazards such as the Laacher See volcanic eruption, the inundation of Doggerland (now the North Sea), and the formation of the Baltic Sea. The European coasts of the North Atlantic were permanently populated about 9–8.5 thousand years ago.This human dispersal left abundant traces along the coasts of the Atlantic Ocean. 50 kya-old, deeply stratified shell middens found in Ysterfontein on the western coast of South Africa are associated with the Middle Stone Age (MSA). The MSA population was small and dispersed and the rate of their reproduction and exploitation was less intense than those of later generations. While their middens resemble 12–11 kya-old Late Stone Age (LSA) middens found on every inhabited continent, the 50–45 kya-old Enkapune Ya Muto in Kenya probably represents the oldest traces of the first modern humans to disperse out of Africa. The same development can be seen in Europe. In La Riera Cave (23–13 kya) in Asturias, Spain, only some 26,600 molluscs were deposited over 10 kya. In contrast, 8–7 kya-old shell middens in Portugal, Denmark, and Brazil generated thousands of tons of debris and artefacts. The Ertebølle middens in Denmark, for example, accumulated 2,000 m3 (71,000 cu ft) of shell deposits representing some 50 million molluscs over only a thousand years. This intensification in the exploitation of marine resources has been described as accompanied by new technologies — such as boats, harpoons, and fish-hooks — because many caves found in the Mediterranean and on the European Atlantic coast have increased quantities of marine shells in their upper levels and reduced quantities in their lower. The earliest exploitation, however, took place on the now submerged shelves, and most settlements now excavated were then located several kilometers from these shelves. The reduced quantities of shells in the lower levels can represent the few shells that were exported inland.
During the LGM the Laurentide Ice Sheet covered most of northern North America while Beringia connected Siberia to Alaska. In 1973 late American geoscientist Paul S. Martin proposed a "blitzkrieg" colonization of the Americas by which Clovis hunters migrated into North America around 13,000 years ago in a single wave through an ice-free corridor in the ice sheet and "spread southward explosively, briefly attaining a density sufficiently large to overkill much of their prey." Others later proposed a "three-wave" migration over the Bering Land Bridge. These hypotheses remained the long-held view regarding the settlement of the Americas, a view challenged by more recent archaeological discoveries: the oldest archaeological sites in the Americas have been found in South America; sites in north-east Siberia report virtually no human presence there during the LGM; and most Clovis artefacts have been found in eastern North America along the Atlantic coast. Furthermore, colonisation models based on mtDNA, yDNA, and atDNA data respectively support neither the "blitzkrieg" nor the "three-wave" hypotheses but they also deliver mutually ambiguous results. Contradictory data from archaeology and genetics will most likely deliver future hypotheses that will, eventually, confirm each other. A proposed route across the Pacific to South America could explain early South American finds and another hypothesis proposes a northern path, through the Canadian Arctic and down the North American Atlantic coast. Early settlements across the Atlantic have been suggested by alternative theories, ranging from purely hypothetical to mostly disputed, including the Solutrean hypothesis and some of the Pre-Columbian trans-oceanic contact theories. The Norse settlement of the Faroe Islands and Iceland began during the 9th and 10th centuries. A settlement on Greenland was established before 1000 CE, but contact with it was lost in 1409 and it was finally abandoned during the early Little Ice Age. This setback was caused by a range of factors: an unsustainable economy resulted in erosion and denudation, while conflicts with the local Inuit resulted in the failure to adapt their Arctic technologies; a colder climate resulted in starvation, and the colony got economically marginalized as the Great Plague and Barbary pirates harvested its victims on Iceland in the 15th century. Iceland was initially settled 865–930 CE following a warm period when winter temperatures hovered around 2 °C (36 °F) which made farming favorable at high latitudes. This did not last, however, and temperatures quickly dropped; at 1080 CE summer temperatures had reached a maximum of 5 °C (41 °F). The Landnámabók (Book of Settlement) records disastrous famines during the first century of settlement — "men ate foxes and ravens" and "the old and helpless were killed and thrown over cliffs" — and by the early 1200s hay had to be abandoned for short-season crops such as barley.
Christopher Columbus reached the Americas in 1492 under Spanish flag. Six years later Vasco da Gama reached India under the Portuguese flag, by navigating south around the Cape of Good Hope, thus proving that the Atlantic and Indian Oceans are connected. In 1500, in his voyage to India following Vasco da Gama, Pedro Alvares Cabral reached Brazil, taken by the currents of the South Atlantic Gyre. Following these explorations, Spain and Portugal quickly conquered and colonized large territories in the New World and forced the Amerindian population into slavery in order to explore the vast quantities of silver and gold they found. Spain and Portugal monopolized this trade in order to keep other European nations out, but conflicting interests nevertheless led to a series of Spanish-Portuguese wars. A peace treaty mediated by the Pope divided the conquered territories into Spanish and Portuguese sectors while keeping other colonial powers away. England, France, and the Dutch Republic enviously watched the Spanish and Portuguese wealth grow and allied themselves with pirates such as Henry Mainwaring and Alexandre Exquemelin. They could explore the convoys leaving the Americas because prevailing winds and currents made the transport of heavy metals slow and predictable. In the colonies of the Americas, depredation, smallpox and others diseases, and slavery quickly reduced the indigenous population of the Americas to the extent that the Atlantic slave trade had to be introduced to replace them — a trade that became the norm and an integral part of the colonization. Between the 15th century and 1888, when Brazil became the last part of the Americas to end the slave trade, an estimated ten million Africans were exported as slaves, most of them destined for agricultural labour. The slave trade was officially abolished in the British Empire and the United States in 1808, and slavery itself was abolished in the British Empire in 1838 and in the United States in 1865 after the Civil War.From Columbus to the Industrial Revolution Trans-Atlantic trade, including colonialism and slavery, became crucial for Western Europe. For European countries with direct access to the Atlantic (including Britain, France, the Netherlands, Portugal, and Spain) 1500–1800 was a period of sustained growth during which these countries grew richer than those in Eastern Europe and Asia. Colonialism evolved as part of the Trans-Atlantic trade, but this trade also strengthened the position of merchant groups at the expense of monarchs. Growth was more rapid in non-absolutist countries, such as Britain and the Netherlands, and more limited in absolutist monarchies, such as Portugal, Spain, and France, where profit mostly or exclusively benefited the monarchy and its allies.Trans-Atlantic trade also resulted in increasing urbanization: in European countries facing the Atlantic, urbanization grew from 8% in 1300, 10.1% in 1500, to 24.5% in 1850; in other European countries from 10% in 1300, 11.4% in 1500, to 17% in 1850. Likewise, GDP doubled in Atlantic countries but rose by only 30% in the rest of Europe. By end of the 17th century, the volume of the Trans-Atlantic trade had surpassed that of the Mediterranean trade.
The Atlantic has contributed significantly to the development and economy of surrounding countries. Besides major transatlantic transportation and communication routes, the Atlantic offers abundant petroleum deposits in the sedimentary rocks of the continental shelves. The Atlantic harbors petroleum and gas fields, fish, marine mammals (seals and whales), sand and gravel aggregates, placer deposits, polymetallic nodules, and precious stones. Gold deposits are a mile or two under water on the ocean floor, however the deposits are also encased in rock that must be mined through. Currently, there is no cost-effective way to mine or extract gold from the ocean to make a profit.Various international treaties attempt to reduce pollution caused by environmental threats such as oil spills, marine debris, and the incineration of toxic wastes at sea.
The shelves of the Atlantic hosts one of the world's richest fishing resources. The most productive areas include the Grand Banks of Newfoundland, the Scotian Shelf, Georges Bank off Cape Cod, the Bahama Banks, the waters around Iceland, the Irish Sea, the Bay of Fundy, the Dogger Bank of the North Sea, and the Falkland Banks. Fisheries have, however, undergone significant changes since the 1950s and global catches can now be divided into three groups of which only two are observed in the Atlantic: fisheries in the Eastern Central and South-West Atlantic oscillate around a globally stable value, the rest of the Atlantic is in overall decline following historical peaks. The third group, "continuously increasing trend since 1950", is only found in the Indian Ocean and Western Pacific. In the North-East Atlantic total catches decreased between the mid-1970s and the 1990s and reached 8.7 million tons in 2013. Blue whiting reached a 2.4 million tons peak in 2004 but was down to 628,000 tons in 2013. Recovery plans for cod, sole, and plaice have reduced mortality in these species. Arctic cod reached its lowest levels in the 1960s–1980s but is now recovered. Arctic saithe and haddock are considered fully fished; Sand eel is overfished as was capelin which has now recovered to fully fished. Limited data makes the state of redfishes and deep-water species difficult to assess but most likely they remain vulnerable to overfishing. Stocks of northern shrimp and Norwegian lobster are in good condition. In the North-East Atlantic 21% of stocks are considered overfished. In the North-West Atlantic landings have decreased from 4.2 million tons in the early 1970s to 1.9 million tons in 2013. During the 21st century some species have shown weak signs of recovery, including Greenland halibut, yellowtail flounder, Atlantic halibut, haddock, spiny dogfish, while other stocks shown no such signs, including cod, witch flounder, and redfish. Stocks of invertebrates, in contrast, remain at record levels of abundance. 31% of stocks are overfished in the North-west Atlantic. In 1497 John Cabot became the first Western European since the Vikings to explore mainland North America and one of his major discoveries was the abundant resources of Atlantic cod off Newfoundland. Referred to as "Newfoundland Currency" this discovery yielded some 200 million tons of fish over five centuries. In the late 19th and early 20th centuries new fisheries started to exploit haddock, mackerel, and lobster. From the 1950s to the 1970s the introduction of European and Asian distant-water fleets in the area dramatically increased the fishing capacity and the number of exploited species. It also expanded the exploited areas from near-shore to the open sea and to great depths to include deep-water species such as redfish, Greenland halibut, witch flounder, and grenadiers. Overfishing in the area was recognised as early as the 1960s but, because this was occurring on international waters, it took until the late 1970s before any attempts to regulate was made. In the early 1990s, this finally resulted in the collapse of the Atlantic northwest cod fishery. The population of a number of deep-sea fishes also collapsed in the process, including American plaice, redfish, and Greenland halibut, together with flounder and grenadier.In the Eastern Central Atlantic small pelagic fishes constitute about 50% of landings with sardine reaching 0.6–1.0 million tons per year. Pelagic fish stocks are considered fully fished or overfished, with sardines south of Cape Bojador the notable exception. Almost half of the stocks are fished at biologically unsustainable levels. Total catches have been fluctuating since the 1970s; reaching 3.9 million tons in 2013 or slightly less than the peak production in 2010. In the Western Central Atlantic, catches have been decreasing since 2000 and reached 1.3 million tons in 2013. The most important species in the area, Gulf menhaden, reached a million tons in the mid-1980s but only half a million tons in 2013 and is now considered fully fished. Round sardinella was an important species in the 1990s but is now considered overfished. Groupers and snappers are overfished and northern brown shrimp and American cupped oyster are considered fully fished approaching overfished. 44% of stocks are being fished at unsustainable levels. In the South-East Atlantic catches have decreased from 3.3 million tons in the early 1970s to 1.3 million tons in 2013. Horse mackerel and hake are the most important species, together representing almost half of the landings. Off South Africa and Namibia deep-water hake and shallow-water Cape hake have recovered to sustainable levels since regulations were introduced in 2006 and the states of Southern African pilchard and anchovy have improved to fully fished in 2013.In the South-West Atlantic, a peak was reached in the mid-1980s and catches now fluctuate between 1.7 and 2.6 million tons. The most important species, the Argentine shortfin squid, which reached half a million tons in 2013 or half the peak value, is considered fully fished to overfished. Another important species was the Brazilian sardinella, with a production of 100,000 tons in 2013 it is now considered overfished. Half the stocks in this area are being fished at unsustainable levels: Whitehead's round herring has not yet reached fully fished but Cunene horse mackerel is overfished. The sea snail perlemoen abalone is targeted by illegal fishing and remain overfished.
Endangered marine species include the manatee, seals, sea lions, turtles, and whales. Drift net fishing can kill dolphins, albatrosses and other seabirds (petrels, auks), hastening the fish stock decline and contributing to international disputes. Municipal pollution comes from the eastern United States, southern Brazil, and eastern Argentina; oil pollution in the Caribbean Sea, Gulf of Mexico, Lake Maracaibo, Mediterranean Sea, and North Sea; and industrial waste and municipal sewage pollution in the Baltic Sea, North Sea, and Mediterranean Sea. North Atlantic hurricane activity has increased over past decades because of increased sea surface temperature (SST) at tropical latitudes, changes that can be attributed to either the natural Atlantic Multidecadal Oscillation (AMO) or to anthropogenic climate change. A 2005 report indicated that the Atlantic meridional overturning circulation (AMOC) slowed down by 30% between 1957 and 2004. If the AMO were responsible for SST variability, the AMOC would have increased in strength, which is apparently not the case. Furthermore, it is clear from statistical analyses of annual tropical cyclones that these changes do not display multidecadal cyclicity. Therefore, these changes in SST must be caused by human activities.The ocean mixed layer plays an important role in heat storage over seasonal and decadal time-scales, whereas deeper layers are affected over millennia and have a heat capacity about 50 times that of the mixed layer. This heat uptake provides a time-lag for climate change but it also results in thermal expansion of the oceans which contributes to sea-level rise. 21st-century global warming will probably result in an equilibrium sea-level rise five times greater than today, whilst melting of glaciers, including that of the Greenland ice-sheet, expected to have virtually no effect during the 21st century, will probably result in a sea-level rise of 3–6 m over a millennium.A USAF C-124 aircraft from Dover Air Force Base, Delaware was carrying three nuclear bombs over the Atlantic Ocean when it experienced a loss of power. For their own safety, the crew jettisoned two nuclear bombs, which were never recovered.On 7 June 2006, Florida's wildlife commission voted to take the manatee off the state's endangered species list. Some environmentalists worry that this could erode safeguards for the popular sea creature. Marine pollution is a generic term for the entry into the ocean of potentially hazardous chemicals or particles. The biggest culprits are rivers and with them many agriculture fertilizer chemicals as well as livestock and human waste. The excess of oxygen-depleting chemicals leads to hypoxia and the creation of a dead zone.Marine debris, which is also known as marine litter, describes human-created waste floating in a body of water. Oceanic debris tends to accumulate at the center of gyres and coastlines, frequently washing aground where it is known as beach litter. The North Atlantic garbage patch is estimated to be hundreds of kilometers across in size.
List of countries and territories bordering the Atlantic Ocean Seven Seas Gulf Stream shutdown Shipwrecks in the Atlantic Ocean Atlantic hurricanes Transatlantic crossing
Atlantic Ocean. Cartage.org.lb. "Map of Atlantic Coast of North America from the Chesapeake Bay to Florida" from 1639 via the World Digital Library
The Southern Ocean, also known as the Antarctic Ocean or the Austral Ocean, comprises the southernmost waters of the World Ocean, generally taken to be south of 60° S latitude and encircling Antarctica. As such, it is regarded as the second-smallest of the five principal oceanic divisions: smaller than the Pacific, Atlantic, and Indian Oceans but larger than the Arctic Ocean.By way of his voyages in the 1770s, James Cook proved that waters encompassed the southern latitudes of the globe. Since then, geographers have disagreed on the Southern Ocean's northern boundary or even existence, considering the waters as various parts of the Pacific, Atlantic, and Indian Oceans, instead. However, according to Commodore John Leech of the International Hydrographic Organization (IHO), recent oceanographic research has discovered the importance of Southern Circulation, and the term Southern Ocean has been used to define the body of water which lies south of the northern limit of that circulation. This remains the current official policy of the IHO, since a 2000 revision of its definitions including the Southern Ocean as the waters south of the 60th parallel has not yet been adopted. Others regard the seasonally-fluctuating Antarctic Convergence as the natural boundary. This oceanic zone is where cold, northward flowing waters from the Antarctic mix with warmer subantarctic waters. The maximum depth of the Southern Ocean, using the definition that it lies south of 60th parallel, was surveyed by the Five Deeps Expedition in early February 2019. The expedition's multibeam sonar team identified the deepest point at 60° 28' 46"S, 025° 32' 32"W, with a depth of 7,434 metres (24,390 ft). The expedition leader and chief submersible pilot Victor Vescovo, has proposed naming this deepest point in the Southern Ocean the "Factorian Deep", based on the name of the manned submersible DSV Limiting Factor, in which he successfully visited the bottom for the first time on February 3, 2019.
Borders and names for oceans and seas were internationally agreed when the International Hydrographic Bureau, the precursor to the IHO, convened the First International Conference on 24 July 1919. The IHO then published these in its Limits of Oceans and Seas, the first edition being 1928. Since the first edition, the limits of the Southern Ocean have moved progressively southwards; since 1953, it has been omitted from the official publication and left to local hydrographic offices to determine their own limits. The IHO included the ocean and its definition as the waters south of the 60th parallel south in its 2000 revisions, but this has not been formally adopted, due to continuing impasses about some of the content, such as the naming dispute over the Sea of Japan. The 2000 IHO definition, however, was circulated in a draft edition in 2002, and is used by some within the IHO and by some other organizations such as the CIA World Factbook and Merriam-Webster.The Australian Government regards the Southern Ocean as lying immediately south of Australia (see § Australian standpoint).The National Geographic Society does not recognize the ocean, depicting it in a typeface different from the other world oceans; instead, it shows the Pacific, Atlantic, and Indian Oceans extending to Antarctica on both its print and online maps. Map publishers using the term Southern Ocean on their maps include Hema Maps and GeoNova.
"Southern Ocean" is an obsolete name for the Pacific Ocean or South Pacific, coined by Vasco Núñez de Balboa, the first European to discover it, who approached it from the north. The "South Seas" is a less archaic synonym. A 1745 British Act of Parliament established a prize for discovering a Northwest Passage to "the Western and Southern Ocean of America".Authors using "Southern Ocean" to name the waters encircling the unknown southern polar regions used varying limits. James Cook's account of his second voyage implies New Caledonia borders it. Peacock's 1795 Geographical Dictionary said it lay "to the southward of America and Africa"; John Payne in 1796 used 40 degrees as the northern limit; the 1827 Edinburgh Gazetteer used 50 degrees. The Family Magazine in 1835 divided the "Great Southern Ocean" into the "Southern Ocean" and the "Antarctick [sic] Ocean" along the Antarctic Circle, with the northern limit of the Southern Ocean being lines joining Cape Horn, the Cape of Good Hope, Van Diemen's Land and the south of New Zealand.The United Kingdom's South Australia Act 1834 described the waters forming the southern limit of the new province of South Australia as "the Southern Ocean". The Colony of Victoria's Legislative Council Act 1881 delimited part of the division of Bairnsdale as "along the New South Wales boundary to the Southern ocean".
In the 1928 first edition of Limits of Oceans and Seas, the Southern Ocean was delineated by land-based limits: Antarctica to the south, and South America, Africa, Australia, and Broughton Island, New Zealand to the north. The detailed land-limits used were from Cape Horn in Chile eastwards to Cape Agulhas in Africa, then further eastwards to the southern coast of mainland Australia to Cape Leeuwin, Western Australia. From Cape Leeuwin, the limit then followed eastwards along the coast of mainland Australia to Cape Otway, Victoria, then southwards across Bass Strait to Cape Wickham, King Island, along the west coast of King Island, then the remainder of the way south across Bass Strait to Cape Grim, Tasmania. The limit then followed the west coast of Tasmania southwards to the South East Cape and then went eastwards to Broughton Island, New Zealand, before returning to Cape Horn.
The northern limits of the Southern Ocean were moved southwards in the IHO's 1937 second edition of the Limits of Oceans and Seas. From this edition, much of the ocean's northern limit ceased to abut land masses. In the second edition, the Southern Ocean then extended from Antarctica northwards to latitude 40°S between Cape Agulhas in Africa (long. 20°E) and Cape Leeuwin in Western Australia (long. 115°E), and extended to latitude 55°S between Auckland Island of New Zealand (165 or 166°E east) and Cape Horn in South America (67°W).As is discussed in more detail below, prior to the 2002 edition the limits of oceans explicitly excluded the seas lying within each of them. The Great Australian Bight was unnamed in the 1928 edition, and delineated as shown in the figure above in the 1937 edition. It therefore encompassed former Southern Ocean waters—as designated in 1928—but was technically not inside any of the three adjacent oceans by 1937. In the 2002 draft edition, the IHO have designated 'seas' as being subdivisions within 'oceans', so the Bight would have still been within the Southern Ocean in 1937 if the 2002 convention were in place then. To perform direct comparisons of current and former limits of oceans it is necessary to consider, or at least be aware of, how the 2002 change in IHO terminology for 'seas' can affect the comparison.
The Southern Ocean did not appear in the 1953 third edition of Limits of Oceans and Seas, a note in the publication read: The Antarctic or Southern Ocean has been omitted from this publication as the majority of opinions received since the issue of the 2nd Edition in 1937 are to the effect that there exists no real justification for applying the term Ocean to this body of water, the northern limits of which are difficult to lay down owing to their seasonal change. The limits of the Atlantic, Pacific and Indian Oceans have therefore been extended South to the Antarctic Continent.Hydrographic Offices who issue separate publications dealing with this area are therefore left to decide their own northern limits (Great Britain uses Latitude of 55 South.) Instead, in the IHO 1953 publication, the Atlantic, Indian and Pacific Oceans were extended southward, the Indian and Pacific Oceans (which had not previously touched pre 1953, as per the first and second editions) now abutted at the meridian of South East Cape, and the southern limits of the Great Australian Bight and the Tasman Sea were moved northwards.
The IHO readdressed the question of the Southern Ocean in a survey in 2000. Of its 68 member nations, 28 responded, and all responding members except Argentina agreed to redefine the ocean, reflecting the importance placed by oceanographers on ocean currents. The proposal for the name Southern Ocean won 18 votes, beating the alternative Antarctic Ocean. Half of the votes supported a definition of the ocean's northern limit at the 60th parallel south—with no land interruptions at this latitude—with the other 14 votes cast for other definitions, mostly the 50th parallel south, but a few for as far north as the 35th parallel south. A draft fourth edition of Limits of Oceans and Seas was circulated to IHO member states in August 2002 (sometimes referred to as the "2000 edition" as it summarized the progress to 2000). It has yet to be published due to 'areas of concern' by several countries relating to various naming issues around the world – primarily the Sea of Japan naming dispute – and there have been various changes, 60 seas were given new names, and even the name of the publication was changed. A reservation had also been lodged by Australia regarding the Southern Ocean limits. Effectively, the third edition—which did not delineate the Southern Ocean leaving delineation to local hydrographic offices—has yet to be superseded. Despite this, the fourth edition definition has partial de facto usage by many nations, scientists and organisations such as the U.S. (the CIA World Factbook uses "Southern Ocean" but none of the other new sea names within the "Southern Ocean" such as "Cosmonauts Sea") and Merriam-Webster, scientists and nations – and even by some within the IHO. Some nations' hydrographic offices have defined their own boundaries; the United Kingdom used the 55th parallel south for example. Other organisations favour more northerly limits for the Southern Ocean. For example, Encyclopædia Britannica describes the Southern Ocean as extending as far north as South America, and confers great significance on the Antarctic Convergence, yet its description of the Indian Ocean contradicts this, describing the Indian Ocean as extending south to Antarctica.Other sources, such as the National Geographic Society, show the Atlantic, Pacific and Indian Oceans as extending to Antarctica on its maps, although articles on the National Geographic web site have begun to reference the Southern Ocean.A radical shift from past IHO practices (1928–1953) was also seen in the 2002 draft edition when the IHO delineated 'seas' as being subdivisions that lay within the boundaries of 'oceans'. While the IHO are often considered the authority for such conventions, the shift brought them into line with the practices of other publications (e.g. the CIA World Fact Book) which already adopted the principle that seas are contained within oceans. This difference in practice is markedly seen for the Pacific Ocean in the adjacent figure. Thus, for example, previously the Tasman Sea between Australia and New Zealand was not regarded by the IHO as being part of the Pacific, but as of the 2002 draft edition it is. The new delineation of seas being subdivisions of oceans has avoided the need to interrupt the northern boundary of the Southern Ocean where intersected by Drake Passage which includes all of the waters from South America to the Antarctic coast, nor interrupt it for the Scotia Sea, which also extends below the 60th parallel south. The new delineation of seas has also meant that the long-time named seas around Antarctica, excluded from the 1953 edition (the 1953 map did not even extend that far south), are 'automatically' part of the Southern Ocean.
In Australia, cartographical authorities define the Southern Ocean as including the entire body of water between Antarctica and the south coasts of Australia and New Zealand, and up to 60°S elsewhere. Coastal maps of Tasmania and South Australia label the sea areas as Southern Ocean and Cape Leeuwin in Western Australia is described as the point where the Indian and Southern Oceans meet.
Exploration of the Southern Ocean was inspired by a belief in the existence of a Terra Australis – a vast continent in the far south of the globe to "balance" the northern lands of Eurasia and North Africa – which had existed since the times of Ptolemy. The doubling of the Cape of Good Hope in 1487 by Bartolomeu Dias first brought explorers within touch of the Antarctic cold, and proved that there was an ocean separating Africa from any Antarctic land that might exist. Ferdinand Magellan, who passed through the Strait of Magellan in 1520, assumed that the islands of Tierra del Fuego to the south were an extension of this unknown southern land. In 1564, Abraham Ortelius published his first map, Typus Orbis Terrarum, an eight-leaved wall map of the world, on which he identified the Regio Patalis with Locach as a northward extension of the Terra Australis, reaching as far as New Guinea.European geographers continued to connect the coast of Tierra del Fuego with the coast of New Guinea on their globes, and allowing their imaginations to run riot in the vast unknown spaces of the south Atlantic, south Indian and Pacific oceans they sketched the outlines of the Terra Australis Incognita ("Unknown Southern Land"), a vast continent stretching in parts into the tropics. The search for this great south land was a leading motive of explorers in the 16th and the early part of the 17th centuries.The Spaniard Gabriel de Castilla, who claimed having sighted "snow-covered mountains" beyond the 64° S in 1603, is recognized as the first explorer that discovered the continent of Antarctica, although he was ignored in his time. In 1606, Pedro Fernández de Quirós took possession for the king of Spain all of the lands he had discovered in Australia del Espiritu Santo (the New Hebrides) and those he would discover "even to the Pole".Francis Drake, like Spanish explorers before him, had speculated that there might be an open channel south of Tierra del Fuego. When Willem Schouten and Jacob Le Maire discovered the southern extremity of Tierra del Fuego and named it Cape Horn in 1615, they proved that the Tierra del Fuego archipelago was of small extent and not connected to the southern land, as previously thought. Subsequently, in 1642, Abel Tasman showed that even New Holland (Australia) was separated by sea from any continuous southern continent.
The first land south of the parallel 60° south latitude was discovered by the Englishman William Smith, who sighted Livingston Island on 19 February 1819. A few months later Smith returned to explore the other islands of the South Shetlands archipelago, landed on King George Island, and claimed the new territories for Britain. In the meantime, the Spanish Navy ship San Telmo sank in September 1819 when trying to cross Cape Horn. Parts of her wreckage were found months later by sealers on the north coast of Livingston Island (South Shetlands). It is unknown if some survivor managed to be the first to set foot on these Antarctic islands. The first confirmed sighting of mainland Antarctica cannot be accurately attributed to one single person. It can, however, be narrowed down to three individuals. According to various sources, three men all sighted the ice shelf or the continent within days or months of each other: Fabian Gottlieb von Bellingshausen, a captain in the Russian Imperial Navy; Edward Bransfield, a captain in the Royal Navy; and Nathaniel Palmer, an American sealer out of Stonington, Connecticut. It is certain that the expedition, led by von Bellingshausen and Lazarev on the ships Vostok and Mirny, reached a point within 32 km (20 mi) from Princess Martha Coast and recorded the sight of an ice shelf at 69°21′28″S 2°14′50″W that became known as the Fimbul Ice Shelf. On 30 January 1820, Bransfield sighted Trinity Peninsula, the northernmost point of the Antarctic mainland, while Palmer sighted the mainland in the area south of Trinity Peninsula in November 1820. Von Bellingshausen's expedition also discovered Peter I Island and Alexander I Island, the first islands to be discovered south of the circle.
In December 1839, as part of the United States Exploring Expedition of 1838–42 conducted by the United States Navy (sometimes called "the Wilkes Expedition"), an expedition sailed from Sydney, Australia, on the sloops-of-war USS Vincennes and USS Peacock, the brig USS Porpoise, the full-rigged ship Relief, and two schooners Sea Gull and USS Flying Fish. They sailed into the Antarctic Ocean, as it was then known, and reported the discovery "of an Antarctic continent west of the Balleny Islands" on 25 January 1840. That part of Antarctica was later named "Wilkes Land", a name it maintains to this day. Explorer James Clark Ross passed through what is now known as the Ross Sea and discovered Ross Island (both of which were named for him) in 1841. He sailed along a huge wall of ice that was later named the Ross Ice Shelf. Mount Erebus and Mount Terror are named after two ships from his expedition: HMS Erebus and HMS Terror. The Imperial Trans-Antarctic Expedition of 1914, led by Ernest Shackleton, set out to cross the continent via the pole, but their ship, Endurance, was trapped and crushed by pack ice before they even landed. The expedition members survived after an epic journey on sledges over pack ice to Elephant Island. Then Shackleton and five others crossed the Southern Ocean, in an open boat called James Caird, and then trekked over South Georgia to raise the alarm at the whaling station Grytviken. In 1946, US Navy Rear Admiral Richard E. Byrd and more than 4,700 military personnel visited the Antarctic in an expedition called Operation Highjump. Reported to the public as a scientific mission, the details were kept secret and it may have actually been a training or testing mission for the military. The expedition was, in both military or scientific planning terms, put together very quickly. The group contained an unusually high amount of military equipment, including an aircraft carrier, submarines, military support ships, assault troops and military vehicles. The expedition was planned to last for eight months but was unexpectedly terminated after only two months. With the exception of some eccentric entries in Admiral Byrd's diaries, no real explanation for the early termination has ever been officially given. Captain Finn Ronne, Byrd's executive officer, returned to Antarctica with his own expedition in 1947–1948, with Navy support, three planes, and dogs. Ronne disproved the notion that the continent was divided in two and established that East and West Antarctica was one single continent, i.e. that the Weddell Sea and the Ross Sea are not connected. The expedition explored and mapped large parts of Palmer Land and the Weddell Sea coastline, and identified the Ronne Ice Shelf, named by Ronne after his wife Edith "Jackie" Ronne. Ronne covered 3,600 miles (5,790 km) by ski and dog sled – more than any other explorer in history. The Ronne Antarctic Research Expedition discovered and mapped the last unknown coastline in the world and was the first Antarctic expedition to ever include women.
The Antarctic Treaty was signed on 1 December 1959 and came into force on 23 June 1961. Among other provisions, this treaty limits military activity in the Antarctic to the support of scientific research. The first person to sail single-handed to Antarctica was the New Zealander David Henry Lewis, in 1972, in a 10-metre (30 ft) steel sloop Ice Bird. A baby, named Emilio Marcos de Palma, was born near Hope Bay on 7 January 1978, becoming the first baby born on the continent. He also was born further south than anyone in history.The MV Explorer was a cruise ship operated by the Swedish explorer Lars-Eric Lindblad. Observers point to Explorer's 1969 expeditionary cruise to Antarctica as the frontrunner for today's sea-based tourism in that region. Explorer was the first cruise ship used specifically to sail the icy waters of the Antarctic Ocean and the first to sink there when she struck an unidentified submerged object on 23 November 2007, reported to be ice, which caused a 10 by 4 inches (25 by 10 cm) gash in the hull. Explorer was abandoned in the early hours of 23 November 2007 after taking on water near the South Shetland Islands in the Southern Ocean, an area which is usually stormy but was calm at the time. Explorer was confirmed by the Chilean Navy to have sunk at approximately position: 62° 24′ South, 57° 16′ West, in roughly 600 m of water.British engineer Richard Jenkins designed an unmanned surface vehicle called a "saildrone" that completed the first autonomous circumnavigation of the Southern Ocean on 3 August 2019 after 196 days at sea.The first completely human-powered expedition on the Southern Ocean was accomplished on 25 December 2019 by a team of rowers comprising captain Fiann Paul (Iceland), first mate Colin O'Brady (US), Andrew Towne (US), Cameron Bellamy (South Africa), Jamie Douglas-Hamilton (UK) and John Petersen (US).
The Southern Ocean probably contains large, and possibly giant, oil and gas fields on the continental margin. Placer deposits, accumulation of valuable minerals such as gold, formed by gravity separation during sedimentary processes are also expected to exist in the Southern Ocean.Manganese nodules are expected to exist in the Southern Ocean. Manganese nodules are rock concretions on the sea bottom formed of concentric layers of iron and manganese hydroxides around a core. The core may be microscopically small and is sometimes completely transformed into manganese minerals by crystallization. Interest in the potential exploitation of polymetallic nodules generated a great deal of activity among prospective mining consortia in the 1960s and 1970s.The icebergs that form each year around in the Southern Ocean hold enough fresh water to meet the needs of every person on Earth for several months. For several decades there have been proposals, none yet to be feasible or successful, to tow Southern Ocean icebergs to more arid northern regions (such as Australia) where they can be harvested.
Icebergs can occur at any time of year throughout the ocean. Some may have drafts up to several hundred meters; smaller icebergs, iceberg fragments and sea-ice (generally 0.5 to 1 m thick) also pose problems for ships. The deep continental shelf has a floor of glacial deposits varying widely over short distances. Sailors know latitudes from 40 to 70 degrees south as the "Roaring Forties", "Furious Fifties" and "Shrieking Sixties" due to high winds and large waves that form as winds blow around the entire globe unimpeded by any land-mass. Icebergs, especially in May to October, make the area even more dangerous. The remoteness of the region makes sources of search and rescue scarce.
The Antarctic Circumpolar Current moves perpetually eastward – chasing and joining itself, and at 21,000 km (13,000 mi) in length – it comprises the world's longest ocean current, transporting 130 million cubic metres per second (4.6×10^9 cu ft/s) of water – 100 times the flow of all the world's rivers. Several processes operate along the coast of Antarctica to produce, in the Southern Ocean, types of water masses not produced elsewhere in the oceans of the Southern Hemisphere. One of these is the Antarctic Bottom Water, a very cold, highly saline, dense water that forms under sea ice. Associated with the Circumpolar Current is the Antarctic Convergence encircling Antarctica, where cold northward-flowing Antarctic waters meet the relatively warmer waters of the subantarctic, Antarctic waters predominantly sink beneath subantarctic waters, while associated zones of mixing and upwelling create a zone very high in nutrients. These nurture high levels of phytoplankton with associated copepods and Antarctic krill, and resultant foodchains supporting fish, whales, seals, penguins, albatrosses and a wealth of other species.The Antarctic Convergence is considered to be the best natural definition of the northern extent of the Southern Ocean.
Large-scale upwelling is found in the Southern Ocean. Strong westerly (eastward) winds blow around Antarctica, driving a significant flow of water northwards. This is actually a type of coastal upwelling. Since there are no continents in a band of open latitudes between South America and the tip of the Antarctic Peninsula, some of this water is drawn up from great depths. In many numerical models and observational syntheses, the Southern Ocean upwelling represents the primary means by which deep dense water is brought to the surface. Shallower, wind-driven upwelling is also found off the west coasts of North and South America, northwest and southwest Africa, and southwest and southeast Australia, all associated with oceanic subtropical high pressure circulations. Some models of the ocean circulation suggest that broad-scale upwelling occurs in the tropics, as pressure driven flows converge water toward the low latitudes where it is diffusively warmed from above. The required diffusion coefficients, however, appear to be larger than are observed in the real ocean. Nonetheless, some diffusive upwelling does probably occur.
The Ross Gyre and Weddell Gyre are two gyres that exist within the Southern Ocean. The gyres are located in the Ross Sea and Weddell Sea respectively, and both rotate clockwise. The gyres are formed by interactions between the Antarctic Circumpolar Current and the Antarctic Continental Shelf. Sea ice has been noted to persist in the central area of the Ross Gyre. There is some evidence that global warming has resulted in some decrease of the salinity of the waters of the Ross Gyre since the 1950s.Due to the Coriolis effect acting to the left in the Southern Hemisphere and the resulting Ekman transport away from the centres of the Weddell Gyre, these regions are very productive due to upwelling of cold, nutrient rich water.
Sea temperatures vary from about −2 to 10 °C (28 to 50 °F). Cyclonic storms travel eastward around the continent and frequently become intense because of the temperature contrast between ice and open ocean. The ocean-area from about latitude 40 south to the Antarctic Circle has the strongest average winds found anywhere on Earth. In winter the ocean freezes outward to 65 degrees south latitude in the Pacific sector and 55 degrees south latitude in the Atlantic sector, lowering surface temperatures well below 0 degrees Celsius. At some coastal points, however, persistent intense drainage winds from the interior keep the shoreline ice-free throughout the winter.
A variety of marine animals exist and rely, directly or indirectly, on the phytoplankton in the Southern Ocean. Antarctic sea life includes penguins, blue whales, orcas, colossal squids and fur seals. The emperor penguin is the only penguin that breeds during the winter in Antarctica, while the Adélie penguin breeds farther south than any other penguin. The rockhopper penguin has distinctive feathers around the eyes, giving the appearance of elaborate eyelashes. King penguins, chinstrap penguins, and gentoo penguins also breed in the Antarctic. The Antarctic fur seal was very heavily hunted in the 18th and 19th centuries for its pelt by sealers from the United States and the United Kingdom. The Weddell seal, a "true seal", is named after Sir James Weddell, commander of British sealing expeditions in the Weddell Sea. Antarctic krill, which congregates in large schools, is the keystone species of the ecosystem of the Southern Ocean, and is an important food organism for whales, seals, leopard seals, fur seals, squid, icefish, penguins, albatrosses and many other birds.The benthic communities of the seafloor are diverse and dense, with up to 155,000 animals found in 1 square metre (10.8 sq ft). As the seafloor environment is very similar all around the Antarctic, hundreds of species can be found all the way around the mainland, which is a uniquely wide distribution for such a large community. Deep-sea gigantism is common among these animals.A census of sea life carried out during the International Polar Year and which involved some 500 researchers was released in 2010. The research is part of the global Census of Marine Life (CoML) and has disclosed some remarkable findings. More than 235 marine organisms live in both polar regions, having bridged the gap of 12,000 km (7,500 mi). Large animals such as some cetaceans and birds make the round trip annually. More surprising are small forms of life such as mudworms, sea cucumbers and free-swimming snails found in both polar oceans. Various factors may aid in their distribution – fairly uniform temperatures of the deep ocean at the poles and the equator which differ by no more than 5 °C (9.0 °F), and the major current systems or marine conveyor belt which transport egg and larva stages. However, among smaller marine animals generally assumed to be the same in the Antarctica and the Arctic, more detailed studies of each population have often—but not always—revealed differences, showing that they are closely related cryptic species rather than a single bipolar species.
The rocky shores of mainland Antarctica and its offshore islands provide nesting space for over 100 million birds every spring. These nesters include species of albatrosses, petrels, skuas, gulls and terns. The insectivorous South Georgia pipit is endemic to South Georgia and some smaller surrounding islands. Freshwater ducks inhabit South Georgia and the Kerguelen Islands.The flightless penguins are all located in the Southern Hemisphere, with the greatest concentration located on and around Antarctica. Four of the 18 penguin species live and breed on the mainland and its close offshore islands. Another four species live on the subantarctic islands. Emperor penguins have four overlapping layers of feathers, keeping them warm. They are the only Antarctic animal to breed during the winter.
There are relatively few fish species in few families in the Southern Ocean. The most species-rich family are the snailfish (Liparidae), followed by the cod icefish (Nototheniidae) and eelpout (Zoarcidae). Together the snailfish, eelpouts and notothenioids (which includes cod icefish and several other families) account for almost ​9⁄10 of the more than 320 described fish species of the Southern Ocean (tens of undescribed species also occur in the region, especially among the snailfish). Southern Ocean snailfish are generally found in deep waters, while the icefish also occur in shallower waters.
Cod icefish (Nototheniidae), as well as several other families, are part of the Notothenioidei suborder, collectively sometimes referred to as icefish. The suborder contains many species with antifreeze proteins in their blood and tissue, allowing them to live in water that is around or slightly below 0 °C (32 °F). Antifreeze proteins are also known from Southern Ocean snailfish.The crocodile icefish (family Channichthyidae), also known as white-blooded fish, are only found in the Southern Ocean. They lack hemoglobin in their blood, resulting in their blood being colourless. One Channichthyidae species, the mackerel icefish (Champsocephalus gunnari), was once the most common fish in coastal waters less than 400 metres (1,312 ft) deep, but was overfished in the 1970s and 1980s. Schools of icefish spend the day at the seafloor and the night higher in the water column eating plankton and smaller fish.There are two species from the genus Dissostichus, the Antarctic toothfish (Dissostichus mawsoni) and the Patagonian toothfish (Dissostichus eleginoides). These two species live on the seafloor 100–3,000 metres (328–9,843 ft) deep, and can grow to around 2 metres (7 ft) long weighing up to 100 kilograms (220 lb), living up to 45 years. The Antarctic toothfish lives close to the Antarctic mainland, whereas the Patagonian toothfish lives in the relatively warmer subantarctic waters. Toothfish are commercially fished, and overfishing has reduced toothfish populations.Another abundant fish group is the genus Notothenia, which like the Antarctic toothfish have antifreeze in their bodies.An unusual species of icefish is the Antarctic silverfish (Pleuragramma antarcticum), which is the only truly pelagic fish in the waters near Antarctica.
Seven pinniped species inhabit Antarctica. The largest, the elephant seal (Mirounga leonina), can reach up to 4,000 kilograms (8,818 lb), while females of the smallest, the Antarctic fur seal (Arctocephalus gazella), reach only 150 kilograms (331 lb). These two species live north of the sea ice, and breed in harems on beaches. The other four species can live on the sea ice. Crabeater seals (Lobodon carcinophagus) and Weddell seals (Leptonychotes weddellii) form breeding colonies, whereas leopard seals (Hydrurga leptonyx) and Ross seals (Ommatophoca rossii) live solitary lives. Although these species hunt underwater, they breed on land or ice and spend a great deal of time there, as they have no terrestrial predators.The four species that inhabit sea ice are thought to make up 50% of the total biomass of the world's seals. Crabeater seals have a population of around 15 million, making them one of the most numerous large animals on the planet. The New Zealand sea lion (Phocarctos hookeri), one of the rarest and most localised pinnipeds, breeds almost exclusively on the subantarctic Auckland Islands, although historically it had a wider range. Out of all permanent mammalian residents, the Weddell seals live the furthest south.There are 10 cetacean species found in the Southern Ocean; six baleen whales, and four toothed whales. The largest of these, the blue whale (Balaenoptera musculus), grows to 24 metres (79 ft) long weighing 84 tonnes. Many of these species are migratory, and travel to tropical waters during the Antarctic winter.
Many aquatic molluscs are present in Antarctica. Bivalves such as Adamussium colbecki move around on the seafloor, while others such as Laternula elliptica live in burrows filtering the water above. There are around 70 cephalopod species in the Southern Ocean, the largest of which is the colossal squid (Mesonychoteuthis hamiltoni), which at up to 14 metres (46 ft) is among the largest invertebrate in the world. Squid makes up most of the diet of some animals, such as grey-headed albatrosses and sperm whales, and the warty squid (Moroteuthis ingens) is one of the subantarctic's most preyed upon species by vertebrates.The sea urchin genus Abatus burrow through the sediment eating the nutrients they find in it. Two species of salps are common in Antarctic waters, Salpa thompsoni and Ihlea racovitzai. Salpa thompsoni is found in ice-free areas, whereas Ihlea racovitzai is found in the high latitude areas near ice. Due to their low nutritional value, they are normally only eaten by fish, with larger animals such as birds and marine mammals only eating them when other food is scarce.Antarctic sponges are long lived, and sensitive to environmental changes due to the specificity of the symbiotic microbial communities within them. As a result, they function as indicators of environmental health.
Increased solar ultraviolet radiation resulting from the Antarctic ozone hole has reduced marine primary productivity (phytoplankton) by as much as 15% and has started damaging the DNA of some fish. Illegal, unreported and unregulated fishing, especially the landing of an estimated five to six times more Patagonian toothfish than the regulated fishery, likely affects the sustainability of the stock. Long-line fishing for toothfish causes a high incidence of seabird mortality.
All international agreements regarding the world's oceans apply to the Southern Ocean. In addition, it is subject to these agreements specific to the region: The Southern Ocean Whale Sanctuary of the International Whaling Commission (IWC) prohibits commercial whaling south of 40 degrees south (south of 60 degrees south between 50 degrees and 130 degrees west). Japan regularly does not recognize this provision, because the sanctuary violates IWC charter. Since the scope of the sanctuary is limited to commercial whaling, in regard to its whaling permit and whaling for scientific research, a Japanese fleet carried out an annual whale-hunt in the region. On 31 March 2014, the International Court of Justice ruled that Japan's whaling program, which Japan has long claimed is for scientific purposes, was a cloak for commercial whaling, and no further permits would be granted. Convention for the Conservation of Antarctic Seals is part of the Antarctic Treaty System. It was signed at the conclusion of a multilateral conference in London on 11 February 1972. Convention for the Conservation of Antarctic Marine Living Resources (CCAMLR) is part of the Antarctic Treaty System. The Convention was entered into force on 7 April 1982 and has its goal is to preserve marine life and environmental integrity in and near Antarctica. It was established in large part to concerns that an increase in krill catches in the Southern Ocean could have a serious impact on populations of other marine life which are dependent upon krill for food.Many nations prohibit the exploration for and the exploitation of mineral resources south of the fluctuating Antarctic Convergence, which lies in the middle of the Antarctic Circumpolar Current and serves as the dividing line between the very cold polar surface waters to the south and the warmer waters to the north. The Antarctic Treaty covers the portion of the globe south of sixty degrees south, it prohibits new claims to Antarctica.The Convention for the Conservation of Antarctic Marine Living Resources applies to the area south of 60° South latitude as well as the areas further north up to the limit of the Antarctic Convergence.
Between 1 July 1998 and 30 June 1999, fisheries landed 119,898 tonnes, of which 85% consisted of krill and 14% of Patagonian toothfish. International agreements came into force in late 1999 to reduce illegal, unreported, and unregulated fishing, which in the 1998–99 season landed five to six times more Patagonian toothfish than the regulated fishery.
Major operational ports include: Rothera Station, Palmer Station, Villa Las Estrellas, Esperanza Base, Mawson Station, McMurdo Station, and offshore anchorages in Antarctica. Few ports or harbors exist on the southern (Antarctic) coast of the Southern Ocean, since ice conditions limit use of most shores to short periods in midsummer; even then some require icebreaker escort for access. Most Antarctic ports are operated by government research stations and, except in an emergency, remain closed to commercial or private vessels; vessels in any port south of 60 degrees south are subject to inspection by Antarctic Treaty observers. The Southern Ocean's southernmost port operates at McMurdo Station at 77°50′S 166°40′E. Winter Quarters Bay forms a small harbor, on the southern tip of Ross Island where a floating ice pier makes port operations possible in summer. Operation Deep Freeze personnel constructed the first ice pier at McMurdo in 1973.Based on the original 1928 IHO delineation of the Southern Ocean (and the 1937 delineation if the Great Australian Bight is considered integral), Australian ports and harbors between Cape Leeuwin and Cape Otway on the Australian mainland and along the west coast of Tasmania would also be identified as ports and harbors existing in the Southern Ocean. These would include the larger ports and harbors of Albany, Thevenard, Port Lincoln, Whyalla, Port Augusta, Port Adelaide, Portland, Warrnambool, and Macquarie Harbour. Even though organizers of several Yacht races define their routes as involving the Southern Ocean, the actual routes don't enter the actual geographical boundaries of the Southern Ocean. The routes involve instead South Atlantic, South Pacific and Indian Ocean.
Borders of the oceans List of countries by southernmost point List of seamounts in the Southern Ocean Seven Seas
Oceanography Image of the Day, from the Woods Hole Oceanographic Institution The CIA World Factbook's entry on the Southern Ocean The Fifth Ocean from Geography.About.com The International Bathymetric Chart of the Southern Ocean (IBCSO) National Geophysical Data Center U.S. National Oceanic and Atmospheric Administration (NOAA): Limits of Oceans and Seas (2nd Edition), extant 1937 to 1953, with limits of Southern Ocean. NOAA In-situ Ocean Data Viewer Plot and download ocean observations NOAA FAQ about the number of oceans Commission for the Conservation of Antarctic Marine Living Resources
Ocean has lived in Sunningdale, Berkshire, England, with his wife Judy, since 1978. They have three children. His son played rugby sevens at the 2014 Commonwealth Games for Barbados.Ocean decided to become vegetarian after the loss of his mother in 1989, who died from ovarian cancer.
Billy Ocean has been nominated three times for a Grammy Award, with one win.
Lists of UK Singles Chart number ones List of Billboard number-one singles List of artists who reached number one in the United States List of Billboard number-one dance club songs List of artists who reached number one on the U.S. Dance Club Songs chart List of Euro disco artists List of Eastern Caribbean people
Media related to Billy Ocean at Wikimedia Commons Billy Ocean official website Billy Ocean at AllMusic
Though the peoples of Asia and Oceania have traveled the Pacific Ocean since prehistoric times, the eastern Pacific was first sighted by Europeans in the early 16th century when Spanish explorer Vasco Núñez de Balboa crossed the Isthmus of Panama in 1513 and discovered the great "southern sea" which he named Mar del Sur (in Spanish). The ocean's current name was coined by Portuguese explorer Ferdinand Magellan during the Spanish circumnavigation of the world in 1521, as he encountered favorable winds on reaching the ocean. He called it Mar Pacífico, which in both Portuguese and Spanish means "peaceful sea".
Important human migrations occurred in the Pacific in prehistoric times. About 3000 BC, the Austronesian peoples on the island of Taiwan mastered the art of long-distance canoe travel and spread themselves and their languages south to the Philippines, Indonesia, and maritime Southeast Asia; west towards Madagascar; southeast towards New Guinea and Melanesia (intermarrying with native Papuans); and east to the islands of Micronesia, Oceania and Polynesia.Long-distance trade developed all along the coast from Mozambique to Japan. Trade, and therefore knowledge, extended to the Indonesian islands but apparently not Australia. In 219 BC Xu Fu sailed out into the Pacific searching for the elixir of immortality. By at least 878 when there was a significant Islamic settlement in Canton much of this trade was controlled by Arabs or Muslims. From 1404 to 1433 Zheng He led expeditions into the Indian Ocean.
The first contact of European navigators with the western edge of the Pacific Ocean was made by the Portuguese expeditions of António de Abreu and Francisco Serrão, via the Lesser Sunda Islands, to the Maluku Islands, in 1512, and with Jorge Álvares's expedition to southern China in 1513, both ordered by Afonso de Albuquerque from Malacca. The eastern side of the ocean was discovered by Spanish explorer Vasco Núñez de Balboa in 1513 after his expedition crossed the Isthmus of Panama and reached a new ocean. He named it Mar del Sur (literally, "Sea of the South" or "South Sea") because the ocean was to the south of the coast of the isthmus where he first observed the Pacific. In 1520, navigator Ferdinand Magellan and his crew were the first to cross the Pacific in recorded history. They were part of a Spanish expedition to the Spice Islands that would eventually result in the first world circumnavigation. Magellan called the ocean Pacífico (or "Pacific" meaning, "peaceful") because, after sailing through the stormy seas off Cape Horn, the expedition found calm waters. The ocean was often called the Sea of Magellan in his honor until the eighteenth century. Magellan stopped at one uninhabited Pacific island before stopping at Guam in March 1521. Although Magellan himself died in the Philippines in 1521, Spanish navigator Juan Sebastián Elcano led the remains of the expedition back to Spain across the Indian Ocean and round the Cape of Good Hope, completing the first world circumnavigation in 1522. Sailing around and east of the Moluccas, between 1525 and 1527, Portuguese expeditions discovered the Caroline Islands, the Aru Islands, and Papua New Guinea. In 1542–43 the Portuguese also reached Japan.In 1564, five Spanish ships carrying 379 explorers crossed the ocean from Mexico led by Miguel López de Legazpi, and sailed to the Philippines and Mariana Islands. For the remainder of the 16th century, Spanish influence was paramount, with ships sailing from Mexico and Peru across the Pacific Ocean to the Philippines via Guam, and establishing the Spanish East Indies. The Manila galleons operated for two and a half centuries, linking Manila and Acapulco, in one of the longest trade routes in history. Spanish expeditions also discovered Tuvalu, the Marquesas, the Cook Islands, the Solomon Islands, and the Admiralty Islands in the South Pacific.Later, in the quest for Terra Australis ("the [great] Southern Land"), Spanish explorations in the 17th century, such as the expedition led by the Portuguese navigator Pedro Fernandes de Queirós, discovered the Pitcairn and Vanuatu archipelagos, and sailed the Torres Strait between Australia and New Guinea, named after navigator Luís Vaz de Torres. Dutch explorers, sailing around southern Africa, also engaged in discovery and trade; Willem Janszoon, made the first completely documented European landing in Australia (1606), in Cape York Peninsula, and Abel Janszoon Tasman circumnavigated and landed on parts of the Australian continental coast and discovered Tasmania and New Zealand in 1642.In the 16th and 17th centuries, Spain considered the Pacific Ocean a mare clausum—a sea closed to other naval powers. As the only known entrance from the Atlantic, the Strait of Magellan was at times patrolled by fleets sent to prevent entrance of non-Spanish ships. On the western side of the Pacific Ocean the Dutch threatened the Spanish Philippines.The 18th century marked the beginning of major exploration by the Russians in Alaska and the Aleutian Islands, such as the First Kamchatka expedition and the Great Northern Expedition, led by the Danish Russian navy officer Vitus Bering. Spain also sent expeditions to the Pacific Northwest, reaching Vancouver Island in southern Canada, and Alaska. The French explored and settled Polynesia, and the British made three voyages with James Cook to the South Pacific and Australia, Hawaii, and the North American Pacific Northwest. In 1768, Pierre-Antoine Véron, a young astronomer accompanying Louis Antoine de Bougainville on his voyage of exploration, established the width of the Pacific with precision for the first time in history. One of the earliest voyages of scientific exploration was organized by Spain in the Malaspina Expedition of 1789–1794. It sailed vast areas of the Pacific, from Cape Horn to Alaska, Guam and the Philippines, New Zealand, Australia, and the South Pacific.
Growing imperialism during the 19th century resulted in the occupation of much of Oceania by European powers, and later Japan and the United States. Significant contributions to oceanographic knowledge were made by the voyages of HMS Beagle in the 1830s, with Charles Darwin aboard; HMS Challenger during the 1870s; the USS Tuscarora (1873–76); and the German Gazelle (1874–76). In Oceania, France obtained a leading position as imperial power after making Tahiti and New Caledonia protectorates in 1842 and 1853, respectively. After navy visits to Easter Island in 1875 and 1887, Chilean navy officer Policarpo Toro negotiated the incorporation of the island into Chile with native Rapanui in 1888. By occupying Easter Island, Chile joined the imperial nations. By 1900 nearly all Pacific islands were in control of Britain, France, United States, Germany, Japan, and Chile.Although the United States gained control of Guam and the Philippines from Spain in 1898, Japan controlled most of the western Pacific by 1914 and occupied many other islands during the Pacific War; however, by the end of that war, Japan was defeated and the U.S. Pacific Fleet was the virtual master of the ocean. The Japanese-ruled Northern Mariana Islands came under the control of the United States. Since the end of World War II, many former colonies in the Pacific have become independent states.
The Pacific Ocean has most of the islands in the world. There are about 25,000 islands in the Pacific Ocean. The islands entirely within the Pacific Ocean can be divided into three main groups known as Micronesia, Melanesia and Polynesia. Micronesia, which lies north of the equator and west of the International Date Line, includes the Mariana Islands in the northwest, the Caroline Islands in the center, the Marshall Islands to the east and the islands of Kiribati in the southeast.Melanesia, to the southwest, includes New Guinea, the world's second largest island after Greenland and by far the largest of the Pacific islands. The other main Melanesian groups from north to south are the Bismarck Archipelago, the Solomon Islands, Santa Cruz, Vanuatu, Fiji and New Caledonia.The largest area, Polynesia, stretching from Hawaii in the north to New Zealand in the south, also encompasses Tuvalu, Tokelau, Samoa, Tonga and the Kermadec Islands to the west, the Cook Islands, Society Islands and Austral Islands in the center, and the Marquesas Islands, Tuamotu, Mangareva Islands, and Easter Island to the east.Islands in the Pacific Ocean are of four basic types: continental islands, high islands, coral reefs and uplifted coral platforms. Continental islands lie outside the andesite line and include New Guinea, the islands of New Zealand, and the Philippines. Some of these islands are structurally associated with nearby continents. High islands are of volcanic origin, and many contain active volcanoes. Among these are Bougainville, Hawaii, and the Solomon Islands.The coral reefs of the South Pacific are low-lying structures that have built up on basaltic lava flows under the ocean's surface. One of the most dramatic is the Great Barrier Reef off northeastern Australia with chains of reef patches. A second island type formed of coral is the uplifted coral platform, which is usually slightly larger than the low coral islands. Examples include Banaba (formerly Ocean Island) and Makatea in the Tuamotu group of French Polynesia.
The volume of the Pacific Ocean, representing about 50.1 percent of the world's oceanic water, has been estimated at some 714 million cubic kilometers (171 million cubic miles). Surface water temperatures in the Pacific can vary from −1.4 °C (29.5 °F), the freezing point of sea water, in the poleward areas to about 30 °C (86 °F) near the equator. Salinity also varies latitudinally, reaching a maximum of 37 parts per thousand in the southeastern area. The water near the equator, which can have a salinity as low as 34 parts per thousand, is less salty than that found in the mid-latitudes because of abundant equatorial precipitation throughout the year. The lowest counts of less than 32 parts per thousand are found in the far north as less evaporation of seawater takes place in these frigid areas. The motion of Pacific waters is generally clockwise in the Northern Hemisphere (the North Pacific gyre) and counter-clockwise in the Southern Hemisphere. The North Equatorial Current, driven westward along latitude 15°N by the trade winds, turns north near the Philippines to become the warm Japan or Kuroshio Current.Turning eastward at about 45°N, the Kuroshio forks and some water moves northward as the Aleutian Current, while the rest turns southward to rejoin the North Equatorial Current. The Aleutian Current branches as it approaches North America and forms the base of a counter-clockwise circulation in the Bering Sea. Its southern arm becomes the chilled slow, south-flowing California Current. The South Equatorial Current, flowing west along the equator, swings southward east of New Guinea, turns east at about 50°S, and joins the main westerly circulation of the South Pacific, which includes the Earth-circling Antarctic Circumpolar Current. As it approaches the Chilean coast, the South Equatorial Current divides; one branch flows around Cape Horn and the other turns north to form the Peru or Humboldt Current.
The climate patterns of the Northern and Southern Hemispheres generally mirror each other. The trade winds in the southern and eastern Pacific are remarkably steady while conditions in the North Pacific are far more varied with, for example, cold winter temperatures on the east coast of Russia contrasting with the milder weather off British Columbia during the winter months due to the preferred flow of ocean currents.In the tropical and subtropical Pacific, the El Niño Southern Oscillation (ENSO) affects weather conditions. To determine the phase of ENSO, the most recent three-month sea surface temperature average for the area approximately 3,000 km (1,900 mi) to the southeast of Hawaii is computed, and if the region is more than 0.5 °C (0.9 °F) above or below normal for that period, then an El Niño or La Niña is considered in progress. In the tropical western Pacific, the monsoon and the related wet season during the summer months contrast with dry winds in the winter which blow over the ocean from the Asian landmass. Worldwide, tropical cyclone activity peaks in late summer, when the difference between temperatures aloft and sea surface temperatures is the greatest; however, each particular basin has its own seasonal patterns. On a worldwide scale, May is the least active month, while September is the most active month. November is the only month in which all the tropical cyclone basins are active. The Pacific hosts the two most active tropical cyclone basins, which are the northwestern Pacific and the eastern Pacific. Pacific hurricanes form south of Mexico, sometimes striking the western Mexican coast and occasionally the southwestern United States between June and October, while typhoons forming in the northwestern Pacific moving into southeast and east Asia from May to December. Tropical cyclones also form in the South Pacific basin, where they occasionally impact island nations. In the arctic, icing from October to May can present a hazard for shipping while persistent fog occurs from June to December. A climatological low in the Gulf of Alaska keeps the southern coast wet and mild during the winter months. The Westerlies and associated jet stream within the Mid-Latitudes can be particularly strong, especially in the Southern Hemisphere, due to the temperature difference between the tropics and Antarctica, which records the coldest temperature readings on the planet. In the Southern hemisphere, because of the stormy and cloudy conditions associated with extratropical cyclones riding the jet stream, it is usual to refer to the Westerlies as the Roaring Forties, Furious Fifties and Shrieking Sixties according to the varying degrees of latitude.
The ocean was first mapped by Abraham Ortelius; he called it Maris Pacifici following Ferdinand Magellan's description of it as "a pacific sea" during his circumnavigation from 1519 to 1522. To Magellan, it seemed much more calm (pacific) than the Atlantic.The andesite line is the most significant regional distinction in the Pacific. A petrologic boundary, it separates the deeper, mafic igneous rock of the Central Pacific Basin from the partially submerged continental areas of felsic igneous rock on its margins. The andesite line follows the western edge of the islands off California and passes south of the Aleutian arc, along the eastern edge of the Kamchatka Peninsula, the Kuril Islands, Japan, the Mariana Islands, the Solomon Islands, and New Zealand's North Island. The dissimilarity continues northeastward along the western edge of the Andes Cordillera along South America to Mexico, returning then to the islands off California. Indonesia, the Philippines, Japan, New Guinea, and New Zealand lie outside the andesite line. Within the closed loop of the andesite line are most of the deep troughs, submerged volcanic mountains, and oceanic volcanic islands that characterize the Pacific basin. Here basaltic lavas gently flow out of rifts to build huge dome-shaped volcanic mountains whose eroded summits form island arcs, chains, and clusters. Outside the andesite line, volcanism is of the explosive type, and the Pacific Ring of Fire is the world's foremost belt of explosive volcanism. The Ring of Fire is named after the several hundred active volcanoes that sit above the various subduction zones. The Pacific Ocean is the only ocean which is almost totally bounded by subduction zones. Only the Antarctic and Australian coasts have no nearby subduction zones.
The Pacific Ocean was born 750 million years ago at the breakup of Rodinia, although it is generally called the Panthalassic Ocean until the breakup of Pangea, about 200 million years ago. The oldest Pacific Ocean floor is only around 180 Ma old, with older crust subducted by now.
The Pacific Ocean contains several long seamount chains, formed by hotspot volcanism. These include the Hawaiian–Emperor seamount chain and the Louisville Ridge.
The exploitation of the Pacific's mineral wealth is hampered by the ocean's great depths. In shallow waters of the continental shelves off the coasts of Australia and New Zealand, petroleum and natural gas are extracted, and pearls are harvested along the coasts of Australia, Japan, Papua New Guinea, Nicaragua, Panama, and the Philippines, although in sharply declining volume in some cases.
Fish are an important economic asset in the Pacific. The shallower shoreline waters of the continents and the more temperate islands yield herring, salmon, sardines, snapper, swordfish, and tuna, as well as shellfish. Overfishing has become a serious problem in some areas. For example, catches in the rich fishing grounds of the Okhotsk Sea off the Russian coast have been reduced by at least half since the 1990s as a result of overfishing.
The quantity of small plastic fragments floating in the north-east Pacific Ocean increased a hundredfold between 1972 and 2012. The ever-growing Great Pacific garbage patch between California and Japan is three times the size of France. An estimated 80,000 metric tons of plastic inhabit the patch, totaling 1.8 trillion pieces. Marine pollution is a generic term for the harmful entry into the ocean of chemicals or particles. The main culprits are those using the rivers for disposing of their waste. The rivers then empty into the ocean, often also bringing chemicals used as fertilizers in agriculture. The excess of oxygen-depleting chemicals in the water leads to hypoxia and the creation of a dead zone.Marine debris, also known as marine litter, is human-created waste that has ended up floating in a lake, sea, ocean, or waterway. Oceanic debris tends to accumulate at the center of gyres and coastlines, frequently washing aground where it is known as beach litter.From 1946 to 1958, Marshall Islands served as the Pacific Proving Grounds for the United States and was the site of 67 nuclear tests on various atolls. Several nuclear weapons were lost in the Pacific Ocean, including one-megaton bomb lost during the 1965 Philippine Sea A-4 incident.In addition, the Pacific Ocean has served as the crash site of satellites, including Mars 96, Fobos-Grunt, and Upper Atmosphere Research Satellite.
EPIC Pacific Ocean Data Collection Viewable on-line collection of observational data NOAA In-situ Ocean Data Viewer plot and download ocean observations NOAA PMEL Argo profiling floats Realtime Pacific Ocean data NOAA TAO El Niño data Realtime Pacific Ocean El Niño buoy data NOAA Ocean Surface Current Analyses – Realtime (OSCAR) Near-realtime Pacific Ocean Surface Currents derived from satellite altimeter and scatterometer data
Marginal seas, gulfs, bays and straits of the Indian Ocean include:Along the east coast of Africa, the Mozambique Channel separates Madagascar from mainland Africa, while the Sea of Zanj is located north of Madagascar. On the northern coast of the Arabian Sea, Gulf of Aden is connected to the Red Sea by the strait of Bab-el-Mandeb. In the Gulf of Aden, the Gulf of Tadjoura is located in Djibouti and the Guardafui Channel separates Socotra island from the Horn of Africa. The northern end of the Red Sea terminates in the Gulf of Aqaba and Gulf of Suez. The Indian Ocean is artificially connected to the Mediterranean Sea through the Suez Canal, which is accessible via the Red Sea. The Arabian Sea is connected to the Persian Gulf by the Gulf of Oman and the Strait of Hormuz. In the Persian Gulf, the Gulf of Bahrain separates Qatar from the Arabic Peninsula. Along the west coast of India, the Gulf of Kutch and Gulf of Khambat are located in Gujarat in the northern end while the Laccadive Sea separates the Maldives from the southern tip of India. The Bay of Bengal is off the east coast of India. The Gulf of Mannar and the Palk Strait separates Sri Lanka from India, while the Adam's Bridge separates the two. The Andaman Sea is located between the Bay of Bengal and the Andaman Islands. In Indonesia, the so-called Indonesian Seaway is composed of the Malacca, Sunda and Torres Straits. The Gulf of Carpentaria of located on the Australian north coast while the Great Australian Bight constitutes a large part of its southern coast.
Several features make the Indian Ocean unique. It constitutes the core of the large-scale Tropical Warm Pool which, when interacting with the atmosphere, affects the climate both regionally and globally. Asia blocks heat export and prevents the ventilation of the Indian Ocean thermocline. That continent also drives the Indian Ocean monsoon, the strongest on Earth, which causes large-scale seasonal variations in ocean currents, including the reversal of the Somali Current and Indian Monsoon Current. Because of the Indian Ocean Walker circulation there are no continuous equatorial easterlies. Upwelling occurs near the Horn of Africa and the Arabian Peninsula in the Northern Hemisphere and north of the trade winds in the Southern Hemisphere. The Indonesian Throughflow is a unique Equatorial connection to the Pacific.The climate north of the equator is affected by a monsoon climate. Strong north-east winds blow from October until April; from May until October south and west winds prevail. In the Arabian Sea, the violent Monsoon brings rain to the Indian subcontinent. In the southern hemisphere, the winds are generally milder, but summer storms near Mauritius can be severe. When the monsoon winds change, cyclones sometimes strike the shores of the Arabian Sea and the Bay of Bengal. Some 80% of the total annual rainfall in India occurs during summer and the region is so dependent on this rainfall that many civilisations perished when the Monsoon failed in the past. The huge variability in the Indian Summer Monsoon has also occurred pre-historically, with a strong, wet phase 33,500–32,500 BP; a weak, dry phase 26,000–23,500 BC; and a very weak phase 17,000–15,000 BP, corresponding to a series of dramatic global events: Bølling-Allerød, Heinrich, and Younger Dryas. The Indian Ocean is the warmest ocean in the world. Long-term ocean temperature records show a rapid, continuous warming in the Indian Ocean, at about 1.2 °C (34.2 °F) (compared to 0.7 °C (33.3 °F) for the warm pool region) during 1901–2012. Research indicates that human induced greenhouse warming, and changes in the frequency and magnitude of El Niño (or the Indian Ocean Dipole), events are a trigger to this strong warming in the Indian Ocean.South of the Equator (20-5°S), the Indian Ocean is gaining heat from June to October, during the austral winter, while it is losing heat from November to March, during the austral summer.In 1999, the Indian Ocean Experiment showed that fossil fuel and biomass burning in South and Southeast Asia caused air pollution (also known as the Asian brown cloud) that reach as far as the Intertropical Convergence Zone at 60°S. This pollution has implications on both a local and global scale.
40% of the sediment of the Indian Ocean is found in the Indus and Ganges fans. The oceanic basins adjacent to the continental slopes mostly contain terrigenous sediments. The ocean south of the polar front (roughly 50° south latitude) is high in biologic productivity and dominated by non-stratified sediment composed mostly of siliceous oozes. Near the three major mid-ocean ridges the ocean floor is relatively young and therefore bare of sediment, except for the Southwest Indian Ridge due to its ultra-slow spreading rate.The ocean's currents are mainly controlled by the monsoon. Two large gyres, one in the northern hemisphere flowing clockwise and one south of the equator moving anticlockwise (including the Agulhas Current and Agulhas Return Current), constitute the dominant flow pattern. During the winter monsoon (November–February), however, circulation is reversed north of 30°S and winds are weakened during winter and the transitional periods between the monsoons.The Indian Ocean contains the largest submarine fans of the world, the Bengal Fan and Indus Fan, and the largest areas of slope terraces and rift valleys. The inflow of deep water into the Indian Ocean is 11 Sv, most of which comes from the Circumpolar Deep Water (CDW). The CDW enters the Indian Ocean through the Crozet and Madagascar basins and crosses the Southwest Indian Ridge at 30°S. In the Mascarene Basin the CDW becomes a deep western boundary current before it is met by a re-circulated branch of itself, the North Indian Deep Water. This mixed water partly flows north into the Somali Basin whilst most of it flows clockwise in the Mascarene Basin where an oscillating flow is produced by Rossby waves.Water circulation in the Indian Ocean is dominated by the Subtropical Anticyclonic Gyre, the eastern extension of which is blocked by the Southeast Indian Ridge and the 90°E Ridge. Madagascar and the Southwest Indian Ridge separates three cells south of Madagascar and off South Africa. North Atlantic Deep Water reaches into the Indian Ocean south of Africa at a depth of 2,000–3,000 m (6,600–9,800 ft) and flows north along the eastern continental slope of Africa. Deeper than NADW, Antarctic Bottom Water flows from Enderby Basin to Agulhas Basin across deep channels (<4,000 m (13,000 ft)) in the Southwest Indian Ridge, from where it continues into the Mozambique Channel and Prince Edward Fracture Zone.North of 20° south latitude the minimum surface temperature is 22 °C (72 °F), exceeding 28 °C (82 °F) to the east. Southward of 40° south latitude, temperatures drop quickly.The Bay of Bengal contributes more than half (2,950 km3 (710 cu mi)) of the runoff water to the Indian Ocean. Mainly in summer, this runoff flows into the Arabian Sea but also south across the Equator where it mixes with fresher seawater from the Indonesian Throughflow. This mixed freshwater joins the South Equatorial Current in the southern tropical Indian Ocean.Sea surface salinity is highest (more than 36 PSU) in the Arabian Sea because evaporation exceeds precipitation there. In the Southeast Arabian Sea salinity drops to less than 34 PSU. It is the lowest (c. 33 PSU) in the Bay of Bengal because of river runoff and precipitation. The Indonesian Throughflow and precipitation results in lower salinity (34 PSU) along the Sumatran west coast. Monsoonal variation results in eastward transportation of saltier water from the Arabian Sea to the Bay of Bengal from June to September and in westerly transport by the East India Coastal Current to the Arabian Sea from January to April.An Indian Ocean garbage patch was discovered in 2010 covering at least 5 million square kilometres (1.9 million square miles). Riding the southern Indian Ocean Gyre, this vortex of plastic garbage constantly circulates the ocean from Australia to Africa, down the Mozambique Channel, and back to Australia in a period of six years, except for debris that gets indefinitely stuck in the centre of the gyre. The garbage patch in the Indian Ocean will, according to a 2012 study, decrease in size after several decades to vanish completely over centuries. Over several millennia, however, the global system of garbage patches will accumulate in the North Pacific.There are two amphidromes of opposite rotation in the Indian Ocean, probably caused by Rossby wave propagation.Icebergs drift as far north as 55° south latitude, similar to the Pacific but less than in the Atlantic where icebergs reach up to 45°S. The volume of iceberg loss in the Indian Ocean between 2004 and 2012 was 24 Gt.Since the 1960s, anthropogenic warming of the global ocean combined with contributions of freshwater from retreating land ice causes a global rise in sea level. Sea level increases in the Indian Ocean too, except in the south tropical Indian Ocean where it decreases, a pattern most likely caused by rising levels of greenhouse gases.
Among the tropical oceans, the western Indian Ocean hosts one of the largest concentration of phytoplankton blooms in summer, due to the strong monsoon winds. The monsoonal wind forcing leads to a strong coastal and open ocean upwelling, which introduces nutrients into the upper zones where sufficient light is available for photosynthesis and phytoplankton production. These phytoplankton blooms support the marine ecosystem, as the base of the marine food web, and eventually the larger fish species. The Indian Ocean accounts for the second largest share of the most economically valuable tuna catch. It's fish are of great and growing importance to the bordering countries for domestic consumption and export. Fishing fleets from Russia, Japan, South Korea, and Taiwan also exploit the Indian Ocean, mainly for shrimp and tuna.Research indicates that increasing ocean temperatures are taking a toll on the marine ecosystem. A study on the phytoplankton changes in the Indian Ocean indicates a decline of up to 20% in the marine plankton in the Indian Ocean, during the past six decades. The tuna catch rates have also declined 50–90% during the past half century, mostly due to increased industrial fisheries, with the ocean warming adding further stress to the fish species.Endangered and vulnerable marine mammals and turtles: 80% of the Indian Ocean is open ocean and includes nine large marine ecosystems: the Agulhas Current, Somali Coastal Current, Red Sea, Arabian Sea, Bay of Bengal, Gulf of Thailand, West Central Australian Shelf, Northwest Australian Shelf, and Southwest Australian Shelf. Coral reefs cover c. 200,000 km2 (77,000 sq mi). The coasts of the Indian Ocean includes beaches and intertidal zones covering 3,000 km2 (1,200 sq mi) and 246 larger estuaries. Upwelling areas are small but important. The hypersaline salterns in India covers between 5,000–10,000 km2 (1,900–3,900 sq mi) and species adapted for this environment, such as Artemia salina and Dunaliella salina, are important to bird life. Coral reefs, sea grass beds, and mangrove forests are the most productive ecosystems of the Indian Ocean — coastal areas produce 20 tones per square kilometre of fish. These areas, however, are also being urbanised with populations often exceeding several thousand people per square kilometre and fishing techniques become more effective and often destructive beyond sustainable levels while increase in sea surface temperature spreads coral bleaching.Mangroves covers 80,984 km2 (31,268 sq mi) in the Indian Ocean region, or almost half of world's mangrove habitat, of which 42,500 km2 (16,400 sq mi) is located in Indonesia, or 50% of mangroves in the Indian Ocean. Mangroves originated in the Indian Ocean region and have adapted to a wide range of its habitats but it is also where it suffers its biggest loss of habitat.In 2016 six new animal species were identified at hydrothermal vents in the Southwest Indian Ridge: a "Hoff" crab, a "giant peltospirid" snail, a whelk-like snail, a limpet, a scaleworm and a polychaete worm.The West Indian Ocean coelacanth was discovered in the Indian Ocean off South Africa in the 1930s and in the late 1990s another species, the Indonesian coelacanth, was discovered off Sulawesi Island, Indonesia. Most extant coelacanths have been found in the Comoros. Although both species represent an order of lobe-finned fishes known from the Early Devonian (410 mya) and though extinct 66 mya, they are morphologically distinct from their Devonian ancestors. Over millions of years, coelacanths evolved to inhabit different environments — lungs adapted for shallow, brackish waters evolved into gills adapted for deep marine waters.
As the youngest of the major oceans, the Indian Ocean has active spreading ridges that are part of the worldwide system of mid-ocean ridges. In the Indian Ocean these spreading ridges meet at the Rodrigues Triple Point with the Central Indian Ridge, including the Carlsberg Ridge, separating the African Plate from the Indian Plate; the Southwest Indian Ridge separating the African Plate from the Antarctic Plate; and the Southeast Indian Ridge separating the Australian Plate from the Antarctic Plate. The Central Indian Ridge is intercepted by the Owen Fracture Zone. Since the late 1990s, however, it has become clear that this traditional definition of the Indo-Australian Plate cannot be correct; it consists of three plates — the Indian Plate, the Capricorn Plate, and Australian Plate — separated by diffuse boundary zones. Since 20 Ma the African Plate is being divided by the East African Rift System into the Nubian and Somalia plates.There are only two trenches in the Indian Ocean: the 6,000 km (3,700 mi)-long Java Trench between Java and the Sunda Trench and the 900 km (560 mi)-long Makran Trench south of Iran and Pakistan.A series of ridges and seamount chains produced by hotspots pass over the Indian Ocean. The Réunion hotspot (active 70–40 million years ago) connects Réunion and the Mascarene Plateau to the Chagos-Laccadive Ridge and the Deccan Traps in north-western India; the Kerguelen hotspot (100–35 million years ago) connects the Kerguelen Islands and Kerguelen Plateau to the Ninety East Ridge and the Rajmahal Traps in north-eastern India; the Marion hotspot (100–70 million years ago) possibly connects Prince Edward Islands to the Eighty Five East Ridge. These hotspot tracks have been broken by the still active spreading ridges mentioned above.There are fewer seamounts in the Indian Ocean than in the Atlantic and Pacific. These are typically deeper than 3,000 m (9,800 ft) and located north of 55°S and west of 80°E. Most originated at spreading ridges but some are now located in basins far away from these ridges. The ridges of the Indian Ocean form ranges of seamounts, sometimes very long, including the Carlsberg Ridge, Madagascar Ridge, Central Indian Ridge, Southwest Indian Ridge, Chagos-Laccadive Ridge, 85°E Ridge, 90°E Ridge, Southeast Indian Ridge, Broken Ridge, and East Indiaman Ridge. The Agulhas Plateau and Mascarene Plateau are the two major shallow areas.The opening of the Indian Ocean began c. 156 Ma when Africa separated from East Gondwana. The Indian Subcontinent began to separate from Australia-Antarctica 135–125 Ma and as the Tethys Ocean north of India began to close 118–84 Ma the Indian Ocean opened behind it.
The Indian Ocean, together with the Mediterranean, has connected people since ancient times, whereas the Atlantic and Pacific have had the roles of barriers or mare incognitum. The written history of the Indian Ocean, however, has been Eurocentric and largely dependent on the availability of written sources from the colonial era. This history is often divided into an ancient period followed by an Islamic period; the subsequent early modern and colonial/modern periods are often subdivided into Portuguese, Dutch, and British periods.A concept of an "Indian Ocean World" (IOW), similar to that of the "Atlantic World", exists but emerged much more recently and is not well established. The IOW is, nevertheless, sometimes referred to as the "first global economy" and was based on the monsoon which linked Asia, China, India, and Mesopotamia. It developed independently from the European global trade in the Mediterranean and Atlantic and remained largely independent from them until European 19th century colonial dominance.The diverse history of the Indian Ocean is a unique mix of cultures, ethnical groups, natural resources, and shipping routes. It grew in importance beginning in the 1960s and 1970s and after the Cold War it has undergone periods of political instability, most recently with the emergence of India and China as regional powers.
Pleistocene fossils of Homo erectus and other pre-H. sapiens homonin fossils, similar to H. heidelbergensis in Europe, have been found in India. According to the Toba catastrophe theory, a supereruption c. 74000 years ago at Lake Toba, Sumatra, covered India with volcanic ashes and wiped out one or more lineages of such archaic humans in India and Southeast Asia.The Out of Africa theory states that Homo sapiens spread from Africa into mainland Eurasia. The more recent Southern Dispersal or Coastal hypothesis instead advocates that modern humans spread along the coasts of the Arabic Peninsula and southern Asia. This hypothesis is supported by mtDNA research which reveals a rapid dispersal event during the Late Pleistocene (11,000 years ago). This coastal dispersal, however, began in East Africa 75,000 years ago and occurred intermittently from estuary to estuary along the northern perimetre of the Indian Ocean at rate of 0.7–4.0 km (0.43–2.49 mi) per year. It eventually resulted in modern humans migrating from Sunda over Wallacea to Sahul (Southeast Asia to Australia). Since then, waves of migration have resettled people and, clearly, the Indian Ocean littoral had been inhabited long before the first civilisations emerged. 5000–6000 years ago six distinct cultural centres had evolved around the Indian Ocean: East Africa, the Middle East, the Indian Subcontinent, South East Asia, the Malay World, and Australia; each interlinked to its neighbours.Food globalisation began on the Indian Ocean littoral c. 4.000 years ago. Five African crops — sorghum, pearl millet, finger millet, cowpea, and hyacinth bean — somehow found their way to Gujarat in India during the Late Harappan (2000–1700 BCE). Gujarati merchants evolved into the first explorers of the Indian Ocean as they traded African goods such as ivory, tortoise shells, and slaves. Broomcorn millet found its way from Central Asia to Africa, together with chicken and zebu cattle, although the exact timing is disputed. Around 2000 BCE black pepper and sesame, both native to Asia, appears in Egypt, albeit in small quantities. Around the same time the black rat and the house mouse emigrates from Asia to Egypt. Banana reached Africa around 3000 years ago.At least eleven prehistoric tsunamis have struck the Indian Ocean coast of Indonesia between 7400 and 2900 years ago. Analysing sand beds in caves in the Aceh region, scientists concluded that the intervals between these tsunamis have varied from series of minor tsunamis over a century to dormant periods of more than 2000 years preceding megathrusts in the Sunda Trench. Although the risk for future tsunamis is high, a major megathrust such as the one in 2004 is likely to be followed by a long dormant period.A group of scientists have argued that two large-scale impact events have occurred in the Indian Ocean: the Burckle Crater in the southern Indian Ocean in 2800 BCE and the Kanmare and Tabban craters in the Gulf of Carpentaria in northern Australia in 536 CE. Evidences for these impacts, the team argue, are micro-ejecta and Chevron dunes in southern Madagascar and in the Australian gulf. Geological evidences suggest the tsunamis caused by these impacts reached 205 m (673 ft) above sea level and 45 km (28 mi) inland. The impact events must have disrupted human settlements and perhaps even contributed to major climate changes.
The history of the Indian Ocean is marked by maritime trade; cultural and commercial exchange probably date back at least seven thousand years. Human culture spread early on the shores of the Indian Ocean and was always linked to the cultures of the Mediterranean and Persian Gulf. Before c. 2000 BCE, however, cultures on its shores were only loosely tied to each other; bronze, for example, was developed in Mesopotamia c. 3000 BCE but remained uncommon in Egypt before 1800 BCE. During this period, independent, short-distance oversea communications along its littoral margins evolved into an all-embracing network. The début of this network was not the achievement of a centralised or advanced civilisation but of local and regional exchange in the Persian Gulf, the Red Sea, and Arabian Sea. Sherds of Ubaid (2500–500 BCE) pottery have been found in the western Gulf at Dilmun, present-day Bahrain; traces of exchange between this trading centre and Mesopotamia. The Sumerians traded grain, pottery, and bitumen (used for reed boats) for copper, stone, timber, tin, dates, onions, and pearls. Coast-bound vessels transported goods between the Indus Valley Civilisation (2600–1900 BCE) in the Indian subcontinent (modern-day Pakistan and Northwest India) and the Persian Gulf and Egypt.The Red Sea, one of the main trade routes in Antiquity, was explored by Egyptians and Phoenicians during the last two millennia BCE. In the 6th century BCE Greek explorer Scylax of Caryanda made a journey to India, working for the Persian king Darius, and his now lost account put the Indian Ocean on the maps of Greek geographers. The Greeks began to explore the Indian Ocean following the conquests of Alexander the Great, who ordered a circumnavigation of the Arabian Peninsula in 323 BCE. During the two centuries that followed the reports of the explorers of Ptolemaic Egypt resulted in the best maps of the region until the Portuguese era many centuries later. The main interest in the region for the Ptolemies was not commercial but military; they explored Africa to hunt for war elephants.The Rub' al Khali desert isolates the southern parts of the Arabic Peninsula and the Indian Ocean from the Arabic world. This encouraged the development of maritime trade in the region linking the Red Sea and the Persian Gulf to East Africa and India. The monsoon (from mawsim, the Arabic word for season), however, was used by sailors long before being "discovered" by Hippalus in the 1st century. Indian wood have been found in Sumerian cities, there is evidence of Akkad coastal trade in the region, and contacts between India and the Red Sea dates back to the 2300 B.C.. The archipelagoes of the central Indian Ocean, the Laccadive and Maldive islands, were probably populated during the 2nd century B.C. from the Indian mainland. They appear in written history in the account of merchant Sulaiman al-Tajir in the 9th century but the treacherous reefs of the islands were most likely cursed by the sailors of Aden long before the islands were even settled.Periplus of the Erythraean Sea, an Alexandrian guide to the world beyond the Red Sea — including Africa and India — from the first century CE, not only gives insights into trade in the region but also shows that Roman and Greek sailors had already gained knowledge about the monsoon winds. The contemporaneous settlement of Madagascar by Austronesian sailors shows that the littoral margins of the Indian Ocean were being both well-populated and regularly traversed at least by this time. Albeit the monsoon must have been common knowledge in the Indian Ocean for centuries.The Indian Ocean's relatively calmer waters opened the areas bordering it to trade earlier than the Atlantic or Pacific oceans. The powerful monsoons also meant ships could easily sail west early in the season, then wait a few months and return eastwards. This allowed ancient Indonesian peoples to cross the Indian Ocean to settle in Madagascar around 1 CE.In the 2nd or 1st century BCE, Eudoxus of Cyzicus was the first Greek to cross the Indian Ocean. The probably fictitious sailor Hippalus is said to have learnt the direct route from Arabia to India around this time. During the 1st and 2nd centuries AD intensive trade relations developed between Roman Egypt and the Tamil kingdoms of the Cheras, Cholas and Pandyas in Southern India. Like the Indonesian people above, the western sailors used the monsoon to cross the ocean. The unknown author of the Periplus of the Erythraean Sea describes this route, as well as the commodities that were traded along various commercial ports on the coasts of the Horn of Africa and India circa 1 CE. Among these trading settlements were Mosylon and Opone on the Red Sea littoral.
Unlike the Pacific Ocean where the civilization of the Polynesians reached most of the far flung islands and atolls and populated them, almost all the islands, archipelagos and atolls of the Indian Ocean were uninhabited until colonial times. Although there were numerous ancient civilizations in the coastal states of Asia and parts of Africa, the Maldives were the only island group in the Central Indian Ocean region where an ancient civilization flourished. Maldivians, on their annual trade trip, took their oceangoing trade ships to Sri Lanka rather than mainland India, which is much closer, because their ships were dependent of the Indian Monsoon Current.Arabic missionaries and merchants began to spread Islam along the western shores of the Indian Ocean from the 8th century, if not earlier. A Swahili stone mosque dating to the 8th–15th centuries have been found in Shanga, Kenya. Trade across the Indian Ocean gradually introduced Arabic script and rice as a staple in Eastern Africa. Muslim merchants traded an estimated 1000 African slaves annually between 800 and 1700, a number that grew to c. 4000 during the 18th century, and 3700 during the period 1800–1870. Slave trade also occurred in the eastern Indian Ocean before the Dutch settled there around 1600 but the volume of this trade is unknown.From 1405 to 1433 admiral Zheng He said to have led large fleets of the Ming Dynasty on several treasure voyages through the Indian Ocean, ultimately reaching the coastal countries of East Africa.The Portuguese navigator Vasco da Gama rounded the Cape of Good Hope during his first voyage in 1497 and became the first European to sail to India. The Swahili people he encountered along the African eastcoast lived in a series of cities and had established trade routes to India and to China. Among them, the Portuguese kidnapped most of their pilots in coastal raids and onboard ships. A few of the pilots, however, were gifts by local Swahili rulers, including the sailor from Gujarat, a gift by a Malindi ruler in Kenya, who helped the Portuguese to reach India. In expeditions after 1500 the Portuguese attacked and colonised cities along the African coast. European slave trade in the Indian Ocean began when Portugal established Estado da Índia in the early 16th century. From then until the 1830s, c. 200 slaves were exported from Mozambique annually and similar figures has been estimated for slaves brought from Asia to the Philippines during the Iberian Union (1580–1640).The Ottoman Empire began its expansion into the Indian Ocean in 1517 with the conquest of Egypt under Sultan Selim I. Although the Ottomans shared the same religion as the trading communities in the Indian Ocean the region was unexplored by them. Maps that included the Indian Ocean had been produced by Muslim geographers centuries before the Ottoman conquests; Muslim scholars, such as Ibn Battuta in the 14th Century, had visited most parts of the known world; contemporarily with Vasco da Gama, Arab navigator Ahmad ibn Mājid had compiled a guide to navigation in the Indian Ocean; the Ottomans, nevertheless, began their own parallel era of discovery which rivaled the European expansion.The establishment of the Dutch East India Company in the early 17th century lead to a quick increase in trade volume; there were perhaps up to 500,000 slaves working in Dutch colonies during the 17th and 18th centuries mostly in the Indian Ocean. For example, some 4000 African slaves were used to build the Colombo fortress in Sri Lanka. Bali and neighbouring islands supplied regional networks with c. 100,000–150,000 slaves 1620–1830. Indian and Chinese traders supplied Dutch Indonesia with perhaps 250,000 slaves during 17th and 18th centuries.The British East India Company was established during the same period and in 1622 its ship first carried slaves from the Indian Coromandel Coast to Indonesia. The British mostly brought slaves from Africa and islands in the Indian Ocean to India and Indonesia but also exported slaves from India. The French colonised Réunion and Mauritius in 1721; by 1735 some 7200 slaves populated the Mascarene Islands, a number which had reached 133,000 in 1807. The British captured the islands in 1810, however, and because the British Parliament had prohibited slavery in 1807 a system of clandestine slave trade developed; resulting in 336,000–388,000 slaves exported to the Mascarane Islands 1670–1848.In all, Europeans traded 567,900–733,200 slaves within the Indian Ocean between 1500 and 1850 and almost that same amount were exported from the Indian Ocean to the Americas during the same period. Slave trade in the Indian Ocean was, nevertheless, very limited compared to c. 12,000,000 slaves exported across the Atlantic.
Scientifically, the Indian Ocean remained poorly explored before the International Indian Ocean Expedition in the early 1960s. However, the Challenger expedition 1872–1876 only reported from south of the polar front. The Valdivia expedition 1898–1899 made deep samples in the Indian Ocean. In the 1930s, the John Murray Expedition mainly studied shallow-water habitats. The Swedish Deep Sea Expedition 1947–1948 also sampled the Indian Ocean on its global tour and the Danish Galathea sampled deep-water fauna from Sri Lanka to South Africa on its second expedition 1950–1952. The Soviet research vessel Vityaz also did research in the Indian Ocean.The Suez Canal opened in 1869 when the Industrial Revolution dramatically changed global shipping – the sailing ship declined in importance as did the importance of European trade in favour of trade in East Asia and Australia. The construction of the canal introduced many non-indigenous species into the Mediterranean. For example, the goldband goatfish (Upeneus moluccensis) has replaced the red mullet (Mullus barbatus); since the 1980s huge swarms of scyphozoan jellyfish (Rhopilema nomadica) have affected tourism and fisheries along the Levantian coast and clogged power and desalination plants. Plans announced in 2014 to build a new, much larger Suez Canal parallel to the 19th century canal will most likely boost economy in the region but also cause ecological damage in a much wider area. Throughout the colonial era, islands such as Mauritius were important shipping nodes for the Dutch, French, and British. Mauritius, an inhabited island, became populated by slaves from Africa and indenture labour from India. The end of World War II marked the end of the colonial era. The British left Mauritius in 1974 and with 70% of the population of Indian descent, Mauritius became a close ally of India. In the 1980s, during the Cold War, the South African regime acted to destabilise several island nations in the Indian Ocean, including the Seychelles, Comoros, and Madagascar. India intervened in Mauritius to prevent a coup d'état, backed-up by the United States who feared the Soviet Union could gain access to Port Louis and threaten the U.S. base on Diego Garcia.Iranrud is an unrealised plan by Iran and the Soviet Union to build a canal between the Caspian Sea and Persian Gulf. Testimonies from the colonial era are stories of African slaves, Indian indentured labourers, and white settlers. But, while there was a clear racial line between free men and slaves in the Atlantic World, this delineation is less distinct in the Indian Ocean — there were Indian slaves and settlers as well as black indentured labourers. There were also a string of prison camps across the Indian Ocean, from Robben Island in South Africa to Cellular Jail in the Andamans, in which prisoners, exiles, POWs, forced labourers, merchants, and people of different faiths were forcefully united. On the islands of the Indian Ocean, therefore, a trend of creolisation emerged.On 26 December 2004 fourteen countries around the Indian Ocean were hit by a wave of tsunamis caused by the 2004 Indian Ocean earthquake. The waves radiated across the ocean at speeds exceeding 500 km/h (310 mph), reached up to 20 m (66 ft) in height, and resulted in an estimated 236,000 deaths.In the late 2000s the ocean evolved into a hub of pirate activity. By 2013, attacks off the Horn region's coast had steadily declined due to active private security and international navy patrols, especially by the Indian Navy.Malaysian Airlines Flight 370, a Boeing 777 airliner with 239 persons on board, disappeared on 8 March 2014 and is alleged to have crashed into the southeastern Indian Ocean about 2,000 km (1,200 mi) from the coast of southwest Western Australia. Despite an extensive search, the whereabouts of the remains of the aircraft are unknown.
The sea lanes in the Indian Ocean are considered among the most strategically important in the world with more than 80 percent of the world's seaborne trade in oil transits through the Indian Ocean and its vital chokepoints, with 40 percent passing through the Strait of Hormuz, 35 percent through the Strait of Malacca and 8 percent through the Bab el-Mandab Strait.The Indian Ocean provides major sea routes connecting the Middle East, Africa, and East Asia with Europe and the Americas. It carries a particularly heavy traffic of petroleum and petroleum products from the oil fields of the Persian Gulf and Indonesia. Large reserves of hydrocarbons are being tapped in the offshore areas of Saudi Arabia, Iran, India, and Western Australia. An estimated 40% of the world's offshore oil production comes from the Indian Ocean. Beach sands rich in heavy minerals, and offshore placer deposits are actively exploited by bordering countries, particularly India, Pakistan, South Africa, Indonesia, Sri Lanka, and Thailand. Chinese companies have made investments in several Indian Ocean ports, including Gwadar, Hambantota, Colombo and Sonadia. This has sparked a debate about the strategic implications of these investments. (See String of Pearls)
Indian Ocean in World War II Indian Ocean literature Indian Ocean Naval Symposium Indian Ocean Research Group List of islands in the Indian Ocean List of ports and harbours of the Indian Ocean List of sovereign states and dependent territories in the Indian Ocean Indian Ocean Rim Association Southern Ocean Antarctica Territorial claims in Antarctica Erythraean Sea
Frank Ocean (born October 28, 1987) is an American singer, songwriter, record producer, photographer, and visual artist. Recognized for his idiosyncratic musical style, introspective and elliptical songwriting, unconventional production techniques, and wide vocal range, Ocean is among the most acclaimed artists of his generation. Music critics have credited him with revitalizing jazz and funk influenced R&B, as well as advancing the genre through his experimental approach. He is considered a representative artist of alternative R&B.Ocean began his musical career as a ghostwriter, prior to joining the hip hop collective Odd Future in 2010. In 2011, Ocean released his critically successful debut mixtape Nostalgia, Ultra and subsequently secured a recording contract with Def Jam Recordings. Drawing on electro-funk, pop-soul, jazz-funk, and psychedelia, Ocean's debut studio album Channel Orange (2012) was one of the most acclaimed albums of 2012. It was nominated for Album of the Year and won Best Urban Contemporary Album at the 2013 Grammy Awards, while the album's hit single "Thinkin Bout You" garnered Ocean a nomination for Record of the Year. Following four years of recluse, Ocean released the visual project Endless in 2016 in order to fulfill contractual obligations with Def Jam before releasing his highly anticipated second album Blonde (2016) independently the next day. Blonde debuted at number one on the US Billboard 200 and was certified platinum by the Recording Industry Association of America (RIAA). Encompassing avant-garde, soul, and psychedelic rock, the album was acclaimed by critics and Ocean was praised for challenging the conventions of contemporary R&B and pop music.Among Ocean's awards are two Grammy Awards, a Brit Award for International Male Solo Artist in 2013 and an NME Award for Best International Male Artist in 2017. He was included in the 2013 edition of the Time's list of the 100 most influential people in the world and the 2017 edition of the Forbes 30 Under 30. Both Insider and The Wall Street Journal regarded Ocean as the most dominant artist of the 2010s decade. As a photographer, he worked with Vogue at the annual Met Gala and the British fashion magazine i-D. Premiered in 2017, he also has his own Beats 1 radio show, Blonded Radio, that often premiers his new singles.
Ocean was born on October 28, 1987, in Long Beach, California. When he was five years old, he and his family relocated to New Orleans. Ocean was first introduced to music through his mother, who would often play jazz music on her car stereo, as well as albums by Celine Dion and Anita Baker and the soundtrack to The Phantom of the Opera. He later frequented New Orleans jazz bars and parlors, which encouraged him to begin recording his own music. In order to raise funds for recording time, he performed several jobs as a teenager such as washing cars, mowing lawns, and walking his neighbors' dogs. After graduating from John Ehret High School in New Orleans in 2005, Ocean enrolled in the University of New Orleans to study English. However, Hurricane Katrina struck New Orleans in August 2005, destroying his home and personal recording facility and forcing him to transfer to the University of Louisiana at Lafayette. He stayed there for a brief time before dropping out to focus on his music career.
Ocean released the cover art for his debut studio album's lead single, titled "Thinkin Bout You", revealing the song would be released to digital retailers on April 10, 2012. However, a month earlier, a re-mastered version of the song had already leaked. About the prospective single he said: "It succinctly defines me as an artist for where I am right now and that was the aim," he said of the follow-up to his acclaimed Nostalgia, Ultra. "It's about the stories. If I write 14 stories that I love, then the next step is to get the environment of music around it to best envelop the story and all kinds of sonic goodness." In 2012, Ocean released his debut studio album Channel Orange to universal acclaim from critics, who later named it the best album of the year in the HMV's Poll of Polls. It also earned Ocean six Grammy Award nominations and was credited by some writers for moving the R&B genre in a different, more challenging direction. Considered as Ocean's first commercial release on a traditional record label, Channel Orange featured unconventional songs that were noted for their storytelling and social commentary, and a dense musical fusion that drew on jazz, soul, and R&B. Funk and electronic music also influenced his album. The songs about unrequited love in particular received the most attention, partly because of Ocean's announcement prior to the album's release, when he revealed that his first love was a man. The announcement made global headlines, and some critics compared its cultural impact to when David Bowie revealed that he was bisexual in 1972. Channel Orange debuted at number two on the Billboard 200 and sold 131,000 copies in its first week. The majority of its first-week sales were digital copies from iTunes, while approximately 3,000 of the sales were physical copies. On January 30, Channel Orange was certified gold by the Recording Industry Association of America (RIAA). By September 2014, it had sold 621,000 copies, according to Nielsen SoundScan. Ocean promoted the album with his 2012 Summer Tour, which featured final appearances at the Coachella and Lollapalooza festivals. At the 2013 Brit Awards, Ocean won the Brit Award for International Male Solo Artist. On May 28, 2013, Ocean announced the You're Not Dead ... 2013 Tour; a fourteen-date European and Canadian tour that began on June 16, 2013, in Munich. He had been scheduled to perform at the first night of OVO Fest on August 4, 2013; however he was forced to cancel his appearance due to a small vocal cord injury. The first night of the music festival was subsequently cancelled and James Blake was booked to appear during the second night as Ocean's replacement. Ocean appeared on John Mayer's album Paradise Valley, as a featured artist on a song called "Wildfire".
In February 2013, Ocean confirmed that he had started work on his second studio album, which he confirmed would be another concept album. He revealed that he was working with Tyler, the Creator, Pharrell Williams, and Danger Mouse on the record. He later stated that he was being influenced by The Beach Boys and The Beatles. He stated he was interested in collaborating with Tame Impala and King Krule and that he would record part of the album in Bora Bora.On March 10, 2014, the song "Hero" was made available for free download on SoundCloud. The song is a collaboration with Mick Jones, Paul Simonon and Diplo and is a part of Converse's Three Artists. One Song series.In April 2014, Ocean stated that his second album was nearly finished. In June, Billboard reported that the singer was working with a string of artists such as Happy Perez (whom he worked with on nostalgia, ULTRA), Charlie Gambetta and Kevin Ristro, while producers Hit-Boy, Rodney Jerkins and Danger Mouse were also said to be on board. On November 29, 2014, Ocean released a snippet of a new song supposedly from his upcoming follow-up to channel ORANGE called "Memrise" on his official Tumblr page. The Guardian described the song as: "...a song which affirms that despite reportedly changing labels and management, he has maintained both his experimentation and sense of melancholy in the intervening years". On April 6, 2015, Ocean announced that his follow-up to channel ORANGE would be released in July with "two versions", as well as a publication, although no further details were released. The album was ultimately not released in July, with no explanation given for its delay. The publication was rumoured to be called Boys Don't Cry, and the album was slated to feature the aforementioned "Memrise". In February 2016, Ocean was featured on Kanye West's album The Life of Pablo on the track "Wolves" along with Vic Mensa and Sia Furler. A month later, the song was re-edited by West, and Ocean's part was separated and listed on the track list as its own song titled "Frank's Track."In July 2016, he hinted at a possible second album with an image on his website pointing to a July release date. The image shows a library card labeled Boys Don't Cry with numerous stamps, implying various due dates. The dates begin with July 2, 2015 and conclude with July 2016. Ocean's brother, Ryan Breaux, further suggested this release with an Instagram caption of the same library card photo reading BOYS DON'T CRY #JULY2016.By August 1, 2016, at approximately 3 a.m., an endless live stream shot in negative lighting in what is allegedly a Brooklyn warehouse, sponsored by Apple Music began to surface on boysdontcry.co which appeared to show Ocean woodworking and sporadically playing instrumentals on loop. It later became clear that these instrumentals were from his upcoming visual album Endless; the full version is estimated to be 140 hours long. That same day, many news outlets reported that August 5, 2016 could be the release date for Boys Don't Cry. That date also turned out to be inaccurate, though in a Reddit AMA session, his collaborator Malay said that Ocean is a perfectionist, constantly tweaking things, and that his art cannot be rushed.On August 18 and 19, 2016, the live stream was accompanied with music and at midnight an Apple Music link was directed to a project called Endless. Endless would be Ocean's last album with Def Jam Recordings to fulfill his contract with the record label. Before the visual album's release on Apple Music, Ocean had already begun making efforts to part ways with Def Jam, who signed the artist in 2009. He describes his negotiations with the label as a "seven-year chess game", while adding that he had replaced many of his representatives (including his lawyer and manager) during the process, as well as having to buy back all of his master recordings that previously belonged to Def Jam.At midnight Pacific time on August 20, 2016, a music video for a song titled "Nikes" was uploaded to Ocean's Connect page on Apple Music and later to his own website. Also on August 20, Ocean announced pop-up shops in Los Angeles, New York City, Chicago, and London for his magazine Boys Don't Cry, and released his second studio album Blonde to widespread acclaim. Blonde debuted at number one in several countries, including the United States and the United Kingdom, and recorded sales of 232,000 copies (275,000 with album-equivalent units) in its first week. Rather than going on a typical promotional tour playing radio festivals and appearing on television shows, Ocean spent a month after the release of Blonde, traveling to countries such as China, Japan and France. He also chose not to submit Blonde for consideration at the Grammy Awards, stating "that institution certainly has nostalgic importance; it just doesn't seem to be representing very well for people who come from where I come from, and hold down what I hold down." Time ranked it as the best album of 2016 on its year-end list. Forbes estimated that Blonde earned Ocean nearly one million in profits after one week of availability, attributing this to him releasing the album independently and as a limited exclusive release on iTunes and Apple Music. On July 9, 2018, Blonde was certified platinum by the Recording Industry Association of America (RIAA).
Ocean's music has been characterized by music writers as idiosyncratic in style. His music generally includes the electronic keyboard, often performed by Ocean himself, and is backed by a subdued rhythm section in the production. His compositions are often midtempo, feature unconventional melodies, and occasionally have an experimental song structure. He has been characterised as both an "avant-garde R&B artist" and a "pop musician". In his songwriting, Jon Pareles of The New York Times observes "open echoes of self-guided, innovative R&B songwriters like Prince, Stevie Wonder, Marvin Gaye, Maxwell, Erykah Badu and particularly R. Kelly and his way of writing melodies that hover between speech and song, asymmetrical and syncopated." Jody Rosen of Rolling Stone calls him a torch singer due to "his feel for romantic tragedy, unfurling in slow-boiling ballads". Ocean's stage presence during live shows has been described by Chris Richards of the Washington Post as "low-key". While nostalgia, ULTRA featured both original music by Ocean and tracks relying on sampled melodies, channel ORANGE showcased Ocean as the primary musical composer, of which music journalist Robert Christgau opines, "when he's the sole composer Ocean resists making a show of himself—resists the dope hook, the smart tempo, the transcendent falsetto itself."Ocean's lyrics deal with themes of love, longing, misgiving, and nostalgia. His debut single "Novacane" juxtaposes the numbness and artificiality of a sexual relationship with that of mainstream radio, while "Voodoo" merges themes of spirituality and sexuality, and is an eccentric take on such subject matter common in R&B. The latter song was released by Ocean on his Tumblr account and references both the traditional spiritual "He's Got the Whole World in His Hands" and the female anatomy in its chorus: "she's got the whole wide world in her juicy fruit / he's got the whole wide world in his pants / he wrapped the whole wide world in a wedding band / then put the whole wide world on her hands / she's got the whole wide world in her hands / he's got the whole wide world in his hands." Certain songs on channel ORANGE allude to Ocean's experience with unrequited love.
Ocean is among the most acclaimed artists of his generation. Music critics have credited him with revitalizing pre-contemporary R&B, as well as approaching the genre differently to his contemporaries through his use of other genres, including avant-garde, electro, rock and psychedelic. His distinctive sound and style have influenced numerous artists of various music genres. Both Insider and The Wall Street Journal regarded Ocean as the most dominant artist of the 2010s decade. He was included in the 2013 edition of the Time's list of the 100 most influential people in the world and the 2017 edition of the Forbes 30 Under 30. Andy Kellman of AllMusic wrote,"Frank Ocean has been one of the more fascinating figures in contemporary music since his early-2010s arrival. A singer and songwriter whose artful output has defied rigid classification as R&B, he has nonetheless pushed that genre forward with seemingly offhanded yet imaginatively detailed narratives in which he has alternated between yearning romantic and easygoing braggart."Culture critic Nelson George asserts that, along with Miguel, Ocean has "staked out ground where [he is] not competing with those hit-driven [commercial R&B] acts" and is "cultivating a sound that balances adult concerns with a sense of young men trying to understand their own desires (an apt description of Ocean, particularly)." Writing for Insider, Callie Ahlgrim said that Ocean "changed our very understanding of modern music", and that he discusses themes like youth, innocence, lost love, loneliness, desire, and mortality in his music in a way that "feels fresh and extraordinary [and] makes the introspective sound universal and transcendent [which] is why he's one of the defining artists of our time." Jacob Shamsian of Business Insider said that Ocean "isn't just one of the most important artists in pop, he's one of the most important artists in all of music." In a GQ article titled 'Why Frank Ocean is a musical icon', Jon Savage described Ocean as "one of the pop elite", a "true pop star of today", and a "consummate contemporary artist in every sense who is immersed in new sonic possibilities, one who is deeply committed to artistic exploration in the most profound sense." Savage praised Ocean for taking R&B to a "new level [through] constructing startling sound pictures that fit his lyrics." Pitchfork regarded Ocean as a "master of confessional songwriting, earning a cult-icon status with his enigmatic persona and idiosyncratic approach to pop."
On August 20, 2016, Ocean released a 360-page magazine, Boys Don't Cry, alongside his second album Blonde. The fashion and automobile-themed publication contains the photo projects from Wolfgang Tillmans, Viviane Sassen, Tyrone Lebon, Ren Hang, Harley Weir, Michael Mayren and Ocean himself. Four months later, British magazine Print published another photowork from Frank Ocean.On May 1, 2017, Ocean attended annual Met Gala as a special photographer for Vogue. On October 23, 2017, he made two covers and a visual essay for British fashion magazine, i-D.
Ocean wrote an open letter, initially intended for the liner notes on Channel Orange, that preemptively addressed speculation about his attraction in the past to another man. Instead, on July 4, 2012, he published an open letter on his Tumblr blog recounting unrequited feelings he had for another young man when he was 19 years old, citing it as his first true love. He used the blog to thank the man for his influence, and also thanked his mother and other friends, saying, "I don't know what happens now, and that's alright. I don't have any secrets I need kept anymore... I feel like a free man." Numerous celebrities publicly voiced their support for Ocean following his announcement, including Beyoncé and Jay-Z. Members of the hip hop industry generally responded positively to the announcement. Tyler, the Creator and other members of OFWGKTA tweeted their support for Ocean. Russell Simmons wrote a congratulatory article in Global Grind in which he said, "Today is a big day for hip-hop. It is a day that will define who we really are. How compassionate will we be? How loving can we be? How inclusive are we? [...] Your decision to go public about your sexual orientation gives hope and light to so many young people still living in fear."In June 2016, following the Orlando nightclub shooting that killed 49 people, Ocean published an essay expressing his sadness and frustration. He mentioned that his first experience with homophobia and transphobia was with his father when he was six years old, and related how many people pass on their hateful ideals to the next generation and send thousands of people down suicidal paths. In 2017, Ocean's father subsequently sued him for defamation and requested $14.5 million. On October 17, 2017, after a hearing that saw Ocean and both of his parents taking the stand, the presiding judge ruled in favor of Ocean, stating that his father had not provided sufficient evidence of defamation.
In a 2011 interview, Ocean stated that he had attempted to change his name to Christopher Francis Ocean through a legal website on his 23rd birthday. The change was reportedly partly inspired by the 1960 film Ocean's 11. In March 2014, it was reported that he was legally changing his name to Frank Ocean. In November 2014, it was revealed that the name change had not been legalized due to multiple speeding offenses. It was finally legalized on April 23, 2015.
Ocean sampled the music from the Eagles' song "Hotel California" on the song "American Wedding" from Nostalgia, Ultra. When asked about it, Ocean stated that Eagles band member "Don Henley is apparently intimidated by my rendition of 'Hotel California'. He threatened to sue if I perform it again." In response to Ocean's comments, the Eagles' legal representative released a statement: "Frank Ocean did not merely 'sample' a portion of the Eagles' 'Hotel California,' he took the entire master track, plus the song's existing melody, and replaced the lyrics with his own; this is not creative, let alone 'intimidating.' It's illegal. For the record, Don Henley has not threatened or instituted any legal action against Frank Ocean, although the Eagles are now considering whether they should." Chris Richards of The Washington Post remarked that "certain boomers don't like Ocean as much" as "information-age babies" due to the controversy.On March 7, 2014, Ocean was sued by Chipotle Mexican Grill for them to recoup the money they paid him in advance for a commercial that he backed out of because he objected to material in the advertisement. The advertisement was to feature Ocean singing the song "Pure Imagination", and was to promote sustainable farming. Ocean backed out of the spot when Chipotle refused to remove their logo and name from the advertisement. The lawsuit was dropped on March 20 after Ocean paid the advance back in full. The commercial, titled The Scarecrow, was ultimately released with Fiona Apple performing the song.
Interview with music video director Zach Merck
The Arctic Ocean is the smallest and shallowest of the world's five major oceans. It is also known as the coldest of all the oceans. The International Hydrographic Organization (IHO) recognizes it as an ocean, although some oceanographers call it the Arctic Mediterranean Sea. It is sometimes classified as an estuary of the Atlantic Ocean, and it is also seen as the northernmost part of the all-encompassing World Ocean. The Arctic Ocean includes the North Pole region in the middle of the Northern Hemisphere, and extends south to about 60°N. The Arctic Ocean is surrounded by Eurasia and North America, and the borders follow topographic features; the Bering Strait on the Pacific side, and the Greenland Scotland Ridge on the Atlantic side. It is mostly covered by sea ice throughout the year and almost completely in winter. The Arctic Ocean's surface temperature and salinity vary seasonally as the ice cover melts and freezes; its salinity is the lowest on average of the five major oceans, due to low evaporation, heavy fresh water inflow from rivers and streams, and limited connection and outflow to surrounding oceanic waters with higher salinities. The summer shrinking of the ice has been quoted at 50%. The US National Snow and Ice Data Center (NSIDC) uses satellite data to provide a daily record of Arctic sea ice cover and the rate of melting compared to an average period and specific past years, showing a continuous decline in sea ice extent. In September 2012, the Arctic ice extent reached a new record minimum. Compared to the average extent (1979-2000), the sea ice had diminished by 49%.
Human habitation in the North American polar region goes back at least 50,000–17,000 years ago, during the Wisconsin glaciation. At this time, falling sea levels allowed people to move across the Bering land bridge that joined Siberia to northwestern North America (Alaska), leading to the Settlement of the Americas. Paleo-Eskimo groups included the Pre-Dorset (c. 3200–850 BC); the Saqqaq culture of Greenland (2500–800 BC); the Independence I and Independence II cultures of northeastern Canada and Greenland (c. 2400–1800 BC and c. 800–1 BC); the Groswater of Labrador and Nunavik, and the Dorset culture (500 BC to AD 1500), which spread across Arctic North America. The Dorset were the last major Paleo-Eskimo culture in the Arctic before the migration east from present-day Alaska of the Thule, the ancestors of the modern Inuit.The Thule Tradition lasted from about 200 BC to AD 1600 around the Bering Strait, the Thule people being the prehistoric ancestors of the Inuit who now live in Northern Labrador.For much of European history, the north polar regions remained largely unexplored and their geography conjectural. Pytheas of Massilia recorded an account of a journey northward in 325 BC, to a land he called "Eschate Thule", where the Sun only set for three hours each day and the water was replaced by a congealed substance "on which one can neither walk nor sail". He was probably describing loose sea ice known today as "growlers" or "bergy bits"; his "Thule" was probably Norway, though the Faroe Islands or Shetland have also been suggested. Early cartographers were unsure whether to draw the region around the North Pole as land (as in Johannes Ruysch's map of 1507, or Gerardus Mercator's map of 1595) or water (as with Martin Waldseemüller's world map of 1507). The fervent desire of European merchants for a northern passage, the Northern Sea Route or the Northwest Passage, to "Cathay" (China) caused water to win out, and by 1723 mapmakers such as Johann Homann featured an extensive "Oceanus Septentrionalis" at the northern edge of their charts. The few expeditions to penetrate much beyond the Arctic Circle in this era added only small islands, such as Novaya Zemlya (11th century) and Spitzbergen (1596), though since these were often surrounded by pack-ice, their northern limits were not so clear. The makers of navigational charts, more conservative than some of the more fanciful cartographers, tended to leave the region blank, with only fragments of known coastline sketched in. This lack of knowledge of what lay north of the shifting barrier of ice gave rise to a number of conjectures. In England and other European nations, the myth of an "Open Polar Sea" was persistent. John Barrow, longtime Second Secretary of the British Admiralty, promoted exploration of the region from 1818 to 1845 in search of this. In the United States in the 1850s and 1860s, the explorers Elisha Kane and Isaac Israel Hayes both claimed to have seen part of this elusive body of water. Even quite late in the century, the eminent authority Matthew Fontaine Maury included a description of the Open Polar Sea in his textbook The Physical Geography of the Sea (1883). Nevertheless, as all the explorers who travelled closer and closer to the pole reported, the polar ice cap is quite thick, and persists year-round. Fridtjof Nansen was the first to make a nautical crossing of the Arctic Ocean, in 1896. The first surface crossing of the ocean was led by Wally Herbert in 1969, in a dog sled expedition from Alaska to Svalbard, with air support. The first nautical transit of the north pole was made in 1958 by the submarine USS Nautilus, and the first surface nautical transit occurred in 1977 by the icebreaker NS Arktika. Since 1937, Soviet and Russian manned drifting ice stations have extensively monitored the Arctic Ocean. Scientific settlements were established on the drift ice and carried thousands of kilometers by ice floes.In World War II, the European region of the Arctic Ocean was heavily contested: the Allied commitment to resupply the Soviet Union via its northern ports was opposed by German naval and air forces. Since 1954 commercial airlines have flown over the Arctic Ocean (see Polar route).
There are several ports and harbors around the Arctic Ocean
The ocean's Arctic shelf comprises a number of continental shelves, including the Canadian Arctic shelf, underlying the Canadian Arctic Archipelago, and the Russian continental shelf, which is sometimes simply called the "Arctic Shelf" because it is greater in extent. The Russian continental shelf consists of three separate, smaller shelves, the Barents Shelf, Chukchi Sea Shelf and Siberian Shelf. Of these three, the Siberian Shelf is the largest such shelf in the world. The Siberian Shelf holds large oil and gas reserves, and the Chukchi shelf forms the border between Russian and the United States as stated in the USSR–USA Maritime Boundary Agreement. The whole area is subject to international territorial claims.
The crystalline basement rocks of mountains around the Arctic Ocean were recrystallized or formed during the Ellesmerian orogeny, the regional phase of the larger Caledonian orogeny in the Paleozoic. Regional subsidence in the Jurassic and Triassic led to significant sediment deposition, creating many of the reservoir for current day oil and gas deposits. During the Cretaceous the Canadian Basin opened and tectonic activity due to the assembly of Alaska caused hydrocarbons to migrate toward what is now Prudhoe Bay. At the same time, sediments shed off the rising Canadian Rockies building out the large Mackenzie Delta. The rifting apart of the supercontinent Pangea, beginning in the Triassic opened the early Atlantic Ocean. Rifting then extended northward, opening the Arctic Ocean as mafic oceanic crust material erupted out of a branch of Mid-Atlantic Ridge. The Amerasia Basin may have opened first, with the Chulkchi Borderland moved along to the northeast by transform faults. Additional spreading helped to create the "triple-junction" of the Alpha-Mendeleev Ridge in the Late Cretaceous. Throughout the Cenozoic, the subduction of the Pacific plate, the collision of India with Eurasia and the continued opening of the North Atlantic created new hydrocarbon traps. The seafloor began spreading from the Gakkel Ridge in the Paleocene and Eocene, causing the Lomonosov Ridge to move farther from land and subside. Because of sea ice and remote conditions, the geology of the Arctic Ocean is still poorly explored. ACEX drilling shed some light on the Lomonosov Ridge, which appears to be continental crust separated from the Barents-Kara Shelf in the Paleocene and then starved of sediment. It may contain up to 10 billion barrels of oil. The Gakkel Ridge rift is also poorly understand and may extend into the Laptev Sea.
In large parts of the Arctic Ocean, the top layer (about 50 m [160 ft]) is of lower salinity and lower temperature than the rest. It remains relatively stable, because the salinity effect on density is bigger than the temperature effect. It is fed by the freshwater input of the big Siberian and Canadian streams (Ob, Yenisei, Lena, Mackenzie), the water of which quasi floats on the saltier, denser, deeper ocean water. Between this lower salinity layer and the bulk of the ocean lies the so-called halocline, in which both salinity and temperature rise with increasing depth. Because of its relative isolation from other oceans, the Arctic Ocean has a uniquely complex system of water flow. It resembles some hydrological features of the Mediterranean Sea, referring to its deep waters having only limited communication through the Fram Strait with the Atlantic Basin, "where the circulation is dominated by thermohaline forcing”. The Arctic Ocean has a total volume of 18.07×106 km3, equal to about 1.3% of the World Ocean. Mean surface circulation is predominately cyclonic on the Eurasian side and anticyclonic in the Canadian Basin.Water enters from both the Pacific and Atlantic Oceans and can be divided into three unique water masses. The deepest water mass is called Arctic Bottom Water and begins around 900 metres (3,000 feet) depth. It is composed of the densest water in the World Ocean and has two main sources: Arctic shelf water and Greenland Sea Deep Water. Water in the shelf region that begins as inflow from the Pacific passes through the narrow Bering Strait at an average rate of 0.8 Sverdrups and reaches the Chukchi Sea. During the winter, cold Alaskan winds blow over the Chukchi Sea, freezing the surface water and pushing this newly formed ice out to the Pacific. The speed of the ice drift is roughly 1–4 cm/s. This process leaves dense, salty waters in the sea that sink over the continental shelf into the western Arctic Ocean and create a halocline. This water is met by Greenland Sea Deep Water, which forms during the passage of winter storms. As temperatures cool dramatically in the winter, ice forms and intense vertical convection allows the water to become dense enough to sink below the warm saline water below. Arctic Bottom Water is critically important because of its outflow, which contributes to the formation of Atlantic Deep Water. The overturning of this water plays a key role in global circulation and the moderation of climate. In the depth range of 150–900 metres (490–2,950 feet) is a water mass referred to as Atlantic Water. Inflow from the North Atlantic Current enters through the Fram Strait, cooling and sinking to form the deepest layer of the halocline, where it circles the Arctic Basin counter-clockwise. This is the highest volumetric inflow to the Arctic Ocean, equalling about 10 times that of the Pacific inflow, and it creates the Arctic Ocean Boundary Current. It flows slowly, at about 0.02 m/s. Atlantic Water has the same salinity as Arctic Bottom Water but is much warmer (up to 3 °C [37 °F]). In fact, this water mass is actually warmer than the surface water, and remains submerged only due to the role of salinity in density. When water reaches the basin it is pushed by strong winds into a large circular current called the Beaufort Gyre. Water in the Beaufort Gyre is far less saline than that of the Chukchi Sea due to inflow from large Canadian and Siberian rivers.The final defined water mass in the Arctic Ocean is called Arctic Surface Water and is found from 150–200 metres (490–660 feet). The most important feature of this water mass is a section referred to as the sub-surface layer. It is a product of Atlantic water that enters through canyons and is subjected to intense mixing on the Siberian Shelf. As it is entrained, it cools and acts a heat shield for the surface layer. This insulation keeps the warm Atlantic Water from melting the surface ice. Additionally, this water forms the swiftest currents of the Arctic, with speed of around 0.3–0.6 m/s. Complementing the water from the canyons, some Pacific water that does not sink to the shelf region after passing through the Bering Strait also contributes to this water mass. Waters originating in the Pacific and Atlantic both exit through the Fram Strait between Greenland and Svalbard Island, which is about 2,700 metres (8,900 feet) deep and 350 kilometres (220 miles) wide. This outflow is about 9 Sv. The width of the Fram Strait is what allows for both inflow and outflow on the Atlantic side of the Arctic Ocean. Because of this, it is influenced by the Coriolis force, which concentrates outflow to the East Greenland Current on the western side and inflow to the Norwegian Current on the eastern side. Pacific water also exits along the west coast of Greenland and the Hudson Strait (1–2 Sv), providing nutrients to the Canadian Archipelago.As noted, the process of ice formation and movement is a key driver in Arctic Ocean circulation and the formation of water masses. With this dependence, the Arctic Ocean experiences variations due to seasonal changes in sea ice cover. Sea ice movement is the result of wind forcing, which is related to a number of meteorological conditions that the Arctic experiences throughout the year. For example, the Beaufort High—an extension of the Siberian High system—is a pressure system that drives the anticyclonic motion of the Beaufort Gyre. During the summer, this area of high pressure is pushed out closer to its Siberian and Canadian sides. In addition, there is a sea level pressure (SLP) ridge over Greenland that drives strong northerly winds through the Fram Strait, facilitating ice export. In the summer, the SLP contrast is smaller, producing weaker winds. A final example of seasonal pressure system movement is the low pressure system that exists over the Nordic and Barents Seas. It is an extension of the Icelandic Low, which creates cyclonic ocean circulation in this area. The low shifts to center over the North Pole in the summer. These variations in the Arctic all contribute to ice drift reaching its weakest point during the summer months. There is also evidence that the drift is associated with the phase of the Arctic Oscillation and Atlantic Multidecadal Oscillation.
Much of the Arctic Ocean is covered by sea ice that varies in extent and thickness seasonally. The mean extent of the Arctic sea ice has been continuously decreasing in the last decades, declining at a rate of currently 12.85% per decade since 1980 from the average winter value of 15,600,000 km2 (6,023,200 sq mi). The seasonal variations are about 7,000,000 km2 (2,702,700 sq mi) with the maximum in April and minimum in September. The sea ice is affected by wind and ocean currents, which can move and rotate very large areas of ice. Zones of compression also arise, where the ice piles up to form pack ice.Icebergs occasionally break away from northern Ellesmere Island, and icebergs are formed from glaciers in western Greenland and extreme northeastern Canada. Icebergs are not sea ice but may become embedded in the pack ice. Icebergs pose a hazard to ships, of which the Titanic is one of the most famous. The ocean is virtually icelocked from October to June, and the superstructure of ships are subject to icing from October to May. Before the advent of modern icebreakers, ships sailing the Arctic Ocean risked being trapped or crushed by sea ice (although the Baychimo drifted through the Arctic Ocean untended for decades despite these hazards).
Due to the pronounced seasonality of 2–6 months of midnight sun and polar night in the Arctic Ocean, the primary production of photosynthesizing organisms such as ice algae and phytoplankton is limited to the spring and summer months (March/April to September). Important consumers of primary producers in the central Arctic Ocean and the adjacent shelf seas include zooplankton, especially copepods (Calanus finmarchicus, Calanus glacialis, and Calanus hyperboreus) and euphausiids, as well as ice-associated fauna (e.g., amphipods). These primary consumers form an important link between the primary producers and higher trophic levels. The composition of higher trophic levels in the Arctic Ocean varies with region (Atlantic side vs. Pacific side), and with the sea-ice cover. Secondary consumers in the Barents Sea, an Atlantic-influenced Arctic shelf sea, are mainly sub-Arctic species including herring, young cod, and capelin. In ice-covered regions of the central Arctic Ocean, polar cod is a central predator of primary consumers. The apex predators in the Arctic Ocean - Marine mammals such as seals, whales, and polar bears, prey upon fish. Endangered marine species in the Arctic Ocean include walruses and whales. The area has a fragile ecosystem, and it is especially exposed to climate change, because it warms faster than the rest of the world. Lion's mane jellyfish are abundant in the waters of the Arctic, and the banded gunnel is the only species of gunnel that lives in the ocean.
Petroleum and natural gas fields, placer deposits, polymetallic nodules, sand and gravel aggregates, fish, seals and whales can all be found in abundance in the region.The political dead zone near the center of the sea is also the focus of a mounting dispute between the United States, Russia, Canada, Norway, and Denmark. It is significant for the global energy market because it may hold 25% or more of the world's undiscovered oil and gas resources.
The Arctic ice pack is thinning, and a seasonal hole in the ozone layer frequently occurs. Reduction of the area of Arctic sea ice reduces the planet's average albedo, possibly resulting in global warming in a positive feedback mechanism. Research shows that the Arctic may become ice-free in the summer for the first time in human history by 2040. Estimates vary for when the last time the Arctic was ice-free: 65 million years ago when fossils indicate that plants existed there to as recently as 5,500 years ago; ice and ocean cores going back 8,000 years to the last warm period or 125,000 during the last intraglacial period.Warming temperatures in the Arctic may cause large amounts of fresh meltwater to enter the north Atlantic, possibly disrupting global ocean current patterns. Potentially severe changes in the Earth's climate might then ensue.As the extent of sea ice diminishes and sea level rises, the effect of storms such as the Great Arctic Cyclone of 2012 on open water increases, as does possible salt-water damage to vegetation on shore at locations such as the Mackenzie's river delta as stronger storm surges become more likely.Global warming has increased encounters between polar bears and humans. Reduced sea ice due to melting is causing polar bears to search for new sources of food. Beginning in December 2018 and coming to an apex in February 2019, a mass invasion of polar bears into the archipelago of Novaya Zemlya caused local authorities to declare a state of emergency. Dozens of polar bears were seen entering homes and public buildings and inhabited areas.
Sea ice, and the cold conditions it sustains, serves to stabilize methane deposits on and near the shoreline, preventing the clathrate breaking down and outgassing methane into the atmosphere, causing further warming. Melting of this ice may release large quantities of methane, a powerful greenhouse gas into the atmosphere, causing further warming in a strong positive feedback cycle and marine genera and species to become extinct.
Other environmental concerns relate to the radioactive contamination of the Arctic Ocean from, for example, Russian radioactive waste dump sites in the Kara Sea Cold War nuclear test sites such as Novaya Zemlya, Camp Century's contaminants in Greenland, or radioactive contamination from Fukushima.On 16 July 2015, five nations (United States, Russia, Canada, Norway, Denmark/Greenland) signed a declaration committing to keep their fishing vessels out of a 1.1 million square mile zone in the central Arctic Ocean near the North Pole. The agreement calls for those nations to refrain from fishing there until there is better scientific knowledge about the marine resources and until a regulatory system is in place to protect those resources.
The Hidden Ocean Arctic 2005 Daily logs, photos and video from exploration mission. Oceanography Image of the Day, from the Woods Hole Oceanographic Institution Arctic Council The Northern Forum Arctic Environmental Atlas Interactive map NOAA Arctic Theme Page Arctic Great Rivers Observatory (ArcticGRO) "Arctic Ocean". The World Factbook. Central Intelligence Agency. Daily Arctic Ocean Rawinsonde Data from Soviet Drifting Ice Stations (1954–1990) at NSIDC NOAA North Pole Web Cam Images from Web Cams deployed in spring on an ice floe NOAA Near-realtime North Pole Weather Data Data from instruments deployed on an ice floe Search for Arctic Life Heats Up by Stephen Leahy International Polar Foundation "Daily report of Arctic ice cover based on satellite data". nsidc.org. National Snow and Ice Data Center. Marine Biodiversity Wiki
Astronomy (from Greek: ἀστρονομία, literally meaning the science that studies the laws of the stars) is a natural science that studies celestial objects and phenomena. It uses mathematics, physics, and chemistry in order to explain their origin and evolution. Objects of interest include planets, moons, stars, nebulae, galaxies, and comets. Relevant phenomena include supernova explosions, gamma ray bursts, quasars, blazars, pulsars, and cosmic microwave background radiation. More generally, astronomy studies everything that originates outside Earth's atmosphere. Cosmology is a branch of astronomy. It studies the Universe as a whole.Astronomy is one of the oldest natural sciences. The early civilizations in recorded history made methodical observations of the night sky. These include the Babylonians, Greeks, Indians, Egyptians, Chinese, Maya, and many ancient indigenous peoples of the Americas. In the past, astronomy included disciplines as diverse as astrometry, celestial navigation, observational astronomy, and the making of calendars. Nowadays, professional astronomy is often said to be the same as astrophysics.Professional astronomy is split into observational and theoretical branches. Observational astronomy is focused on acquiring data from observations of astronomical objects. This data is then analyzed using basic principles of physics. Theoretical astronomy is oriented toward the development of computer or analytical models to describe astronomical objects and phenomena. These two fields complement each other. Theoretical astronomy seeks to explain observational results and observations are used to confirm theoretical results. Astronomy is one of the few sciences in which amateurs play an active role. This is especially true for the discovery and observation of transient events. Amateur astronomers have helped with many important discoveries, such as finding new comets.
"Astronomy" and "astrophysics" are synonyms. Based on strict dictionary definitions, "astronomy" refers to "the study of objects and matter outside the Earth's atmosphere and of their physical and chemical properties," while "astrophysics" refers to the branch of astronomy dealing with "the behavior, physical properties, and dynamic processes of celestial objects and phenomena". In some cases, as in the introduction of the introductory textbook The Physical Universe by Frank Shu, "astronomy" may be used to describe the qualitative study of the subject, whereas "astrophysics" is used to describe the physics-oriented version of the subject. However, since most modern astronomical research deals with subjects related to physics, modern astronomy could actually be called astrophysics. Some fields, such as astrometry, are purely astronomy rather than also astrophysics. Various departments in which scientists carry out research on this subject may use "astronomy" and "astrophysics", partly depending on whether the department is historically affiliated with a physics department, and many professional astronomers have physics rather than astronomy degrees. Some titles of the leading scientific journals in this field include The Astronomical Journal, The Astrophysical Journal, and Astronomy & Astrophysics.
In early historic times, astronomy only consisted of the observation and predictions of the motions of objects visible to the naked eye. In some locations, early cultures assembled massive artifacts that possibly had some astronomical purpose. In addition to their ceremonial uses, these observatories could be employed to determine the seasons, an important factor in knowing when to plant crops and in understanding the length of the year.Before tools such as the telescope were invented, early study of the stars was conducted using the naked eye. As civilizations developed, most notably in Mesopotamia, Greece, Persia, India, China, Egypt, and Central America, astronomical observatories were assembled and ideas on the nature of the Universe began to develop. Most early astronomy consisted of mapping the positions of the stars and planets, a science now referred to as astrometry. From these observations, early ideas about the motions of the planets were formed, and the nature of the Sun, Moon and the Earth in the Universe were explored philosophically. The Earth was believed to be the center of the Universe with the Sun, the Moon and the stars rotating around it. This is known as the geocentric model of the Universe, or the Ptolemaic system, named after Ptolemy. A particularly important early development was the beginning of mathematical and scientific astronomy, which began among the Babylonians, who laid the foundations for the later astronomical traditions that developed in many other civilizations. The Babylonians discovered that lunar eclipses recurred in a repeating cycle known as a saros. Following the Babylonians, significant advances in astronomy were made in ancient Greece and the Hellenistic world. Greek astronomy is characterized from the start by seeking a rational, physical explanation for celestial phenomena. In the 3rd century BC, Aristarchus of Samos estimated the size and distance of the Moon and Sun, and he proposed a model of the Solar System where the Earth and planets rotated around the Sun, now called the heliocentric model. In the 2nd century BC, Hipparchus discovered precession, calculated the size and distance of the Moon and invented the earliest known astronomical devices such as the astrolabe. Hipparchus also created a comprehensive catalog of 1020 stars, and most of the constellations of the northern hemisphere derive from Greek astronomy. The Antikythera mechanism (c. 150–80 BC) was an early analog computer designed to calculate the location of the Sun, Moon, and planets for a given date. Technological artifacts of similar complexity did not reappear until the 14th century, when mechanical astronomical clocks appeared in Europe.
Medieval Europe housed a number of important astronomers. Richard of Wallingford (1292–1336) made major contributions to astronomy and horology, including the invention of the first astronomical clock, the Rectangulus which allowed for the measurement of angles between planets and other astronomical bodies, as well as an equatorium called the Albion which could be used for astronomical calculations such as lunar, solar and planetary longitudes and could predict eclipses. Nicole Oresme (1320–1382) and Jean Buridan (1300–1361) first discussed evidence for the rotation of the Earth, furthermore, Buridan also developed the theory of impetus (predecessor of the modern scientific theory of inertia) which was able to show planets were capable of motion without the intervention of angels. Georg von Peuerbach (1423–1461) and Regiomontanus (1436–1476) helped make astronomical progress instrumental to Copernicus's development of the heliocentric model decades later. Astronomy flourished in the Islamic world and other parts of the world. This led to the emergence of the first astronomical observatories in the Muslim world by the early 9th century. In 964, the Andromeda Galaxy, the largest galaxy in the Local Group, was described by the Persian Muslim astronomer Abd al-Rahman al-Sufi in his Book of Fixed Stars. The SN 1006 supernova, the brightest apparent magnitude stellar event in recorded history, was observed by the Egyptian Arabic astronomer Ali ibn Ridwan and Chinese astronomers in 1006. Some of the prominent Islamic (mostly Persian and Arab) astronomers who made significant contributions to the science include Al-Battani, Thebit, Abd al-Rahman al-Sufi, Biruni, Abū Ishāq Ibrāhīm al-Zarqālī, Al-Birjandi, and the astronomers of the Maragheh and Samarkand observatories. Astronomers during that time introduced many Arabic names now used for individual stars.It is also believed that the ruins at Great Zimbabwe and Timbuktu may have housed astronomical observatories. In Post-classical West Africa, Astronomers studied the movement of stars and relation to seasons, crafting charts of the heavens as well as precise diagrams of orbits of the other planets based on complex mathematical calculations. Songhai historian Mahmud Kati documented a meteor shower in August 1583. Europeans had previously believed that there had been no astronomical observation in sub-Saharan Africa during the pre-colonial Middle Ages, but modern discoveries show otherwise.For over six centuries (from the recovery of ancient learning during the late Middle Ages into the Enlightenment), the Roman Catholic Church gave more financial and social support to the study of astronomy than probably all other institutions. Among the Church's motives was finding the date for Easter.
During the Renaissance, Nicolaus Copernicus proposed a heliocentric model of the solar system. His work was defended by Galileo Galilei and expanded upon by Johannes Kepler. Kepler was the first to devise a system that correctly described the details of the motion of the planets around the Sun. However, Kepler did not succeed in formulating a theory behind the laws he wrote down. It was Isaac Newton, with his invention of celestial dynamics and his law of gravitation, who finally explained the motions of the planets. Newton also developed the reflecting telescope.Improvements in the size and quality of the telescope led to further discoveries. The English astronomer John Flamsteed catalogued over 3000 stars, More extensive star catalogues were produced by Nicolas Louis de Lacaille. The astronomer William Herschel made a detailed catalog of nebulosity and clusters, and in 1781 discovered the planet Uranus, the first new planet found.During the 18–19th centuries, the study of the three-body problem by Leonhard Euler, Alexis Claude Clairaut, and Jean le Rond d'Alembert led to more accurate predictions about the motions of the Moon and planets. This work was further refined by Joseph-Louis Lagrange and Pierre Simon Laplace, allowing the masses of the planets and moons to be estimated from their perturbations.Significant advances in astronomy came about with the introduction of new technology, including the spectroscope and photography. Joseph von Fraunhofer discovered about 600 bands in the spectrum of the Sun in 1814–15, which, in 1859, Gustav Kirchhoff ascribed to the presence of different elements. Stars were proven to be similar to the Earth's own Sun, but with a wide range of temperatures, masses, and sizes.The existence of the Earth's galaxy, the Milky Way, as its own group of stars was only proved in the 20th century, along with the existence of "external" galaxies. The observed recession of those galaxies led to the discovery of the expansion of the Universe. Theoretical astronomy led to speculations on the existence of objects such as black holes and neutron stars, which have been used to explain such observed phenomena as quasars, pulsars, blazars, and radio galaxies. Physical cosmology made huge advances during the 20th century. In the early 1900s the model of the Big Bang theory was formulated, heavily evidenced by cosmic microwave background radiation, Hubble's law, and the cosmological abundances of elements. Space telescopes have enabled measurements in parts of the electromagnetic spectrum normally blocked or blurred by the atmosphere. In February 2016, it was revealed that the LIGO project had detected evidence of gravitational waves in the previous September.
The main source of information about celestial bodies and other objects is visible light, or more generally electromagnetic radiation. Observational astronomy may be categorized according to the corresponding region of the electromagnetic spectrum on which the observations are made. Some parts of the spectrum can be observed from the Earth's surface, while other parts are only observable from either high altitudes or outside the Earth's atmosphere. Specific information on these subfields is given below.
Radio astronomy uses radiation with wavelengths greater than approximately one millimeter, outside the visible range. Radio astronomy is different from most other forms of observational astronomy in that the observed radio waves can be treated as waves rather than as discrete photons. Hence, it is relatively easier to measure both the amplitude and phase of radio waves, whereas this is not as easily done at shorter wavelengths.Although some radio waves are emitted directly by astronomical objects, a product of thermal emission, most of the radio emission that is observed is the result of synchrotron radiation, which is produced when electrons orbit magnetic fields. Additionally, a number of spectral lines produced by interstellar gas, notably the hydrogen spectral line at 21 cm, are observable at radio wavelengths.A wide variety of other objects are observable at radio wavelengths, including supernovae, interstellar gas, pulsars, and active galactic nuclei.
Infrared astronomy is founded on the detection and analysis of infrared radiation, wavelengths longer than red light and outside the range of our vision. The infrared spectrum is useful for studying objects that are too cold to radiate visible light, such as planets, circumstellar disks or nebulae whose light is blocked by dust. The longer wavelengths of infrared can penetrate clouds of dust that block visible light, allowing the observation of young stars embedded in molecular clouds and the cores of galaxies. Observations from the Wide-field Infrared Survey Explorer (WISE) have been particularly effective at unveiling numerous Galactic protostars and their host star clusters. With the exception of infrared wavelengths close to visible light, such radiation is heavily absorbed by the atmosphere, or masked, as the atmosphere itself produces significant infrared emission. Consequently, infrared observatories have to be located in high, dry places on Earth or in space. Some molecules radiate strongly in the infrared. This allows the study of the chemistry of space; more specifically it can detect water in comets.
Historically, optical astronomy, also called visible light astronomy, is the oldest form of astronomy. Images of observations were originally drawn by hand. In the late 19th century and most of the 20th century, images were made using photographic equipment. Modern images are made using digital detectors, particularly using charge-coupled devices (CCDs) and recorded on modern medium. Although visible light itself extends from approximately 4000 Å to 7000 Å (400 nm to 700 nm), that same equipment can be used to observe some near-ultraviolet and near-infrared radiation.
Ultraviolet astronomy employs ultraviolet wavelengths between approximately 100 and 3200 Å (10 to 320 nm). Light at those wavelengths is absorbed by the Earth's atmosphere, requiring observations at these wavelengths to be performed from the upper atmosphere or from space. Ultraviolet astronomy is best suited to the study of thermal radiation and spectral emission lines from hot blue stars (OB stars) that are very bright in this wave band. This includes the blue stars in other galaxies, which have been the targets of several ultraviolet surveys. Other objects commonly observed in ultraviolet light include planetary nebulae, supernova remnants, and active galactic nuclei. However, as ultraviolet light is easily absorbed by interstellar dust, an adjustment of ultraviolet measurements is necessary.
X-ray astronomy uses X-ray wavelengths. Typically, X-ray radiation is produced by synchrotron emission (the result of electrons orbiting magnetic field lines), thermal emission from thin gases above 107 (10 million) kelvins, and thermal emission from thick gases above 107 Kelvin. Since X-rays are absorbed by the Earth's atmosphere, all X-ray observations must be performed from high-altitude balloons, rockets, or X-ray astronomy satellites. Notable X-ray sources include X-ray binaries, pulsars, supernova remnants, elliptical galaxies, clusters of galaxies, and active galactic nuclei.
Gamma ray astronomy observes astronomical objects at the shortest wavelengths of the electromagnetic spectrum. Gamma rays may be observed directly by satellites such as the Compton Gamma Ray Observatory or by specialized telescopes called atmospheric Cherenkov telescopes. The Cherenkov telescopes do not detect the gamma rays directly but instead detect the flashes of visible light produced when gamma rays are absorbed by the Earth's atmosphere.Most gamma-ray emitting sources are actually gamma-ray bursts, objects which only produce gamma radiation for a few milliseconds to thousands of seconds before fading away. Only 10% of gamma-ray sources are non-transient sources. These steady gamma-ray emitters include pulsars, neutron stars, and black hole candidates such as active galactic nuclei.
In addition to electromagnetic radiation, a few other events originating from great distances may be observed from the Earth. In neutrino astronomy, astronomers use heavily shielded underground facilities such as SAGE, GALLEX, and Kamioka II/III for the detection of neutrinos. The vast majority of the neutrinos streaming through the Earth originate from the Sun, but 24 neutrinos were also detected from supernova 1987A. Cosmic rays, which consist of very high energy particles (atomic nuclei) that can decay or be absorbed when they enter the Earth's atmosphere, result in a cascade of secondary particles which can be detected by current observatories. Some future neutrino detectors may also be sensitive to the particles produced when cosmic rays hit the Earth's atmosphere.Gravitational-wave astronomy is an emerging field of astronomy that employs gravitational-wave detectors to collect observational data about distant massive objects. A few observatories have been constructed, such as the Laser Interferometer Gravitational Observatory LIGO. LIGO made its first detection on 14 September 2015, observing gravitational waves from a binary black hole. A second gravitational wave was detected on 26 December 2015 and additional observations should continue but gravitational waves require extremely sensitive instruments.The combination of observations made using electromagnetic radiation, neutrinos or gravitational waves and other complementary information, is known as multi-messenger astronomy.
One of the oldest fields in astronomy, and in all of science, is the measurement of the positions of celestial objects. Historically, accurate knowledge of the positions of the Sun, Moon, planets and stars has been essential in celestial navigation (the use of celestial objects to guide navigation) and in the making of calendars. Careful measurement of the positions of the planets has led to a solid understanding of gravitational perturbations, and an ability to determine past and future positions of the planets with great accuracy, a field known as celestial mechanics. More recently the tracking of near-Earth objects will allow for predictions of close encounters or potential collisions of the Earth with those objects.The measurement of stellar parallax of nearby stars provides a fundamental baseline in the cosmic distance ladder that is used to measure the scale of the Universe. Parallax measurements of nearby stars provide an absolute baseline for the properties of more distant stars, as their properties can be compared. Measurements of the radial velocity and proper motion of stars allows astronomers to plot the movement of these systems through the Milky Way galaxy. Astrometric results are the basis used to calculate the distribution of speculated dark matter in the galaxy.During the 1990s, the measurement of the stellar wobble of nearby stars was used to detect large extrasolar planets orbiting those stars.
Theoretical astronomers use several tools including analytical models and computational numerical simulations; each has its particular advantages. Analytical models of a process are better for giving broader insight into the heart of what is going on. Numerical models reveal the existence of phenomena and effects otherwise unobserved.Theorists in astronomy endeavor to create theoretical models and from the results predict observational consequences of those models. The observation of a phenomenon predicted by a model allows astronomers to select between several alternate or conflicting models as the one best able to describe the phenomena. Theorists also try to generate or modify models to take into account new data. In the case of an inconsistency between the data and model's results, the general tendency is to try to make minimal modifications to the model so that it produces results that fit the data. In some cases, a large amount of inconsistent data over time may lead to total abandonment of a model. Phenomena modeled by theoretical astronomers include: stellar dynamics and evolution; galaxy formation; large-scale distribution of matter in the Universe; origin of cosmic rays; general relativity and physical cosmology, including string cosmology and astroparticle physics. Astrophysical relativity serves as a tool to gauge the properties of large scale structures for which gravitation plays a significant role in physical phenomena investigated and as the basis for black hole (astro)physics and the study of gravitational waves. Some widely accepted and studied theories and models in astronomy, now included in the Lambda-CDM model are the Big Bang, dark matter and fundamental theories of physics. A few examples of this process: Along with Cosmic inflation, dark matter and dark energy are the current leading topics in astronomy, as their discovery and controversy originated during the study of the galaxies.
Astrophysics is the branch of astronomy that employs the principles of physics and chemistry "to ascertain the nature of the astronomical objects, rather than their positions or motions in space". Among the objects studied are the Sun, other stars, galaxies, extrasolar planets, the interstellar medium and the cosmic microwave background. Their emissions are examined across all parts of the electromagnetic spectrum, and the properties examined include luminosity, density, temperature, and chemical composition. Because astrophysics is a very broad subject, astrophysicists typically apply many disciplines of physics, including mechanics, electromagnetism, statistical mechanics, thermodynamics, quantum mechanics, relativity, nuclear and particle physics, and atomic and molecular physics. In practice, modern astronomical research often involves a substantial amount of work in the realms of theoretical and observational physics. Some areas of study for astrophysicists include their attempts to determine the properties of dark matter, dark energy, and black holes; whether or not time travel is possible, wormholes can form, or the multiverse exists; and the origin and ultimate fate of the universe. Topics also studied by theoretical astrophysicists include Solar System formation and evolution; stellar dynamics and evolution; galaxy formation and evolution; magnetohydrodynamics; large-scale structure of matter in the universe; origin of cosmic rays; general relativity and physical cosmology, including string cosmology and astroparticle physics.
Astrochemistry is the study of the abundance and reactions of molecules in the Universe, and their interaction with radiation. The discipline is an overlap of astronomy and chemistry. The word "astrochemistry" may be applied to both the Solar System and the interstellar medium. The study of the abundance of elements and isotope ratios in Solar System objects, such as meteorites, is also called cosmochemistry, while the study of interstellar atoms and molecules and their interaction with radiation is sometimes called molecular astrophysics. The formation, atomic and chemical composition, evolution and fate of molecular gas clouds is of special interest, because it is from these clouds that solar systems form. Studies in this field contribute to the understanding of the formation of the Solar System, Earth's origin and geology, abiogenesis, and the origin of climate and oceans.
Astrobiology is an interdisciplinary scientific field concerned with the origins, early evolution, distribution, and future of life in the universe. Astrobiology considers the question of whether extraterrestrial life exists, and how humans can detect it if it does. The term exobiology is similar.Astrobiology makes use of molecular biology, biophysics, biochemistry, chemistry, astronomy, physical cosmology, exoplanetology and geology to investigate the possibility of life on other worlds and help recognize biospheres that might be different from that on Earth. The origin and early evolution of life is an inseparable part of the discipline of astrobiology. Astrobiology concerns itself with interpretation of existing scientific data, and although speculation is entertained to give context, astrobiology concerns itself primarily with hypotheses that fit firmly into existing scientific theories. This interdisciplinary field encompasses research on the origin of planetary systems, origins of organic compounds in space, rock-water-carbon interactions, abiogenesis on Earth, planetary habitability, research on biosignatures for life detection, and studies on the potential for life to adapt to challenges on Earth and in outer space.
Cosmology (from the Greek κόσμος (kosmos) "world, universe" and λόγος (logos) "word, study" or literally "logic") could be considered the study of the Universe as a whole. Observations of the large-scale structure of the Universe, a branch known as physical cosmology, have provided a deep understanding of the formation and evolution of the cosmos. Fundamental to modern cosmology is the well-accepted theory of the Big Bang, wherein our Universe began at a single point in time, and thereafter expanded over the course of 13.8 billion years to its present condition. The concept of the Big Bang can be traced back to the discovery of the microwave background radiation in 1965.In the course of this expansion, the Universe underwent several evolutionary stages. In the very early moments, it is theorized that the Universe experienced a very rapid cosmic inflation, which homogenized the starting conditions. Thereafter, nucleosynthesis produced the elemental abundance of the early Universe. (See also nucleocosmochronology.) When the first neutral atoms formed from a sea of primordial ions, space became transparent to radiation, releasing the energy viewed today as the microwave background radiation. The expanding Universe then underwent a Dark Age due to the lack of stellar energy sources.A hierarchical structure of matter began to form from minute variations in the mass density of space. Matter accumulated in the densest regions, forming clouds of gas and the earliest stars, the Population III stars. These massive stars triggered the reionization process and are believed to have created many of the heavy elements in the early Universe, which, through nuclear decay, create lighter elements, allowing the cycle of nucleosynthesis to continue longer.Gravitational aggregations clustered into filaments, leaving voids in the gaps. Gradually, organizations of gas and dust merged to form the first primitive galaxies. Over time, these pulled in more matter, and were often organized into groups and clusters of galaxies, then into larger-scale superclusters.Various fields of physics are crucial to studying the universe. Interdisciplinary studies involve the fields of quantum mechanics, particle physics, plasma physics, condensed matter physics, statistical mechanics, optics, and nuclear physics. Fundamental to the structure of the Universe is the existence of dark matter and dark energy. These are now thought to be its dominant components, forming 96% of the mass of the Universe. For this reason, much effort is expended in trying to understand the physics of these components.
The study of objects outside our galaxy is a branch of astronomy concerned with the formation and evolution of Galaxies, their morphology (description) and classification, the observation of active galaxies, and at a larger scale, the groups and clusters of galaxies. Finally, the latter is important for the understanding of the large-scale structure of the cosmos. Most galaxies are organized into distinct shapes that allow for classification schemes. They are commonly divided into spiral, elliptical and Irregular galaxies.As the name suggests, an elliptical galaxy has the cross-sectional shape of an ellipse. The stars move along random orbits with no preferred direction. These galaxies contain little or no interstellar dust, few star-forming regions, and older stars. Elliptical galaxies are more commonly found at the core of galactic clusters, and may have been formed through mergers of large galaxies. A spiral galaxy is organized into a flat, rotating disk, usually with a prominent bulge or bar at the center, and trailing bright arms that spiral outward. The arms are dusty regions of star formation within which massive young stars produce a blue tint. Spiral galaxies are typically surrounded by a halo of older stars. Both the Milky Way and one of our nearest galaxy neighbors, the Andromeda Galaxy, are spiral galaxies. Irregular galaxies are chaotic in appearance, and are neither spiral nor elliptical. About a quarter of all galaxies are irregular, and the peculiar shapes of such galaxies may be the result of gravitational interaction. An active galaxy is a formation that emits a significant amount of its energy from a source other than its stars, dust and gas. It is powered by a compact region at the core, thought to be a super-massive black hole that is emitting radiation from in-falling material. A radio galaxy is an active galaxy that is very luminous in the radio portion of the spectrum, and is emitting immense plumes or lobes of gas. Active galaxies that emit shorter frequency, high-energy radiation include Seyfert galaxies, Quasars, and Blazars. Quasars are believed to be the most consistently luminous objects in the known universe.The large-scale structure of the cosmos is represented by groups and clusters of galaxies. This structure is organized into a hierarchy of groupings, with the largest being the superclusters. The collective matter is formed into filaments and walls, leaving large voids between.
The Solar System orbits within the Milky Way, a barred spiral galaxy that is a prominent member of the Local Group of galaxies. It is a rotating mass of gas, dust, stars and other objects, held together by mutual gravitational attraction. As the Earth is located within the dusty outer arms, there are large portions of the Milky Way that are obscured from view. In the center of the Milky Way is the core, a bar-shaped bulge with what is believed to be a supermassive black hole at its center. This is surrounded by four primary arms that spiral from the core. This is a region of active star formation that contains many younger, population I stars. The disk is surrounded by a spheroid halo of older, population II stars, as well as relatively dense concentrations of stars known as globular clusters.Between the stars lies the interstellar medium, a region of sparse matter. In the densest regions, molecular clouds of molecular hydrogen and other elements create star-forming regions. These begin as a compact pre-stellar core or dark nebulae, which concentrate and collapse (in volumes determined by the Jeans length) to form compact protostars.As the more massive stars appear, they transform the cloud into an H II region (ionized atomic hydrogen) of glowing gas and plasma. The stellar wind and supernova explosions from these stars eventually cause the cloud to disperse, often leaving behind one or more young open clusters of stars. These clusters gradually disperse, and the stars join the population of the Milky Way.Kinematic studies of matter in the Milky Way and other galaxies have demonstrated that there is more mass than can be accounted for by visible matter. A dark matter halo appears to dominate the mass, although the nature of this dark matter remains undetermined.
The study of stars and stellar evolution is fundamental to our understanding of the Universe. The astrophysics of stars has been determined through observation and theoretical understanding; and from computer simulations of the interior. Star formation occurs in dense regions of dust and gas, known as giant molecular clouds. When destabilized, cloud fragments can collapse under the influence of gravity, to form a protostar. A sufficiently dense, and hot, core region will trigger nuclear fusion, thus creating a main-sequence star.Almost all elements heavier than hydrogen and helium were created inside the cores of stars.The characteristics of the resulting star depend primarily upon its starting mass. The more massive the star, the greater its luminosity, and the more rapidly it fuses its hydrogen fuel into helium in its core. Over time, this hydrogen fuel is completely converted into helium, and the star begins to evolve. The fusion of helium requires a higher core temperature. A star with a high enough core temperature will push its outer layers outward while increasing its core density. The resulting red giant formed by the expanding outer layers enjoys a brief life span, before the helium fuel in the core is in turn consumed. Very massive stars can also undergo a series of evolutionary phases, as they fuse increasingly heavier elements.The final fate of the star depends on its mass, with stars of mass greater than about eight times the Sun becoming core collapse supernovae; while smaller stars blow off their outer layers and leave behind the inert core in the form of a white dwarf. The ejection of the outer layers forms a planetary nebula. The remnant of a supernova is a dense neutron star, or, if the stellar mass was at least three times that of the Sun, a black hole. Closely orbiting binary stars can follow more complex evolutionary paths, such as mass transfer onto a white dwarf companion that can potentially cause a supernova. Planetary nebulae and supernovae distribute the "metals" produced in the star by fusion to the interstellar medium; without them, all new stars (and their planetary systems) would be formed from hydrogen and helium alone.
At a distance of about eight light-minutes, the most frequently studied star is the Sun, a typical main-sequence dwarf star of stellar class G2 V, and about 4.6 billion years (Gyr) old. The Sun is not considered a variable star, but it does undergo periodic changes in activity known as the sunspot cycle. This is an 11-year oscillation in sunspot number. Sunspots are regions of lower-than- average temperatures that are associated with intense magnetic activity.The Sun has steadily increased in luminosity by 40% since it first became a main-sequence star. The Sun has also undergone periodic changes in luminosity that can have a significant impact on the Earth. The Maunder minimum, for example, is believed to have caused the Little Ice Age phenomenon during the Middle Ages.The visible outer surface of the Sun is called the photosphere. Above this layer is a thin region known as the chromosphere. This is surrounded by a transition region of rapidly increasing temperatures, and finally by the super-heated corona. At the center of the Sun is the core region, a volume of sufficient temperature and pressure for nuclear fusion to occur. Above the core is the radiation zone, where the plasma conveys the energy flux by means of radiation. Above that is the convection zone where the gas material transports energy primarily through physical displacement of the gas known as convection. It is believed that the movement of mass within the convection zone creates the magnetic activity that generates sunspots.A solar wind of plasma particles constantly streams outward from the Sun until, at the outermost limit of the Solar System, it reaches the heliopause. As the solar wind passes the Earth, it interacts with the Earth's magnetic field (magnetosphere) and deflects the solar wind, but traps some creating the Van Allen radiation belts that envelop the Earth. The aurora are created when solar wind particles are guided by the magnetic flux lines into the Earth's polar regions where the lines then descend into the atmosphere.
Planetary science is the study of the assemblage of planets, moons, dwarf planets, comets, asteroids, and other bodies orbiting the Sun, as well as extrasolar planets. The Solar System has been relatively well-studied, initially through telescopes and then later by spacecraft. This has provided a good overall understanding of the formation and evolution of the Sun's planetary system, although many new discoveries are still being made.The Solar System is subdivided into the inner planets, the asteroid belt, and the outer planets. The inner terrestrial planets consist of Mercury, Venus, Earth, and Mars. The outer gas giant planets are Jupiter, Saturn, Uranus, and Neptune. Beyond Neptune lies the Kuiper belt, and finally the Oort Cloud, which may extend as far as a light-year. The planets were formed 4.6 billion years ago in the protoplanetary disk that surrounded the early Sun. Through a process that included gravitational attraction, collision, and accretion, the disk formed clumps of matter that, with time, became protoplanets. The radiation pressure of the solar wind then expelled most of the unaccreted matter, and only those planets with sufficient mass retained their gaseous atmosphere. The planets continued to sweep up, or eject, the remaining matter during a period of intense bombardment, evidenced by the many impact craters on the Moon. During this period, some of the protoplanets may have collided and one such collision may have formed the Moon.Once a planet reaches sufficient mass, the materials of different densities segregate within, during planetary differentiation. This process can form a stony or metallic core, surrounded by a mantle and an outer crust. The core may include solid and liquid regions, and some planetary cores generate their own magnetic field, which can protect their atmospheres from solar wind stripping.A planet or moon's interior heat is produced from the collisions that created the body, by the decay of radioactive materials (e.g. uranium, thorium, and 26Al), or tidal heating caused by interactions with other bodies. Some planets and moons accumulate enough heat to drive geologic processes such as volcanism and tectonics. Those that accumulate or retain an atmosphere can also undergo surface erosion from wind or water. Smaller bodies, without tidal heating, cool more quickly; and their geological activity ceases with the exception of impact cratering.
Astronomy and astrophysics have developed significant interdisciplinary links with other major scientific fields. Archaeoastronomy is the study of ancient or traditional astronomies in their cultural context, utilizing archaeological and anthropological evidence. Astrobiology is the study of the advent and evolution of biological systems in the Universe, with particular emphasis on the possibility of non-terrestrial life. Astrostatistics is the application of statistics to astrophysics to the analysis of vast amount of observational astrophysical data. The study of chemicals found in space, including their formation, interaction and destruction, is called astrochemistry. These substances are usually found in molecular clouds, although they may also appear in low temperature stars, brown dwarfs and planets. Cosmochemistry is the study of the chemicals found within the Solar System, including the origins of the elements and variations in the isotope ratios. Both of these fields represent an overlap of the disciplines of astronomy and chemistry. As "forensic astronomy", finally, methods from astronomy have been used to solve problems of law and history.
Astronomy is one of the sciences to which amateurs can contribute the most.Collectively, amateur astronomers observe a variety of celestial objects and phenomena sometimes with equipment that they build themselves. Common targets of amateur astronomers include the Sun, the Moon, planets, stars, comets, meteor showers, and a variety of deep-sky objects such as star clusters, galaxies, and nebulae. Astronomy clubs are located throughout the world and many have programs to help their members set up and complete observational programs including those to observe all the objects in the Messier (110 objects) or Herschel 400 catalogues of points of interest in the night sky. One branch of amateur astronomy, amateur astrophotography, involves the taking of photos of the night sky. Many amateurs like to specialize in the observation of particular objects, types of objects, or types of events which interest them.Most amateurs work at visible wavelengths, but a small minority experiment with wavelengths outside the visible spectrum. This includes the use of infrared filters on conventional telescopes, and also the use of radio telescopes. The pioneer of amateur radio astronomy was Karl Jansky, who started observing the sky at radio wavelengths in the 1930s. A number of amateur astronomers use either homemade telescopes or use radio telescopes which were originally built for astronomy research but which are now available to amateurs (e.g. the One-Mile Telescope).Amateur astronomers continue to make scientific contributions to the field of astronomy and it is one of the few scientific disciplines where amateurs can still make significant contributions. Amateurs can make occultation measurements that are used to refine the orbits of minor planets. They can also discover comets, and perform regular observations of variable stars. Improvements in digital technology have allowed amateurs to make impressive advances in the field of astrophotography.
Although the scientific discipline of astronomy has made tremendous strides in understanding the nature of the Universe and its contents, there remain some important unanswered questions. Answers to these may require the construction of new ground- and space-based instruments, and possibly new developments in theoretical and experimental physics. What is the origin of the stellar mass spectrum? That is, why do astronomers observe the same distribution of stellar masses—the initial mass function—apparently regardless of the initial conditions? A deeper understanding of the formation of stars and planets is needed. Is there other life in the Universe? Especially, is there other intelligent life? If so, what is the explanation for the Fermi paradox? The existence of life elsewhere has important scientific and philosophical implications. Is the Solar System normal or atypical? What is the nature of dark matter and dark energy? These dominate the evolution and fate of the cosmos, yet their true nature remains unknown. What will be the ultimate fate of the universe? How did the first galaxies form? How did supermassive black holes form? What is creating the ultra-high-energy cosmic rays? Why is the abundance of lithium in the cosmos four times lower than predicted by the standard Big Bang model? What really happens beyond the event horizon?
NASA/IPAC Extragalactic Database (NED) (NED-Distances) International Year of Astronomy 2009 IYA2009 Main website Cosmic Journey: A History of Scientific Cosmology from the American Institute of Physics Southern Hemisphere Astronomy Celestia Motherlode Educational site for Astronomical journeys through space Kroto, Harry, Astrophysical Chemistry Lecture Series. Core books and Core journals in Astronomy, from the Smithsonian/NASA Astrophysics Data System A Journey with Fred Hoyle by Wickramasinghe, Chandra. Astronomy books from the History of Science Collection at Linda Hall Library
Astronomy is the oldest of the natural sciences, dating back to antiquity, with its origins in the religious, mythological, cosmological, calendrical, and astrological beliefs and practices of prehistory: vestiges of these are still found in astrology, a discipline long interwoven with public and governmental astronomy. It was not completely separated in Europe (see astrology and astronomy) during the Copernican Revolution starting in 1543. In some cultures, astronomical data was used for astrological prognostication. The study of astronomy has received financial and social support from many institutions, especially the Church, which was its largest source of support between the 12th century to the Enlightenment.Ancient astronomers were able to differentiate between stars and planets, as stars remain relatively fixed over the centuries while planets will move an appreciable amount during a comparatively short time.
Early cultures identified celestial objects with gods and spirits. They related these objects (and their movements) to phenomena such as rain, drought, seasons, and tides. It is generally believed that the first astronomers were priests, and that they understood celestial objects and events to be manifestations of the divine, hence early astronomy's connection to what is now called astrology. A 32,500 year old carved ivory Mammoth tusk could contain the oldest known star chart (resembling the constellation Orion). It has also been suggested that drawing on the wall of the Lascaux caves in France dating from 33,000 to 10,000 years ago could be a graphical representation of the Pleiades, the Summer Triangle, and the Northern Crown. Ancient structures with possibly astronomical alignments (such as Stonehenge) probably fulfilled astronomical, religious, and social functions. Calendars of the world have often been set by observations of the Sun and Moon (marking the day, month and year), and were important to agricultural societies, in which the harvest depended on planting at the correct time of year, and for which the nearly full moon was the only lighting for night-time travel into city markets. The common modern calendar is based on the Roman calendar. Although originally a lunar calendar, it broke the traditional link of the month to the phases of the Moon and divided the year into twelve almost-equal months, that mostly alternated between thirty and thirty-one days. Julius Caesar instigated calendar reform in 46 BCE and introduced what is now called the Julian calendar, based upon the 365 ​1⁄4 day year length originally proposed by the 4th century BCE Greek astronomer Callippus.
The origins of Western astronomy can be found in Mesopotamia, the "land between the rivers" Tigris and Euphrates, where the ancient kingdoms of Sumer, Assyria, and Babylonia were located. A form of writing known as cuneiform emerged among the Sumerians around 3500–3000 BC. Our knowledge of Sumerian astronomy is indirect, via the earliest Babylonian star catalogues dating from about 1200 BC. The fact that many star names appear in Sumerian suggests a continuity reaching into the Early Bronze Age. Astral theology, which gave planetary gods an important role in Mesopotamian mythology and religion, began with the Sumerians. They also used a sexagesimal (base 60) place-value number system, which simplified the task of recording very large and very small numbers. The modern practice of dividing a circle into 360 degrees, or an hour into 60 minutes, began with the Sumerians. For more information, see the articles on Babylonian numerals and mathematics. Classical sources frequently use the term Chaldeans for the astronomers of Mesopotamia, who were, in reality, priest-scribes specializing in astrology and other forms of divination. The first evidence of recognition that astronomical phenomena are periodic and of the application of mathematics to their prediction is Babylonian. Tablets dating back to the Old Babylonian period document the application of mathematics to the variation in the length of daylight over a solar year. Centuries of Babylonian observations of celestial phenomena are recorded in the series of cuneiform tablets known as the Enūma Anu Enlil. The oldest significant astronomical text that we possess is Tablet 63 of the Enūma Anu Enlil, the Venus tablet of Ammi-saduqa, which lists the first and last visible risings of Venus over a period of about 21 years and is the earliest evidence that the phenomena of a planet were recognized as periodic. The MUL.APIN, contains catalogues of stars and constellations as well as schemes for predicting heliacal risings and the settings of the planets, lengths of daylight measured by a water clock, gnomon, shadows, and intercalations. The Babylonian GU text arranges stars in 'strings' that lie along declination circles and thus measure right-ascensions or time-intervals, and also employs the stars of the zenith, which are also separated by given right-ascensional differences.A significant increase in the quality and frequency of Babylonian observations appeared during the reign of Nabonassar (747–733 BC). The systematic records of ominous phenomena in Babylonian astronomical diaries that began at this time allowed for the discovery of a repeating 18-year cycle of lunar eclipses, for example. The Greek astronomer Ptolemy later used Nabonassar's reign to fix the beginning of an era, since he felt that the earliest usable observations began at this time. The last stages in the development of Babylonian astronomy took place during the time of the Seleucid Empire (323–60 BC). In the 3rd century BC, astronomers began to use "goal-year texts" to predict the motions of the planets. These texts compiled records of past observations to find repeating occurrences of ominous phenomena for each planet. About the same time, or shortly afterwards, astronomers created mathematical models that allowed them to predict these phenomena directly, without consulting past records. A notable Babylonian astronomer from this time was Seleucus of Seleucia, who was a supporter of the heliocentric model. Babylonian astronomy was the basis for much of what was done in Greek and Hellenistic astronomy, in classical Indian astronomy, in Sassanian Iran, in Byzantium, in Syria, in Islamic astronomy, in Central Asia, and in Western Europe.
Astronomy in the Indian subcontinent dates back to the period of Indus Valley Civilization during 3rd millennium BCE, when it was used to create calendars. As the Indus Valley civilization did not leave behind written documents, the oldest extant Indian astronomical text is the Vedanga Jyotisha, dating from the Vedic period. Vedanga Jyotisha describes rules for tracking the motions of the Sun and the Moon for the purposes of ritual. During the 6th century, astronomy was influenced by the Greek and Byzantine astronomical traditions.Aryabhata (476–550), in his magnum opus Aryabhatiya (499), propounded a computational system based on a planetary model in which the Earth was taken to be spinning on its axis and the periods of the planets were given with respect to the Sun. He accurately calculated many astronomical constants, such as the periods of the planets, times of the solar and lunar eclipses, and the instantaneous motion of the Moon. Early followers of Aryabhata's model included Varahamihira, Brahmagupta, and Bhaskara II. Astronomy was advanced during the Shunga Empire and many star catalogues were produced during this time. The Shunga period is known as the "Golden age of astronomy in India". It saw the development of calculations for the motions and places of various planets, their rising and setting, conjunctions, and the calculation of eclipses. Indian astronomers by the 6th century believed that comets were celestial bodies that re-appeared periodically. This was the view expressed in the 6th century by the astronomers Varahamihira and Bhadrabahu, and the 10th-century astronomer Bhattotpala listed the names and estimated periods of certain comets, but it is unfortunately not known how these figures were calculated or how accurate they were.Bhāskara II (1114–1185) was the head of the astronomical observatory at Ujjain, continuing the mathematical tradition of Brahmagupta. He wrote the Siddhantasiromani which consists of two parts: Goladhyaya (sphere) and Grahaganita (mathematics of the planets). He also calculated the time taken for the Earth to orbit the Sun to 9 decimal places. The Buddhist University of Nalanda at the time offered formal courses in astronomical studies. Other important astronomers from India include Madhava of Sangamagrama, Nilakantha Somayaji and Jyeshtadeva, who were members of the Kerala school of astronomy and mathematics from the 14th century to the 16th century. Nilakantha Somayaji, in his Aryabhatiyabhasya, a commentary on Aryabhata's Aryabhatiya, developed his own computational system for a partially heliocentric planetary model, in which Mercury, Venus, Mars, Jupiter and Saturn orbit the Sun, which in turn orbits the Earth, similar to the Tychonic system later proposed by Tycho Brahe in the late 16th century. Nilakantha's system, however, was mathematically more efficient than the Tychonic system, due to correctly taking into account the equation of the centre and latitudinal motion of Mercury and Venus. Most astronomers of the Kerala school of astronomy and mathematics who followed him accepted his planetary model.
The Ancient Greeks developed astronomy, which they treated as a branch of mathematics, to a highly sophisticated level. The first geometrical, three-dimensional models to explain the apparent motion of the planets were developed in the 4th century BC by Eudoxus of Cnidus and Callippus of Cyzicus. Their models were based on nested homocentric spheres centered upon the Earth. Their younger contemporary Heraclides Ponticus proposed that the Earth rotates around its axis. A different approach to celestial phenomena was taken by natural philosophers such as Plato and Aristotle. They were less concerned with developing mathematical predictive models than with developing an explanation of the reasons for the motions of the Cosmos. In his Timaeus, Plato described the universe as a spherical body divided into circles carrying the planets and governed according to harmonic intervals by a world soul. Aristotle, drawing on the mathematical model of Eudoxus, proposed that the universe was made of a complex system of concentric spheres, whose circular motions combined to carry the planets around the earth. This basic cosmological model prevailed, in various forms, until the 16th century. In the 3rd century BC Aristarchus of Samos was the first to suggest a heliocentric system, although only fragmentary descriptions of his idea survive. Eratosthenes estimated the circumference of the Earth with great accuracy.Greek geometrical astronomy developed away from the model of concentric spheres to employ more complex models in which an eccentric circle would carry around a smaller circle, called an epicycle which in turn carried around a planet. The first such model is attributed to Apollonius of Perga and further developments in it were carried out in the 2nd century BC by Hipparchus of Nicea. Hipparchus made a number of other contributions, including the first measurement of precession and the compilation of the first star catalog in which he proposed our modern system of apparent magnitudes. The Antikythera mechanism, an ancient Greek astronomical observational device for calculating the movements of the Sun and the Moon, possibly the planets, dates from about 150–100 BC, and was the first ancestor of an astronomical computer. It was discovered in an ancient shipwreck off the Greek island of Antikythera, between Kythera and Crete. The device became famous for its use of a differential gear, previously believed to have been invented in the 16th century, and the miniaturization and complexity of its parts, comparable to a clock made in the 18th century. The original mechanism is displayed in the Bronze collection of the National Archaeological Museum of Athens, accompanied by a replica. Depending on the historian's viewpoint, the acme or corruption of physical Greek astronomy is seen with Ptolemy of Alexandria, who wrote the classic comprehensive presentation of geocentric astronomy, the Megale Syntaxis (Great Synthesis), better known by its Arabic title Almagest, which had a lasting effect on astronomy up to the Renaissance. In his Planetary Hypotheses, Ptolemy ventured into the realm of cosmology, developing a physical model of his geometric system, in a universe many times smaller than the more realistic conception of Aristarchus of Samos four centuries earlier.
The precise orientation of the Egyptian pyramids affords a lasting demonstration of the high degree of technical skill in watching the heavens attained in the 3rd millennium BC. It has been shown the Pyramids were aligned towards the pole star, which, because of the precession of the equinoxes, was at that time Thuban, a faint star in the constellation of Draco. Evaluation of the site of the temple of Amun-Re at Karnak, taking into account the change over time of the obliquity of the ecliptic, has shown that the Great Temple was aligned on the rising of the midwinter Sun. The length of the corridor down which sunlight would travel would have limited illumination at other times of the year. The Egyptians also found the position of Sirius (the dog star) who they believed was Anubis their Jackal headed god moving through the heavens. Its position was critical to their civilisation as when it rose heliacal in the east before sunrise it foretold the flooding of the Nile. It is also where we get the phrase 'dog days of summer' from. Astronomy played a considerable part in religious matters for fixing the dates of festivals and determining the hours of the night. The titles of several temple books are preserved recording the movements and phases of the sun, moon and stars. The rising of Sirius (Egyptian: Sopdet, Greek: Sothis) at the beginning of the inundation was a particularly important point to fix in the yearly calendar. Writing in the Roman era, Clement of Alexandria gives some idea of the importance of astronomical observations to the sacred rites: And after the Singer advances the Astrologer (ὡροσκόπος), with a horologium (ὡρολόγιον) in his hand, and a palm (φοίνιξ), the symbols of astrology. He must know by heart the Hermetic astrological books, which are four in number. Of these, one is about the arrangement of the fixed stars that are visible; one on the positions of the Sun and Moon and five planets; one on the conjunctions and phases of the Sun and Moon; and one concerns their risings. The Astrologer's instruments (horologium and palm) are a plumb line and sighting instrument. They have been identified with two inscribed objects in the Berlin Museum; a short handle from which a plumb line was hung, and a palm branch with a sight-slit in the broader end. The latter was held close to the eye, the former in the other hand, perhaps at arm's length. The "Hermetic" books which Clement refers to are the Egyptian theological texts, which probably have nothing to do with Hellenistic Hermetism.From the tables of stars on the ceiling of the tombs of Rameses VI and Rameses IX it seems that for fixing the hours of the night a man seated on the ground faced the Astrologer in such a position that the line of observation of the pole star passed over the middle of his head. On the different days of the year each hour was determined by a fixed star culminating or nearly culminating in it, and the position of these stars at the time is given in the tables as in the centre, on the left eye, on the right shoulder, etc. According to the texts, in founding or rebuilding temples the north axis was determined by the same apparatus, and we may conclude that it was the usual one for astronomical observations. In careful hands it might give results of a high degree of accuracy.
The astronomy of East Asia began in China. Solar term was completed in Warring States period. The knowledge of Chinese astronomy was introduced into East Asia. Astronomy in China has a long history. Detailed records of astronomical observations were kept from about the 6th century BC, until the introduction of Western astronomy and the telescope in the 17th century. Chinese astronomers were able to precisely predict eclipses. Much of early Chinese astronomy was for the purpose of timekeeping. The Chinese used a lunisolar calendar, but because the cycles of the Sun and the Moon are different, astronomers often prepared new calendars and made observations for that purpose. Astrological divination was also an important part of astronomy. Astronomers took careful note of "guest stars"(Chinese: 客星; pinyin: kèxīng; lit.: 'guest star') which suddenly appeared among the fixed stars. They were the first to record a supernova, in the Astrological Annals of the Houhanshu in 185 AD. Also, the supernova that created the Crab Nebula in 1054 is an example of a "guest star" observed by Chinese astronomers, although it was not recorded by their European contemporaries. Ancient astronomical records of phenomena like supernovae and comets are sometimes used in modern astronomical studies. The world's first star catalogue was made by Gan De, a Chinese astronomer, in the 4th century BC.
Maya astronomical codices include detailed tables for calculating phases of the Moon, the recurrence of eclipses, and the appearance and disappearance of Venus as morning and evening star. The Maya based their calendrics in the carefully calculated cycles of the Pleiades, the Sun, the Moon, Venus, Jupiter, Saturn, Mars, and also they had a precise description of the eclipses as depicted in the Dresden Codex, as well as the ecliptic or zodiac, and the Milky Way was crucial in their Cosmology. A number of important Maya structures are believed to have been oriented toward the extreme risings and settings of Venus. To the ancient Maya, Venus was the patron of war and many recorded battles are believed to have been timed to the motions of this planet. Mars is also mentioned in preserved astronomical codices and early mythology.Although the Maya calendar was not tied to the Sun, John Teeple has proposed that the Maya calculated the solar year to somewhat greater accuracy than the Gregorian calendar. Both astronomy and an intricate numerological scheme for the measurement of time were vitally important components of Maya religion.
Since 1990 our understanding of prehistoric Europeans has been radically changed by discoveries of ancient astronomical artifacts throughout Europe. The artifacts demonstrate that Neolithic and Bronze Age Europeans had a sophisticated knowledge of mathematics and astronomy. Among the discoveries are: Paleolithic archaeologist Alexander Marshack put forward a theory in 1972 that bone sticks from locations like Africa and Europe from possibly as long ago as 35,000 BCE could be marked in ways that tracked the Moon's phases, an interpretation that has met with criticism. The Warren Field calendar in the Dee River valley of Scotland's Aberdeenshire. First excavated in 2004 but only in 2013 revealed as a find of huge significance, it is to date the world's oldest known calendar, created around 8000 BC and predating all other calendars by some 5,000 years. The calendar takes the form of an early Mesolithic monument containing a series of 12 pits which appear to help the observer track lunar months by mimicking the phases of the Moon. It also aligns to sunrise at the winter solstice, thus coordinating the solar year with the lunar cycles. The monument had been maintained and periodically reshaped, perhaps up to hundreds of times, in response to shifting solar/lunar cycles, over the course of 6,000 years, until the calendar fell out of use around 4,000 years ago. Goseck circle is located in Germany and belongs to the linear pottery culture. First discovered in 1991, its significance was only clear after results from archaeological digs became available in 2004. The site is one of hundreds of similar circular enclosures built in a region encompassing Austria, Germany, and the Czech Republic during a 200-year period starting shortly after 5000 BC. The Nebra sky disc is a Bronze Age bronze disc that was buried in Germany, not far from the Goseck circle, around 1600 BC. It measures about 30 cm diameter with a mass of 2.2 kg and displays a blue-green patina (from oxidization) inlaid with gold symbols. Found by archeological thieves in 1999 and recovered in Switzerland in 2002, it was soon recognized as a spectacular discovery, among the most important of the 20th century. Investigations revealed that the object had been in use around 400 years before burial (2000 BC), but that its use had been forgotten by the time of burial. The inlaid gold depicted the full moon, a crescent moon about 4 or 5 days old, and the Pleiades star cluster in a specific arrangement forming the earliest known depiction of celestial phenomena. Twelve lunar months pass in 354 days, requiring a calendar to insert a leap month every two or three years in order to keep synchronized with the solar year's seasons (making it lunisolar). The earliest known descriptions of this coordination were recorded by the Babylonians in 6th or 7th centuries BC, over one thousand years later. Those descriptions verified ancient knowledge of the Nebra sky disc's celestial depiction as the precise arrangement needed to judge when to insert the intercalary month into a lunisolar calendar, making it an astronomical clock for regulating such a calendar a thousand or more years before any other known method. The Kokino site, discovered in 2001, sits atop an extinct volcanic cone at an elevation of 1,013 metres (3,323 ft), occupying about 0.5 hectares overlooking the surrounding countryside in North Macedonia. A Bronze Age astronomical observatory was constructed there around 1900 BC and continuously served the nearby community that lived there until about 700 BC. The central space was used to observe the rising of the Sun and full moon. Three markings locate sunrise at the summer and winter solstices and at the two equinoxes. Four more give the minimum and maximum declinations of the full moon: in summer, and in winter. Two measure the lengths of lunar months. Together, they reconcile solar and lunar cycles in marking the 235 lunations that occur during 19 solar years, regulating a lunar calendar. On a platform separate from the central space, at lower elevation, four stone seats (thrones) were made in north-south alignment, together with a trench marker cut in the eastern wall. This marker allows the rising Sun's light to fall on only the second throne, at midsummer (about July 31). It was used for ritual ceremony linking the ruler to the local sun god, and also marked the end of the growing season and time for harvest. Golden hats of Germany, France and Switzerland dating from 1400–800 BC are associated with the Bronze Age Urnfield culture. The Golden hats are decorated with a spiral motif of the Sun and the Moon. They were probably a kind of calendar used to calibrate between the lunar and solar calendars. Modern scholarship has demonstrated that the ornamentation of the gold leaf cones of the Schifferstadt type, to which the Berlin Gold Hat example belongs, represent systematic sequences in terms of number and types of ornaments per band. A detailed study of the Berlin example, which is the only fully preserved one, showed that the symbols probably represent a lunisolar calendar. The object would have permitted the determination of dates or periods in both lunar and solar calendars.
The Arabic and the Persian world under Islam had become highly cultured, and many important works of knowledge from Greek astronomy and Indian astronomy and Persian astronomy were translated into Arabic, used and stored in libraries throughout the area. An important contribution by Islamic astronomers was their emphasis on observational astronomy. This led to the emergence of the first astronomical observatories in the Muslim world by the early 9th century. Zij star catalogues were produced at these observatories. In the 10th century, Abd al-Rahman al-Sufi (Azophi) carried out observations on the stars and described their positions, magnitudes, brightness, and colour and drawings for each constellation in his Book of Fixed Stars. He also gave the first descriptions and pictures of "A Little Cloud" now known as the Andromeda Galaxy. He mentions it as lying before the mouth of a Big Fish, an Arabic constellation. This "cloud" was apparently commonly known to the Isfahan astronomers, very probably before 905 AD. The first recorded mention of the Large Magellanic Cloud was also given by al-Sufi. In 1006, Ali ibn Ridwan observed SN 1006, the brightest supernova in recorded history, and left a detailed description of the temporary star. In the late 10th century, a huge observatory was built near Tehran, Iran, by the astronomer Abu-Mahmud al-Khujandi who observed a series of meridian transits of the Sun, which allowed him to calculate the tilt of the Earth's axis relative to the Sun. He noted that measurements by earlier (Indian, then Greek) astronomers had found higher values for this angle, possible evidence that the axial tilt is not constant but was in fact decreasing. In 11th-century Persia, Omar Khayyám compiled many tables and performed a reformation of the calendar that was more accurate than the Julian and came close to the Gregorian. Other Muslim advances in astronomy included the collection and correction of previous astronomical data, resolving significant problems in the Ptolemaic model, the development of the universal latitude-independent astrolabe by Arzachel, the invention of numerous other astronomical instruments, Ja'far Muhammad ibn Mūsā ibn Shākir's belief that the heavenly bodies and celestial spheres were subject to the same physical laws as Earth, the first elaborate experiments related to astronomical phenomena, the introduction of exacting empirical observations and experimental techniques, and the introduction of empirical testing by Ibn al-Shatir, who produced the first model of lunar motion which matched physical observations.Natural philosophy (particularly Aristotelian physics) was separated from astronomy by Ibn al-Haytham (Alhazen) in the 11th century, by Ibn al-Shatir in the 14th century, and Qushji in the 15th century, leading to the development of an astronomical physics.
After the significant contributions of Greek scholars to the development of astronomy, it entered a relatively static era in Western Europe from the Roman era through the 12th century. This lack of progress has led some astronomers to assert that nothing happened in Western European astronomy during the Middle Ages. Recent investigations, however, have revealed a more complex picture of the study and teaching of astronomy in the period from the 4th to the 16th centuries.Western Europe entered the Middle Ages with great difficulties that affected the continent's intellectual production. The advanced astronomical treatises of classical antiquity were written in Greek, and with the decline of knowledge of that language, only simplified summaries and practical texts were available for study. The most influential writers to pass on this ancient tradition in Latin were Macrobius, Pliny, Martianus Capella, and Calcidius. In the 6th century Bishop Gregory of Tours noted that he had learned his astronomy from reading Martianus Capella, and went on to employ this rudimentary astronomy to describe a method by which monks could determine the time of prayer at night by watching the stars.In the 7th century the English monk Bede of Jarrow published an influential text, On the Reckoning of Time, providing churchmen with the practical astronomical knowledge needed to compute the proper date of Easter using a procedure called the computus. This text remained an important element of the education of clergy from the 7th century until well after the rise of the Universities in the 12th century.The range of surviving ancient Roman writings on astronomy and the teachings of Bede and his followers began to be studied in earnest during the revival of learning sponsored by the emperor Charlemagne. By the 9th century rudimentary techniques for calculating the position of the planets were circulating in Western Europe; medieval scholars recognized their flaws, but texts describing these techniques continued to be copied, reflecting an interest in the motions of the planets and in their astrological significance.Building on this astronomical background, in the 10th century European scholars such as Gerbert of Aurillac began to travel to Spain and Sicily to seek out learning which they had heard existed in the Arabic-speaking world. There they first encountered various practical astronomical techniques concerning the calendar and timekeeping, most notably those dealing with the astrolabe. Soon scholars such as Hermann of Reichenau were writing texts in Latin on the uses and construction of the astrolabe and others, such as Walcher of Malvern, were using the astrolabe to observe the time of eclipses in order to test the validity of computistical tables.By the 12th century, scholars were traveling to Spain and Sicily to seek out more advanced astronomical and astrological texts, which they translated into Latin from Arabic and Greek to further enrich the astronomical knowledge of Western Europe. The arrival of these new texts coincided with the rise of the universities in medieval Europe, in which they soon found a home. Reflecting the introduction of astronomy into the universities, John of Sacrobosco wrote a series of influential introductory astronomy textbooks: the Sphere, a Computus, a text on the Quadrant, and another on Calculation.In the 14th century, Nicole Oresme, later bishop of Liseux, showed that neither the scriptural texts nor the physical arguments advanced against the movement of the Earth were demonstrative and adduced the argument of simplicity for the theory that the Earth moves, and not the heavens. However, he concluded "everyone maintains, and I think myself, that the heavens do move and not the earth: For God hath established the world which shall not be moved." In the 15th century, Cardinal Nicholas of Cusa suggested in some of his scientific writings that the Earth revolved around the Sun, and that each star is itself a distant sun.
During the renaissance period, astronomy began to undergo a revolution in thought known as the Copernican revolution, which gets the name from the astronomer Nicolaus Copernicus, who proposed a heliocentric system, in which the planets revolved around the Sun and not the Earth. His De Revolutionibus Orbium Coelestium was published in 1543. While in the long term this was a very controversial claim, in the very beginning it only brought minor controversy. The theory became the dominant view because many figures, most notably Galileo Galilei, Johannes Kepler and Isaac Newton championed and improved upon the work. Other figures also aided this new model despite not believing the overall theory, like Tycho Brahe, with his well-known observations.Brahe, a Danish noble, was an essential astronomer in this period. He came on the astronomical scene with the publication of De Nova Stella in which he disproved conventional wisdom on the supernova SN 1572. He also created the Tychonic System in which he blended the mathematical benefits of the Copernican system and the “physical benefits” of the Ptolemaic system. This was one of the systems people believed in when they did not accept heliocentrism, but could no longer accept the Ptolemaic system. He is most known for his highly accurate observations of the stars and the solar system. Later he moved to Prague and continued his work. In Prague he was at work on the Rudolphine Tables, that were not finished until after his death. The Rudolphine Tables was a star map designed to be more accurate than either the Alphonsine Tables, made in the 1300s and the Prutenic Tables which were inaccurate. He was assisted at this time by his assistant Johannes Kepler, who would later use his observations to finish Brahe's works and for his theories as well.After the death of Brahe, Kepler was deemed his successor and was given the job of completing Brahe's uncompleted works, like the Rudolphine Tables. He completed the Rudolphine Tables in 1624, although it was not published for several years. Like many other figures of this era, he was subject to religious and political troubles, like the Thirty Years War, which led to chaos that almost destroyed some of his works. Kepler was, however, the first to attempt to derive mathematical predictions of celestial motions from assumed physical causes. He discovered the three Kepler's Laws of Planetary Motion that now carry his name, those laws being as follows: The orbit of a planet is an ellipse with the Sun at one of the two foci. A line segment joining a planet and the Sun sweeps out equal areas during equal intervals of time. The square of the orbital period of a planet is proportional to the cube of the semi-major axis of its orbit. With these laws, he managed to improve upon the existing Heliocentric model. The first two were published in 1609. Kepler's contributions improved upon the overall system, giving it more credibility because it adequately explained events and could cause more reliable predictions. Before this the Copernican model was just as unreliable as the Ptolemaic model. This improvement came because Kepler realized the orbits were not perfect circles, but ellipses.Galileo Galilei was among the first to use a telescope to observe the sky, and after constructing a 20x refractor telescope. He discovered the four largest moons of Jupiter in 1610, which are now collectively known as the Galilean moons, in his honor. This discovery was the first known observation of satellites orbiting another planet. He also found that our Moon had craters and observed, and correctly explained, sunspots, and that Venus exhibited a full set of phases resembling lunar phases. Galileo argued that these facts demonstrated incompatibility with the Ptolemaic model, which could not explain the phenomenon and would even contradict it. With the moons it demonstrated that the Earth does not have to have everything orbiting it and that other parts of the Solar System could orbit another object, such as the Earth orbiting the Sun. In the Ptolemaic system the celestial bodies were supposed to be perfect so such objects should not have craters or sunspots. The phases of Venus could only happen in the event that Venus's orbit is insides Earth's orbit, which could not happen if the Earth was the center. He, as the most famous example, had to face challenges from church officials, more specifically the Roman Inquisition. They accused him of heresy because these beliefs went against the teachings of the Roman Catholic Church and were challenging the Catholic church's authority when it was at its weakest. While he was able to avoid punishment for a little while he was eventually tried and pled guilty to heresy in 1633. Although this came at some expense, his book was banned, and he was put under house arrest until he died in 1642.Sir Isaac Newton developed further ties between physics and astronomy through his law of universal gravitation. Realizing that the same force that attracts objects to the surface of the Earth held the Moon in orbit around the Earth, Newton was able to explain – in one theoretical framework – all known gravitational phenomena. In his Philosophiae Naturalis Principia Mathematica, he derived Kepler's laws from first principles. Those first principles are as follows: In an inertial frame of reference, an object either remains at rest or continues to move at constant velocity, unless acted upon by a force. In an inertial reference frame, the vector sum of the forces F on an object is equal to the mass m of that object multiplied by the acceleration a of the object: F = ma. (It is assumed here that the mass m is constant) When one body exerts a force on a second body, the second body simultaneously exerts a force equal in magnitude and opposite in direction on the first body.Thus while Kepler explained how the planets moved, Newton accurately managed to explain why the planets moved the way they do. Newton's theoretical developments laid many of the foundations of modern physics.
Outside of England, Newton's theory took some time to become established. Descartes' theory of vortices held sway in France, and Huygens, Leibniz and Cassini accepted only parts of Newton's system, preferring their own philosophies. Voltaire published a popular account in 1738. In 1748, the French Academy of Sciences offered a reward for solving the perturbations of Jupiter and Saturn which was eventually solved by Euler and Lagrange. Laplace completed the theory of the planets, publishing from 1798 to 1825. Edmund Halley succeeded Flamsteed as Astronomer Royal in England and succeeded in predicting the return in 1758 of the comet that bears his name. Sir William Herschel found the first new planet, Uranus, to be observed in modern times in 1781. The gap between the planets Mars and Jupiter disclosed by the Titius–Bode law was filled by the discovery of the asteroids Ceres and Pallas in 1801 and 1802 with many more following. At first, astronomical thought in America was based on Aristotelian philosophy, but interest in the new astronomy began to appear in Almanacs as early as 1659.
In the 19th century, Joseph von Fraunhofer discovered that when sunlight was dispersed, a multitude of spectral lines were observed (regions where there was less or no light). Experiments with hot gases showed that the same lines could be observed in the spectra of gases, with specific lines corresponding to unique elements. It was proved that the chemical elements found in the Sun (chiefly hydrogen and helium) were also found on Earth. During the 20th century spectroscopy (the study of these lines) advanced, especially because of the advent of quantum physics, which was necessary to understand the observations. Although in previous centuries noted astronomers were exclusively male, at the turn of the 20th century women began to play a role in the great discoveries. In this period prior to modern computers, women at the United States Naval Observatory (USNO), Harvard University, and other astronomy research institutions began to be hired as human "computers", who performed the tedious calculations while scientists performed research requiring more background knowledge. A number of discoveries in this period were originally noted by the women "computers" and reported to their supervisors. For example, at the Harvard Observatory Henrietta Swan Leavitt discovered the cepheid variable star period-luminosity relation which she further developed into a method of measuring distance outside of the Solar System. Annie Jump Cannon, also at Harvard, organized the stellar spectral types according to stellar temperature. In 1847, Maria Mitchell discovered a comet using a telescope. According to Lewis D. Eigen, Cannon alone, "in only 4 years discovered and catalogued more stars than all the men in history put together." Most of these women received little or no recognition during their lives due to their lower professional standing in the field of astronomy. Although their discoveries and methods are taught in classrooms around the world, few students of astronomy can attribute the works to their authors or have any idea that there were active female astronomers at the end of the 19th century.
Most of our current knowledge was gained during the 20th century. With the help of the use of photography, fainter objects were observed. The Sun was found to be part of a galaxy made up of more than 1010 stars (10 billion stars). The existence of other galaxies, one of the matters of the great debate, was settled by Edwin Hubble, who identified the Andromeda nebula as a different galaxy, and many others at large distances and receding, moving away from our galaxy. Physical cosmology, a discipline that has a large intersection with astronomy, made huge advances during the 20th century, with the model of the hot Big Bang heavily supported by the evidence provided by astronomy and physics, such as the redshifts of very distant galaxies and radio sources, the cosmic microwave background radiation, Hubble's law and cosmological abundances of elements.
In the 19th century, scientists began discovering forms of light which were invisible to the naked eye: X-Rays, gamma rays, radio waves, microwaves, ultraviolet radiation, and infrared radiation. This had a major impact on astronomy, spawning the fields of infrared astronomy, radio astronomy, x-ray astronomy and finally gamma-ray astronomy. With the advent of spectroscopy it was proven that other stars were similar to the Sun, but with a range of temperatures, masses and sizes. The existence of our galaxy, the Milky Way, as a separate group of stars was only proven in the 20th century, along with the existence of "external" galaxies, and soon after, the expansion of the universe seen in the recession of most galaxies from us.
DIO: The International Journal of Scientific History Journal for the History of Astronomy Journal of Astronomical History and Heritage
Paris Observatory books and manuscripts UNESCO-IAU Portal to the Heritage of Astronomy Astronomiae Historia / History of Astronomy at the Astronomical Institutes of Bonn University. Society for the History of Astronomy Mayan Astronomy Caelum Antiquum: Ancient Astronomy and Astrology at LacusCurtius Mesoamerican Archaeoastronomy "The Book of Instruction on Deviant Planes and Simple Planes" is a manuscript in Arabic that dates back to 1740 and talks about practical astronomy, with diagrams. More information on women astronomers Astronomy & Empire, BBC Radio 4 discussion with Simon Schaffer, Kristen Lippincott & Allan Chapman (In Our Time, May 4, 2006)
An economy (from Greek οίκος – "household" and νέμoμαι – "manage") is an area of the production, distribution and trade, as well as consumption of goods and services by different agents. Understood in its broadest sense, 'The economy is defined as a social domain that emphasize the practices, discourses, and material expressions associated with the production, use, and management of resources'. A given economy is the result of a set of processes that involves its culture, values, education, technological evolution, history, social organization, political structure and legal systems, as well as its geography, natural resource endowment, and ecology, as main factors. These factors give context, content, and set the conditions and parameters in which an economy functions. In other words, the economic domain is a social domain of human practices and transactions. It does not stand alone. Economic agents can be individuals, businesses, organizations, or governments. Economic transactions occur when two groups or parties agree to the value or price of the transacted good or service, commonly expressed in a certain currency. However, monetary transactions only account for a small part of the economic domain. Economic activity is spurred by production which uses natural resources, labor and capital. It has changed over time due to technology (automation, accelerator of process, reduction of cost functions), innovation (new products, services, processes, expanding markets, diversification of markets, niche markets, increases revenue functions) such as, that which produces intellectual property and changes in industrial relations (most notably child labor being replaced in some parts of the world with universal access to education). A market-based economy is one where goods and services are produced and exchanged according to demand and supply between participants (economic agents) by barter or a medium of exchange with a credit or debit value accepted within the network, such as a unit of currency. A command-based economy is one where political agents directly control what is produced and how it is sold and distributed. A green economy is low-carbon, resource efficient and socially inclusive. In a green economy, growth in income and employment is driven by public and private investments that reduce carbon emissions and pollution, enhance energy and resource efficiency, and prevent the loss of biodiversity and ecosystem services. A gig economy is one in which short-term jobs are assigned or chosen via online platforms. New economy is a term that referred to the whole emerging ecosystem where new standards and practices were introduced, usually as a result of technological innovations. The global economy refers to humanity's economic system or systems overall.
Today the range of fields of study examining the economy revolves around the social science of economics, but may include sociology (economic sociology), history (economic history), anthropology (economic anthropology), and geography (economic geography). Practical fields directly related to the human activities involving production, distribution, exchange, and consumption of goods and services as a whole are engineering, management, business administration, applied science, and finance. All professions, occupations, economic agents or economic activities, contribute to the economy. Consumption, saving, and investment are variable components in the economy that determine macroeconomic equilibrium. There are three main sectors of economic activity: primary, secondary, and tertiary. Due to the growing importance of the economical sector in modern times, the term real economy is used by analysts as well as politicians to denote the part of the economy that is concerned with the actual production of goods and services, as ostensibly contrasted with the paper economy, or the financial side of the economy, which is concerned with buying and selling on the financial markets. Alternate and long-standing terminology distinguishes measures of an economy expressed in real values (adjusted for inflation), such as real GDP, or in nominal values (unadjusted for inflation).
As long as someone has been making, supplying and distributing goods or services, there has been some sort of economy; economies grew larger as societies grew and became more complex. Sumer developed a large-scale economy based on commodity money, while the Babylonians and their neighboring city states later developed the earliest system of economics as we think of, in terms of rules/laws on debt, legal contracts and law codes relating to business practices, and private property.The Babylonians and their city state neighbors developed forms of economics comparable to currently used civil society (law) concepts. They developed the first known codified legal and administrative systems, complete with courts, jails, and government records. The ancient economy was mainly based on subsistence farming. The Shekel referred to an ancient unit of weight and currency. The first usage of the term came from Mesopotamia circa 3000 BC. and referred to a specific mass of barley which related other values in a metric such as silver, bronze, copper etc. A barley/shekel was originally both a unit of currency and a unit of weight, just as the British Pound was originally a unit denominating a one-pound mass of silver. For most people, the exchange of goods occurred through social relationships. There were also traders who bartered in the marketplaces. In Ancient Greece, where the present English word 'economy' originated, many people were bond slaves of the freeholders. The economic discussion was driven by scarcity.
The European captures became branches of the European states, the so-called colonies. The rising nation-states Spain, Portugal, France, Great Britain and the Netherlands tried to control the trade through custom duties and (from mercator, lat.: merchant) was a first approach to intermediate between private wealth and public interest. The secularization in Europe allowed states to use the immense property of the church for the development of towns. The influence of the nobles decreased. The first Secretaries of State for economy started their work. Bankers like Amschel Mayer Rothschild (1773–1855) started to finance national projects such as wars and infrastructure. Economy from then on meant national economy as a topic for the economic activities of the citizens of a state.
The first economist in the true modern meaning of the word was the Scotsman Adam Smith (1723–1790) who was inspired partly by the ideas of physiocracy, a reaction to mercantilism and also later Economics student, Adam Mari. He defined the elements of a national economy: products are offered at a natural price generated by the use of competition - supply and demand - and the division of labor. He maintained that the basic motive for free trade is human self-interest. The so-called self-interest hypothesis became the anthropological basis for economics. Thomas Malthus (1766–1834) transferred the idea of supply and demand to the problem of overpopulation. The Industrial Revolution was a period from the 18th to the 19th century where major changes in agriculture, manufacturing, mining, and transport had a profound effect on the socioeconomic and cultural conditions starting in the United Kingdom, then subsequently spreading throughout Europe, North America, and eventually the world. The onset of the Industrial Revolution marked a major turning point in human history; almost every aspect of daily life was eventually influenced in some way. In Europe wild capitalism started to replace the system of mercantilism (today: protectionism) and led to economic growth. The period today is called industrial revolution because the system of Production, production and division of labor enabled the mass production of goods.
The contemporary concept of "the economy" wasn't popularly known until the American Great Depression in the 1930s.After the chaos of two World Wars and the devastating Great Depression, policymakers searched for new ways of controlling the course of the economy. This was explored and discussed by Friedrich August von Hayek (1899–1992) and Milton Friedman (1912–2006) who pleaded for a global free trade and are supposed to be the fathers of the so-called neoliberalism. However, the prevailing view was that held by John Maynard Keynes (1883–1946), who argued for a stronger control of the markets by the state. The theory that the state can alleviate economic problems and instigate economic growth through state manipulation of aggregate demand is called Keynesianism in his honor. In the late 1950s, the economic growth in America and Europe—often called Wirtschaftswunder (ger: economic miracle) —brought up a new form of economy: mass consumption economy. In 1958, John Kenneth Galbraith (1908–2006) was the first to speak of an affluent society. In most of the countries the economic system is called a social market economy.
With the fall of the Iron Curtain and the transition of the countries of the Eastern Bloc towards democratic government and market economies, the idea of the post-industrial society is brought into importance as its role is to mark together the significance that the service sector receives instead of industrialization. Some attribute the first use of this term to Daniel Bell's 1973 book, The Coming of Post-Industrial Society, while others attribute it to social philosopher Ivan Illich's book, Tools for Conviviality. The term is also applied in philosophy to designate the fading of postmodernism in the late 90s and especially in the beginning of the 21st century. With the spread of Internet as a mass media and communication medium especially after 2000-2001, the idea for the Internet and information economy is given place because of the growing importance of e-commerce and electronic businesses, also the term for a global information society as understanding of a new type of "all-connected" society is created. In the late 2000s, the new type of economies and economic expansions of countries like China, Brazil, and India bring attention and interest to different from the usually dominating Western type economies and economic models.
The economy may be considered as having developed through the following phases or degrees of precedence. The ancient economy was mainly based on subsistence farming. The industrial revolution phase lessened the role of subsistence farming, converting it to more extensive and mono-cultural forms of agriculture in the last three centuries. The economic growth took place mostly in mining, construction and manufacturing industries. Commerce became more significant due to the need for improved exchange and distribution of produce throughout the community. In the economies of modern consumer societies phase there is a growing part played by services, finance, and technology—the knowledge economy.In modern economies, these phase precedences are somewhat differently expressed by the three-sector theory. Primary stage/degree of the economy: Involves the extraction and production of raw materials, such as corn, coal, wood and iron. (A coal miner and a fisherman would be workers in the primary degree.) Secondary stage/degree of the economy: Involves the transformation of raw or intermediate materials into goods e.g. manufacturing steel into cars, or textiles into clothing. (A builder and a dressmaker would be workers in the secondary degree.) At this stage the associated industrial economy is also sub-divided into several economic sectors (also called industries). Their separate evolution during the Industrial Revolution phase is dealt with elsewhere. Tertiary stage/degree of the economy: Involves the provision of services to consumers and businesses, such as baby-sitting, cinema and banking. (A shopkeeper and an accountant would be workers in the tertiary degree.) Quaternary stage/degree of the economy: Involves the research and development needed to produce products from natural resources and their subsequent by-products. (A logging company might research ways to use partially burnt wood to be processed so that the undamaged portions of it can be made into pulp for paper.) Note that education is sometimes included in this sector.Other sectors of the developed community include : the public sector or state sector (which usually includes: parliament, law-courts and government centers, various emergency services, public health, shelters for impoverished and threatened people, transport facilities, air/sea ports, post-natal care, hospitals, schools, libraries, museums, preserved historical buildings, parks/gardens, nature-reserves, some universities, national sports grounds/stadiums, national arts/concert-halls or theaters and centers for various religions). the private sector or privately run businesses. the social sector or voluntary sector.
There are a number of concepts associated with the economy, such as these:
The GDP (gross domestic product) of a country is a measure of the size of its economy. The most conventional economic analysis of a country relies heavily on economic indicators like the GDP and GDP per capita. While often useful, GDP only includes economic activity for which money is exchanged.
An informal economy is economic activity that is neither taxed nor monitored by a government, contrasted with a formal economy. The informal economy is thus not included in that government's gross national product (GNP). Although the informal economy is often associated with developing countries, all economic systems contain an informal economy in some proportion. Informal economic activity is a dynamic process that includes many aspects of economic and social theory including exchange, regulation, and enforcement. By its nature, it is necessarily difficult to observe, study, define, and measure. No single source readily or authoritatively defines informal economy as a unit of study. The terms "underground", "under the table" and "off the books" typically refer to this type of economy. The term black market refers to a specific subset of the informal economy. The term "informal sector" was used in many earlier studies, and has been mostly replaced in more recent studies which use the newer term. The informal sector makes up a significant portion of the economies in developing countries but it is often stigmatized as troublesome and unmanageable. However, the informal sector provides critical economic opportunities for the poor and has been expanding rapidly since the 1960s. As such, integrating the informal economy into the formal sector is an important policy challenge.
Economic research is conducted in fields as different as economics, economic sociology, economic anthropology, and economic history.
Aristotle, Politics, Book I-IIX, translated by Benjamin Jowett, Classics.mit.edu Barnes, Peter, Capitalism 3.0, A Guide to Reclaiming the Commons, San Francisco 2006, Whatiseconomy.com Dill, Alexander, Reclaiming the Hidden Assets, Towards a Global Freeware Index, Global Freeware Research Paper 01-07, 2007, Whatiseconomy.com Fehr Ernst, Schmidt, Klaus M., The Economics Of Fairness, Reciprocity and Altruism - experimental Evidence and new Theories, 2005, Discussion PAPER 2005-20, Munich Economics, Whatiseconomy.com Marx, Karl, Engels, Friedrich, 1848, The Communist Manifesto, Marxists.org Stiglitz, Joseph E., Global public goods and global finance: does global governance ensure that the global public interest is served? In: Advancing Public Goods, Jean-Philippe Touffut, (ed.), Paris 2006, pp. 149/164, GSB.columbia.edu Where is the Wealth of Nations? Measuring Capital for the 21st Century. Wealth of Nations Report 2006, Ian Johnson and Francois Bourguignon, World Bank, Washington 2006, Whatiseconomy.com.
Keynesian economics ( KAYN-zee-ən; sometimes Keynesianism, named for the economist John Maynard Keynes) are various macroeconomic theories about how economic output is strongly influenced by aggregate demand (total spending in the economy). In the Keynesian view, aggregate demand does not necessarily equal the productive capacity of the economy. Instead, it is influenced by a host of factors. According to Keynes, the productive capacity of the economy sometimes behaves erratically, affecting production, employment, and inflation.Keynesian economics developed during and after the Great Depression from the ideas presented by Keynes in his 1936 book, The General Theory of Employment, Interest and Money. Keynes' approach was a stark contrast to the aggregate supply-focused classical economics that preceded his book. Interpreting Keynes's work is a contentious topic, and several schools of economic thought claim his legacy. Keynesian economics served as the standard economic model in the developed nations during the later part of the Great Depression, World War II, and the post-war economic expansion (1945–1973). It lost some influence following the oil shock and resulting stagflation of the 1970s. Keynesian economics was later redeveloped as New Keynesian economics, becoming part of the contemporary new neoclassical synthesis. The advent of the financial crisis of 2007–2008 caused a resurgence of popular interest in Keynesian thought.Keynesian economists generally argue that aggregate demand is volatile and unstable. They propose that a market economy often experiences inefficient macroeconomic outcomes in the form of economic recessions (when demand is low) and inflation (when demand is high), and that these can be mitigated by economic policy responses. In particular, monetary policy actions by the central bank and fiscal policy actions by the government can help stabilize output over the business cycle. Keynesian economists generally advocate a market economy – predominantly private sector, but with an active role for government intervention during recessions and depressions.
Macroeconomics is the study of the factors applying to an economy as a whole. Influential economic factors include the overall price level, the interest rate, and the level of employment (or equivalently, of income/output measured in real terms). The classical tradition of partial equilibrium theory had been to split the economy into separate markets, each of whose equilibrium conditions could be stated as a single equation determining a single variable. The theoretical apparatus of supply and demand curves developed by Fleeming Jenkin and Alfred Marshall provided a unified mathematical basis for this approach, which the Lausanne School generalized to general equilibrium theory. For macroeconomics, relevant partial theories included the Quantity theory of money determining the price level and the classical theory of the interest rate. In regards to employment, the condition referred to by Keynes as the "first postulate of classical economics" stated that the wage is equal to the marginal product, which is a direct application of the marginalist principles developed during the nineteenth century (see The General Theory). Keynes sought to supplant all three aspects of the classical theory.
Although Keynes's work was crystallized and given impetus by the advent of the Great Depression, it was part of a long-running debate within economics over the existence and nature of general gluts. A number of the policies Keynes advocated to address the Great Depression (notably government deficit spending at times of low private investment or consumption), and many of the theoretical ideas he proposed (effective demand, the multiplier, the paradox of thrift), had been advanced by various authors in the 19th and early 20th centuries. Keynes's unique contribution was to provide a general theory of these, which proved acceptable to the economic establishment. An intellectual precursor of Keynesian economics was underconsumption theories associated with John Law, Thomas Malthus, the Birmingham School of Thomas Attwood, and the American economists William Trufant Foster and Waddill Catchings, who were influential in the 1920s and 1930s. Underconsumptionists were, like Keynes after them, concerned with failure of aggregate demand to attain potential output, calling this "underconsumption" (focusing on the demand side), rather than "overproduction" (which would focus on the supply side), and advocating economic interventionism. Keynes specifically discussed underconsumption (which he wrote "under-consumption") in the General Theory, in Chapter 22, Section IV and Chapter 23, Section VII. Numerous concepts were developed earlier and independently of Keynes by the Stockholm school during the 1930s; these accomplishments were described in a 1937 article, published in response to the 1936 General Theory, sharing the Swedish discoveries.The paradox of thrift was stated in 1892 by John M. Robertson in his The Fallacy of Saving, in earlier forms by mercantilist economists since the 16th century, and similar sentiments date to antiquity.
In 1923 Keynes published his first contribution to economic theory, A Tract on Monetary Reform, whose point of view is classical but incorporates ideas that later played a part in the General Theory. In particular, looking at the hyperinflation in European economies, he drew attention to the opportunity cost of holding money (identified with inflation rather than interest) and its influence on the velocity of circulation.In 1930 he published A Treatise on Money, intended as a comprehensive treatment of its subject "which would confirm his stature as a serious academic scholar, rather than just as the author of stinging polemics", and marks a large step in the direction of his later views. In it, he attributes unemployment to wage stickiness and treats saving and investment as governed by independent decisions: the former varying positively with the interest rate, the latter negatively. The velocity of circulation is expressed as a function of the rate of interest. He interpreted his treatment of liquidity as implying a purely monetary theory of interest.Keynes's younger colleagues of the Cambridge Circus and Ralph Hawtrey believed that his arguments implicitly assumed full employment, and this influenced the direction of his subsequent work. During 1933, he wrote essays on various economic topics "all of which are cast in terms of movement of output as a whole".
At the time that Keynes's wrote the General Theory, it had been a tenet of mainstream economic thought that the economy would automatically revert to a state of general equilibrium: it had been assumed that, because the needs of consumers are always greater than the capacity of the producers to satisfy those needs, everything that is produced would eventually be consumed once the appropriate price was found for it. This perception is reflected in Say's law and in the writing of David Ricardo, which states that individuals produce so that they can either consume what they have manufactured or sell their output so that they can buy someone else's output. This argument rests upon the assumption that if a surplus of goods or services exists, they would naturally drop in price to the point where they would be consumed. Given the backdrop of high and persistent unemployment during the Great Depression, Keynes argued that there was no guarantee that the goods that individuals produce would be met with adequate effective demand, and periods of high unemployment could be expected, especially when the economy was contracting in size. He saw the economy as unable to maintain itself at full employment automatically, and believed that it was necessary for the government to step in and put purchasing power into the hands of the working population through government spending. Thus, according to Keynesian theory, some individually rational microeconomic-level actions such as not investing savings in the goods and services produced by the economy, if taken collectively by a large proportion of individuals and firms, can lead to outcomes wherein the economy operates below its potential output and growth rate. Prior to Keynes, a situation in which aggregate demand for goods and services did not meet supply was referred to by classical economists as a general glut, although there was disagreement among them as to whether a general glut was possible. Keynes argued that when a glut occurred, it was the over-reaction of producers and the laying off of workers that led to a fall in demand and perpetuated the problem. Keynesians therefore advocate an active stabilization policy to reduce the amplitude of the business cycle, which they rank among the most serious of economic problems. According to the theory, government spending can be used to increase aggregate demand, thus increasing economic activity, reducing unemployment and deflation.
As the 1929 election approached "Keynes was becoming a strong public advocate of capital development" as a public measure to alleviate unemployment. Winston Churchill, the Conservative Chancellor, took the opposite view: It is the orthodox Treasury dogma, steadfastly held ... [that] very little additional employment and no permanent additional employment can, in fact, be created by State borrowing and State expenditure. Keynes pounced on a chink in the Treasury view. Cross-examining Sir Richard Hopkins, a Second Secretary in the Treasury, before the Macmillan Committee on Finance and Industry in 1930 he referred to the "first proposition" that "schemes of capital development are of no use for reducing unemployment" and asked whether "it would be a misunderstanding of the Treasury view to say that they hold to the first proposition". Hopkins responded that "The first proposition goes much too far. The first proposition would ascribe to us an absolute and rigid dogma, would it not?"Later the same year, speaking in a newly created Committee of Economists, Keynes tried to use Kahn's emerging multiplier theory to argue for public works, "but Pigou's and Henderson's objections ensured that there was no sign of this in the final product". In 1933 he gave wider publicity to his support for Kahn's multiplier in a series of articles titled "The road to prosperity" in The Times newspaper.A. C. Pigou was at the time the sole economics professor at Cambridge. He had a continuing interest in the subject of unemployment, having expressed the view in his popular Unemployment (1913) that it was caused by "maladjustment between wage-rates and demand" – a view Keynes may have shared prior to the years of the General Theory. Nor were his practical recommendations very different: "on many occasions in the thirties" Pigou "gave public support ... to State action designed to stimulate employment." Where the two men differed is in the link between theory and practice. Keynes was seeking to build theoretical foundations to support his recommendations for public works while Pigou showed no disposition to move away from classical doctrine. Referring to him and Dennis Robertson, Keynes asked rhetorically: "Why do they insist on maintaining theories from which their own practical conclusions cannot possibly follow?"
John Maynard Keynes (1883–1946) set forward the ideas that became the basis for Keynesian economics in his main work, The General Theory of Employment, Interest and Money (1936). It was written during the Great Depression, when unemployment rose to 25% in the United States and as high as 33% in some countries. It is almost wholly theoretical, enlivened by occasional passages of satire and social commentary. The book had a profound impact on economic thought, and ever since it was published there has been debate over its meaning.
Keynes begins the General Theory with a summary of the classical theory of employment, which he encapsulates in his formulation of Say's Law as the dictum "Supply creates its own demand". Under the classical theory, the wage rate is determined by the marginal productivity of labour, and as many people are employed as are willing to work at that rate. Unemployment may arise through friction or may be "voluntary," in the sense that it arises from a refusal to accept employment owing to "legislation or social practices ... or mere human obstinacy", but "...the classical postulates do not admit of the possibility of the third category," which Keynes defines as involuntary unemployment.Keynes raises two objections to the classical theory's assumption that "wage bargains ... determine the real wage". The first lies in the fact that "labour stipulates (within limits) for a money-wage rather than a real wage". The second is that classical theory assumes that, "The real wages of labour depend on the wage bargains which labour makes with the entrepreneurs," whereas, "If money wages change, one would have expected the classical school to argue that prices would change in almost the same proportion, leaving the real wage and the level of unemployment practically the same as before." Keynes considers his second objection the more fundamental, but most commentators concentrate on his first one: it has been argued that the quantity theory of money protects the classical school from the conclusion Keynes expected from it.
Saving is that part of income not devoted to consumption, and consumption is that part of expenditure not allocated to investment, i.e., to durable goods. Hence saving encompasses hoarding (the accumulation of income as cash) and the purchase of durable goods. The existence of net hoarding, or of a demand to hoard, is not admitted by the simplified liquidity preference model of the General Theory. Once he rejects the classical theory that unemployment is due to excessive wages, Keynes proposes an alternative based on the relationship between saving and investment. In his view, unemployment arises whenever entrepreneurs' incentive to invest fails to keep pace with society's propensity to save (propensity is one of Keynes's synonyms for "demand"). The levels of saving and investment are necessarily equal, and income is therefore held down to a level where the desire to save is no greater than the incentive to invest. The incentive to invest arises from the interplay between the physical circumstances of production and psychological anticipations of future profitability; but once these things are given the incentive is independent of income and depends solely on the rate of interest r. Keynes designates its value as a function of r as the "schedule of the marginal efficiency of capital".The propensity to save behaves quite differently. Saving is simply that part of income not devoted to consumption, and: ... the prevailing psychological law seems to be that when aggregate income increases, consumption expenditure will also increase but to a somewhat lesser extent. Keynes adds that "this psychological law was of the utmost importance in the development of my own thought".
Keynes viewed the money supply as one of the main determinants of the state of the real economy. The significance he attributed to it is one of the innovative features of his work, and was influential on the politically hostile monetarist school. Money supply comes into play through the liquidity preference function, which is the demand function that corresponds to money supply. It specifies the amount of money people will seek to hold according to the state of the economy. In Keynes's first (and simplest) account – that of Chapter 13 – liquidity preference is determined solely by the interest rate r—which is seen as the earnings forgone by holding wealth in liquid form: hence liquidity preference can be written L(r ) and in equilibrium must equal the externally fixed money supply M̂.
Money supply, saving and investment combine to determine the level of income as illustrated in the diagram, where the top graph shows money supply (on the vertical axis) against interest rate. M̂ determines the ruling interest rate r̂ through the liquidity preference function. The rate of interest determines the level of investment Î through the schedule of the marginal efficiency of capital, shown as a blue curve in the lower graph. The red curves in the same diagram show what the propensities to save are for different incomes Y ; and the income Ŷ corresponding to the equilibrium state of the economy must be the one for which the implied level of saving at the established interest rate is equal to Î. In Keynes's more complicated liquidity preference theory (presented in Chapter 15) the demand for money depends on income as well as on the interest rate and the analysis becomes more complicated. Keynes never fully integrated his second liquidity preference doctrine with the rest of his theory, leaving that to John Hicks: see the IS-LM model below.
Keynes rejects the classical explanation of unemployment based on wage rigidity, but it is not clear what effect the wage rate has on unemployment in his system. He treats wages of all workers as proportional to a single rate set by collective bargaining, and chooses his units so that this rate never appears separately in his discussion. It is present implicitly in those quantities he expresses in wage units, while being absent from those he expresses in money terms. It is therefore difficult to see whether, and in what way, his results differ for a different wage rate, nor is it clear what he thought about the matter.
An increase in the money supply, according to Keynes's theory, leads to a drop in the interest rate and an increase in the amount of investment that can be undertaken profitably, bringing with it an increase in total income.
The liquidity trap is a phenomenon that may impede the effectiveness of monetary policies in reducing unemployment. Economists generally think the rate of interest will not fall below a certain limit, often seen as zero or a slightly negative number. Keynes suggested that the limit might be appreciably greater than zero but did not attach much practical significance to it. The term "liquidity trap" was coined by Dennis Robertson in his comments on the General Theory, but it was John Hicks in "Mr. Keynes and the Classics" who recognised the significance of a slightly different concept. If the economy is in a position such that the liquidity preference curve is almost vertical, as must happen as the lower limit on r is approached, then a change in the money supply M̂ makes almost no difference to the equilibrium rate of interest r̂ or, unless there is compensating steepness in the other curves, to the resulting income Ŷ. As Hicks put it, "Monetary means will not force down the rate of interest any further." Paul Krugman has worked extensively on the liquidity trap, claiming that it was the problem confronting the Japanese economy around the turn of the millennium. In his later words: Short-term interest rates were close to zero, long-term rates were at historical lows, yet private investment spending remained insufficient to bring the economy out of deflation. In that environment, monetary policy was just as ineffective as Keynes described. Attempts by the Bank of Japan to increase the money supply simply added to already ample bank reserves and public holdings of cash...
Keynes argued that the solution to the Great Depression was to stimulate the country ("incentive to invest") through some combination of two approaches: A reduction in interest rates (monetary policy), and Government investment in infrastructure (fiscal policy).If the interest rate at which businesses and consumers can borrow decreases, investments that were previously uneconomic become profitable, and large consumer sales normally financed through debt (such as houses, automobiles, and, historically, even appliances like refrigerators) become more affordable. A principal function of central banks in countries that have them is to influence this interest rate through a variety of mechanisms collectively called monetary policy. This is how monetary policy that reduces interest rates is thought to stimulate economic activity, i.e., "grow the economy"—and why it is called expansionary monetary policy. Expansionary fiscal policy consists of increasing net public spending, which the government can effect by a) taxing less, b) spending more, or c) both. Investment and consumption by government raises demand for businesses' products and for employment, reversing the effects of the aforementioned imbalance. If desired spending exceeds revenue, the government finances the difference by borrowing from capital markets by issuing government bonds. This is called deficit spending. Two points are important to note at this point. First, deficits are not required for expansionary fiscal policy, and second, it is only change in net spending that can stimulate or depress the economy. For example, if a government ran a deficit of 10% both last year and this year, this would represent neutral fiscal policy. In fact, if it ran a deficit of 10% last year and 5% this year, this would actually be contractionary. On the other hand, if the government ran a surplus of 10% of GDP last year and 5% this year, that would be expansionary fiscal policy, despite never running a deficit at all. But – contrary to some critical characterizations of it – Keynesianism does not consist solely of deficit spending, since it recommends adjusting fiscal policies according to cyclical circumstances. An example of a counter-cyclical policy is raising taxes to cool the economy and to prevent inflation when there is abundant demand-side growth, and engaging in deficit spending on labour-intensive infrastructure projects to stimulate employment and stabilize wages during economic downturns. Keynes's ideas influenced Franklin D. Roosevelt's view that insufficient buying-power caused the Depression. During his presidency, Roosevelt adopted some aspects of Keynesian economics, especially after 1937, when, in the depths of the Depression, the United States suffered from recession yet again following fiscal contraction. But to many the true success of Keynesian policy can be seen at the onset of World War II, which provided a kick to the world economy, removed uncertainty, and forced the rebuilding of destroyed capital. Keynesian ideas became almost official in social-democratic Europe after the war and in the U.S. in the 1960s. The Keynesian advocacy of deficit spending contrasted with the classical and neoclassical economic analysis of fiscal policy. They admitted that fiscal stimulus could actuate production. But, to these schools, there was no reason to believe that this stimulation would outrun the side-effects that "crowd out" private investment: first, it would increase the demand for labour and raise wages, hurting profitability; Second, a government deficit increases the stock of government bonds, reducing their market price and encouraging high interest rates, making it more expensive for business to finance fixed investment. Thus, efforts to stimulate the economy would be self-defeating. The Keynesian response is that such fiscal policy is appropriate only when unemployment is persistently high, above the non-accelerating inflation rate of unemployment (NAIRU). In that case, crowding out is minimal. Further, private investment can be "crowded in": Fiscal stimulus raises the market for business output, raising cash flow and profitability, spurring business optimism. To Keynes, this accelerator effect meant that government and business could be complements rather than substitutes in this situation. Second, as the stimulus occurs, gross domestic product rises—raising the amount of saving, helping to finance the increase in fixed investment. Finally, government outlays need not always be wasteful: government investment in public goods that is not provided by profit-seekers encourages the private sector's growth. That is, government spending on such things as basic research, public health, education, and infrastructure could help the long-term growth of potential output. In Keynes's theory, there must be significant slack in the labour market before fiscal expansion is justified. Keynesian economists believe that adding to profits and incomes during boom cycles through tax cuts, and removing income and profits from the economy through cuts in spending during downturns, tends to exacerbate the negative effects of the business cycle. This effect is especially pronounced when the government controls a large fraction of the economy, as increased tax revenue may aid investment in state enterprises in downturns, and decreased state revenue and investment harm those enterprises.
In the last few years of his life, John Maynard Keynes was much preoccupied with the question of balance in international trade. He was the leader of the British delegation to the United Nations Monetary and Financial Conference in 1944 that established the Bretton Woods system of international currency management. He was the principal author of a proposal – the so-called Keynes Plan – for an International Clearing Union. The two governing principles of the plan were that the problem of settling outstanding balances should be solved by 'creating' additional 'international money', and that debtor and creditor should be treated almost alike as disturbers of equilibrium. In the event, though, the plans were rejected, in part because "American opinion was naturally reluctant to accept the principle of equality of treatment so novel in debtor-creditor relationships".The new system is not founded on free trade (liberalisation of foreign trade) but rather on regulating international trade to eliminate trade imbalances. Nations with a surplus would have a powerful incentive to get rid of it, which would automatically clear other nations' deficits. Keynes proposed a global bank that would issue its own currency—the bancor—which was exchangeable with national currencies at fixed rates of exchange and would become the unit of account between nations, which means it would be used to measure a country's trade deficit or trade surplus. Every country would have an overdraft facility in its bancor account at the International Clearing Union. He pointed out that surpluses lead to weak global aggregate demand – countries running surpluses exert a "negative externality" on trading partners, and posed far more than those in deficit, a threat to global prosperity. Keynes thought that surplus countries should be taxed to avoid trade imbalances. In "National Self-Sufficiency" The Yale Review, Vol. 22, no. 4 (June 1933), he already highlighted the problems created by free trade. His view, supported by many economists and commentators at the time, was that creditor nations may be just as responsible as debtor nations for disequilibrium in exchanges and that both should be under an obligation to bring trade back into a state of balance. Failure for them to do so could have serious consequences. In the words of Geoffrey Crowther, then editor of The Economist, "If the economic relationships between nations are not, by one means or another, brought fairly close to balance, then there is no set of financial arrangements that can rescue the world from the impoverishing results of chaos."These ideas were informed by events prior to the Great Depression when – in the opinion of Keynes and others – international lending, primarily by the U.S., exceeded the capacity of sound investment and so got diverted into non-productive and speculative uses, which in turn invited default and a sudden stop to the process of lending.Influenced by Keynes, economic texts in the immediate post-war period put a significant emphasis on balance in trade. For example, the second edition of the popular introductory textbook, An Outline of Money, devoted the last three of its ten chapters to questions of foreign exchange management and in particular the 'problem of balance'. However, in more recent years, since the end of the Bretton Woods system in 1971, with the increasing influence of Monetarist schools of thought in the 1980s, and particularly in the face of large sustained trade imbalances, these concerns – and particularly concerns about the destabilising effects of large trade surpluses – have largely disappeared from mainstream economics discourse and Keynes' insights have slipped from view. They are receiving some attention again in the wake of the financial crisis of 2007–08.
Keynes's ideas became widely accepted after World War II, and until the early 1970s, Keynesian economics provided the main inspiration for economic policy makers in Western industrialized countries. Governments prepared high quality economic statistics on an ongoing basis and tried to base their policies on the Keynesian theory that had become the norm. In the early era of social liberalism and social democracy, most western capitalist countries enjoyed low, stable unemployment and modest inflation, an era called the Golden Age of Capitalism. In terms of policy, the twin tools of post-war Keynesian economics were fiscal policy and monetary policy. While these are credited to Keynes, others, such as economic historian David Colander, argue that they are, rather, due to the interpretation of Keynes by Abba Lerner in his theory of functional finance, and should instead be called "Lernerian" rather than "Keynesian".Through the 1950s, moderate degrees of government demand leading industrial development, and use of fiscal and monetary counter-cyclical policies continued, and reached a peak in the "go go" 1960s, where it seemed to many Keynesians that prosperity was now permanent. In 1971, Republican US President Richard Nixon even proclaimed "I am now a Keynesian in economics."Beginning in the late 1960s, a new classical macroeconomics movement arose, critical of Keynesian assumptions (see sticky prices), and seemed, especially in the 1970s, to explain certain phenomena better. It was characterized by explicit and rigorous adherence to microfoundations, as well as use of increasingly sophisticated mathematical modelling. With the oil shock of 1973, and the economic problems of the 1970s, Keynesian economics began to fall out of favour. During this time, many economies experienced high and rising unemployment, coupled with high and rising inflation, contradicting the Phillips curve's prediction. This stagflation meant that the simultaneous application of expansionary (anti-recession) and contractionary (anti-inflation) policies appeared necessary. This dilemma led to the end of the Keynesian near-consensus of the 1960s, and the rise throughout the 1970s of ideas based upon more classical analysis, including monetarism, supply-side economics, and new classical economics. However, by the late 1980s, certain failures of the new classical models, both theoretical (see Real business cycle theory) and empirical (see the "Volcker recession") hastened the emergence of New Keynesian economics, a school that sought to unite the most realistic aspects of Keynesian and neo-classical assumptions and place them on more rigorous theoretical foundation than ever before. One line of thinking, utilized also as a critique of the notably high unemployment and potentially disappointing GNP growth rates associated with the new classical models by the mid-1980s, was to emphasize low unemployment and maximal economic growth at the cost of somewhat higher inflation (its consequences kept in check by indexing and other methods, and its overall rate kept lower and steadier by such potential policies as Martin Weitzman's share economy).
Multiple schools of economic thought that trace their legacy to Keynes currently exist, the notable ones being neo-Keynesian economics, New Keynesian economics, post-Keynesian economics, and the new neoclassical synthesis. Keynes's biographer Robert Skidelsky writes that the post-Keynesian school has remained closest to the spirit of Keynes's work in following his monetary theory and rejecting the neutrality of money. Today these ideas, regardless of provenance, are referred to in academia under the rubric of "Keynesian economics", due to Keynes's role in consolidating, elaborating, and popularizing them. In the postwar era, Keynesian analysis was combined with neoclassical economics to produce what is generally termed the "neoclassical synthesis", yielding neo-Keynesian economics, which dominated mainstream macroeconomic thought. Though it was widely held that there was no strong automatic tendency to full employment, many believed that if government policy were used to ensure it, the economy would behave as neoclassical theory predicted. This post-war domination by neo-Keynesian economics was broken during the stagflation of the 1970s. There was a lack of consensus among macroeconomists in the 1980s, and during this period New Keynesian economics was developed, ultimately becoming- along with new classical macroeconomics- a part of the current consensus, known as the new neoclassical synthesis.Post-Keynesian economists, on the other hand, reject the neoclassical synthesis and, in general, neoclassical economics applied to the macroeconomy. Post-Keynesian economics is a heterodox school that holds that both neo-Keynesian economics and New Keynesian economics are incorrect, and a misinterpretation of Keynes's ideas. The post-Keynesian school encompasses a variety of perspectives, but has been far less influential than the other more mainstream Keynesian schools.Interpretations of Keynes have emphasized his stress on the international coordination of Keynesian policies, the need for international economic institutions, and the ways in which economic forces could lead to war or could promote peace.
In a 2014 paper, economist Alan Blinder argues that, "for not very good reasons," public opinion in the United States has associated Keynesianism with liberalism, and he states that such is incorrect. For example, both Presidents Ronald Reagan (1981-89) and George W. Bush (2001-09) supported policies that were, in fact, Keynesian, even though both men were conservative leaders. And tax cuts can provide highly helpful fiscal stimulus during a recession, just as much as infrastructure spending can. Blinder concludes, "If you are not teaching your students that 'Keynesianism' is neither conservative nor liberal, you should be."
The Keynesian schools of economics are situated alongside a number of other schools that have the same perspectives on what the economic issues are, but differ on what causes them and how best to resolve them. Today, most of these schools of thought have been subsumed into modern macroeconomic theory.
The Stockholm school rose to prominence at about the same time that Keynes published his General Theory and shared a common concern in business cycles and unemployment. The second generation of Swedish economists also advocated government intervention through spending during economic downturns although opinions are divided over whether they conceived the essence of Keynes's theory before he did.
There was debate between monetarists and Keynesians in the 1960s over the role of government in stabilizing the economy. Both monetarists and Keynesians agree that issues such as business cycles, unemployment, and deflation are caused by inadequate demand. However, they had fundamentally different perspectives on the capacity of the economy to find its own equilibrium, and the degree of government intervention that would be appropriate. Keynesians emphasized the use of discretionary fiscal policy and monetary policy, while monetarists argued the primacy of monetary policy, and that it should be rules-based.The debate was largely resolved in the 1980s. Since then, economists have largely agreed that central banks should bear the primary responsibility for stabilizing the economy, and that monetary policy should largely follow the Taylor rule – which many economists credit with the Great Moderation. The financial crisis of 2007–08, however, has convinced many economists and governments of the need for fiscal interventions and highlighted the difficulty in stimulating economies through monetary policy alone during a liquidity trap.
Some Marxist economists criticized Keynesian economics. For example, in his 1946 appraisal Paul Sweezy—while admitting that there was much in the General Theory's analysis of effective demand that Marxists could draw on—described Keynes as a prisoner of his neoclassical upbringing. Sweezy argued that Keynes had never been able to view the capitalist system as a totality. He argued that Keynes regarded the class struggle carelessly, and overlooked the class role of the capitalist state, which he treated as a deus ex machina, and some other points. While Michał Kalecki was generally enthusiastic about the Keynesian revolution, he predicted that it would not endure, in his article "Political Aspects of Full Employment". In the article Kalecki predicted that the full employment delivered by Keynesian policy would eventually lead to a more assertive working class and weakening of the social position of business leaders, causing the elite to use their political power to force the displacement of the Keynesian policy even though profits would be higher than under a laissez faire system: The erosion of social prestige and political power would be unacceptable to the elites despite higher profits.
James M. Buchanan criticized Keynesian economics on the grounds that governments would in practice be unlikely to implement theoretically optimal policies. The implicit assumption underlying the Keynesian fiscal revolution, according to Buchanan, was that economic policy would be made by wise men, acting without regard to political pressures or opportunities, and guided by disinterested economic technocrats. He argued that this was an unrealistic assumption about political, bureaucratic and electoral behaviour. Buchanan blamed Keynesian economics for what he considered a decline in America's fiscal discipline. Buchanan argued that deficit spending would evolve into a permanent disconnect between spending and revenue, precisely because it brings short-term gains, so, ending up institutionalizing irresponsibility in the federal government, the largest and most central institution in our society.Martin Feldstein argues that the legacy of Keynesian economics–the misdiagnosis of unemployment, the fear of saving, and the unjustified government intervention–affected the fundamental ideas of policy makers.Milton Friedman thought that Keynes's political bequest was harmful for two reasons. First, he thought whatever the economic analysis, benevolent dictatorship is likely sooner or later to lead to a totalitarian society. Second, he thought Keynes's economic theories appealed to a group far broader than economists primarily because of their link to his political approach.Alex Tabarrok argues that Keynesian politics–as distinct from Keynesian policies–has failed pretty much whenever it's been tried, at least in liberal democracies.In response to this argument, John Quiggin, wrote about these theories' implication for a liberal democratic order. He thought that if it is generally accepted that democratic politics is nothing more than a battleground for competing interest groups, then reality will come to resemble the model. Paul Krugman wrote "I don’t think we need to take that as an immutable fact of life; but still, what are the alternatives?" Daniel Kuehn, criticized James M. Buchanan. He argued, "if you have a problem with politicians - criticize politicians," not Keynes. He also argued that empirical evidence makes it pretty clear that Buchanan was wrong.James Tobin argued, if advising government officials, politicians, voters, it's not for economists to play games with them. Keynes implicitly rejected this argument, in "soon or late it is ideas not vested interests which are dangerous for good or evil."Brad DeLong has argued that politics is the main motivator behind objections to the view that government should try to serve a stabilizing macroeconomic role. Paul Krugman argued that a regime that by and large lets markets work, but in which the government is ready both to rein in excesses and fight slumps is inherently unstable, due to intellectual instability, political instability, and financial instability.
Another influential school of thought was based on the Lucas critique of Keynesian economics. This called for greater consistency with microeconomic theory and rationality, and in particular emphasized the idea of rational expectations. Lucas and others argued that Keynesian economics required remarkably foolish and short-sighted behaviour from people, which totally contradicted the economic understanding of their behaviour at a micro level. New classical economics introduced a set of macroeconomic theories that were based on optimizing microeconomic behaviour. These models have been developed into the real business-cycle theory, which argues that business cycle fluctuations can to a large extent be accounted for by real (in contrast to nominal) shocks. Beginning in the late 1950s new classical macroeconomists began to disagree with the methodology employed by Keynes and his successors. Keynesians emphasized the dependence of consumption on disposable income and, also, of investment on current profits and current cash flow. In addition, Keynesians posited a Phillips curve that tied nominal wage inflation to unemployment rate. To support these theories, Keynesians typically traced the logical foundations of their model (using introspection) and supported their assumptions with statistical evidence. New classical theorists demanded that macroeconomics be grounded on the same foundations as microeconomic theory, profit-maximizing firms and rational, utility-maximizing consumers.The result of this shift in methodology produced several important divergences from Keynesian macroeconomics: Independence of consumption and current income (life-cycle permanent income hypothesis) Irrelevance of current profits to investment (Modigliani–Miller theorem) Long run independence of inflation and unemployment (natural rate of unemployment) The inability of monetary policy to stabilize output (rational expectations) Irrelevance of taxes and budget deficits to consumption (Ricardian equivalence)
Adam Smith Economic theories Game theory Invisible hand Job guarantee Pareto principle
Works by John Maynard Keynes at Project Gutenberg "We are all Keynesians now" – Historic article from Time magazine, 1965
Agricultural economics is an applied field of economics concerned with the application of economic theory in optimizing the production and distribution of food and fiber. Agricultural economics began as a branch of economics that specifically dealt with land usage, it focused on maximizing the crop yield while maintaining a good soil ecosystem. Throughout the 20th century the discipline expanded and the current scope of the discipline is much broader. Agricultural economics today includes a variety of applied areas, having considerable overlap with conventional economics. Agricultural economists have made substantial contributions to research in economics, econometrics, development economics, and environmental economics. Agricultural economics influences food policy, agricultural policy, and environmental policy.
Economics has been defined as the study of resource allocation under scarcity. Agricultural economics, or the application of economic methods to optimizing the decisions made by agricultural producers, grew to prominence around the turn of the 20th century. The field of agricultural economics can be traced back to works on land economics. Henry Charles Taylor was the greatest contributor with the establishment of the Department of Agricultural Economics at Wisconsin in 1909.Another contributor, 1979 Nobel Economics Prize winner Theodore Schultz, was among the first to examine development economics as a problem related directly to agriculture. Schultz was also instrumental in establishing econometrics as a tool for use in analyzing agricultural economics empirically; he noted in his landmark 1956 article that agricultural supply analysis is rooted in "shifting sand", implying that it was and is simply not being done correctly.One scholar summarizes the development of agricultural economics as follows: Agricultural economics arose in the late 19th century, combined the theory of the firm with marketing and organization theory, and developed throughout the 20th century largely as an empirical branch of general economics. The discipline was closely linked to empirical applications of mathematical statistics and made early and significant contributions to econometric methods. In the 1960s and afterwards, as agricultural sectors in the OECD countries contracted, agricultural economists were drawn to the development problems of poor countries, to the trade and macroeconomic policy implications of agriculture in rich countries, and to a variety of production, consumption, and environmental and resource problems. Agricultural economists have made many well-known contributions to the economics field with such models as the cobweb model, hedonic regression pricing models, new technology and diffusion models (Zvi Griliches), multifactor productivity and efficiency theory and measurement, and the random coefficients regression. The farm sector is frequently cited as a prime example of the perfect competition economic paradigm. In Asia, agricultural economics was offered first by the University of the Philippines Los Baños Department of Agricultural Economics in 1919. Today, the field of agricultural economics has transformed into a more integrative discipline which covers farm management and production economics, rural finance and institutions, agricultural marketing and prices, agricultural policy and development, food and nutrition economics, and environmental and natural resource economics. Since the 1970s, agricultural economics has primarily focused on seven main topics, according to a scholar in the field: agricultural environment and resources; risk and uncertainty; food and consumer economics; prices and incomes; market structures; trade and development; and technical change and human capital.
In the field of environmental economics, agricultural economists have contributed in three main areas: designing incentives to control environmental externalities (such as water pollution due to agricultural production), estimating the value of non-market benefits from natural resources and environmental amenities (such as an appealing rural landscape), and the complex interrelationship between economic activities and environmental consequences. With regard to natural resources, agricultural economists have developed quantitative tools for improving land management, preventing erosion, managing pests, protecting biodiversity, and preventing livestock diseases.
While at one time, the field of agricultural economics was focused primarily on farm-level issues, in recent years agricultural economists have studied diverse topics related to the economics of food consumption. In addition to economists' long-standing emphasis on the effects of prices and incomes, researchers in this field have studied how information and quality attributes influence consumer behavior. Agricultural economists have contributed to understanding how households make choices between purchasing food or preparing it at home, how food prices are determined, definitions of poverty thresholds, how consumers respond to price and income changes in a consistent way, and survey and experimental tools for understanding consumer preferences.
Agricultural economics research has addressed diminishing returns in agricultural production, as well as farmers' costs and supply responses. Much research has applied economic theory to farm-level decisions. Studies of risk and decision-making under uncertainty have real-world applications to crop insurance policies and to understanding how farmers in developing countries make choices about technology adoption. These topics are important for understanding prospects for producing sufficient food for a growing world population, subject to new resource and environmental challenges such as water scarcity and global climate change.
Development economics is broadly concerned with the improvement of living conditions in low-income countries, and the improvement of economic performance in low-income settings. Because agriculture is a large part of most developing economies, both in terms of employment and share of GDP, agricultural economists have been at the forefront of empirical research on development economics, contributing to our understanding of agriculture's role in economic development, economic growth and structural transformation. Many agricultural economists are interested in the food systems of developing economies, the linkages between agriculture and nutrition, and the ways in which agriculture interact with other domains, such as the natural environment.
The International Association of Agricultural Economists (IAAE) is a worldwide professional association, which holds its major conference every three years. The association publishes the journal Agricultural Economics. There also is a European Association of Agricultural Economists (EAAE), an African Association of Agricultural Economists (AAAE) and an Australian Agricultural and Resource Economics Society. Substantial work in agricultural economics internationally is conducted by the International Food Policy Research Institute. In the United States, the primary professional association is the Agricultural & Applied Economics Association (AAEA), which holds its own annual conference and also co-sponsors the annual meetings of the Allied Social Sciences Association (ASSA). The AAEA publishes the American Journal of Agricultural Economics and Applied Economic Perspectives and Policy.
Graduates from agricultural and applied economics departments find jobs in many sectors of the economy: agricultural management, agribusiness, commodities markets, education, financial sector, government, natural resource and environmental management, real estate, and public relations. Careers in agricultural economics require at least a bachelor's degree, and research careers in the field require graduate-level training; see Masters in Agricultural Economics. A 2011 study by the Georgetown Center on Education and the Workforce rated agricultural economics tied for 8th out of 171 fields in terms of employability.
Independent research institutions International Food Policy Research Institute (IFPRI) Academic and professional associations African Association of Agricultural Economists (AAAE) Australian Agricultural and Resource Economics Association Agricultural & Applied Economics Association (AAEA) Canadian Agricultural Economics Society (CAES) European Association of Agricultural Economists International Association of Agricultural Economists (IAAE) Government agencies U.S. Agency for International Development, Bureau for Economic Growth, Agriculture, and Trade U.S. Department of Agriculture, Economic Research Service European Commission on FarmingAcademic journals Agricultural Economics (AgEcon) American Journal of Agricultural Economics (AJAE) Applied Economic Perspectives and Policy (AEPP) Department of Agricultural and Resource Economics records at the University of Maryland libraries
Positive economics (as opposed to normative economics) is the branch of economics that concerns the description and explanation of economic phenomena. It focuses on facts and cause-and-effect behavioral relationships and includes the development and testing of economic theories. An earlier term was value-free (German: wertfrei) economics. Positive economics as science, concerns analysis of economic behavior. Positive economics concerns what is. To illustrate, an example of a positive economic statement is as follows: "The unemployment rate in France is higher than that in the United States." Another is: “An increase in government spending would lower the unemployment rate.” Either of these is potentially falsifiable. In contrast, a normative statement is, for example, “Government spending should be increased.” A standard theoretical statement of positive economics as operationally meaningful theorems is in Paul Samuelson's Foundations of Economic Analysis (1947). Positive economics as such avoids economic value judgements. For example, a positive economic theory might describe how money supply growth affects inflation, but it does not provide any instruction on what policy ought to be followed. Still, positive economics is commonly deemed necessary for the ranking of economic policies or outcomes as to acceptability, which is normative economics. Positive economics is sometimes defined as the economics of "what is", whereas normative economics discusses "what ought to be". The distinction was exposited by John Neville Keynes (1891) and elaborated by Milton Friedman in an influential 1953 essay.The methodological basis for a positive/normative distinction has its roots in the fact-value distinction in philosophy, the principal proponents of such distinctions being David Hume and G. E. Moore. The logical basis of such a relation as a dichotomy has been disputed in the philosophical literature. Such debates are reflected in discussion of positive science and specifically in economics, where critics, such as Gunnar Myrdal (1954), and proponents of Feminist Economics such as Julie A. Nelson, Geoff Schneider and Jean Shackelford, and Diana Strassmann, dispute the idea that economics can be completely neutral and agenda-free.
Consumer theory Distribution (economics) Economic methodology Feminist economics Normative economics Philosophy of economics Production possibilities frontier Supply and demand
Essays in Positive Economics by Milton Friedman Milton Friedman ([1953] 1966). "The Methodology of Positive Economics," excerpts from Friedman's essay
In economics, capital consists of human-created assets that can enhance one's power to perform economically useful work. For example, a stone arrowhead is capital for a hunter-gatherer who can use it as a hunting instrument; similarly, roads are capital for inhabitants of a city. Capital is distinct from land and other non-renewable resources in that it can be increased by human labor, and does not include certain durable goods like homes and personal automobiles that are not used in the production of saleable goods and services. Adam Smith defined capital as "that part of man's stock which he expects to afford him revenue". In economic models, capital is an input in the production function. The total physical capital at any given moment in time is referred to as the capital stock (not to be confused with the capital stock of a business entity). Capital goods, real capital, or capital assets are already-produced, durable goods or any non-financial asset that is used in production of goods or services.In Marxian economics, capital is money used to buy something only in order to sell it again to realize a profit. For Marx, capital only exists within the process of the economic circuit (represented by M-C-M')—it is wealth that grows out of the process of circulation itself, and for Marx it formed the basis of the economic system of capitalism. In more contemporary schools of economics, this form of capital is generally referred to as "financial capital" and is distinguished from "capital goods".
Classical and neoclassical economics regard capital as one of the factors of production (alongside the other factors: land and labour). All other inputs to production are called intangibles in classical economics. This includes organization, entrepreneurship, knowledge, goodwill, or management (which some characterize as talent, social capital or instructional capital). This is what makes it a factor of production: The good is not used up immediately in the process of production unlike raw materials or intermediate goods. (The significant exception to this is depreciation allowance, which like intermediate goods, is treated as a business expense.) The good can be produced or increased (in contrast to land and non-renewable resources).These distinctions of convenience have carried over to contemporary economic theory. Adam Smith provided the further clarification that capital is a stock. As such, its value can be estimated at a point in time. By contrast, investment, as production to be added to the capital stock, is described as taking place over time ("per year"), thus a flow. Marxian economics distinguishes between different forms of capital: constant capital, which refers to capital goods variable capital, which refers to labor-inputs, where the cost is "variable" based on the amount of wages and salaries paid during an employee's contract/employment, fictitious capital, which refers to intangible representations or abstractions of physical capital, such as stocks, bonds and securities (or "tradable paper claims to wealth")Earlier illustrations often described capital as physical items, such as tools, buildings, and vehicles that are used in the production process. Since at least the 1960s economists have increasingly focused on broader forms of capital. For example, investment in skills and education can be viewed as building up human capital or knowledge capital, and investments in intellectual property can be viewed as building up intellectual capital. These terms lead to certain questions and controversies discussed in those articles.
Detailed classifications of capital that have been used in various theoretical or applied uses generally respect the following division: Financial capital, which represents obligations, and is liquidated as money for trade, and owned by legal entities. It is in the form of capital assets, traded in financial markets. Its market value is not based on the historical accumulation of money invested but on the perception by the market of its expected revenues and of the risk entailed. Natural capital, which is inherent in ecologies and which increases the supply of human wealth Social capital, which in private enterprise is partly captured as goodwill or brand value, but is a more general concept of inter-relationships between human beings having money-like value that motivates actions in a similar fashion to paid compensation. Instructional capital, defined originally in academia as that aspect of teaching and knowledge transfer that is not inherent in individuals or social relationships but transferable. Various theories use names like knowledge or intellectual capital to describe similar concepts but these are not strictly defined as in the academic definition and have no widely agreed accounting treatment. Human capital, a broad term that generally includes social, instructional and individual human talent in combination. It is used in technical economics to define “balanced growth”, which is the goal of improving human capital as much as economic capital. Public capital is a blanket term that attempts to characterize physical capital that is considered infrastructure and which supports production in unclear or poorly accounted ways. This encompasses the aggregate body of all government-owned assets that are used to promote private industry productivity, including highways, railways, airports, water treatment facilities, telecommunications, electric grids, energy utilities, municipal buildings, public hospitals and schools, police, fire protection, courts and still others. However it is a problematic term insofar as many of these assets can be either publicly or privately owned. Ecological capital is the world's stock of natural resources, which includes geology, soils, air, water and all living organisms. Some natural capital assets provide people with free goods and services, often called ecosystem services. Two of these (clean water and fertile soil) underpin our economy and society and make human life possible.Separate literatures have developed to describe both natural capital and social capital. Such terms reflect a wide consensus that nature and society both function in such a similar manner as traditional industrial infrastructural capital, that it is entirely appropriate to refer to them as different types of capital in themselves. In particular, they can be used in the production of other goods, are not used up immediately in the process of production, and can be enhanced (if not created) by human effort. There is also a literature of intellectual capital and intellectual property law. However, this increasingly distinguishes means of capital investment, and collection of potential rewards for patent, copyright (creative or individual capital), and trademark (social trust or social capital) instruments. Building on Marx, and on the theories of the sociologist and philosopher Pierre Bourdieu, scholars have recently argued for the significance of "culinary capital" in the arena of food. The idea is that the production, consumption, and distribution of knowledge about food can confer power and status.
Within classical economics, Adam Smith (Wealth of Nations, Book II, Chapter 1) distinguished fixed capital from circulating capital. The former designated physical assets not consumed in the production of a product (e.g. machines and storage facilities), while the latter referred to physical assets consumed in the process of production (e.g. raw materials and intermediate products). For an enterprise, both were types of capital. Economist Henry George argued that financial instruments like stocks, bonds, mortgages, promissory notes, or other certificates for transferring wealth is not really capital, because "Their economic value merely represents the power of one class to appropriate the earnings of another" and "their increase or decrease does not affect the sum of wealth in the community".Some thinkers, such as Werner Sombart and Max Weber, locate the concept of capital as originating in double-entry bookkeeping, which is thus a foundational innovation in capitalism, Sombart writing in "Medieval and Modern Commercial Enterprise" that: The very concept of capital is derived from this way of looking at things; one can say that capital, as a category, did not exist before double-entry bookkeeping. Capital can be defined as that amount of wealth which is used in making profits and which enters into the accounts."Karl Marx adds a distinction that is often confused with David Ricardo's. In Marxian theory, variable capital refers to a capitalist's investment in labor-power, seen as the only source of surplus-value. It is called "variable" since the amount of value it can produce varies from the amount it consumes, i.e., it creates new value. On the other hand, constant capital refers to investment in non-human factors of production, such as plant and machinery, which Marx takes to contribute only its own replacement value to the commodities it is used to produce. Investment or capital accumulation, in classical economic theory, is the production of increased capital. Investment requires that some goods be produced that are not immediately consumed, but instead used to produce other goods as capital goods. Investment is closely related to saving, though it is not the same. As Keynes pointed out, saving involves not spending all of one's income on current goods or services, while investment refers to spending on a specific type of goods, i.e., capital goods. Austrian School economist Eugen Boehm von Bawerk maintained that capital intensity was measured by the roundaboutness of production processes. Since capital is defined by him as being goods of higher-order, or goods used to produce consumer goods, and derived their value from them, being future goods. Human development theory describes human capital as being composed of distinct social, imitative and creative elements: Social capital is the value of network trusting relationships between individuals in an economy. Individual capital, which is inherent in persons, protected by societies, and trades labour for trust or money. Close parallel concepts are "talent", "ingenuity", "leadership", "trained bodies", or "innate skills" that cannot reliably be reproduced by using any combination of any of the others above. In traditional economic analysis individual capital is more usually called labour. Instructional capital in the academic sense is clearly separate from either individual persons or social bonds between them.This theory is the basis of triple bottom line accounting and is further developed in ecological economics, welfare economics and the various theories of green economics. All of which use a particularly abstract notion of capital in which the requirement of capital being produced like durable goods is effectively removed. The Cambridge capital controversy was a dispute between economists at Cambridge, Massachusetts based MIT and University of Cambridge in the UK about the measurement of capital. The Cambridge, UK economists, including Joan Robinson and Piero Sraffa claimed that there is no basis for aggregating the heterogeneous objects that constitute 'capital goods.' Political economists Jonathan Nitzan and Shimshon Bichler have suggested that capital is not a productive entity, but solely financial and that capital values measure the relative power of owners over the broad social processes that bear on profits.
Capital deepening Capitalist mode of production Das Kapital Means of production Organic composition of capital Organizational capital The Accumulation of Capital Venture capital Wealth (economics)
Quotations related to Capital at Wikiquote Media related to Capital (economics) at Wikimedia Commons
An economy (from Greek οίκος – "household" and νέμoμαι – "manage") is an area of the production, distribution and trade, as well as consumption of goods and services by different agents. Understood in its broadest sense, 'The economy is defined as a social domain that emphasize the practices, discourses, and material expressions associated with the production, use, and management of resources'. A given economy is the result of a set of processes that involves its culture, values, education, technological evolution, history, social organization, political structure and legal systems, as well as its geography, natural resource endowment, and ecology, as main factors. These factors give context, content, and set the conditions and parameters in which an economy functions. In other words, the economic domain is a social domain of human practices and transactions. It does not stand alone. Economic agents can be individuals, businesses, organizations, or governments. Economic transactions occur when two groups or parties agree to the value or price of the transacted good or service, commonly expressed in a certain currency. However, monetary transactions only account for a small part of the economic domain. Economic activity is spurred by production which uses natural resources, labor and capital. It has changed over time due to technology (automation, accelerator of process, reduction of cost functions), innovation (new products, services, processes, expanding markets, diversification of markets, niche markets, increases revenue functions) such as, that which produces intellectual property and changes in industrial relations (most notably child labor being replaced in some parts of the world with universal access to education). A market-based economy is one where goods and services are produced and exchanged according to demand and supply between participants (economic agents) by barter or a medium of exchange with a credit or debit value accepted within the network, such as a unit of currency. A command-based economy is one where political agents directly control what is produced and how it is sold and distributed. A green economy is low-carbon, resource efficient and socially inclusive. In a green economy, growth in income and employment is driven by public and private investments that reduce carbon emissions and pollution, enhance energy and resource efficiency, and prevent the loss of biodiversity and ecosystem services. A gig economy is one in which short-term jobs are assigned or chosen via online platforms. New economy is a term that referred to the whole emerging ecosystem where new standards and practices were introduced, usually as a result of technological innovations. The global economy refers to humanity's economic system or systems overall.
Today the range of fields of study examining the economy revolves around the social science of economics, but may include sociology (economic sociology), history (economic history), anthropology (economic anthropology), and geography (economic geography). Practical fields directly related to the human activities involving production, distribution, exchange, and consumption of goods and services as a whole are engineering, management, business administration, applied science, and finance. All professions, occupations, economic agents or economic activities, contribute to the economy. Consumption, saving, and investment are variable components in the economy that determine macroeconomic equilibrium. There are three main sectors of economic activity: primary, secondary, and tertiary. Due to the growing importance of the economical sector in modern times, the term real economy is used by analysts as well as politicians to denote the part of the economy that is concerned with the actual production of goods and services, as ostensibly contrasted with the paper economy, or the financial side of the economy, which is concerned with buying and selling on the financial markets. Alternate and long-standing terminology distinguishes measures of an economy expressed in real values (adjusted for inflation), such as real GDP, or in nominal values (unadjusted for inflation).
As long as someone has been making, supplying and distributing goods or services, there has been some sort of economy; economies grew larger as societies grew and became more complex. Sumer developed a large-scale economy based on commodity money, while the Babylonians and their neighboring city states later developed the earliest system of economics as we think of, in terms of rules/laws on debt, legal contracts and law codes relating to business practices, and private property.The Babylonians and their city state neighbors developed forms of economics comparable to currently used civil society (law) concepts. They developed the first known codified legal and administrative systems, complete with courts, jails, and government records. The ancient economy was mainly based on subsistence farming. The Shekel referred to an ancient unit of weight and currency. The first usage of the term came from Mesopotamia circa 3000 BC. and referred to a specific mass of barley which related other values in a metric such as silver, bronze, copper etc. A barley/shekel was originally both a unit of currency and a unit of weight, just as the British Pound was originally a unit denominating a one-pound mass of silver. For most people, the exchange of goods occurred through social relationships. There were also traders who bartered in the marketplaces. In Ancient Greece, where the present English word 'economy' originated, many people were bond slaves of the freeholders. The economic discussion was driven by scarcity.
The European captures became branches of the European states, the so-called colonies. The rising nation-states Spain, Portugal, France, Great Britain and the Netherlands tried to control the trade through custom duties and (from mercator, lat.: merchant) was a first approach to intermediate between private wealth and public interest. The secularization in Europe allowed states to use the immense property of the church for the development of towns. The influence of the nobles decreased. The first Secretaries of State for economy started their work. Bankers like Amschel Mayer Rothschild (1773–1855) started to finance national projects such as wars and infrastructure. Economy from then on meant national economy as a topic for the economic activities of the citizens of a state.
The first economist in the true modern meaning of the word was the Scotsman Adam Smith (1723–1790) who was inspired partly by the ideas of physiocracy, a reaction to mercantilism and also later Economics student, Adam Mari. He defined the elements of a national economy: products are offered at a natural price generated by the use of competition - supply and demand - and the division of labor. He maintained that the basic motive for free trade is human self-interest. The so-called self-interest hypothesis became the anthropological basis for economics. Thomas Malthus (1766–1834) transferred the idea of supply and demand to the problem of overpopulation. The Industrial Revolution was a period from the 18th to the 19th century where major changes in agriculture, manufacturing, mining, and transport had a profound effect on the socioeconomic and cultural conditions starting in the United Kingdom, then subsequently spreading throughout Europe, North America, and eventually the world. The onset of the Industrial Revolution marked a major turning point in human history; almost every aspect of daily life was eventually influenced in some way. In Europe wild capitalism started to replace the system of mercantilism (today: protectionism) and led to economic growth. The period today is called industrial revolution because the system of Production, production and division of labor enabled the mass production of goods.
The contemporary concept of "the economy" wasn't popularly known until the American Great Depression in the 1930s.After the chaos of two World Wars and the devastating Great Depression, policymakers searched for new ways of controlling the course of the economy. This was explored and discussed by Friedrich August von Hayek (1899–1992) and Milton Friedman (1912–2006) who pleaded for a global free trade and are supposed to be the fathers of the so-called neoliberalism. However, the prevailing view was that held by John Maynard Keynes (1883–1946), who argued for a stronger control of the markets by the state. The theory that the state can alleviate economic problems and instigate economic growth through state manipulation of aggregate demand is called Keynesianism in his honor. In the late 1950s, the economic growth in America and Europe—often called Wirtschaftswunder (ger: economic miracle) —brought up a new form of economy: mass consumption economy. In 1958, John Kenneth Galbraith (1908–2006) was the first to speak of an affluent society. In most of the countries the economic system is called a social market economy.
With the fall of the Iron Curtain and the transition of the countries of the Eastern Bloc towards democratic government and market economies, the idea of the post-industrial society is brought into importance as its role is to mark together the significance that the service sector receives instead of industrialization. Some attribute the first use of this term to Daniel Bell's 1973 book, The Coming of Post-Industrial Society, while others attribute it to social philosopher Ivan Illich's book, Tools for Conviviality. The term is also applied in philosophy to designate the fading of postmodernism in the late 90s and especially in the beginning of the 21st century. With the spread of Internet as a mass media and communication medium especially after 2000-2001, the idea for the Internet and information economy is given place because of the growing importance of e-commerce and electronic businesses, also the term for a global information society as understanding of a new type of "all-connected" society is created. In the late 2000s, the new type of economies and economic expansions of countries like China, Brazil, and India bring attention and interest to different from the usually dominating Western type economies and economic models.
The economy may be considered as having developed through the following phases or degrees of precedence. The ancient economy was mainly based on subsistence farming. The industrial revolution phase lessened the role of subsistence farming, converting it to more extensive and mono-cultural forms of agriculture in the last three centuries. The economic growth took place mostly in mining, construction and manufacturing industries. Commerce became more significant due to the need for improved exchange and distribution of produce throughout the community. In the economies of modern consumer societies phase there is a growing part played by services, finance, and technology—the knowledge economy.In modern economies, these phase precedences are somewhat differently expressed by the three-sector theory. Primary stage/degree of the economy: Involves the extraction and production of raw materials, such as corn, coal, wood and iron. (A coal miner and a fisherman would be workers in the primary degree.) Secondary stage/degree of the economy: Involves the transformation of raw or intermediate materials into goods e.g. manufacturing steel into cars, or textiles into clothing. (A builder and a dressmaker would be workers in the secondary degree.) At this stage the associated industrial economy is also sub-divided into several economic sectors (also called industries). Their separate evolution during the Industrial Revolution phase is dealt with elsewhere. Tertiary stage/degree of the economy: Involves the provision of services to consumers and businesses, such as baby-sitting, cinema and banking. (A shopkeeper and an accountant would be workers in the tertiary degree.) Quaternary stage/degree of the economy: Involves the research and development needed to produce products from natural resources and their subsequent by-products. (A logging company might research ways to use partially burnt wood to be processed so that the undamaged portions of it can be made into pulp for paper.) Note that education is sometimes included in this sector.Other sectors of the developed community include : the public sector or state sector (which usually includes: parliament, law-courts and government centers, various emergency services, public health, shelters for impoverished and threatened people, transport facilities, air/sea ports, post-natal care, hospitals, schools, libraries, museums, preserved historical buildings, parks/gardens, nature-reserves, some universities, national sports grounds/stadiums, national arts/concert-halls or theaters and centers for various religions). the private sector or privately run businesses. the social sector or voluntary sector.
There are a number of concepts associated with the economy, such as these:
The GDP (gross domestic product) of a country is a measure of the size of its economy. The most conventional economic analysis of a country relies heavily on economic indicators like the GDP and GDP per capita. While often useful, GDP only includes economic activity for which money is exchanged.
An informal economy is economic activity that is neither taxed nor monitored by a government, contrasted with a formal economy. The informal economy is thus not included in that government's gross national product (GNP). Although the informal economy is often associated with developing countries, all economic systems contain an informal economy in some proportion. Informal economic activity is a dynamic process that includes many aspects of economic and social theory including exchange, regulation, and enforcement. By its nature, it is necessarily difficult to observe, study, define, and measure. No single source readily or authoritatively defines informal economy as a unit of study. The terms "underground", "under the table" and "off the books" typically refer to this type of economy. The term black market refers to a specific subset of the informal economy. The term "informal sector" was used in many earlier studies, and has been mostly replaced in more recent studies which use the newer term. The informal sector makes up a significant portion of the economies in developing countries but it is often stigmatized as troublesome and unmanageable. However, the informal sector provides critical economic opportunities for the poor and has been expanding rapidly since the 1960s. As such, integrating the informal economy into the formal sector is an important policy challenge.
Economic research is conducted in fields as different as economics, economic sociology, economic anthropology, and economic history.
Aristotle, Politics, Book I-IIX, translated by Benjamin Jowett, Classics.mit.edu Barnes, Peter, Capitalism 3.0, A Guide to Reclaiming the Commons, San Francisco 2006, Whatiseconomy.com Dill, Alexander, Reclaiming the Hidden Assets, Towards a Global Freeware Index, Global Freeware Research Paper 01-07, 2007, Whatiseconomy.com Fehr Ernst, Schmidt, Klaus M., The Economics Of Fairness, Reciprocity and Altruism - experimental Evidence and new Theories, 2005, Discussion PAPER 2005-20, Munich Economics, Whatiseconomy.com Marx, Karl, Engels, Friedrich, 1848, The Communist Manifesto, Marxists.org Stiglitz, Joseph E., Global public goods and global finance: does global governance ensure that the global public interest is served? In: Advancing Public Goods, Jean-Philippe Touffut, (ed.), Paris 2006, pp. 149/164, GSB.columbia.edu Where is the Wealth of Nations? Measuring Capital for the 21st Century. Wealth of Nations Report 2006, Ian Johnson and Francois Bourguignon, World Bank, Washington 2006, Whatiseconomy.com.
A market economy is an economic system in which the decisions regarding investment, production and distribution are guided by the price signals created by the forces of supply and demand. The major characteristic of a market economy is the existence of factor markets that play a dominant role in the allocation of capital and the factors of production.Market economies range from minimally regulated free-market and laissez-faire systems where state activity is restricted to providing public goods and services and safeguarding private ownership, to interventionist forms where the government plays an active role in correcting market failures and promoting social welfare. State-directed or dirigist economies are those where the state plays a directive role in guiding the overall development of the market through industrial policies or indicative planning—which guides yet does not substitute the market for economic planning—a form sometimes referred to as a mixed economy.Market economies are contrasted with planned economies where investment and production decisions are embodied in an integrated economy-wide economic plan. In a centrally planned economy, economic planning is the principal allocation mechanism between firms rather than markets, with the economy's means of production being owned and operated by a single organizational body.
For market economies to function efficiently, governments must establish clearly defined and enforceable property rights for assets and capital goods. However, property rights does not specifically mean private property rights and market economies do not logically presuppose the existence of private ownership of the means of production. Market economies can and often do include various types of cooperatives or autonomous state-owned enterprises that acquire capital goods and raw materials in capital markets. These enterprises utilize a market-determined free price system to allocate capital goods and labor. In addition, there are many variations of market socialism where the majority of capital assets are socially owned with markets allocating resources between socially owned firms. These models range from systems based on employee-owned enterprises based on self-management to a combination of public ownership of the means of production with factor markets.
Market economies rely upon a price system to signal market actors to adjust production and investment. Price formation relies on the interaction of supply and demand to reach or approximate an equilibrium where unit price for a particular good or service is at a point where the quantity demanded equals the quantity supplied. Governments can intervene by establishing price ceilings or price floors in specific markets (such as minimum wage laws in the labor market), or use fiscal policy to discourage certain consumer behavior or to address market externalities generated by certain transactions (Pigovian taxes). Different perspectives exist on the role of government in both regulating and guiding market economies and in addressing social inequalities produced by markets. Fundamentally, a market economy requires that a price system affected by supply and demand exists as the primary mechanism for allocating resources irrespective of the level of regulation.
Capitalism is an economic system where the means of production are largely or entirely privately owned and operated for a profit, structured on the process of capital accumulation. In general, in capitalist systems investment, distribution, income and prices are determined by markets, whether regulated or unregulated. There are different variations of capitalism with different relationships to markets. In laissez-faire and free-market variations of capitalism, markets are utilized most extensively with minimal or no state intervention and minimal or no regulation over prices and the supply of goods and services. In interventionist, welfare capitalism and mixed economies, markets continue to play a dominant role, but they are regulated to some extent by government in order to correct market failures or to promote social welfare. In state capitalist systems, markets are relied upon the least, with the state relying heavily on either indicative planning and/or state-owned enterprises to accumulate capital. Capitalism has been dominant in the Western world since the end of feudalism. However, it is argued that the term mixed economies more precisely describes most contemporary economies due to their containing both private-owned and state-owned enterprises. In capitalism, prices determine the demand-supply scale. Higher demand for certain goods and services lead to higher prices and lower demand for certain goods lead to lower prices.
A capitalist free-market economy is an economic system where prices for goods and services are set freely by the forces of supply and demand and are allowed to reach their point of equilibrium without intervention by government policy. It typically entails support for highly competitive markets, private ownership of productive enterprises. Laissez-faire is a more extensive form of free-market economy where the role of the state is limited to protecting property rights.
Laissez-faire is synonymous with what was referred to as strict capitalist free-market economy during the early and mid-19th century as a classical liberal ideal to achieve. It is generally understood that the necessary components for the functioning of an idealized free market include the complete absence of government regulation, subsidies, artificial price pressures and government-granted monopolies (usually classified as coercive monopoly by free market advocates) and no taxes or tariffs other than what is necessary for the government to provide protection from coercion and theft, maintaining peace and property rights and providing for basic public goods. Right-libertarian advocates of anarcho-capitalism see the state as morally illegitimate and economically unnecessary and destructive. Although laissez-faire has been commonly associated with capitalism, there is a similar left-wing laissez-faire system called free-market anarchism, also known as free-market anti-capitalism and free-market socialism to distinguish it from laissez-faire capitalism. Thus, critics of laissez-faire as commonly understood argues that a truly laissez-faire system would be anti-capitalist and socialist.
Welfare capitalism is a capitalist economy that includes public policies favoring extensive provisions for social welfare services. The economic mechanism involves a free market and the predominance of privately owned enterprises in the economy, but public provision of universal welfare services aimed at enhancing individual autonomy and maximizing equality. Examples of contemporary welfare capitalism include the Nordic model of capitalism predominant in Northern Europe.
Anglo-Saxon capitalism is the form of capitalism predominant in Anglophone countries and typified by the economy of the United States. It is contrasted with European models of capitalism such as the continental social market model and the Nordic model. Anglo-Saxon capitalism refers to a macroeconomic policy regime and capital market structure common to the Anglophone economies. Among these characteristics are low rates of taxation, more open financial markets, lower labor market protections and a less generous welfare state eschewing collective bargaining schemes found in the continental and northern European models of capitalism.
The East Asian model of capitalism involves a strong role for state investment and in some instances involves state-owned enterprises. The state takes an active role in promoting economic development through subsidies, the facilitation of "national champions" and an export-based model of growth. The actual practice of this model varies by country. This designation has been applied to the economies of China, Japan, Singapore, South Korea and Taiwan. A related concept in political science is the developmental state.
The social market economy was implemented by Alfred Müller-Armack and Ludwig Erhard after World War II in West Germany. The social market economic model, sometimes called Rhine capitalism, is based upon the idea of realizing the benefits of a free-market economy, especially economic performance and high supply of goods while avoiding disadvantages such as market failure, destructive competition, concentration of economic power and the socially harmful effects of market processes. The aim of the social market economy is to realize greatest prosperity combined with best possible social security. One difference from the free market economy is that the state is not passive, but instead takes active regulatory measures. The social policy objectives include employment, housing and education policies, as well as a socio-politically motivated balancing of the distribution of income growth. Characteristics of social market economies are a strong competition policy and a contractionary monetary policy. The philosophical background is neoliberalism or ordoliberalism.
Market socialism is a form of market economy where the means of production are socially owned. In a market socialist economy, firms operate according to the rules of supply and demand and operate to maximize profit; the principal difference between market socialism and capitalism being that the profits accrue to society as a whole as opposed to private owners.The distinguishing feature between non-market socialism and market socialism is the existence of a market for factors of production and the criteria of profitability for enterprises. Profits derived from publicly owned enterprises can variously be used to reinvest in further production, to directly finance government and social services, or be distributed to the public at large through a social dividend or basic income system.Advocates of market socialism such as Jaroslav Vanek argue that genuinely free markets are not possible under conditions of private ownership of productive property. Instead, he contends that the class differences and inequalities in income and power that result from private ownership enable the interests of the dominant class to skew the market to their favor, either in the form of monopoly and market power, or by utilizing their wealth and resources to legislate government policies that benefit their specific business interests. Additionally, Vanek states that workers in a socialist economy based on cooperative and self-managed enterprises have stronger incentives to maximize productivity because they would receive a share of the profits (based on the overall performance of their enterprise) in addition to receiving their fixed wage or salary. The stronger incentives to maximize productivity that he conceives as possible in a socialist economy based on cooperative and self-managed enterprises might be accomplished in a free-market economy if employee-owned companies were the norm as envisioned by various thinkers including Louis O. Kelso and James S. Albus.
Market socialism traces its roots to classical economics and the works of Adam Smith, the Ricardian socialists and mutualist philosophers.In the 1930s, the economists Oskar Lange and Abba Lerner developed a model of socialism that posited that a public body (dubbed the Central Planning Board) could set prices through a trial-and-error approach until they equaled the marginal cost of production in order to achieve perfect competition and pareto optimality. In this model of socialism, firms would be state-owned and managed by their employees and the profits would be disbursed among the population in a social dividend. This model came to be referred to as market socialism because it involved the use of money, a price system and simulated capital markets, all of which were absent from traditional of non-market socialism. A more contemporary model of market socialism is that put forth by the American economist John Roemer, referred to as economic democracy. In this model, social ownership is achieved through public ownership of equity in a market economy. A Bureau of Public Ownership would own controlling shares in publicly listed firms, so that the profits generated would be used for public finance and the provision of a basic income. Some anarchists and libertarian socialists promote a form of market socialism in which enterprises are owned and managed cooperatively by their workforce so that the profits directly remunerate the employee-owners. These cooperative enterprises would compete with each other in the same way private companies compete with each other in a capitalist market. The first major elaboration of this type of market socialism was made by Pierre-Joseph Proudhon and was called mutualism. Self-managed market socialism was promoted in Yugoslavia by economists Branko Horvat and Jaroslav Vanek. In the self-managed model of socialism, firms would be directly owned by their employees and the management board would be elected by employees. These cooperative firms would compete with each other in a market for both capital goods and for selling consumer goods.
Following the 1978 reforms, China developed what it calls a socialist market economy in which most of the economy is under state ownership, with the state enterprises organized as joint-stock companies with various government agencies owning controlling shares through a shareholder system. Prices are set by a largely free-price system and the state-owned enterprises are not subjected to micromanagement by a government planning agency. A similar system called socialist-oriented market economy has emerged in Vietnam following the Đổi Mới reforms in 1986. This system is frequently characterized as state capitalism instead of market socialism because there is no meaningful degree of employee self-management in firms, because the state enterprises retain their profits instead of distributing them to the workforce or government and because many function as de facto private enterprises. The profits neither finance a social dividend to benefit the population at large, nor do they accrue to their employees. In China, this economic model is presented as a preliminary stage of socialism to explain the dominance of capitalistic management practices and forms of enterprise organization in both the state and non-state sectors.
A wide range of philosophers and theologians have linked market economies to monotheistic values. Michael Novak described capitalism as being closely related to Catholicism, but Max Weber drew a connection between capitalism and Protestantism. The economist Jeffrey Sachs has stated that his work was inspired by the healing characteristics of Judaism. Chief Rabbi Lord Sacks of the United Synagogue draws a correlation between modern capitalism and the Jewish image of the Golden Calf.
In the Christian faith, the liberation theology movement advocated involving the church in labor market capitalism. Many priests and nuns integrated themselves into labor organizations while others moved into the slums to live among the poor. The Holy Trinity was interpreted as a call for social equality and the elimination of poverty. However, the Pope was highly active in his criticism of liberation theology. He was particularly concerned about the increased fusion between Christianity and Marxism. He closed Catholic institutions that taught liberation theology and dismissed some of its activists from the church.
The Buddhist approach to the market economy was dealt with in E. F. Schumacher’s 1966 essay "Buddhist Economics". Schumacher asserted that a market economy guided by Buddhist principles would more successfully meet the needs of its people. He emphasized the importance or pursuing occupations that adhered to Buddhist teachings. The essay would later become required reading for a course that Clair Brown offered at University of California, Berkeley.
The economist Joseph Stiglitz argues that markets suffer from informational inefficiency and the presumed efficiency of markets stems from the faulty assumptions of neoclassical welfare economics, particularly the assumption of perfect and costless information and related incentive problems. Neoclassical economics assumes static equilibrium and efficient markets require that there be no non-convexities, even though nonconvexities are pervasive in modern economies. Stiglitz's critique applies to both existing models of capitalism and to hypothetical models of market socialism. However, Stiglitz does not advocate replacing markets, but instead states that there is a significant role for government intervention to boost the efficiency of markets and to address the pervasive market failures that exist in contemporary economies. A fair market economy is in fact a martingale or a Brownian motion model and for a participant competitor in such a model there is no more than 50% of success chances at any given moment. Due to the fractal nature of any fair market and being market participants subject to the law of competition which impose reinvesting an increasing part of profits, the mean statistical chance of bankruptcy within the half life of any participant is also 50% and 100% whether an infinite sample of time is considered. Robin Hahnel and Michael Albert claim that "markets inherently produce class division". Albert states that even if everyone started out with a balanced job complex (doing a mix of roles of varying creativity, responsibility and empowerment) in a market economy, class divisions would arise, arguing: Without taking the argument that far, it is evident that in a market system with uneven distribution of empowering work, such as Economic Democracy, some workers will be more able than others to capture the benefits of economic gain. For example, if one worker designs cars and another builds them, the designer will use his cognitive skills more frequently than the builder. In the long term, the designer will become more adept at conceptual work than the builder, giving the former greater bargaining power in a firm over the distribution of income. A conceptual worker who is not satisfied with his income can threaten to work for a company that will pay him more. The effect is a class division between conceptual and manual laborers, and ultimately managers and workers, and a de facto labor market for conceptual workers. David McNally argues in the Marxist tradition that the logic of the market inherently produces inequitable outcomes and leads to unequal exchanges, arguing that Adam Smith's moral intent and moral philosophy espousing equal exchange was undermined by the practice of the free markets he championed. The development of the market economy involved coercion, exploitation and violence that Smith's moral philosophy could not countenance. McNally also criticizes market socialists for believing in the possibility of fair markets based on equal exchanges to be achieved by purging parasitical elements from the market economy such as private ownership of the means of production. McNally argues that market socialism is an oxymoron when socialism is defined as an end to wage-based labor.
Market Systems at Encyclopædia Britannica Online.
The economy of Greenland can be characterized as small, mixed and vulnerable. Greenland's economy consists of a large public sector and comprehensive foreign trade. This has resulted in an economy with periods of strong growth, considerable inflation, unemployment problems and extreme dependence on capital inflow from the Kingdom Government.GDP per capita is close to the average for European economies, but the economy is critically dependent upon substantial support from the Danish government, which supplies about half the revenues of the Self-rule Government, which in turn employs 10,307 Greenlanders out of 25,620 currently in employment (2015). Unemployment nonetheless remains high, with the rest of the economy dependent upon demand for exports of shrimp and fish.
Except for an abortive royal colony established under Major Claus Paarss between 1728 and 1730, colonial Greenland was administered by companies under royal charter until 1908. Hans Egede's Hope Colony was organized under the auspices of the Bergen Greenland Company prior to its bankruptcy in 1727; it was succeeded by the merchant Jacob Severin (1733–1749), the General Trade Company (Det almindelige Handelskompagni; 1749–1774), and finally the Royal Greenland Trading Department (KGH; 1776–1908). Early hopes of mineral or agricultural wealth were dashed, and open trade proved a failure owing to other nations' better quality, lower priced goods and hostility. Kale, lettuce, and other vegetables were successfully introduced, but repeated attempts to cultivate wheat or clover failed throughout Greenland, limiting the ability to raise European livestock. After government-funded whaling failed, the KGH eventually settled on maintaining the native Greenlanders in their traditional pursuits of hunting and whaling and enforced a monopoly on trade between them and Europe. Repeated attempts to open trade were opposed on both commercial and humanitarian grounds, although minor reforms in the 1850s and 60s lowered the prices charged to the natives for "luxuries" like sugar and coffee; transferred more of the KGH's profits to local communities; and granted the important Ivigtut cryolite concession to a separate company.During the years before World War I, the KGH's independence was curtailed and the company folded into the Ministry of the Interior. Climate change, apparent since the 1920s, disrupted traditional Kalaallit life as the milder weather reduced the island's seal populations but filled the waters offshore with cod. After World War II, reforms were finally enacted by the Danish Greenland Commission composed of Greenland Provincial Council members and Danish economists. The report outlined a program to end the KGH model and establish a modern welfare state on the Danish model and supported by the Kingdom Government. The KGH monopolies were ended in 1950; Greenland was made an equal part of the Kingdom of Denmark in 1953 and Home Rule granted in 1979. The KGH had long opposed urbanization of the Kalaallit Greenlanders, but during the 1950s and 1960s the Danish government introduced an urbanization and modernization program aimed at consolidating existing settlements. The program was intended to reduce costs, improve access to education and health care, and provide workers for modernized cod fisheries, which were growing rapidly at the time. The program faced a number of problems including the collapse of the fisheries and the shoddy construction of many of the buildings, particularly the infamous Blok P, and produced a number of problems of its own, including continuing unemployment and alcoholism. Greenland left the European Economic Community in February 1985, principally due to EEC policies on fishing and sealskin. Most EU laws do not apply to Greenland; however, owing to its connection with Denmark, Greenland continues to enjoy preferential access to EU markets. In the same year, Greenland exercised its new control over the Royal Greenland Trading Company to reestablish it as KNI. Over the next few decades, divisions of the conglomerate were slowly spun off and competition within the Greenlandic economy somewhat increased. Following the closure of the Maarmorilik lead and zinc mine in 1990 and the collapse of the cod fisheries amid colder ocean currents, Greenland faced foreign trade deficits and a shrinking economy, but it has been growing since 1993.
The Greenland economy is extremely dependent on exports of fish and on support from the Danish Government, which supplies about half of government revenues. The public sector, including publicly owned enterprises and the municipalities, plays the dominant role in the economy.
The largest employers in Greenland are the various levels of administration, including the central Kingdom Government in Denmark, the Local Greenland Self-Rule Government, and the municipalities. Most of these positions are in the capital Nuuk. In addition to this direct employment, the government heavily subsidizes other major employers in other areas of the economy, including Great Greenland's sealskin purchases, Pilersuisoq's rural stores, and some of Air Greenland and Royal Arctic's regional routes.
The second-largest sector by employment is Greenland's fishing industry. The commercial fishing fleet consists of approximately 5,000 dinghies, 300 cutters, and 25 trawlers. While cod was formerly the main catch, today the industry centers on cold-water shrimp and Greenland halibut.The fish processing industry is almost entirely centered on Royal Greenland, the world's largest retailer of cold-water shrimp.
Whaling and seal hunting were once traditional mainstays of Greenland's economy. Greenlanders still kill an estimated 170,000 seals a year and 175 whales a year, ranking them second and third in the world respectively. Both whaling and sealing have become controversial, limiting the potential market for their products. As such, the only seal tannery in the country – Great Greenland in Qaqortoq – is heavily subsidized by the government to maintain the livelihood of smaller communities which are economically dependent on the hunt.Reindeer or caribou are found in the northwest of the island, while muskoxen are found in the northeast and at Kangerlussuaq. Because the muskoxen's natural range favors the protected Northeast Greenland National Park, it is a less common object of hunting than in the past. Polar bear and reindeer hunting in Greenland still occur but are regulated to avoid endangering the populations.
Approximately half of total sales are conducted by KNI, the state-owned successor to the Royal Greenland Trade Department; its rural sales division Pilersuisoq; or its daughter company – which has been purchased by the Danish Dagrofa – Pisiffik. The third major chain is the Brugsen association of cooperatives.
Ivigtut used to be the world's premier source of natural cryolite, an important mineral in aluminum extraction, but the commercially viable reserves were depleted in the 1980s. Similarly, deposits of coal, diamonds, and many metals – including silver, nickel, platinum, copper, molybdenum, iron, niobium, tantalum, uranium, and rare earths – are known to exist, but not yet in commercially viable deposits. Greenland's Bureau of Minerals and Petroleum is working to promote Greenland as an attractive destination for prospectors. Improvements in technology and increases in mineral prices have led to some mines being reopened, such as the lead and zinc mine at Maarmorilik and the gold mine at Nalunaq.Greenland is expected to be one of the world’s next great mining frontiers as global warming starts to uncover precious metals from the frozen surroundings. Substantial volumes of minerals are now within reach of geological land mapping technologies, according to research conducted by GlobalData, a natural resources business intelligence provider.
At 70%, Greenland has one of the highest shares of renewable energy in the world, mostly coming from hydropower.While the Greenland Home Rule Government has primary sovereignty over mineral deposits on the mainland, oil resources are within the domain of the Danish exclusive economic zone. Nonetheless, prospecting takes place under the auspices of NUNAOIL, a partnership between the two governments. Some geologists believe Greenland has some of the world's largest remaining oil resources: in 2001, the U.S. Geological Survey found that the waters off north-eastern Greenland (north and south of the Arctic Circle) could contain up to 110 billion barrels (17×10^9 m3) of oil, and in 2010 the British petrochemical company Cairns Oil reported "the first firm indications" of commercially viable oil deposits. Nonetheless, all six wells drilled since the 1970s have been dry. Greenland has offered eight license blocks for tender along its west coast by Baffin Bay. Seven of those blocks have been bid for by a combination of multinational oil companies and NUNAOIL. Companies that have participated successfully in the previous license rounds and have formed a partnership for the licenses with NUNAOIL are DONG Energy, Chevron, ExxonMobil, Husky Energy, and Cairn Energy. The area available known as the West Disko licensing round is of interest due to its relative accessibility compared to other Arctic basins, as the area remains largely free of ice and contains a number of promising geological leads and prospects from the Paleocene era. Coal used to be mined at Qullissat but this has been suspended. Electricity generation is controlled by the state-owned Nukissiorfiit. It is distributed at 220 V and 50 Hz and sockets of Danish type K are used. Electricity has historically been generated by oil or diesel power plants, even though there is a large surplus of potential hydropower. Because of rising oil prices, there is a program to build hydro power plants. Since the success of the 1993 Buksefjord dam, – whose distribution path to Nuuk includes the Ameralik Span – the long-term policy of the Greenland government is to produce the island's electricity from renewable domestic sources. A third turbine at Buksefjord brought its capacity up to 45 MW in 2008; in 2007, a second, 7.2 MW dam was constructed at Qorlortorsuaq; and in 2010, a third, 15 MW dam was constructed at Sisimiut. There is a plan for an Aluminium smelter plant, which requires multiple large (total 600-750 MW) hydropower plants. Domestic heating is provided by electricity at locations where there is a hydro power plant.
Tourism is limited by the short summers and high costs. Access is almost exclusively by air, mainly from Scandinavia and Iceland. Some tourists arrive by cruise ship (but they don't spend much locally, since the ship provides accommodation and meals). There have been tests with direct flights from the US East Coast from 2007 to 2008, but these were discontinued. The state-owned tourism agency Visit Greenland has the web address Greenland.com.
Agriculture is of little importance in the economy but due to climate change – in southern Greenland, the growing season averages about three weeks longer than a decade ago – which has enabled expanded production of existing crops. At present, local production accounts for 10% of potatoes consumption in Greenland, but that is projected to grow to 15% by 2020. Similarly, it has enabled new crops like apples, strawberries, broccoli, cauliflower, cabbage, and carrots to be grown and for the cultivated areas of the country to be extended although even now only about 1% of Greenland is considered arable. Expanded production is subsidized by the government through purchase guarantees by the state-owned Neqi A/S grocery store chain. The only forest in Greenland is in the Qinngua Valley near Nanortalik. It is protected and not used for timber production.
Animal husbandry consists mainly of sheep farming, with free-grazing flocks. Modern sheep farming methods were introduced in the early 20th century, with the first farm built in 1906. The farms provide meat for local consumption and wool mainly for export. Some 20,000 lambs are slaughtered annually in Narsaq by the state-owned Neqi A/S. The lack of private land ownership rights on Greenland forces farmers to jointly agree to terms of land usage. In the south, there is also a small cattle farm.Reindeer herding has been introduced to Greenland in waves since 1952. Supervision by Scandinavian Sami ended in 1978 and subsequent results were dismal. Repeated attempts in mid-west Greenland in the 1980s and the 1990s failed due to the immobility of the herds, which destroyed their forage. In 1998, the remaining herd was sold to the Nuuk municipality and removed through hunting. At that point, only one Greenlander was still a deerherd; the rest – about 20 people – were still hired Norwegian Sami. Although the conclusion was drawn that reindeer herding was incompatible with the local culture, the southern herds continue to prosper. In 2008, there was still a strong herd at the Isortoq Reindeer Station maintained by the Icelander Stefán Magnússon and Norwegian Ole Kristiansen.
Greenland krone Bank of Greenland
"Setting up a Business in Greenland", from the Greenland Home Rule Government
A planned economy is a type of economic system where investment, production and the allocation of capital goods take place according to economy-wide economic plans and production plans. A planned economy may use centralized, decentralized, participatory or Soviet-type forms of economic planning. The level of centralization or decentralization in decision-making and participation depends on the specific type of planning mechanism employed.Socialist states based on the Soviet model have used central planning, although a minority such as the Socialist Federal Republic of Yugoslavia have adopted some degree of market socialism. Market abolitionist socialism replaces factor markets with direct calculation as the means to coordinate the activities of the various socially-owned economic enterprises that make up the economy. More recent approaches to socialist planning and allocation have come from some economists and computer scientists proposing planning mechanisms based on advances in computer science and information technology.Planned economies contrast with unplanned economies, specifically market economies, where autonomous firms operating in markets make decisions about production, distribution, pricing and investment. Market economies that use indicative planning are variously referred to as planned market economies, mixed economies and mixed market economies. A command economy follows an administrative-command system and uses Soviet-type economic planning which was characteristic of the former Soviet Union and Eastern Bloc before most of these countries converted to market economies. This highlights the central role of hierarchical administration and public ownership of production in guiding the allocation of resources in these economic systems.In command economies, important allocation decisions are made by government authorities and are imposed by law. This goes against the Marxist understanding of conscious planning. Decentralized planning has been proposed as a basis for socialism and has been variously advocated by anarchists, council communists, libertarian Marxists and other democratic and libertarian socialists who advocate a non-market form of socialism, in total rejection of the type of planning adopted in the economy of the Soviet Union.
In the Hellenistic and post-Hellenistic world, "compulsory state planning was the most characteristic trade condition for the Egyptian countryside, for Hellenistic India, and to a lesser degree the more barbaric regions of the Seleucid, the Pergamenian, the southern Arabian, and the Parthian empires". Scholars have argued that the Incan economy was a flexible type of command economy, centered around the movement and utilization of labor instead of goods. One view of mercantilism sees it as a planned economy.The Soviet-style planned economy started with war communism in 1918 and ended in 1921, when the Soviet government founded Gosplan in 1921. However, the period of the New Economic Policy intervened before regular five-year plans started in 1928. Dirigisme, or government direction of the economy through non-coercive means, was practiced in France and in Great Britain after World War II. The Swedish government planned public-housing models in a similar fashion as urban planning in a project called Million Programme, implemented from 1965 to 1974. Some decentralised participation in economic planning has been implemented across Revolutionary Spain, most notably in Catalonia, during the Spanish Revolution of 1936.
While socialism is not equivalent to economic planning or to the concept of a planned economy, an influential conception of socialism involves the replacement of capital markets with some form of economic planning in order to achieve ex-ante coordination of the economy. The goal of such an economic system would be to achieve conscious control over the economy by the population, specifically so that the use of the surplus product is controlled by the producers. The specific forms of planning proposed for socialism and their feasibility are subjects of the socialist calculation debate.
When the development of computer technology was still its early stages in 1971, the socialist Allende administration of Chile launched Project Cybersyn to install a telex machine in every corporation and organisation in the economy for the communication of economic data between firms and the government. The data was also fed into a computer-simulated economy for forecasting. A control room was built for realtime observation and management of the overall economy. The prototype-stage of the project showed promise when it was used to redirect supplies around a trucker's strike, but after CIA-backed Augusto Pinochet led a coup in 1973 that established a military dictatorship under his rule the program was abolished and Pinochet moved Chile towards a more liberalized market economy. In their book Towards a New Socialism (1993), the computer scientist Paul Cockshott from the University of Glasgow and the economist Allin Cottrell from the Wake Forest University claim to demonstrate how a democratically planned economy built on modern computer technology is possible and drives the thesis that it would be both economically more stable than the free-market economies and also morally desirable.
The use of computers to coordinate production in an optimal fashion has been variously proposed for socialist economies. The Polish economist Oskar Lange argued that the computer is more efficient than the market process at solving the multitude of simultaneous equations required for allocating economic inputs efficiently (either in terms of physical quantities or monetary prices).The 1970 Chilean distributed decision support system Project Cybersyn was pioneered by Salvador Allende's socialist government in an attempt to move towards a decentralised planned economy with the experimental viable system model of computed organisational structure of autonomous operative units though an algedonic feedback setting and bottom-up participative decision-making in the form of participative democracy by the Cyberfolk component.
The 1888 novel Looking Backward by Edward Bellamy depicts a fictional planned economy in a United States around the year 2000 which has become a socialist utopia. The World State in Aldous Huxley's Brave New World and Airstrip One in George Orwell's Nineteen Eighty-Four are both fictional examples of command economies, albeit with diametrically opposed aims. The former is a consumer economy designed to engender productivity while the latter is a shortage economy designed as an agent of totalitarian social control. Airstrip One is organized by the euphemistically named Ministry of Plenty. Other literary portrayals of planned economies were Yevgeny Zamyatin's We which was an influence on Orwell's work. Like Nineteen Eighty-Four, Ayn Rand's dystopian story Anthem was also an artistic portrayal of a command economy that was influenced by We. The difference is that it was a primitivist planned economy as opposed to the advanced technology of We or Brave New World.
The government can harness land, labour and capital to serve the economic objectives of the state. Consumer demand can be restrained in favor of greater capital investment for economic development in a desired pattern. In international comparisons, state-socialist nations compared favorably with capitalist nations in health indicators such as infant mortality and life expectancy. However, the reality of this, at least in regards to infant mortality, varied depending on whether official Soviet statistics or WHO definitions were used.The state can begin building a heavy industry at once in an underdeveloped economy without waiting years for capital to accumulate through the expansion of light industry and without reliance on external financing. This is what happened in the Soviet Union during the 1930s when the government forced the share of gross national income dedicated to private consumption from eighty percent to fifty percent. As a result of this development, the Soviet Union experienced massive growth in heavy industry, with a concurrent massive contraction of its agricultural sector due to labour shortage, in both relative and absolute terms.
Studies of command economies of the Eastern Bloc in the 1950s and 1960s by both American and Eastern European economists found that contrary to the expectations of both groups they showed greater fluctuations in output than market economies during the same period.
Critics of planned economies argue that planners cannot detect consumer preferences, shortages and surpluses with sufficient accuracy and therefore cannot efficiently co-ordinate production (in a market economy, a free price system is intended to serve this purpose). This difficulty was notably written about by economists Ludwig von Mises and Friedrich Hayek, who referred to subtly distinct aspects of the problem as the economic calculation problem and local knowledge problem, respectively.Whereas the former stressed the theoretical underpinnings of a market economy to subjective value theory while attacking the labor theory of value, the latter argued that the only way to satisfy individuals who have a constantly changing hierarchy of needs and are the only ones to possess their particular individual's circumstances is by allowing those with the most knowledge of their needs to have it in their power to use their resources in a competing marketplace to meet the needs of the most consumers most efficiently. This phenomenon is recognized as spontaneous order. Additionally, misallocation of resources would naturally ensue by redirecting capital away from individuals with direct knowledge and circumventing it into markets where a coercive monopoly influences behavior, ignoring market signals. According to Tibor Machan, "[w]ithout a market in which allocations can be made in obedience to the law of supply and demand, it is difficult or impossible to funnel resources with respect to actual human preferences and goals".
Economist Robin Hahnel, who supports participatory economics, a form of socialist decentralized planned economy, notes that even if central planning overcame its inherent inhibitions of incentives and innovation, it would nevertheless be unable to maximize economic democracy and self-management, which he believes are concepts that are more intellectually coherent, consistent and just than mainstream notions of economic freedom. Furthermore, Hahnel states: Combined with a more democratic political system, and redone to closer approximate a best case version, centrally planned economies no doubt would have performed better. But they could never have delivered economic self-management, they would always have been slow to innovate as apathy and frustration took their inevitable toll, and they would always have been susceptible to growing inequities and inefficiencies as the effects of differential economic power grew. Under central planning neither planners, managers, nor workers had incentives to promote the social economic interest. Nor did impeding markets for final goods to the planning system enfranchise consumers in meaningful ways. But central planning would have been incompatible with economic democracy even if it had overcome its information and incentive liabilities. And the truth is that it survived as long as it did only because it was propped up by unprecedented totalitarian political power.
Planned economies contrast with command economies. A planned economy is "an economic system in which the government controls and regulates production, distribution, prices, etc." whereas a command economy necessarily has substantial public ownership of industry while also having this type of regulation. Most of a command economy is organized in a top-down administrative model by a central authority, where decisions regarding investment and production output requirements are decided upon at the top in the chain of command, with little input from lower levels. Advocates of economic planning have sometimes been staunch critics of these command economies. Leon Trotsky believed that those at the top of the chain of command, regardless of their intellectual capacity, operated without the input and participation of the millions of people who participate in the economy and who understand/respond to local conditions and changes in the economy. Therefore, they would be unable to effectively coordinate all economic activity.Historians have associated planned economies with Marxist–Leninist states and the Soviet economic model. Since the 1980s, it was recognized that the Soviet economic model did not actually constitute a planned economy in that a comprehensive and binding plan did not guide production and investment. The further distinction of an administrative-command system emerged as a more accurate designation for the economic system that existed in the former Soviet Union and Eastern Bloc, highlighting the role of centralized hierarchical decision-making in the absence of popular control over the economy. The possibility of a digital planned economy was explored in Chile between 1971 and 1973 with the development of Project Cybersyn and by Alexander Kharkevich, head of the Department of Technical Physics in Kiev in 1962.While both economic planning and a planned economy can be either authoritarian or democratic and participatory, democratic socialist critics argue that command economies have been authoritarian or undemocratic in practice. Indicative planning is a form of economic planning in market economies that directs the economy through incentive-based methods. Economic planning can be practiced in a decentralized manner through different government authorities. In some predominantly market-oriented and Western mixed economies, the state utilizes economic planning in strategic industries such as the aerospace industry. Mixed economies usually employ macroeconomic planning while micro-economic affairs are left to the market and price system.
A decentralized-planned economy, occasionally called horizontally-planned economy due to its horizontalism, is a type of planned economy in which the investment and allocation of consumer and capital goods is explicated accordingly to an economy-wide plan built and operatively coordinated through a distributed network of disparate economic agents or even production units itself. Decentralized planning is usually held in contrast to centralized planning, in particular the Soviet-type economic planning of the Soviet Union's command economy, where economic information is aggregated and used to formulate a plan for production, investment and resource allocation by a single central authority. Decentralized planning can take shape both in the context of a mixed economy as well as in a post-capitalist economic system. This form of economic planning implies some process of democratic and participatory decision-making within the economy and within firms itself in the form of industrial democracy. Computer-based forms of democratic economic planning and coordination between economic enterprises have also been proposed by various computer scientists and radical economists. Proponents present decentralized and participatory economic planning as an alternative to market socialism for a post-capitalist society.Decentralized planning has been a feature of anarchist and other socialist economics. Variations of decentralized planning such as economic democracy, industrial democracy and participatory economics have been promoted by various political groups, most notably anarchists, democratic socialists, guild socialists, libertarian Marxists, libertarian socialists, revolutionary syndicalists and Trotskyists. During the Spanish Revolution, some areas where anarchist and libertarian socialist influence through the CNT and UGT was extensive, particularly rural regions, were run on the basis of decentralized planning resembling the principles laid out by anarcho-syndicalist Diego Abad de Santillan in the book After the Revolution.
Economist Pat Devine has created a model of decentralized economic planning called "negotiated coordination" which is based upon social ownership of the means of production by those affected by the use of the assets involved, with the allocation of consumer and capital goods made through a participatory form of decision-making by those at the most localized level of production. Moreover, organizations that utilize modularity in their production processes may distribute problem solving and decision making.
The planning structure of a decentralized planned economy is generally based on a consumers council and producer council (or jointly, a distributive cooperative) which is sometimes called a consumers' cooperative. Producers and consumers, or their representatives, negotiate the quality and quantity of what is to be produced. This structure is central to guild socialism, participatory economics and the economic theories related to anarchism.
Some decentralised participation in economic planning has been implemented in various regions and states in India, most notably in Kerala. Local level planning agencies assess the needs of people who are able to give their direct input through the Gram Sabhas (village-based institutions) and the planners subsequently seek to plan accordingly.
Some decentralised participation in economic planning has been implemented across Revolutionary Spain, most notably in Catalonia, during the Spanish Revolution of 1936.
The United Nations has developed local projects that promote participatory planning on a community level. Members of communities take decisions regarding community development directly.
Case studies (Soviet-type economies)Analysis of Soviet-type economic planning Eastern Bloc economies Economy of Cuba Economy of North Korea Five-year plans in the Soviet Union Five-year plans of China OGAS, a plan for creating a computer network to supervise the Soviet economy Project Cybersyn, a project for a computer network controlling the economy of Chile under Salvador AllendeCase studies (mixed-market economies)Dirigisme (indicative planning in France) Economy of India Economy of Singapore First Malaysia Plan Five-year plans of Argentina Five-year plans of South Korea
"The Myth of the Permanent Arms Economy" "The Stalin Model for the Control and Coordination of Enterprises in a Socialist Economy"
The economy of Bangladesh is a developing market economy. It's the 35th largest in the world in nominal terms, and 30th largest by purchasing power parity; it is classified among the Next Eleven emerging market middle income economies and a frontier market. In the first quarter of 2019, Bangladesh's was the world's seventh fastest growing economy with a rate of 7.3% real GDP annual growth. Dhaka and Chittagong are the principal financial centers of the country, being home to the Dhaka Stock Exchange and the Chittagong Stock Exchange. The financial sector of Bangladesh is the second largest in the Indian subcontinent. Bangladesh is one of the world's fastest growing economies. In the decade since 2004, Bangladesh averaged a GDP growth of 6.5%, that has been largely driven by its exports of ready made garments, remittances and the domestic agricultural sector. The country has pursued export-oriented industrialisation, with its key export sectors include textiles, shipbuilding, fish and seafood, jute and leather goods. It has also developed self-sufficient industries in pharmaceuticals, steel and food processing. Bangladesh's telecommunication industry has witnessed rapid growth over the years, receiving high investment from foreign companies. Bangladesh also has substantial reserves of natural gas and is Asia's seventh largest gas producer. Offshore exploration activities are increasing in its maritime territory in the Bay of Bengal. It also has large deposits of limestone. The government promotes the Digital Bangladesh scheme as part of its efforts to develop the country's growing information technology sector. Bangladesh is strategically important for the economies of Northeast India, Nepal and Bhutan, as Bangladeshi seaports provide maritime access for these landlocked regions and countries. China also views Bangladesh as a potential gateway for its landlocked southwest, including Tibet, Sichuan and Yunnan. As of 2019, Bangladesh's GDP per capita income is estimated as per IMF data at US$5,028 (PPP) and US$1,906 (nominal). Bangladesh is a member of the D-8 Organization for Economic Cooperation, the South Asian Association for Regional Cooperation, the International Monetary Fund, the World Bank, the World Trade Organization and the Asian Infrastructure Investment Bank. The economy faces challenges of infrastructure bottlenecks, bureaucratic corruption, and youth unemployment.
East Bengal—the eastern segment of Bengal—was a historically prosperous region. The Ganges Delta provided advantages of a mild, almost tropical climate, fertile soil, ample water, and an abundance of fish, wildlife, and fruit. The standard of living is believed to have been higher compared with other parts of South Asia. As early as the thirteenth century, the region was developing as an agrarian economy. Bengal was the junction of trade routes on the Southeastern Silk Road.
The economy of the Bengal Sultanate inherited earlier aspects of the Delhi Sultanate, including mint towns, a salaried bureaucracy and the jagirdar system of land ownership. The production of silver coins inscribed with the name of the Sultan of Bengal was a mark of Bengali sovereignty. Bengal was more successful in perpetuating purely silver coinage than Delhi and other contemporary Asian and European governments. There were three sources of silver. The first source was the leftover silver reserve of previous kingdoms. The second source was the tribute payments of subordinate kingdoms which were paid in silver bullion. The third source was during military campaigns when Bengali forces sacked neighboring states.The apparent vibrancy of the Bengal economy in the beginning of the 15th-century is attributed to the end of tribute payments to Delhi, which ceased after Bengali independence and stopped the outflow of wealth. Ma Huan's testimony of a flourishing shipbuilding industry was part of the evidence that Bengal enjoyed significant seaborne trade. The expansion of muslin production, sericulture and the emergence of several other crafts were indicated in Ma Huan's list of items exported from Bengal to China. Bengali shipping co-existed with Chinese shipping until the latter withdrew from the Indian Ocean in the mid-15th-century. The testimony of European travelers such as Ludovico di Varthema, Duarte Barbosa and Tomé Pires attest to the presence of a large number of wealthy Bengali merchants and shipowners in Malacca. Historian Rila Mukherjee wrote that ports in Bengal may have been entrepots, importing goods and re-exporting them to China.A vigorous riverine shipbuilding tradition existed in Bengal. The shipbuilding tradition is evidenced in the sultanate's naval campaigns in the Ganges delta. The trade between Bengal and the Maldives, based on rice and cowry shells, was probably done on Arab-style baghlah ships. Chinese accounts point to Bengali ships being prominent in Southeast Asian waters. A vessel from Bengal, probably owned by the Sultan of Bengal, could accommodate three tribute missions- from Bengal, Brunei and Sumatra- and was evidently the only vessel capable of such a task. Bengali ships were the largest vessels plying in those decades in Southeast Asian waters.All large business transactions were done in terms of silver taka. Smaller purchases involved shell currency. One silver coin was worth 10,250 cowry shells. Bengal relied on shiploads of cowry shell imports from the Maldives. Due to the fertile land, there was an abundance of agricultural commodities, including bananas, jackfruits, pomegranate, sugarcane, and honey. Native crops included rice and sesame. Vegetables included ginger, mustard, onions, and garlic among others. There were four types of wines, including coconut, rice, tarry and kajang. Bengali streets were well provided with eating establishments, drinking houses and bathhouses. At least six varieties of fine muslin cloth existed. Silk fabrics were also abundant. Pearls, rugs and ghee were other important products. The finest variety of paper was made in Bengal from the bark of mulberry trees. The high quality of paper was compared with the lightweight white muslin cloth.Europeans referred to Bengal as "the richest country to trade with". Bengal was the eastern pole of Islamic India. Like the Gujarat Sultanate in the western coast of India, Bengal in the east was open to the sea and accumulated profits from trade. Merchants from around the world traded in the Bay of Bengal. Cotton textile exports were a unique aspect of the Bengali economy. Marco Polo noted Bengal's prominence in the textile trade. In 1569, Venetian explorer Caesar Frederick wrote about how merchants from Pegu in Burma traded in silver and gold with Bengalis. Overland trade routes such as the Grand Trunk Road connected Bengal to northern India, Central Asia and the Middle East.
Under Mughal rule, Bengal operated as a centre of the worldwide muslin, silk and pearl trades. Domestically, much of India depended on Bengali products such as rice, silks and cotton textiles. Overseas, Europeans depended on Bengali products such as cotton textiles, silks and opium; Bengal accounted for 40% of Dutch imports from Asia, for example. Bengal shipped saltpeter to Europe, sold opium in Indonesia, exported raw silk to Japan and the Netherlands, and produced cotton and silk textiles for export to Europe, Indonesia and Japan. Real wages and living standards in 18th-century Bengal were comparable to Britain, which in turn had the highest living standards in Europe.During the Mughal era, the most important centre of cotton production was Bengal, particularly around its capital city of Dhaka, leading to muslin being called "daka" in distant markets such as Central Asia. Bengali agriculturalists rapidly learned techniques of mulberry cultivation and sericulture, establishing Bengal as a major silk-producing region of the world. Bengal accounted for more than 50% of textiles and around 80% of silks imported by the Dutch from Asia, for example.Bengal also had a large shipbuilding industry. Indrajit Ray estimates shipbuilding output of Bengal during the sixteenth and seventeenth centuries at 223,250 tons annually, compared with 23,061 tons produced in nineteen colonies in North America from 1769 to 1771. He also assesses ship repairing as very advanced in Bengal. Bengali shipbuilding was advanced compared to European shipbuilding at the time. An important innovation in shipbuilding was the introduction of a flushed deck design in Bengal rice ships, resulting in hulls that were stronger and less prone to leak than the structurally weak hulls of traditional European ships built with a stepped deck design. The British East India Company later duplicated the flushed-deck and hull designs of Bengal rice ships in the 1760s, leading to significant improvements in seaworthiness and navigation for European ships during the Industrial Revolution.
The British East India Company, that took complete control of Bengal in 1793 by abolishing Nizamat (local rule), chose to develop Calcutta, now the capital city of West Bengal, as their commercial and administrative center for the Company-held territories in South Asia. The development of East Bengal was thereafter limited to agriculture. The administrative infrastructure of the late eighteenth and nineteenth centuries reinforced East Bengal's function as the primary agricultural producer—chiefly of rice, tea, teak, cotton, sugar cane and jute — for processors and traders from around Asia and beyond.
After its independence from Pakistan, Bangladesh followed a socialist economy by nationalising all industries, proving to be a critical blunder undertaken by the Awami League government. Some of the same factors that had made East Bengal a prosperous region became disadvantages during the nineteenth and twentieth centuries. As life expectancy increased, the limitations of land and the annual floods increasingly became constraints on economic growth. Traditional agricultural methods became obstacles to the modernisation of agriculture. Geography severely limited the development and maintenance of a modern transportation and communications system.The partition of British India and the emergence of India and Pakistan in 1947 severely disrupted the economic system. The united government of Pakistan expanded the cultivated area and some irrigation facilities, but the rural population generally became poorer between 1947 and 1971 because improvements did not keep pace with rural population increase. Pakistan's five-year plans opted for a development strategy based on industrialisation, but the major share of the development budget went to West Pakistan, that is, contemporary Pakistan. The lack of natural resources meant that East Pakistan was heavily dependent on imports, creating a balance of payments problem. Without a substantial industrialisation programme or adequate agrarian expansion, the economy of East Pakistan steadily declined. Blame was placed by various observers, but especially those in East Pakistan, on the West Pakistani leaders who not only dominated the government but also most of the fledgling industries in East Pakistan.Since Bangladesh followed a socialist economy by nationalising all industries after its independence, it underwent a slow growth of producing experienced entrepreneurs, managers, administrators, engineers, and technicians. There were critical shortages of essential food grains and other staples because of wartime disruptions. External markets for jute had been lost because of the instability of supply and the increasing popularity of synthetic substitutes. Foreign exchange resources were minuscule, and the banking and monetary systems were unreliable. Although Bangladesh had a large work force, the vast reserves of under trained and underpaid workers were largely illiterate, unskilled, and underemployed. Commercially exploitable industrial resources, except for natural gas, were lacking. Inflation, especially for essential consumer goods, ran between 300 and 400 percent. The war of independence had crippled the transportation system. Hundreds of road and railroad bridges had been destroyed or damaged, and rolling stock was inadequate and in poor repair. The new country was still recovering from a severe cyclone that hit the area in 1970 and caused 250,000 deaths. India came forward immediately with critically measured economic assistance in the first months after Bangladesh achieved independence from Pakistan. Between December 1971 and January 1972, India committed US$232 million in aid to Bangladesh from the politico-economic aid India received from the US and USSR. Official amount of disbursement yet undisclosed.After 1975, Bangladeshi leaders began to turn their attention to developing new industrial capacity and rehabilitating its economy. The static economic model adopted by these early leaders, however—including the nationalisation of much of the industrial sector—resulted in inefficiency and economic stagnation. Beginning in late 1975, the government gradually gave greater scope to private sector participation in the economy, a pattern that has continued. Many state-owned enterprises have been privatised, like banking, telecommunication, aviation, media, and jute. Inefficiency in the public sector has been rising however at a gradual pace; external resistance to developing the country's richest natural resources is mounting; and power sectors including infrastructure have all contributed to slowing economic growth.In the mid-1980s, there were encouraging signs of progress. Economic policies aimed at encouraging private enterprise and investment, privatising public industries, reinstating budgetary discipline, and liberalising the import regime were accelerated. From 1991 to 1993, the government successfully followed an enhanced structural adjustment facility (ESAF) with the International Monetary Fund (IMF) but failed to follow through on reforms in large part because of preoccupation with the government's domestic political troubles. In the late 1990s the government's economic policies became more entrenched, and some gains were lost, which was highlighted by a precipitous drop in foreign direct investment in 2000 and 2001. In June 2003 the IMF approved 3-year, $490-million plan as part of the Poverty Reduction and Growth Facility (PRGF) for Bangladesh that aimed to support the government's economic reform programme up to 2006. Seventy million dollars was made available immediately. In the same vein the World Bank approved $536 million in interest-free loans. The economy saw continuous real GDP growth of at least 5% since 2003. In 2010, Government of India extended a line of credit worth $1 billion to counterbalance China's close relationship with Bangladesh. Bangladesh historically has run a large trade deficit, financed largely through aid receipts and remittances from workers overseas. Foreign reserves dropped markedly in 2001 but stabilised in the US$3 to US$4 billion range (or about 3 months' import cover). In January 2007, reserves stood at $3.74 billion, and then increased to $5.8 billion by January 2008, in November 2009 it surpassed $10.0 billion, and as of April 2011 it surpassed the US$12 billion according to the Bank of Bangladesh, the central bank. The dependence on foreign aid and imports has also decreased gradually since the early 1990s. According to Bangladesh bank the reserve is $30 billion in August 2016 In last decade, poverty dropped by around one third with significant improvement in human development index, literacy, life expectancy and per capita food consumption. With economy growing close to 6% per year, more than 15 million people have moved out of poverty since 1992.
This is a chart of trend of gross domestic product of Bangladesh at market prices estimated by the International Monetary Fund with figures in millions of Bangladeshi Taka. However, this reflects only the formal sector of the economy. Mean wages were $0.58 per man-hour in 2009. The following table shows the main economic indicators in 1980–2019. Inflation below 5% is in green.
Most Bangladeshis earn their living from agriculture. Although rice and jute are the primary crops, maize and vegetables are assuming greater importance. Due to the expansion of irrigation networks, some wheat producers have switched to cultivation of maize which is used mostly as poultry feed. Tea is grown in the northeast. Because of Bangladesh's fertile soil and normally ample water supply, rice can be grown and harvested three times a year in many areas. Due to a number of factors, Bangladesh's labour-intensive agriculture has achieved steady increases in food grain production despite the often unfavourable weather conditions. These include better flood control and irrigation, a generally more efficient use of fertilisers, and the establishment of better distribution and rural credit networks. With 28.8 million metric tons produced in 2005–2006 (July–June), rice is Bangladesh's principal crop. By comparison, wheat output in 2005–2006 was 9 million metric tons. Population pressure continues to place a severe burden on productive capacity, creating a food deficit, especially of wheat. Foreign assistance and commercial imports fill the gap, but seasonal hunger ("monga") remains a problem. Underemployment remains a serious problem, and a growing concern for Bangladesh's agricultural sector will be its ability to absorb additional manpower. Finding alternative sources of employment will continue to be a daunting problem for future governments, particularly with the increasing numbers of landless peasants who already account for about half the rural labour force. Due to farmers' vulnerability to various risks, Bangladesh's poorest face numerous potential limitations on their ability to enhance agriculture production and their livelihoods. These include an actual and perceived risk to investing in new agricultural technologies and activities (despite their potential to increase income), a vulnerability to shocks and stresses and a limited ability to mitigate or cope with these and limited access to market information.
Many new jobs – mostly for women – have been created by the country's dynamic private ready-made garment industry, which grew at double-digit rates through most of the 1990s. By the late 1990s, about 1.5 million people, mostly women, were employed in the garments sector as well as Leather products specially Footwear (Shoe manufacturing unit). During 2001–2002, export earnings from ready-made garments reached $3,125 million, representing 52% of Bangladesh's total exports. Bangladesh has overtaken India in apparel exports in 2009, its exports stood at 2.66 billion US dollar, ahead of India's 2.27 billion US dollar and in 2014 the export rose to $3.12 billion every month. At the fiscal year 2018, Bangladesh has been able to garner US$36.67 billion export earnings by exporting manufactured goods, of which, 83.49 percent has come from the apparel manufacturing sector.Eastern Bengal was known for its fine muslin and silk fabric before the British period. The dyes, yarn, and cloth were the envy of much of the premodern world. Bengali muslin, silk, and brocade were worn by the aristocracy of Asia and Europe. The introduction of machine-made textiles from England in the late eighteenth century spelled doom for the costly and time-consuming hand loom process. Cotton growing died out in East Bengal, and the textile industry became dependent on imported yarn. Those who had earned their living in the textile industry were forced to rely more completely on farming. Only the smallest vestiges of a once-thriving cottage industry survived.Other industries which have shown very strong growth include the pharmaceutical industry, shipbuilding industry, information technology, leather industry, steel industry, and light engineering industry. Bangladesh's textile industry, which includes knitwear and ready-made garments (RMG) along with specialised textile products, is the nation's number one export earner, accounting for $21.5 billion in 2013 – 80% of Bangladesh's total exports of $27 billion. Bangladesh is 2nd in world textile exports, behind China, which exported $120.1 billion worth of textiles in 2009. The industry employs nearly 3.5 million workers. Current exports have doubled since 2004. Wages in Bangladesh's textile industry were the lowest in the world as of 2010. The country was considered the most formidable rival to China where wages were rapidly rising and currency was appreciating. As of 2012 wages remained low for the 3 million people employed in the industry, but labour unrest was increasing despite vigorous government action to enforce labour peace. Owners of textile firms and their political allies were a powerful political influence in Bangladesh. The urban garment industry has created more than one million formal sector jobs for women, contributing to the high female labour participation in Bangladesh. While it can be argued that women working in the garment industry are subjected to unsafe labour conditions and low wages, Dina M. Siddiqi argues that even though conditions in Bangladesh garment factories "are by no means ideal," they still give women in Bangladesh the opportunity to earn their own wages. As evidence she points to the fear created by the passage of the 1993 Harkins Bill (Child Labor Deterrence Bill), which caused factory owners to dismiss "an estimated 50,000 children, many of whom helped support their families, forcing them into a completely unregulated informal sector, in lower-paying and much less secure occupations such as brick-breaking, domestic service and rickshaw pulling."Even though the working conditions in garment factories are not ideal, they tend to financially be more reliable than other occupations and, "enhance women’s economic capabilities to spend, save and invest their incomes." Both married and unmarried women send money back to their families as remittances, but these earned wages have more than just economic benefits. Many women in the garment industry are marrying later, have lower fertility rates, and attain higher levels of education, then women employed elsewhere.After massive labour unrest in 2006 the government formed a Minimum Wage Board including business and worker representatives which in 2006 set a minimum wage equivalent to 1,662.50 taka, $24 a month, up from Tk950. In 2010, following widespread labour protests involving 60,000 workers in June 2010, a controversial proposal was being considered by the Board which would raise the monthly minimum to the equivalent of $50 a month, still far below worker demands of 5,000 taka, $72, for entry level wages, but unacceptably high according to textile manufacturers who are asking for a wage below $30. On 28 July 2010 it was announced that the minimum entry level wage would be increased to 3,000 taka, about $43.The government also seems to believe some change is necessary. On 21 September 2006 then ex-Prime Minister Khaleda Zia called on textile firms to ensure the safety of workers by complying with international labour law at a speech inaugurating the Bangladesh Apparel & Textile Exposition (BATEXPO). Many Western multinationals use labour in Bangladesh, which is one of the cheapest in the world: 30 euros per month compared to 150 or 200 in China. Four days is enough for the CEO of one of the top five global textile brands to earn what a Bangladeshi garment worker will earn in her lifetime. In April 2013, at least 1,135 textile workers died in the collapse of their factory. Other fatal accidents due to unsanitary factories have affected Bangladesh: in 2005 a factory collapsed and caused the death of 64 people. In 2006, a series of fires killed 85 people and injured 207 others. In 2010, some 30 people died of asphyxiation and burns in two serious fires.In 2006, tens of thousands of workers mobilized in one of the country's largest strike movements, affecting almost all of the 4,000 factories. The Bangladesh Garment Manufacturers and Exporters Association (BGMEA) uses police forces to crack down. Three workers were killed, hundreds more were wounded by bullets, or imprisoned. In 2010, after a new strike movement, nearly 1,000 people were injured among workers as a result of the repression.
Shipbuilding is a growing industry in Bangladesh with great potential. Due to the potential of shipbuilding in Bangladesh, the country has been compared to countries like China, Japan and South Korea. Referring to the growing amount of export deals secured by the shipbuilding companies as well as the low cost labour available in the country, experts suggest that Bangladesh could emerge as a major competitor in the global market of small to medium ocean-going vessels.Bangladesh also has the world's largest ship breaking industry which employs over 200,000 Bangladeshis and accounts for half of all the steel in Bangladesh. Chittagong Ship Breaking Yard is the world's second-largest ship breaking area. Khulna Shipyard Limited (KSY) with over five decades of reputation has been leading the Bangladesh Shipbuilding industry and had built a wide spectrum of ships for domestic and international clients. KSY built ships for Bangladesh Navy, Bangladesh Army and Bangladesh Coast Guard under the contract of ministry of defence.
Until the 1980s, the financial sector of Bangladesh was dominated by state-owned banks. With the grand-scale reform made in finance, private commercial banks were established through privatisation. The next finance sector reform programme was launched from 2000 to 2006 with focus on the development of financial institutions and adoption of risk-based regulations and supervision by Bangladesh Bank. As of date, the banking sector consisted of 4 SCBs, 4 government-owned specialized banks dealing in development financing, 39 private commercial banks, and 9 foreign commercial banks.
Tourism in Bangladesh
Bangladesh's information technology sector is growing example of what can be achieved after the current government's relentless effort to create a skilled workforce in ICT sector. The ICT workforce consisted of private sector and freelance skilled ICT workforce. The ICT sector also contributed to Bangladesh's economic growth. The ICT adviser to the prime minister, Sajeeb Wazed Joy is hopeful that Bangladesh will become a major player in the ICT sector in the future. In the last 3 years, Bangladesh has seen a tremendous growth in the ICT sector. Bangladesh is a market of 160 million people with vast consumer spending around mobile phones, telco and internet. Bangladesh has 80 million internet users, an estimated 9% growth in internet use by June 2017 powered by mobile internet. Bangladesh currently has an active 23 million Facebook users. Bangladesh currently has 143.1 million mobile phone customers. Bangladesh has exported $800 million worth of software, games, outsourcing and services to European countries, the United States, Canada, Russia and India by 30 June 2017. The Junior Minister for ICT division of the Ministry of Post, Telecommunications and Information Technology said that Bangladesh aims to raise its export earnings from the information and communications technology (ICT) sector to $5 billion by 2021.
The stock market capitalisation of the Dhaka Stock Exchange in Bangladesh crossed $10 billion in November 2007 and the $30 billion mark in 2009, and US$50 billion in August 2010. Bangladesh had the best performing stock market in Asia during the recent global recession between 2007 and 2010, due to relatively low correlations with developed country stock markets.Major investment in real estate by domestic and foreign-resident Bangladeshis has led to a massive building boom in Dhaka and Chittagong. Recent (2011) trends for investing in Bangladesh as Saudi Arabia trying to secure public and private investment in oil and gas, power and transportation projects, United Arab Emirates (UAE) is keen to invest in growing shipbuilding industry in Bangladesh encouraged by comparative cost advantage, Tata, an India-based leading industrial multinational to invest Taka 1500 crore to set up an automobile industry in Bangladesh, World Bank to invest in rural roads improving quality of live, the Rwandan entrepreneurs are keen to invest in Bangladesh's pharmaceuticals sector considering its potentiality in international market, Samsung sought to lease 500 industrial plots from the export zones authority to set up an electronics hub in Bangladesh with an investment of US$1.25 billion, National Board of Revenue (NBR) is set to withdraw tax rebate facilities on investment in the capital market by individual taxpayers from the fiscal 2011–12. In 2011, Japan Bank for International Cooperation ranked Bangladesh as the 15th best investment destination for foreign investors.
The bullish capital market turned bearish during 2010, with the exchange losing 1,800 points between December 2010 and January 2011. Millions of investors have been rendered bankrupt as a result of the market crash. The crash is believed to be caused artificially to benefit a handful of players at the expense of the big players.
The list includes ten largest Bangladeshi companies by trading value (millions in BDT) in 2018.
The Bangladesh Garments Manufacturers and Exporters Association (BGMEA) has predicted textile exports will rise from US$7.90 billion earned in 2005–06 to US$15 billion by 2011. In part this optimism stems from how well the sector has fared since the end of textile and clothing quotas, under the Multifibre Agreement, in early 2005. According to a United Nations Development Programme report "Sewing Thoughts: How to Realize Human Development Gains in the Post-Quota World" Bangladesh has been able to offset a decline in European sales by cultivating new markets in the United States."[In 2005] we had tremendous growth. The quota-free textile regime has proved to be a big boost for our factories," said BGMEA president S.M. Fazlul Hoque told reporters, after the sector's 24 per cent growth rate was revealed.The Bangladesh Knitwear Manufacturers and Exporters Association (BKMEA) president Md Fazlul Hoque has also struck an optimistic tone. In an interview with United News Bangladesh he lauded the blistering growth rate, saying "The quality of our products and its competitiveness in terms of prices helped the sector achieve such... tremendous success." Knitwear posted the strongest growth of all textile products in 2005–06, surging 35.38 per cent to US$2.82 billion. On the downside however, the sector's strong growth came amid sharp falls in prices for textile products on the world market, with growth subsequently dependent upon large increases in volume. Bangladesh's quest to boost the quantity of textile trade was also helped by US and EU caps on Chinese textiles. The US cap restricts growth in imports of Chinese textiles to 12.5 per cent next year and between 15 and 16 per cent in 2008. The EU deal similarly manages import growth until 2008. Bangladesh may continue to benefit from these restrictions over the next two years, however a climate of falling global textile prices forces wage rates the centre of the nation's efforts to increase market share. They offer a range of incentives to potential investors including 10-year tax holidays, duty-free import of capital goods, raw materials and building materials, exemptions on income tax on salaries paid to foreign nationals for three years and dividend tax exemptions for the period of the tax holiday. All goods produced in the zones are able to be exported duty-free, in addition to which Bangladesh benefits from the Generalised System of Preferences in US, European and Japanese markets and is also endowed with Most Favoured Nation status from the United States. Furthermore, Bangladesh imposes no ceiling on investment in the EPZs and allows full repatriation of profits. The formation of labour unions within the EPZs is prohibited as are strikes.Bangladesh has been a world leader in its efforts to end the use of child labour in garment factories. On 4 July 1995, the Bangladesh Garment Manufacturers and Exporters Association, International Labour Organization, and UNICEF signed a memorandum of understanding on the elimination of child labour in the garment sector. Implementation of this pioneering agreement began in fall 1995, and by the end of 1999, child labour in the garment trade virtually had been eliminated. The labour-intensive process of ship breaking for scrap has developed to the point where it now meets most of Bangladesh's domestic steel needs. Other industries include sugar, tea, leather goods, newsprint, pharmaceutical, and fertilizer production. The Bangladesh government continues to court foreign investment, something it has done fairly successfully in private power generation and gas exploration and production, as well as in other sectors such as cellular telephony, textiles, and pharmaceuticals. In 1989, the same year it signed a bilateral investment treaty with the United States, it established a Board of Investment to simplify approval and start-up procedures for foreign investors, although in practice the board has done little to increase investment. The government created the Bangladesh Export Processing Zone Authority to manage the various export processing zones. The agency currently manages EPZs in Adamjee, Chittagong, Comilla, Dhaka, Ishwardi, Karnaphuli, Mongla, and Uttara. An EPZ has also been proposed for Sylhet. The government has given the private sector permission to build and operate competing EPZs-initial construction on a Korean EPZ started in 1999. In June 1999, the AFL-CIO petitioned the U.S. Government to deny Bangladesh access to U.S. markets under the Generalized System of Preferences (GSP), citing the country's failure to meet promises made in 1992 to allow freedom of association in EPZs.
As of 2014, female participation in the labour force is 58% as per World Bank data, and male participation at 82%. A 2007 World Bank report stated that the areas in which women's work force participation have increased the most are in the fields of agriculture, education and health and social work. Over three-quarters of women in the labour force work in the agricultural sector. On the other hand, the International Labour Organization reports that women's workforce participation has only increased in the professional and administrative areas between 2000 and 2005, demonstrating women's increased participation in sectors that require higher education. Employment and labour force participation data from the World Bank, the UN, and the ILO vary and often under report on women's work due to unpaid labour and informal sector jobs. Though these fields are mostly paid, women experience very different work conditions than men, including wage differences and work benefits. Women's wages are significantly lower than men's wages for the same job with women being paid as much as 60–75 percent less than what men make.One example of action that is being taken to improve female conditions in the work force is Non-Governmental Organisations. These NGOs encourage women to rely on their own self-savings, rather than external funds provide women with increased decision-making and participation within the family and society. However, some NGOs that address microeconomic issues among individual families fail to deal with broader macroeconomic issues that prevent women's complete autonomy and advancement.
Bangladesh has made significant strides in its economic sector performance since independence in 1971. Although the economy has improved vastly in the 1990s, Bangladesh still suffers in the area of foreign trade in South Asian. Despite major impediments to growth like the inefficiency of state-owned enterprises, a rapidly growing labour force that cannot be absorbed by agriculture, inadequate power supplies, and slow implementation of economic reforms, Bangladesh has made some headway improving the climate for foreign investors and liberalising the capital markets; for example, it has negotiated with foreign firms for oil and gas exploration, bettered the countrywide distribution of cooking gas, and initiated the construction of natural gas pipelines and power stations. Progress on other economic reforms has been halting because of opposition from the bureaucracy, public sector unions, and other vested interest groups. The especially severe floods of 1998 increased the flow of international aid. So far the global financial crisis has not had a major impact on the economy. Foreign aid has seen a gradual decline over the last few decades but economists see this as a good sign for self-reliance. There has been a dramatic growth in exports and remittance inflow which has helped the economy to expand at a steady rate.Bangladesh has been on the list of UN Least Developed Countries (LDC) since 1975. Bangladesh met the requirements to be recognised as a developing country in March, 2018. Bangladesh's Gross National Income (GNI) $1,724 per capita, the Human Assets Index (HAI) 72 and the Economic Vulnerability (EVI) Index 25.2.
Bangladesh Academy for Rural Development Electricity sector in Bangladesh Automotive industry in Bangladesh Bangladeshi RMG Sector List of companies of Bangladesh List of the largest trading partners of Bangladesh 3G (countries) Corruption in Bangladesh
Bangladesh Economic Development at Curlie Bangladesh Economic News Bangladesh Budget 2007 – 2008 Budget in Brief 2016–17 World Bank Summary Trade Statistics Bangladesh, 2007 This article incorporates public domain material from the United States Department of State website https://www.state.gov/countries-areas/. (U.S. Bilateral Relations Fact Sheets) This article incorporates public domain material from the CIA World Factbook website https://www.cia.gov/library/publications/the-world-factbook/index.html.
The economy of China, described as Socialism with Chinese characteristics since the 12th National Congress of the CCP in 1982, is a mixed socialist market economy which is composed of state-owned enterprises and domestic and foreign private businesses and uses economic planning. The income generated by state-owned enterprises accounted for about 40% of China's GDP of USD 14.4 trillion in 2019, with domestic and foreign private businesses and investment accounting for the remaining 60%. As of the end of 2019, the total assets of all China's SOEs, including those operating in the financial sector, reached USD 78.08 trillion. Ninety-one (91) of these SOEs belong to the 2020 Fortune Global 500 companies. Direct foreign investment in China, which totaled about USD 1.6 trillion as of the end of October 2016, directly and indirectly contributed about one-third of China's GDP and a quarter of jobs there. As of the end of June 2020, FDI stock in China reached USD 2.947 trillion, and China's outgoing FDI stock stood at USD 2.128 trillion. Total foreign financial assets owned by China reached USD 7.860 trillion, and its foreign financial liabilities USD 5.716 trillion, making China the second largest creditor nation after Japan in the world.The government began its economic reforms in 1978 under the leadership of Deng Xiaoping. China has four of the top ten most competitive financial centers (Shanghai, Hong Kong, Beijing, and Shenzhen) in the 2020 Global Financial Centres Index, more than any other country. China has three out of the ten world's largest stock exchanges—Shanghai, Hong Kong and Shenzhen by market capitalization and trade volume. As of October 12, 2020, the total market capitalization of Mainland Chinese stock markets, consisting of the Shanghai Stock Exchange and Shenzhen Stock Exchange, topped USD 10 trillion, excluding the Hong Kong Stock Exchange, with about USD 5.9 trillion. As of the end of June 2020, foreign investors had bought a total of USD 440 billion in Chinese stocks, representing about 2.9% of the total value, and indicating that foreign investors scooped up a total of USD 156.6 billion in the stocks just in the first half of 2020.The total value of China's bond market topped USD 15.4 trillion, ranked above that of Japan and the U.K., and second only to that of the U.S. with USD 40 trillion, as of the beginning of September 2020. As of the end of September 2020, foreign holdings of Chinese bonds reached USD 388 billion, or 2.5%, of the total value, notwithstanding an increase by 44.66% year on year.According to the 2019 Global Wealth Report by Credit Suisse Group, China surpassed the US in the wealth of the top 10% of the world's population: China had 100 million wealthy people (each owning a net wealth of over USD 110,000) and the US 99 million. At USD 63.8 trillion as of end of 2019, representing a 17-fold increase from USD 3.7 trillion in 2001, the total amount of China's household wealth stood behind only that of the US with USD 105.6 trillion. The economy, as of 2019, ranked as the second largest in the world by nominal GDP and as of 2017 the largest in the world by purchasing power parity. China has the world's fastest-growing major economy, with growth rates averaging 10% over 30 years.As of 2019, China's public sector accounted for 63% of total employment. According to the IMF, on a per capita income basis, China ranked 73rd by GDP (PPP) per capita in 2019. China's GDP was $14.4 trillion (99 trillion Yuan) in 2019. The country has natural resources with an estimated worth of $23 trillion, 90% of which are coal and rare earth metals. China also has the world's largest total banking sector assets of around $45.838 trillion (309.41 trillion CNY) with $42.063 trillion in total deposits and other liabilities. It has the second largest inward foreign direct investment at USD 141 billion in 2019 alone, and the second largest outward foreign direct investment, at USD 136.91 billion for 2019 alone, following Japan at USD 226.65 billion for the same period. As of 2018, China was first in the world in total number of billionaires and second in millionaires—there were 658 Chinese billionaires and 3.5 million millionaires. As of 2020, China is home to the largest companies in the Fortune Global 500 and 129 are headquartered in China. It has the world's largest foreign-exchange reserves worth $3.1 trillion, but if the foreign assets of China's state-owned commercial banks are included, the value of China's reserves rises to nearly $4 trillion. Historically, China was one of the world's foremost economic powers for most of the two millennia from the 1st until the 19th century. China accounted for around one-quarter of the global GDP until the late 1700s and approximately one-third of the global GDP in 1820 as the Industrial Revolution was beginning in Great Britain. China's GDP in 1820 was six times as large as Britain’s, the largest economy in Europe — and almost 20 times the GDP of the nascent United States.Currently, China is the world's largest manufacturing economy and exporter of goods. China is classified as a newly industrialized economy. It is also the world's fastest-growing consumer market and second-largest importer of goods. China is a net importer of services products. It is the largest trading nation in the world and plays a prominent role in international trade and has increasingly engaged in trade organizations and treaties in recent years. China became a member of the World Trade Organization in 2001. It also has free trade agreements with several nations, including ASEAN, Australia, New Zealand, Pakistan, South Korea and Switzerland. The provinces in the coastal regions of China tend to be more industrialized while regions in the hinterland are less developed. As China's economic importance has grown, so has attention to the structure and health of the economy. China's largest trading partners are the US, EU, Japan, Hong Kong, South Korea, India, Taiwan, Australia, Vietnam, Malaysia, and Brazil. With 778 million workers, the Chinese labour force is the world's largest as of 2020. It ranks 31st on the Ease of doing business index and 28th on the Global Competitiveness Report. China ranks 14th on the Global Innovation Index and is the only middle-income economy and the only emerging country in the top 30. By the end of July 2020, China's 5G users had already surpassed 88 million, accounting for over 80% of users worldwide -- far ahead of the previously projected 70% share for the whole of 2020. By the end of this year, the number of 5G base stations in China is expected to reach nearly one million, by far the biggest tally globally. As of the end of June 2020, the number of automatic teller machines (ATMs) stood at 1.0521 million, the most globally.To avoid the long-term socioeconomic cost of environmental pollution in China, it has been suggested by Nicholas Stern and Fergus Green of the Grantham Research Institute on Climate Change and the Environment that the economy of China be shifted to more advanced industrial development with low carbon dioxide emissions and better allocation of national resources to innovation and R&D for sustainable economic growth in order to reduce the impact of China's heavy industry. This is in accord with the planning goals of the central government. Communist Party general secretary Xi Jinping's Chinese Dream is described as achieving the "Two Centenaries", namely the material goal of China becoming a "moderately prosperous society" by 2021, and the modernization goal of China becoming a fully developed nation by 2049, the 100th anniversary of the founding of the People's Republic. The internationalization of the Chinese economy continues to affect the standardized economic forecast officially launched in China by the Purchasing Managers Index in 2005. As China's economy grows, so does China's Renminbi, which undergoes the process needed for its internationalization. China initiated the founding of the Asian Infrastructure Investment Bank in 2015 and the Silk Road Fund in 2014, an investment fund of the Chinese government to foster increased investment and provide financial supports in countries along the One Belt, One Road. The economic development of Shenzhen has caused the city to be referred to as the world's next Silicon Valley.
China's unequal transportation system—combined with important differences in the availability of natural and human resources and in industrial infrastructure—has produced significant variations in the regional economies of China. Economic development has generally been more rapid in coastal provinces than in the interior and there are large disparities in per capita income between regions. The three wealthiest regions are the Yangtze River Delta in East China; the Pearl River Delta in South China; and Jingjinji region in North China. It is the rapid development of these areas that is expected to have the most significant effect on the Asian regional economy as a whole and Chinese government policy is designed to remove the obstacles to accelerated growth in these wealthier regions. See also: List of administrative regions by GDP, List of administrative regions by GDP per capita and List of cities by GDP per capita.
In accordance with the One country, two systems policy, the economies of the former British colony of Hong Kong and Portuguese colony of Macau are separate from the rest of China and each other. Both Hong Kong and Macau are free to conduct and engage in economic negotiations with foreign countries, as well as participating as full members in various international economic organizations such as the World Customs Organization, the World Trade Organization and the Asia-Pacific Economic Cooperation forum, often under the names "Hong Kong, China" and "Macau, China".
See also: List of administrative divisions by Human Development Index (HDI). The economic reforms implemented in 1978 helped to propel China into the ranks of the world's major economic powers. To guide economic development, the Chinese central government adopts "five-year plans" that detail its economic priorities and essential policies. The Thirteenth Five-Year Plan (2016–2020) is currently being implemented. In contrast to most countries, where the private sector dominates the economy, China's economy is still largely dominated by the public sector.Like Japan and South Korea before it, China has grown steadily, raising the income levels and living standards of its citizens while producing goods that are consumed globally. Between 1978 and 2005, China's per capita GDP grew from $153 to $1,284. Its current account surplus increased more than twelve-fold between 1982 and 2004, from $5.7 billion to $71 billion. During this time, China also became an industrial powerhouse, moving beyond initial successes in low-wage sectors like clothing and footwear to the increasingly sophisticated production of computers, pharmaceuticals, and automobiles.However, it remains unclear how long the Chinese economy can maintain this trajectory. According to the 11th five-year plan, China needed to sustain an annual growth rate of 8% for the foreseeable future. Only with such levels of growth, the leadership argued, could China continue to develop its industrial prowess, raise its citizen's standard of living, and redress the inequalities that were cropping up across the country. Yet no country had ever before maintained the kind of growth that China was predicting. Moreover, China had to some extent already undergone the easier parts of development. In the 1980s, it had transformed its vast and inefficient agricultural sector, freeing its peasants from the confines of central planning and winning them to the cause of reform. In the 1990s, it had likewise started to restructure its stagnant industrial sector, wooing foreign investors for the first time. These policies had catalysed the country's phenomenal growth. Instead, China had to take what many regarded as the final step toward the market, liberalizing the banking sector and launching the beginnings of a real capital market. According to an article in Journal of the Asia Pacific Economy by Mete Feridun of University of Greenwich Business School and Abdul Jalil from Wuhan University in China, financial development leads to a reduction in the income inequality in China. This process, however, would not be easy. As of 2004, China's state-owned enterprises were still only partially reorganized, and its banks were dealing with the burden of over $205 billion (1.7 trillion RMB) in non-performing loans, monies that had little chance of ever being repaid. The country had a floating exchange rate, and strict controls on both the current and capital accounts.In mid-2014 China announced it was taking steps to boost the economy, which at the time was running at a rate 7.4% per annum, but was slowing. The measures included plans to build a multi-tier transport network, comprising railways, roads and airports, to create a new economic belt alongside the Yangtze River.
Chinese provinces and cities have long been suspected of cooking their numbers, with the focus on local government officials, whose performance are often assessed based on how well their respective economies have performed. In recent years, China claimed growth numbers have come under increased scrutiny, with both domestic and international observers alleging the government has been inflating its economic output. Instances of overclaiming officially came to light when: Binhai New Area in the northern Chinese city of Tianjin. Tianjin's trillion yuan GDP claim for 2016, was in fact a third lower, at 665 billion yuan ($103 billion). Inner Mongolia's government also stated, that about 40% of the region's reported industrial output in 2016, as well as 26% of reported fiscal revenues, did not exist. Liaoning, frequently called China's rust belt, admitted in 2017, that local GDP numbers from 2011 to 2014 had been inflated artificially by about 20%.A Wall Street Journal survey of 64 select economists found that 96% of respondents think China's GDP estimates don't "accurately reflect the state of the Chinese economy." However, more than half the economists in the survey estimated that China's annual growth was somewhere in the 5% to 7% range, which represents robust growth. Chinese premier Li Keqiang has said he is far from confident in the country's GDP estimates, calling them "man-made" and unreliable, according to a leaked document from 2007 obtained by WikiLeaks. He said government data releases, especially the GDP numbers, should be used "for reference only".Analysts like Wilbur Ross and Donald Straszheim believe China's recent economic growth has been overstated, and estimate a growth rate at around 4% or less. According to a research from the Brookings Institution, China's economic growth may have been overstated by 1.7 percent between 2008 and 2016, meaning that the government may have been overstating the size of the Chinese economy by 16 percent in 2016.
Several sources have supported the claim that China's economy is likely to be underestimated. For example, a paper by the US-based National Bureau of Economic Research in 2017 employs an innovative method of satellite-recorded nighttime lights, which the authors claim to be a best-unbiased predictor of the economic growth in Chinese cities. The results suggest that the Chinese economic growth rate is higher than the official reported data. The Li Keqiang index is an alternative measurement of Chinese economic performance that uses three variables Li preferred. Satellite measurements of light pollution are used by some analysts to model Chinese economic growth and suggest recent growth rate numbers in Chinese official data are more reliable although are likely to be smoothed. According to an article by the Federal Reserve Bank of St. Louis, China's official statistics are of a high quality compared to other developing, middle-income and low-income countries. In 2016, China was at the 83rd percentile of middle and low-income countries, up from the 38th percentile in 2004. A study by the Federal Reserve Bank of San Francisco found that China's official GDP statistics are "significantly and positively correlated" with externally verifiable measures of economic activity such as import and export data from China’s trade partners, suggesting that China's economic growth was no slower than the official figures indicated.
These strategies are aimed at the relatively poorer regions in China in an attempt to prevent widening inequalities: China Western Development, designed to increase the economic situation of the western provinces through capital investment and development of natural resources. Revitalize Northeast China, to rejuvenate the industrial bases in Northeast China. It covers the three provinces of Heilongjiang, Jilin, and Liaoning, as well as the five eastern prefectures of Inner Mongolia. Rise of Central China Plan, to accelerate the development of its central regions. It covers six provinces: Shanxi, Henan, Anhui, Hubei, Hunan, and Jiangxi. Third Front, focused on the southwestern provinces.Foreign investment abroad: Go Global, to encourage its enterprises to invest overseas.
The "West-to-East Electricity Transmission", the "West-to-East Gas Transmission", and the "South–North Water Transfer Project" are the government's three key strategic projects, aimed at realigning overall of 12 billion cu m per year. Construction of the "South-to-North Water Diversion" project was officially launched on 27 December 2002 and completion of Phase I is scheduled for 2010; this will relieve serious water shortfall in northern China and realize a rational distribution of the water resources of the Yangtze, Yellow, Huaihe, and Haihe river valleys.
In January 1985, the State Council of China approved to establish a SNA (System of National Accounting), use the gross domestic product (GDP) to measure the national economy. China started the study of theoretical foundation, guiding, and accounting model etc., for establishing a new system of national economic accounting. In 1986, as the first citizen of the People's Republic of China to receive a PhD in economics from an overseas country, Dr. Fengbo Zhang headed Chinese Macroeconomic Research – the key research project of the seventh Five-Year Plan of China, as well as completing and publishing the China GDP data by China's own research. The summary of the above has been included in the book Chinese Macroeconomic Structure and Policy (1988) Editor: Fengbo Zhang, collectively authored by the Research Center of the State Council of China. This is the first GDP data published by China. The State Council of China issued "The notice regarding implementation of System of National Accounting" in August 1992, the SNA system officially is introduced to China, replaced Soviet Union's MPS system, Western economic indicator GDP became China's most important economic indicator (WikiChina: China GDP, The First China GDP). The table below shows the trend of the GDP of China at market prices estimated by the International Monetary Fund (IMF) with figures in millions (Chinese yuan). See also. For purchasing power parity comparisons, the US dollar is exchanged at 2.05 CNY only. The following table shows important economic indicators in 1980–2017.
Over the years, large subsidies were built into the price structure of certain commodities and these subsidies grew substantially in the late 1970s and 1980s.By 2010, rapidly rising wages and a general increase in the standard of living had put increased energy use on a collision course with the need to reduce carbon emissions in order to control global warming. There were diligent efforts to increase energy efficiency and increase use of renewable sources; over 1,000 inefficient power plants had been closed, but projections continued to show a dramatic rise in carbon emissions from burning fossil fuels. Since the late 2010s, a reduction of state-backed subsidies has negatively impacted several Chinese companies, with BYD Auto being one of them.
In 2014, many analysts expressed concern over the overall size of China's government debt. At the end of 2014, the International Monetary Fund reported that China's general government gross debt-to-GDP ratio was 41.44 percent. In 2015, a report by the International Monetary Fund concluded that China's public debt is relatively low "and on a stable path in all standard stress tests except for the scenario with contingent liability shocks", such as "a large-scale bank recapitalization or financial system bailout to deal, for example, with a potential rise in NPLs from deleveraging".Chinese authorities have dismissed analysts' worries, insisting "the country still has room to increase government debt." Former Fed Chairman Ben Bernanke, earlier in 2016, commented that "the ... debt pile facing China [is] an 'internal' problem, given the majority of the borrowings was issued in local currency. Many economists have expressed the same views as Bernanke. A 2019 survey by the OECD found that China's corporate debt is higher than other major countries."Shadow banking" has risen in China, posing risks to the financial system.
Though China's economy has expanded rapidly, its regulatory environment has not kept pace. Since Deng Xiaoping's open market reforms, the growth of new businesses has outpaced the government's ability to regulate them. This has created a situation where businesses, faced with mounting competition and poor oversight, take drastic measures to increase profit margins, often at the expense of consumer safety. This issue became more prominent in 2007, with a number of restrictions being placed on problematic Chinese exports by the United States.From the 1950s to the 1980s, the central government's revenues derived chiefly from the profits of the state enterprises, which were remitted to the state. Some government revenues also came from taxes, of which the most important was the general industrial and commercial tax. The trend, however, has been for remitted profits of the state enterprises to be replaced with taxes on those profits. Initially, this tax system was adjusted so as to allow for differences in the market capitalization and pricing situations of various firms, but more-uniform tax schedules were introduced in the early 1990s. In addition, personal income and value-added taxes were implemented at that time.
During the winter of 2007–2008, inflation ran about 7% on an annual basis, rising to 8.7% in statistics for February 2008, released in March 2008.Shortages of gasoline and diesel fuel developed in the fall of 2007 due to reluctance of refineries to produce fuel at low prices set by the state. These prices were slightly increased in November 2007 with fuel selling for $2.65 a gallon, still slightly below world prices. Price controls were in effect on numerous basic products and services, but were ineffective with food, prices of which were rising at an annual rate of 18.2% in November 2007. The problem of inflation has caused concern at the highest levels of the Chinese government. On 9 January 2008, the government of China issued the following statement on its official website: "The Chinese government decided on Wednesday to take further measures to stabilize market prices and increase the severity of punishments for those guilty of driving up prices through hoarding or cheating."Pork is an important part of the Chinese economy with a per capita consumption of 90 grams per day. The worldwide rise in the price of animal feed associated with increased production of ethanol from corn resulted in steep rises in pork prices in China in 2007. Increased cost of production interacted badly with increased demand resulting from rapidly rising wages. The state responded by subsidizing pork prices for students and the urban poor and called for increased production. Release of pork from the nation's strategic pork reserve was considered.By January 2008, the inflation rate rose to 7.1%, which BBC News described as the highest inflation rate since 1997, due to the winter storms that month. China's inflation rate jumped to a new decade high of 8.7 percent in February 2008 after severe winter storms disrupted the economy and worsened food shortages, the government said 11 March 2008. Throughout the summer and fall, however, inflation fell again to a low of 6.6% in October 2008.By November 2010, the inflation rate rose up to 5.1%, driven by an 11.7% increase in food prices year on year. According to the bureau, industrial output went up 13.3 percent. As supplies have run short, prices for fuel and other commodities have risen.
Chinese investment has always been highly cyclical. Ever since the 1958 Great Leap Forward, growth in fixed capital formation has typically peaked about every five years. Recent peaks occurred in 1978, 1984, 1988, 1993, 2003 and 2009. The corresponding troughs were in 1981, 1986, 1989, 1997 and 2005. In China, the majority of investment is carried out by entities that are at least partially state-owned. Most of these are under the control of local governments. Thus booms are primarily the result of perverse incentives at the local-government level. Unlike entrepreneurs in a free-enterprise economy, Chinese local officials are motivated primarily by political considerations. As their performance evaluations are based, to a large extent, on GDP growth within their jurisdictions, they have a strong incentive to promote large-scale investment projects. They also don't face any real bankruptcy risk. When localities get into trouble, they are invariably bailed out by state-owned banks. Under these circumstances, overinvestment is inevitable. A typical cycle begins with a relaxation of central government credit and industrial policy. This allows local governments to push investment aggressively, both through state-sector entities they control directly and by offering investment-promotion incentives to private investors and enterprises outside their jurisdictions. The resulting boom puts upward pressure on prices and may also result in shortages of key inputs such as coal and electricity (as was the case in 2003). Once inflation has risen to a level at which it begins to threaten social stability, the central government will intervene by tightening enforcement of industrial and credit policy. Projects that went ahead without required approvals will be halted. Bank lending to particular types of investors will be restricted. Credit then becomes tight and investment growth begins to decline.Eventually such centrally-imposed busts alleviate shortages and bring inflation down to acceptable levels. At that point, the central government yields to local-government demands for looser policy and the cycle begins again.
Most of China's financial institutions are state-owned and governed. The chief instruments of financial and fiscal control are the People's Bank of China (PBC) and the Ministry of Finance, both under the authority of the State Council. The People's Bank of China replaced the Central Bank of China in 1950 and gradually took over private banks. It fulfills many of the functions of other central and commercial banks. It issues the currency, controls circulation, and plays an important role in disbursing budgetary expenditures. Additionally, it administers the accounts, payments, and receipts of government organizations and other bodies, which enables it to exert thorough supervision over their financial and general performances in consideration of the government's economic plans. The PBC is also responsible for international trade and other overseas transactions. Remittances by overseas Chinese are managed by the Bank of China (BOC), which has a number of branch offices in several countries. Other financial institutions that are crucial, include the China Development Bank (CDB), which funds economic development and directs foreign investment; the Agricultural Bank of China (ABC), which provides for the agricultural sector; the China Construction Bank (CCB), which is responsible for capitalizing a portion of overall investment and for providing capital funds for certain industrial and construction enterprises; and the Industrial and Commercial Bank of China (ICBC), which conducts ordinary commercial transactions and acts as a savings bank for the public. China's economic reforms greatly increased the economic role of the banking system. In theory any enterprises or individuals can go to the banks to obtain loans outside the state plan, in practice, 75% of state bank loans go to State Owned Enterprises. (SOEs) Even though nearly all investment capital was previously provided on a grant basis according to the state plan, policy has since the start of the reform shifted to a loan basis through the various state-directed financial institutions. It is estimated that, as of 2011, 14 trillion Yuan in loans were outstanding to local governments. Much of that total is believed by outside observers to be nonperforming. Increasing amounts of funds are made available through the banks for economic and commercial purposes. Foreign sources of capital have also increased. China has received loans from the World Bank and several United Nations programs, as well as from countries (particularly Japan) and, to a lesser extent, commercial banks. Hong Kong has been a major conduit of this investment, as well as a source itself. On 23 February 2012, the PBC evinced its inclination to liberalise its capital markets when it circulated a telling ten-year timetable. Following on the heels of this development, Shenzhen banks were able to launch cross-border yuan remittances for individuals, a significant shift in the PBC's capital control strictures since Chinese nationals had been previously barred from transferring their yuan to overseas account.With two stock exchanges (Shanghai Stock Exchange and Shenzhen Stock Exchange), mainland China's stock market had a market value of $4.48 trillion as of November 2014, which makes it the second largest stock market in the world.In August 2013, creation of an as yet unnamed high-level body to gather and analyze financial information and trends was announced by the central government. The central bank would participate as would people from other organizations engaged in financial matters. It would not have direct regulatory authority, but would attempt to function at the highest professional level in order to provide appropriate guidance to regulators with respect to matters such as shadow banking that are potential sources of instability. An article published in International Review of Economics & Finance in 2010 by Mete Feridun (University of Greenwich Business School) and his colleagues provide empirical evidence that financial development fosters economic growth in China.
As of 2014 and the first quarter of 2015 the financial industry had been providing about 1.5% of China's 7% annual growth rate.Despite slowing of the economy, as of June 2015 the Chinese stock index, the CSI 300 Index, which is based on 300 stocks traded in the Shanghai and Shenzhen stock exchanges, had risen nearly 150% over the past 12 months. In an effort to forestall damage from collapse of a possible economic bubble fueled by margin trading the central government raised requirements for margin lending. Economic damage from a crash in 2007–2008 was limited due to margin lending being highly restricted. In early July, after a fall in the markets of nearly 30% from their 12 June highs, there were efforts by blue-chip, often state-owned, firms, the Chinese securities industry, and the central government to stabilize the market by buying back stock and increasing purchases of the stock of established firms; however, much of the volatility has been in smaller, less-established firms that had been heavily invested in by unsophisticated, often working class, investors who had purchased stock based solely on its rapid increase in valuation. 80% of Chinese stocks are owned by individual investors, many novices. As of 10 July 2015 efforts by the China Securities Finance Corporation, CFS, a firm created by China's commodities and stock exchanges to finance trades, had apparently stabilized the market. Major Chinese securities firms were required by the China Securities Regulatory Commission to buy, and hold, a substantial amount of securities affected by the downturn. Using funds supplied by the central bank and commercial banks the China Securities Finance Corporation purchased enough stocks to halt the slide acquiring as much as 5% of the stock in some firms. Lines of credit were extended by CFS to 21 securities firms, some of which also purchased up to 5% of some companies stocks. Some of the small cap stocks acquired may be overvalued.Chinese stocks fell about 10% during the last week of July 2015 with record breaking losses on Monday.
China is the world's largest producer and consumer of agricultural products – and some 300 million Chinese farm workers are in the industry, mostly laboring on pieces of land about the size of U.S farms. Virtually all arable land is used for food crops. China is the world's largest producer of rice and is among the principal sources of wheat, corn (maize), tobacco, soybeans, potatoes, sorghum, peanuts, tea, millet, barley, oilseed, pork, and fish. Major non-food crops, including cotton, other fibers, and oilseeds, furnish China with a small proportion of its foreign trade revenue. Agricultural exports, such as vegetables and fruits, fish and shellfish, grain and meat products, are exported to Hong Kong. Yields are high because of intensive cultivation, for example, China's cropland area is only 75% of the U.S. total, but China still produces about 30% more crops and livestock than the United States. China hopes to further increase agricultural production through improved plant stocks, fertilizers, and technology. According to the government statistics issued in 2005, after a drop in the yield of farm crops in 2000, output has been increasing annually. According to the United Nations World Food Program, in 2003, China fed 20 percent of the world's population with only 7 percent of the world's arable land. China ranks first worldwide in farm output, and, as a result of topographic and climatic factors, only about 10–15 percent of the total land area is suitable for cultivation. Of this, slightly more than half is unirrigated, and the remainder is divided roughly equally between paddy fields and irrigated areas. Nevertheless, about 60 percent of the population lives in the rural areas, and until the 1980s a high percentage of them made their living directly from farming. Since then, many have been encouraged to leave the fields and pursue other activities, such as light manufacturing, commerce, and transportation; and by the mid-1980s farming accounted for less than half of the value of rural output. Today, agriculture contributes only 13% of China's GDP. Animal husbandry constitutes the second most important component of agricultural production. China is the world's leading producer of pigs, chickens, and eggs, and it also has sizable herds of sheep and cattle. Since the mid-1970s, greater emphasis has been placed on increasing the livestock output. China has a long tradition of ocean and freshwater fishing and of aquaculture. Pond raising has always been important and has been increasingly emphasized to supplement coastal and inland fisheries threatened by overfishing and to provide such valuable export commodities as prawns. China is also unmatched in the size and reach of its fishing armada with anywhere from 200,000 to 800,000 boats, some as far afield as Argentina. Fueled primarily by government subsidies, its growth and activities have largely gone unchecked. Environmental problems such as floods, drought, and erosion pose serious threats to farming in many parts of the country. The wholesale destruction of forests gave way to an energetic reforestation program that proved inadequate, and forest resources are still fairly meagre. The principal forests are found in the Qin Mountains and the central mountains and on the Yunnan–Guizhou Plateau. Because they are inaccessible, the Qinling forests are not worked extensively, and much of the country's timber comes from Heilongjiang, Jilin, Sichuan, and Yunnan. Western China, comprising Tibet, Xinjiang, and Qinghai, has little agricultural significance except for areas of floriculture and cattle raising. Rice, China's most important crop, is dominant in the southern provinces and many of the farms here yield two harvests a year. In the north, wheat is of the greatest importance, while in central China wheat and rice vie with each other for the top place. Millet and kaoliang (a variety of grain sorghum) are grown mainly in the northeast and some central provinces, which, together with some northern areas, also provide considerable quantities of barley. Most of the soybean crop is derived from the north and the northeast; corn (maize) is grown in the center and the north, while tea comes mainly from the warm and humid hilly areas of the south. Cotton is grown extensively in the central provinces, but it is also found to a lesser extent in the southeast and in the north. Tobacco comes from the center and parts of the south. Other important crops are potatoes, sugar beets, and oilseeds. In the past decade, the government has been encouraging agricultural mechanization and land consolidation to raise yields and compensate for the loss of rural workers who have migrated to the cities. According to the most recent statistics by the UN Food and Agriculture Organization, the annual growth rate of agricultural mechanization in China is 6.38 percent. By 2014, the integrated mechanization rate had risen to nearly 60 percent, with the rate for wheat surpassing 90 percent and that for maize approaching 80 percent. In addition to standard agricultural equipment like tractors, China's agriculture cooperatives have begun using high-tech equipment, including unmanned aerial vehicles, which are used to spay crops with pesticides. Good progress has been made in increasing water conservancy, and about half the cultivated land is under irrigation. In the late 1970s and early 1980s, economic reforms were introduced. First of all this began with the shift of farming work to a system of household responsibility and a phasing out of collectivized agriculture. Later this expanded to include a gradual liberalization of price controls; fiscal decentralization; massive privatization of state enterprises, thereby allowing a wide variety of private enterprises in the services and light manufacturing; the foundation of a diversified banking system (but with large amounts of state control); the development of a stock market; and the opening of the economy to increased foreign trade and foreign investment.
The real estate industry is about 20% of the Chinese economy.
Since 1980, China's energy production has grown dramatically, as has the proportion allocated to domestic consumption. Some 80 percent of all power is generated from fossil fuel at thermal plants, with about 17 percent at hydroelectric installations; only about two percent is from nuclear energy, mainly from plants located in Guangdong and Zhejiang. Though China has rich overall energy potential, most have yet to be developed. In addition, the geographical distribution of energy puts most of these resources relatively far from their major industrial users. Basically the northeast is rich in coal and oil, the central part of north China has abundant coal, and the southwest has immense hydroelectric potential. But the industrialized regions around Guangzhou and the Lower Yangtze region around Shanghai have too little energy, while there is relatively little heavy industry located near major energy resource areas other than in the southern part of the northeast. Due in large part to environmental concerns, China has wanted to shift China's current energy mix from a heavy reliance on coal, which accounts for 70–75% of China's energy, toward greater reliance on oil, natural gas, renewable energy, and nuclear power. China has closed thousands of coal mines over the past five to ten years to cut overproduction. According to Chinese statistics, this has reduced coal production by over 25%. Since 1993, China has been a net importer of oil, a large portion of which comes from the Middle East. Imported oil accounts for 20% of the processed crude in China. Net imports are expected to rise to 3.5 million barrels (560,000 m3) per day by 2010. China is interested in diversifying the sources of its oil imports and has invested in oil fields around the world. China is developing oil imports from Central Asia and has invested in Kazakhstani oil fields. Beijing also plans to increase China's natural gas production, which currently accounts for only 3% of China's total energy consumption and incorporated a natural gas strategy in its 10th Five-Year Plan (2001–2005), with the goal of expanding gas use from a 2% share of total energy production to 4% by 2005 (gas accounts for 25% of U.S. energy production). Analysts expect China's consumption of natural gas to more than double by 2010. The 11th Five-Year Program (2006–10), announced in 2005 and approved by the National People's Congress in March 2006, called for greater energy conservation measures, including development of renewable energy sources and increased attention to environmental protection. Guidelines called for a 20% reduction in energy consumption per unit of GDP by 2010. Moving away from coal towards cleaner energy sources including oil, natural gas, renewable energy, and nuclear power is an important component of China's development program. Beijing also intends to continue to improve energy efficiency and promote the use of clean coal technology. China has abundant hydroelectric resources; the Three Gorges Dam, for example, will have a total capacity of 18 gigawatts when fully on-line (projected for 2009). In addition, the share of electricity generated by nuclear power is projected to grow from 1% in 2000 to 5% in 2030. China's renewable energy law, which went into effect in 2006, calls for 10% of its energy to come from renewable energy sources by 2020.
Outdated mining and ore-processing technologies are being replaced with modern techniques, but China's rapid industrialization requires imports of minerals from abroad. In particular, iron ore imports from Australia and the United States have soared in the early 2000s as steel production rapidly outstripped domestic iron ore production. Also China has become increasingly active in several African countries to mine the reserves it requires for economic growth, particularly in countries such as the Democratic Republic of the Congo and Gabon. The major areas of production in 2004 were coal (nearly 2 billion tons), iron ore (310 million tons), crude petroleum (175 million tons), natural gas (41 million cubic meters), antimony ore (110,000 tons), tin concentrates (110,000 tons), nickel ore (64,000 tons), tungsten concentrates (67,000 tons), unrefined salt (37 million tons), vanadium (40,000 tons), and molybdenum ore (29,000 tons). In order of magnitude, produced minerals were bauxite, gypsum, barite, magnesite, talc and related minerals, manganese ore, fluorspar, and zinc. In addition, China produced 2,450 tons of silver and 215 tons of gold in 2004. The mining sector accounted for less than 0.9% of total employment in 2002 but produced about 5.3% of total industrial production.
China has an abundant potential for hydroelectric power production due to its considerable river network and mountainous terrain. Most of the total hydroelectric capacity is situated in the southwest of the country, where coal supplies are poor but demand for energy is rising swiftly. The potential in the northeast is fairly small, but it was there that the first hydroelectric stations were built—by the Japanese during its occupation of Manchuria.Thirteen years in construction at a cost of $24 billion, the immense Three Gorges Dam across the Yangtze River was essentially completed in 2006 and produced more than 100TWh of energy in 2018.
China is well endowed with mineral resources, the most important of which is coal. China's mineral resources include large reserves of coal and iron ore, plus adequate to abundant supplies of nearly all other industrial minerals. Although coal deposits are widely scattered (some coal is found in every province), most of the total is located in the northern part of the country. The province of Shanxi, in fact, is thought to contain about half of the total; other important coal-bearing provinces include Heilongjiang, Liaoning, Jilin, Hebei, and Shandong. Apart from these northern provinces, significant quantities of coal are present in Sichuan, and there are some deposits of importance in Guangdong, Guangxi, Yunnan, and Guizhou. A large part of the country's reserves consists of good bituminous coal, but there are also large deposits of lignite. Anthracite is present in several places (especially Liaoning, Guizhou, and Henan), but overall it is not very significant.To ensure a more even distribution of coal supplies and to reduce the strain on the less than adequate transportation network, the authorities pressed for the development of a large number of small, locally run mines throughout the country. This campaign was energetically pursued after the 1960s, with the result that thousands of small pits have been established, and they produce more than half the country's coal. This output, however, is typically expensive and is used for local consumption. It has also led to a less than stringent implementation of safety measures in these unregulated mines, which cause several thousands of deaths each year.Coal makes up the bulk of China's energy consumption (70% in 2005), and China is the largest producer and consumer of coal in the world. As China's economy continues to grow, China's coal demand is projected to rise significantly. Although coal's share of China's overall energy consumption will decrease, coal consumption will continue to rise in absolute terms. China's continued and increasing reliance on coal as a power source has contributed significantly to putting China on the path to becoming the world's largest emitter of acid rain-causing sulfur dioxide and greenhouse gases, including carbon dioxide. As of 2015, falling coal prices resulted in layoffs at coal mines in the northeast.
China's onshore oil resources are mostly located in the Northeast and in Xinjiang, Gansu, Qinghai, Sichuan, Shandong, and Henan provinces. Oil shale is found in a number of places, especially at Fushun in Liaoning, where the deposits overlie the coal reserves, as well as in Guangdong. High quality light oil has been found in the Pearl River estuary of the South China Sea, the Qaidam Basin in Qinghai, and the Tarim Basin in Xinjiang. The country consumes most of its oil output but does export some crude oil and oil products. China has explored and developed oil deposits in the South China Sea and East China Sea, the Yellow Sea, the Gulf of Tonkin, and the Bohai Sea. In 2013, the pace of China's economic growth exceeded the domestic oil capacity and floods damaged the nation's oil fields in the middle of the year. Consequently, China imported oil to compensate for the supply reduction and surpassed the US in September 2013 to become the world's largest importer of oil.The total extent of China's natural gas reserves is unknown, as relatively little exploration for natural gas has been done. Sichuan accounts for almost half of the known natural gas reserves and production. Most of the rest of China's natural gas is associated gas produced in the Northeast's major oil fields, especially Daqing oilfield. Other gas deposits have been found in the Qaidam Basin, Hebei, Jiangsu, Shanghai, and Zhejiang, and offshore to the southwest of Hainan Island. According to an article published in Energy Economics in 2011 by economists Mete Feridun (University of Greenwich) and Abdul Jalil (Wuhan University in China), financial development in China has not taken place at the expense of environmental pollution and financial development has led to a decrease in environmental pollution. Authors conclude that carbon emissions are mainly determined by income, energy consumption and trade openness and their findings confirm the existence of an Environmental Kuznets Curve in the case of China.
Iron ore reserves are found in most provinces, including Hainan. Gansu, Guizhou, southern Sichuan, and Guangdong provinces have rich deposits. The largest mined reserves are located north of the Yangtze River and supply neighboring iron and steel enterprises. With the exception of nickel, chromium, and cobalt, China is well supplied with ferroalloys and manganese. Reserves of tungsten are also known to be fairly large. Copper resources are moderate, and high-quality ore is present only in a few deposits. Discoveries have been reported from Ningxia. Lead and zinc are available, and bauxite resources are thought to be plentiful. China's antimony reserves are the largest in the world. Tin resources are plentiful, and there are fairly rich deposits of gold. China is the world's fifth largest producer of gold and in the early 21st century became an important producer and exporter of rare metals needed in high-technology industries. China also produces a fairly wide range of nonmetallic minerals. One of the most important of these is salt, which is derived from coastal evaporation sites in Jiangsu, Hebei, Shandong, and Liaoning, as well as from extensive salt fields in Sichuan, Ningxia, and the Qaidam Basin. There are important deposits of phosphate rock in a number of areas; Jiangxi, Guangxi, Yunnan and Hubei. Production has been accelerating every year. As of 2013 China is producing 97,000,000 metric tons of phosphate rock a year. Pyrites occur in several places; Liaoning, Hebei, Shandong, and Shanxi have the most important deposits. China also has large resources of fluorite (fluorspar), gypsum, asbestos, and has the world's largest reserves and production of cement, clinker and limestone.
Industry and construction account for 46.8% of China's GDP. Between the years 2011 and 2013, China used more cement than the United States consumed during the entire 20th century. In 2009 around 8% of the total manufacturing output in the world came from China itself and China ranked third worldwide in industrial output that year (first was EU and second United States). Research by IHS Global Insight states that in 2010 China contributed to 19.8% of world's manufacturing output and became the largest manufacturer in the world that year, after the US had held that position for about 110 years. In November 2012, the State Council of the People's Republic of China mandated a "social risk assessment" for all major industrial projects. This requirement followed mass public protests in some locations for planned projects or expansions.Major industries include mining and ore processing; iron and steel; aluminium; coal; machinery; armaments; textiles and apparel; petroleum; cement; chemical; fertilizers; food processing; automobiles and other transportation equipment including rail cars and locomotives, ships, and aircraft; consumer products including footwear, toys, and electronics; telecommunications and information technology. China has become a preferred destination for the relocation of global manufacturing facilities. Its strength as an export platform has contributed to incomes and employment in China. Since the founding of the People's Republic, industrial development has been given considerable attention; as of 2011 46% of China's national output continued to be devoted to investment; a percentage far higher than any other nation. Among the various industrial branches the machine-building and metallurgical industries have received the highest priority. These two areas alone now account for about 20–30 percent of the total gross value of industrial output. In these, as in most other areas of industry, however, innovation has generally suffered at the hands of a system that has rewarded increases in gross output rather than improvements in variety, sophistication and quality. China, therefore, still imports significant quantities of specialized steels. Overall industrial output has grown at an average rate of more than 10 percent per year, having surpassed all other sectors in economic growth and degree of modernization. Some heavy industries and products deemed to be of national strategic importance remain state-owned, but an increasing proportion of lighter and consumer-oriented manufacturing firms are privately held or are private-state joint ventures. The predominant focus of development in the chemical industry is to expand the output of chemical fertilizers, plastics, and synthetic fibers. The growth of this industry has placed China among the world's leading producers of nitrogenous fertilizers. In the consumer goods sector the main emphasis is on textiles and clothing, which also form an important part of China's exports. Textile manufacturing, a rapidly growing proportion of which consists of synthetics, account for about 10 percent of the gross industrial output and continues to be important, but less so than before. The industry tends to be scattered throughout the country, but there are a number of important textile centers, including Shanghai, Guangzhou, and Harbin.
In 2018, China was the largest producer of steel in the world, accounting for more than 50% of the world's steel In 2018, China produced 928 million tons of steel, an increase of almost 100% from 2008. 6 of 10 of largest steel producers in the world are in China. Profits are low despite continued high demand due to high debt and overproduction of high end products produced with the equipment financed by the high debt. The central government is aware of this problem but there is no easy way to resolve it as local governments strongly support local steel production. Meanwhile, each firm aggressively increases production. Iron ore production kept pace with steel production in the early 1990s but was soon outpaced by imported iron ore and other metals in the early 2000s. Steel production, an estimated 140 million tons in 2000 increased to 419 million tons in 2006 and 928 million tons by 2018. Much of the country's steel output comes from a large number of small-scale producing centers, one of the largest being Anshan in Liaoning. China was the top exporter of steel in the world in 2018; export volumes in 2018 were 66.9 million tons, a 9% decrease over the previous year. The decline slowed China's decade-old steel export growth. As of 2012 steel exports faced widespread anti-dumping taxes and had not returned to pre-2008 levels. However, in 2015, China's steel exports reached a record high of 110 million metric tons. Domestic demand remained strong, particularly in the developing west where steel production in Xinjiang was expanding.On 26 April 2012, a warning was issued by China's bank regulator to use caution with respect to lending money to steel companies who, as profits from the manufacture and sale of steel have fallen, have sometimes used borrowed money for speculative purposes. According to the China Iron and Steel Association the Chinese steel industry lost 1 billion Rmb in the first quarter of 2012, its first loss since 2000. For the year 2018, China's steel industry reported profits of CNY 470 billion ($70 billion), which was 39% higher than the year before.
China is the world's largest automobile producer, manufacturing more than 27 million vehicles in 2018—for comparison, the corresponding numbers for the US and Japan were 11.3 million and 9.7 million respectively. By 2006 China had become the world's third largest automotive vehicle manufacturer (after US and Japan) and the second largest consumer (only after the US). However, four years later, in 2010, China was manufacturing more vehicles than the U.S. and Japan combined. Automobile manufacturing has soared during the reform period. In 1975 only 139,800 automobiles were produced annually, but by 1985 production had reached 443,377, then jumped to nearly 1.1 million by 1992 and increased fairly evenly each year up until 2001, when it reached 2.3 million. In 2002 production rose to nearly 3.25 million and then jumped to 4.44 million in 2003, 5.07 million in 2004, 5.71 million in 2005, 7.28 million in 2006, 8.88 million in 2007, 9.35 million in 2008 and 13.83 million in 2009. China has become the number-one automaker in the world in 2009. Domestic sales have kept pace with production. After respectable annual increases in the mid- and late 1990s, passenger car sales soared in the early 2000s. In 2006, a total of 7.22 million automobiles were sold, including 5.18 million units of passenger cars and 2 million units of commercial vehicles. In 2010, China became the world's largest automotive vehicle manufacturer as well as the largest consumer ahead of the United States with an estimated 18 million new cars sold. However, new car sales grew only by an estimated 1% between 2011 and 2012 due to the escalation in the Spratly Islands dispute, which involved Japan, the world's third largest producer of vehicles.China's automotive industry has been so successful that it began exporting car parts in 1999. China began to plan major moves into the automobile and components export business starting in 2005. A new Honda factory in Guangzhou was built in 2004 solely for the export market and was expected to ship 30,000 passenger vehicles to Europe in 2005. By 2004, 12 major foreign automotive manufacturers had joint-venture plants in China. They produced a wide range of automobiles, minivans, sport utility vehicles, buses, and trucks. In 2003 China exported US$4.7 billion worth of vehicles and components. The vehicle export was 78,000 units in 2004, 173,000 units in 2005, and 340,000 units in 2006. The vehicle and component export is targeted to reach US$70 billion by 2010. The market for domestically produced cars, under a local name, is likely to continue to grow both inside China and outside. Companies such as Geely, Qiantu and Chery are constantly evaluating new international locations, both in developing and developed countries.
Substantial investments were made in the manufacture of solar panels and wind generators by a number of companies, supported by liberal loans by banks and local governments. However, by 2012 manufacturing capacity had far outstripped domestic and global demand for both products, particularly solar panels, which were subjected to anti-dumping penalties by both the United States and Europe. The global oversupply has resulted in bankruptcies and production cutbacks both inside and outside China. China has budgeted $50 billion to subsidize production of solar power over the two decades following 2015 but, even at the sharply reduced price resulting from oversupply, as of 2012 cost of solar power in China remained three times that of power produced by conventional coal-fired power plants. China is the world's biggest sex toy producer and accounts for 70% of the worldwide sex toys production. In the country, 1,000 manufacturers are active in this industry, which generates about two billion dollars a year.As of 2011, China was the world's largest market for personal computers Since 2019, Samsung Electronics, Hyundai Motors, Kia Motors and LG Electronics are moving some or all production out of China, in order to reduce overdependence on the Chinese market and avoid risks related with increased local competition, China's economic slowdown and the US-China trade war.
The output of China's services in 2015 ranks second worldwide after the United States. High power and telecom density has ensured that the country has remained on a high-growth trajectory over the long term. In 2015 the services sector produced 52.9% of China's annual GDP, second only to manufacturing. However, its proportion of GDP is still low compared to the ratio in more developed countries, and the agricultural sector still employs a larger workforce. Prior to the onset of economic reforms in 1978, China's services sector was characterized by state-operated shops, rationing, and regulated prices—with reform came private markets, individual entrepreneurs, and a commercial sector. The wholesale and retail trade has expanded quickly, with numerous shopping malls, retail shops, restaurant chains and hotels constructed in urban areas. Public administration remains a main component of the service sector, while tourism has become a significant factor in employment and a source of foreign exchange.
China hosts the world's largest number of World Heritage Sites (55). China's tourism industry is one of the fastest-growing industries in the national economy and is also one of the industries with a very distinct global competitive edge. According to the World Travel and Tourism Council, travel and tourism directly contributed CNY 1,362 billion (US$216 billion) to the Chinese economy (about 2.6% of GDP). In 2011, total international tourist arrivals was 58 million, and international tourism receipts were US$48 billion.Domestic tourism market makes up more than 90% of the country's tourism traffic, and contributes more than 70% of total tourism revenue. In 2002, domestic tourists reached 878 million and tourism revenue was $46.9 billion. A large middle class with strong consumption power is emerging in China, especially in major cities. China's outbound tourists reached 20.22 million in 2003, overtaking Japan for the first time. It is forecast by the World Tourism Organization that China's tourism industry will take up to 8.6% of world market share to become the world's top tourism industry by 2020. Chinese business travel spending is also forecast to be the highest in the world by 2014, overtaking the United States. According to a Global Business Travel Association study, total business-travel spending is expected to reach US$195 billion in 2012.It is forecast by Euromonitor International that China will overtake France as the leading travel destination by 2030.
Luxury spending in China has skyrocketed, an indicator of the country's newfound wealth. For example, the Chinese bottled water industry is forecast to more than double in size in 2008, becoming a $10.5 billion industry. Meanwhile, as those who once had no recourse but low-quality tap water take advantage of its availability in supermarkets, those who had little or no running water are now capitalizing on its availability. Tap water production and supply is expected to grow by 29.3% in 2008, to $11.9 billion. China's automotive industry is expected to expand by 29.5% to nearly $200 billion. Also, consumption of chocolate and other confectionery is to increase by 24.3%, as the industry expands to $4.6 billion. Additionally China's fast food industry has been growing at a 20.8% annual rate. The LVMH Group, who own brands including Louis Vuitton apparel, Moët & Chandon wines and champagne and Hennessy cognacs, reported earnings growth of over 25% in 2007 in China, with the country accounting for around 16% of LVMH's global business.After an October 2012 ban on government agencies purchasing luxury goods, often used as "gifts", sales of luxury goods in China remained strong but slowed, even falling slightly for some luxury retailers in the 4th quarter of 2012, with sales of shark fins and edible swallow nests (once staples of lavish government banquets) down sharply.Retail sales in China account for only 7% of global retail sales; however, Chinese buyers account for 25% of global retail sales of luxury consumer goods. Many shops in international travel destinations have specialized staff devoted to Chinese customers.
As of 2016, computer crime is a lucrative illicit practice in China. An academic study released in August 2012 by the University of California (UC) Institute on Global Conflict and Cooperation, claimed that China's "cyber black market" involved over 90,000 participants, cost the local economy 5.36 billion yuan (£536m), negatively impacted upon 110 million internet users (22%), and affected 1.1 million websites (20%) in 2011. In July 2012, China's State Council released a set of information security guidelines as a measure to combat cyber crime that included increased auditing, security reporting, and monitoring, and a commitment to "reduce the number of internet connection points".
One of the hallmarks of China's socialist economy was its promise of employment to all able and willing to work and job-security with virtually lifelong tenure. This socialist policy is known as the iron rice bowl. In 1979–1980, the state reformed factories by giving wage increases to workers, which was immediately offset by sharply rising inflation rates of 6–7%. The reforms also dismantled the iron rice bowl, which meant it witnessed a rise in unemployment in the economy. In 1979–80 there were 20 million unemployed people. China's estimated employed labor force in 2005 totaled 791 million persons, about 60% of the total population. During 2003, 49% of the labor force worked in agriculture, forestry, and fishing; 22% in mining, manufacturing, energy, and construction industries; and 29% in the services sector and other categories. In 2004 some 25 million persons were employed by 743,000 private enterprises. Urban wages rose rapidly from 2004 to 2007, at a rate of 13 to 19% per year with average wages near $200/month in 2007. By 2016 the average monthly wage for workers engaged in manufacturing goods for export was $424. This wage, combined with other costs of doing business in China, had, more or less, equalized any Chinese cost advantage with respect to developed economies.The All-China Federation of Trade Unions (ACFTU) was established in 1925 to represent the interests of national and local trade unions and trade union councils. The ACFTU reported a membership of 130 million, out of an estimated 248 million urban workers, at the end of 2002. Chinese trade unions are organized on a broad industrial basis. Membership is open to those who rely on wages for the whole or a large part of their income, a qualification that excludes most agricultural workers. In 2010, the issues of manufacturing wages caused a strike at a Honda parts plant. This resulted in wage increases both at the struck plant and other industrial plants.The 2010 census found that China was now half urban and rapidly aging due to the one child policy. This is expected to lead to increased demand for labor to take care of an elderly population and a reduced supply of migrant labor from the countryside.Due to worsening pollution, the corruption and political uncertainties of the one-party state and the limited economic freedom in an economy dominated by large state-owned enterprises, many skilled professionals are either leaving the country or preparing safety nets for themselves abroad. In the decade up to 2014, 10 million Chinese emigrated to other countries, taking assets and their technical skills. Perceived corruption continued to grow worse in China as it dropped from 75th to 80th place in Transparency International's index of state corruption.A law approved February 2013 will mandate a nationwide minimum wage at 40% average urban salaries to be phased in fully by 2015.
International trade makes up a sizeable portion of China's overall economy. Being a Second World country at the time, a meaningful segment of China's trade with the Third World was financed through grants, credits, and other forms of assistance. The principal efforts were made in Asia, especially to Indonesia, Myanmar, Pakistan, and Sri Lanka, but large loans were also granted in Africa (Ghana, Algeria, Tanzania) and in the Middle East (Egypt). However, after Mao Zedong's death in 1976, these efforts were scaled back. After which, trade with developing countries became negligible, though during that time, Hong Kong and Taiwan both began to emerge as major trading partners. Since economic reforms began in the late 1970s, China sought to decentralize its foreign trade system to integrate itself into the international trading system. In November 1991, China joined the Asia-Pacific Economic Cooperation (APEC) group, which promotes free trade and cooperation in the economic, trade, investment, and technology spheres. China served as APEC chair in 2001, and Shanghai hosted the annual APEC leaders meeting in October of that year. After reaching a bilateral WTO agreement with the EU and other trading partners in summer 2000, China worked on a multilateral WTO accession package. China concluded multilateral negotiations on its accession to the WTO in September 2001. The completion of its accession protocol and Working Party Report paved the way for its entry into the WTO on 11 December 2001, after 16 years of negotiations, the longest in the history of the General Agreement on Tariffs and Trade. However, U.S. exporters continue to have concerns about fair market access due to China's restrictive trade policies and U.S. export restrictions. In October 2019, Chinese Vice Premier Han Zheng promised to further decrease tariffs and remove non-tariff barriers for global investors, he also welcomed multinational companies to invest more in China. China's global trade exceeded $4.16 trillion at the end of 2013. It first broke the $100 billion mark in 1988, $200 billion in 1994, $500 billion in 2001, $1 trillion mark ($1.15 trillion) in 2004,$2 trillion mark ($2.17 trillion) in 2007,$3 trillion mark ($3.64 trillion) in 2011, and $4 trillion mark ($4.16 trillion) in 2013. The table below shows the average annual growth (in nominal US dollar terms) of China's foreign trade during the reform era. The vast majority of China's imports consists of industrial supplies and capital goods, notably machinery and high-technology equipment, the majority of which comes from the developed countries, primarily Japan and the United States. Regionally, almost half of China's imports come from East and Southeast Asia, and about one-fourth of China's exports go to the same destinations. About 80 percent of China's exports consist of manufactured goods, most of which are textiles and electronic equipment, with agricultural products and chemicals constituting the remainder. Out of the five busiest ports in the world, three are in China. The U.S. trade deficit with China reached $233 billion in 2006, as imports grew 18%. China's share of total U.S. imports has grown from 7% to 15% since 1996. Trade volume between China and Russia reached $29.1 billion in 2005, an increase of 37.1% compared with 2004. A spokesman for the Ministry of Commerce, Van Jingsun, said that the volume of trade between China and Russia could exceed 40 billion dollars in 2007. China's export of machinery and electronic goods to Russia grew 70%, which is 24% of China's total export to Russia in the first 11 months of 2005. During the same time, China's export of high-tech products to Russia increased by 58%, and that is 7% of China's total exports to Russia. Also at that time period, border trade between the two countries reached $5.13 billion, growing 35% and accounting for nearly 20% of the total trade. Most of China's exports to Russia remain apparel and footwear. Russia is China's eighth largest trade partner and China is now Russia's fourth largest trade partner, and China now has over 750 investment projects in Russia, involving $1.05 billion. China's contracted investment in Russia totaled $368 million during January–September 2005, twice that in 2004. Chinese imports from Russia are mainly those of energy sources, such as crude oil, which is mostly transported by rail, and electricity exports from neighboring Siberian and Far Eastern regions. In the near future, exports of both of these commodities are set to increase, as Russia is building the Eastern Siberia-Pacific Ocean oil pipeline with a branch going to the Chinese border, and Russian power grid monopoly UES is building some of its hydropower stations with a view of future exports to China. Export growth has continued to be a major component supporting China's rapid economic growth. To increase exports, China pursued policies such as fostering the rapid development of foreign-invested factories, which assembled imported components into consumer goods for export and liberalizing trading rights. In its 11th Five-Year Program, adopted in 2005, China placed greater emphasis on developing a consumer demand-driven economy to sustain economic growth and address imbalances.
China's investment climate has changed dramatically with more than two decades of reform. In the early 1980s, China restricted foreign investments to export-oriented operations and required foreign investors to form joint-venture partnerships with Chinese firms. The Encouraged Industry Catalogue sets out the degree of foreign involvement allowed in various industry sectors. From the beginning of the reforms legalizing foreign investment, capital inflows expanded every year until 1999. Foreign-invested enterprises account for 58–60% of China's imports and exports.Since the early 1990s, the government has allowed foreign investors to manufacture and sell a wide range of goods on the domestic market, eliminated time restrictions on the establishment of joint ventures, provided some assurances against nationalization, allowed foreign partners to become chairs of joint venture boards, and authorized the establishment of wholly foreign-owned enterprises, now the preferred form of FDI. In 1991, China granted more preferential tax treatment for Wholly Foreign Owned Enterprises and contractual ventures and for foreign companies, which invested in selected economic zones or in projects encouraged by the state, such as energy, communications and transportation.China also authorized some foreign banks to open branches in Shanghai and allowed foreign investors to purchase special "B" shares of stock in selected companies listed on the Shanghai and Shenzhen Securities Exchanges. These "B" shares sold to foreigners carried no ownership rights in a company. In 1997, China approved 21,046 foreign investment projects and received over $45 billion in foreign direct investment. China revised significantly its laws on Wholly Foreign-Owned Enterprises and China Foreign Equity Joint Ventures in 2000 and 2001, easing export performance and domestic content requirements. The Vice Minister of Finance Zhu Guangyao announced, foreign investors will be allowed to own up to 51% on domestic financial service companies. Formerly foreign ownership was limited to a 49% stake in these firms.Foreign investment remains a strong element in China's rapid expansion in world trade and has been an important factor in the growth of urban jobs. In 1998, foreign-invested enterprises produced about 40% of China's exports, and foreign exchange reserves totalled about $145 billion. Foreign-invested enterprises today produce about half of China's exports (the majority of China's foreign investment come from Hong Kong, Macau and Taiwan), and China continues to attract large investment inflows. However, the Chinese government's emphasis on guiding FDI into manufacturing has led to market saturation in some industries, while leaving China's services sectors underdeveloped. From 1993 to 2001, China was the world's second-largest recipient of foreign direct investment after the United States. China received $39 billion FDI in 1999 and $41 billion FDI in 2000. China is now one of the leading FDI recipients in the world, receiving almost $80 billion in 2005 according to World Bank statistics. In 2006, China received $69.47 billion in foreign direct investment. By 2011, with the U.S. seeing a decline in foreign investment following the 2008 financial crisis, China overtook it as the top destination for FDI, receiving over $280 billion that year.Amid slowing economic conditions and a weakening yuan in 2015, December of that year saw a 5.8% drop in FDI to China. While China's rank as the top receiver of FDI continued through 2014, the slowing of inbound investment in 2015 combined with a massive rebound in foreign investment to the United States resulted in the U.S. reclaiming its position as the top investment destination. Data from the American Chamber of Commerce in China's 2016 China Business Climate Survey confirms this trend, although it also demonstrates that China remains a top investment destination. This survey of over 500 members found that "China remains a top three investment priority for six out of ten member companies," though this is a decline from the 2012 high of eight out of ten respondents considering China a top priority.Foreign exchange reserves totaled $155 billion in 1999 and $165 billion in 2000. Foreign exchange reserves exceeded $800 billion in 2005, more than doubling from 2003. Foreign exchange reserves were $819 billion at the end of 2005, $1.066 trillion at the end of 2006, $1.9 trillion by June 2008. In addition, by the end of September 2008 China replaced Japan for the first time as the largest foreign holder of US treasury securities with a total of $585 billion, vs Japan $573 billion. China's foreign exchange reserves are the largest in the world.As part of its WTO accession, China undertook to eliminate certain trade-related investment measures and to open up specified sectors that had previously been closed to foreign investment. New laws, regulations, and administrative measures to implement these commitments are being issued. Major remaining barriers to foreign investment include opaque and inconsistently enforced laws and regulations and the lack of a rules-based legal infrastructure. Warner Bros., for instance, withdrew its cinema business in China as a result of a regulation that requires Chinese investors to own at least a 51 percent stake or play a leading role in a foreign joint venture.Another major development in the history of foreign investment in China was the establishment of the Shanghai Free Trade Zone in September 2013. The Zone is considered a testing ground for a number of economic and social reforms. Critically, foreign investment is controlled via a "negative list" approach, where FDI is permitted in all sectors unless explicitly prohibited by the inclusion of a given sector on the negative list published by the Shanghai Municipal Government.On 15 March 2019, China's National People's Congress adopted the Foreign Investment Law, which comes into effect on 1 January 2020.
Outward foreign direct investment is a new feature of Chinese globalization, where local Chinese firms seek to make investments in both developing and developed countries. It was reported in 2011 that there was increasing investment by capital rich Chinese firms in promising firms in the United States. Such investments offer access to expertise in marketing and distribution potentially useful in exploiting the developing Chinese domestic market.Since 2005, Chinese companies have been actively expanding outside of China, in both developed and developing countries. In 2013, Chinese companies invested US$90 billion globally in non-financial sectors, 16% more than 2012.Between January 2009 and December 2013, China contributed a total of $161.03bn in outward FDI, creating almost 300,000 jobs. Western Europe was the largest regional recipient of Chinese outward FDI, with Germany receiving the highest number of FDI projects for any country globally.There are two ways Chinese companies choose to enter a foreign market: organic growth and Merge & Acquisition (M&A). Many Chinese companies would prefer M&A for the following reasons: Fast. M&A is the fastest way for a company to expand into another country by acquiring brand, distribution, talents, and technology. Chinese CEOs has been used to growing at 50%+ speed and do not want to spend capital. China market. China has become the world's largest economy. Many Chinese acquire foreign companies and then bring their products/services to China, anything from premium cars to fashion clothing to meat to Hollywood movies. Cheap capital access. The huge Chinese domestic market help many Chinese companies accumulated financial capital to do M&A. Chinese government also provides long-term, low-interest capital for companies to expand abroad. Low risk. M&A helped Chinese companies avoid risk of failure of organic growth as they got an established company with everything in place. Cheap labor. Some companies may move part of the manufacturing in high labor cost countries to China to reduce the cost and make the product more attractive in price. Trade and policy barrier. Chinese companies in many sectors face quota limitation and high tax, which prevent them from being competitive in foreign markets. Depressed assets. 2008–2010 global economic crisis created liquidity problems for a lot of western companies and reduced their market value. Chinese companies believe it is a great opportunity for them to buy these depressed assets at discount. China's direct foreign investment in non-financial sector growth from US$25 billion in 2007 to US$90 billion in 2013, more than three times. China is growing in investments and influencing power over Europe, and the EU has begun to take notice.At the beginning, state-owned enterprises dominate the foreign acquisition and most of the money goes to oil and minerals. Since 2005, more and more private companies start to acquire non-raw material foreign companies. Below is a list of the top 15 outbound deals from Chinese companies: However, the fast growth and M&A deals did not change consumers' low quality and low price perception of Chinese goods and brands. According to market consecutive researches by the Monogram Group, a Chicago-based advertising agency, in 2007, 2009, 2011 and 2012, American consumers' willingness to purchase Chinese products across all categories except PC remained the same or became worse during 2007–2012. The only sector in which Americans were more likely to purchase was personal computers. Moreover, many M&A deals have failed because companies underestimated the challenges and failed to restructure the company. Case 1: Shanghai Auto acquired 48.9% of Korean Ssangyong at US$500 million in 2004, making it the most ambitious acquisition in Chinese auto industry at the time. Shanghai Auto wanted the brand and technology to expand its footprint in China. However, the cultural difference, the objection to transfer the technology and the failed sales of new SUV model put Shanghai Auto's ambition of expansion in jeopardy. It caused huge conflict between Ssangyong employees and Shanghai Auto as things didn't go well as planned. And the 2008 global economic crisis put Ssangyong on a survival mode, let alone expansion. After the negotiation with the labor union to reduce wages failed, Shanghai Auto decided to exit from Ssangyong and didn't get a penny back for their US$500 million investment.Case 2: In 2004, the television manufacturer TCL acquired TV business including Thomson and RCA brand from Thomson Electronics of France to form a joint vendure called TCL-Thomson Electronics (TTE). For the coming two years, the company recorded huge loss, especially in Europe. Several factors contributed to the failure: Failure of Due Diligence. Right after TCL acquired Thomson's TV business, the TV market shifted to LCD technology, put Thomson out of date. As CEO of TCL, Dongsheng Li, said in 2012 "They betted on the wrong thing where the market would go. They thought Thomson's DLP could be the best choice." Lack of understanding of rules and regulations. According to the book Resumption of Trading by Chong Chen, soon after acquisition, Thomson found it in a situation that they couldn't recruit the talents they wanted and can't fire ones they didn't want. Underestimate of the challenges in cultural difference. Xuesong Tong, vice president of TTE, said in an interview with "China Operation" newspaper in 2005: "The French look down upon their Chinese boss. For example, they wanted to share the design model with TTE, but French just dislike it even though it is a popular one in US market. Also, French feel superior in their language and don't want to speak English, which created huge problem in communication. It takes hours to discuss simple issues and can't reach agreement." According to Scott Markman, president of Monogram, Chinese companies often moved their business model to developed countries and it doesn't work. Thomson has the problem, they are very good and distribution and operation in China but France and Europe is a totally different world.
Since the 1950s medical care, public hygiene and sanitation improved considerably, and epidemics were controlled. Consecutive generations continuously experienced better health. The population growth rate surged as the mortality rate dropped more rapidly than the birth rate. China's massive population has always been a major difficulty for the government as it has struggled to provide for it. In the 1950s, food supply was inadequate and the standard of living was generally low. This spurred the authorities to initiate a major birth control program. The Great Leap Forward industrial plan in 1958–60 was partially responsible for a huge famine that caused the death rate to surpass the birth rate, and by 1960, the overall population was declining. A second population control drive began in 1962 with major efforts focused on promoting late marriages and the use of contraceptives. By 1963 the country was in the beginning of recovery from the famine and the birth rate soared to its highest since 1949 with an annual population growth rate of 3%. In 1966, the Cultural Revolution suspended this second family planning program, but resumed four years later with the third attempt by making later marriage and family size limitation an obligation. Since 1970, the efforts have been much more effective. The third family planning program continued until 1979 when the one child per family policy was implemented. By the early 1980s, China's population reached around 1 billion and by the early 2000s, surpassed 1.3 billion. In the 1980s, the average overall population growth was around 1.5%. In the 1990s, this fell to about 1%. Today it is about 0.6%. China's population growth rate is now among the lowest for a developing country, although, due to its large population, annual net population growth is still considerable. One demographic consequence of the one-child policy is that China is now one of the most rapidly ageing countries in the world. From 100 million to 150 million surplus rural workers are adrift between the villages and the cities, many subsisting through part-time, low-paying jobs. According to the latest Forbes China Rich List (2007), China had 66 billionaires, the second largest number after the United States, which had 415. In the 2006 Forbes Rich List it stated that there were 15 Chinese billionaires. In the latest 2007 Hurun Report, it lists 106 billionaires in China.
China's transportation policy, influenced by political, military, and economic concerns, have undergone major changes since 1949.Immediately after the People's Republic was founded, the primary goal was to repair existing transportation infrastructure in order to meet military transport and logistics needs as well as to strengthen territorial integrity. During most of the 1950s, new road and rail links were built, while at the same time old ones were improved. During the 1960s much of the improvement of regional transportation became the responsibility of the local governments, and many small railways were constructed. Emphasis was also placed on developing transportation in remote rural, mountainous, and forested areas, in order to integrate poorer regions of the country and to help promote economies of scale in the agricultural sector. Before the reform era began in the late 1970s, China's transportation links were mostly concentrated in the coastal areas and access to the inner regions was generally poor. This situation has been improved considerably since then, as railways and highways have been built in the remote and frontier regions of the northwest and southwest. At the same time, the development of international transportation was also pursued, and the scope of ocean shipping was broadened considerably. Freight haulage is mainly provided by rail transport. The rail sector is monopolized by China Railway and there is wide variation in services provided. In late 2007 China became one of the few countries in the world to launch its own indigenously developed high-speed train. As rail capacity is struggling to meet demand for the transport of goods and raw materials such as coal, air routes, roads and waterways are rapidly being developed to provide an increasing proportion of China's overall transportation needs.Some economic experts have argued that the development gap between China and other emerging economies such as Brazil, Argentina and India can be attributed to a large extent to China's early focus on ambitious infrastructure projects: while China invested roughly 9% of its GDP on infrastructure in the 1990s and 2000s, most emerging economies invested only 2% to 5% of their GDP. This considerable spending gap allowed the Chinese economy to grow at near optimal conditions while many South American economies suffered from various development bottlenecks such as poor transportation networks, aging power grids and mediocre schools.
Science and technology in China has in recent decades developed rapidly. The Chinese government has placed emphasis through funding, reform, and societal status on science and technology as a fundamental part of the socio-economic development of the country as well as for national prestige. China has made rapid advances in areas such as education, infrastructure, high-tech manufacturing, artificial intelligence, academic publishing, patents and commercial applications and is now in some areas and by some measures a world leader. China is now increasingly targeting indigenous innovation and aims to reform remaining weaknesses. These initiatives are dependent on attracting highly educated overseas Chinese back to China to work in the innovation economy and to teach the next generation of Chinese students.
A circular economy (often referred to simply as "circularity") is an economic system aimed at eliminating waste and the continual use of resources. Circular systems employ reuse, sharing, repair, refurbishment, remanufacturing and recycling to create a closed-loop system, minimising the use of resource inputs and the creation of waste, pollution and carbon emissions. The circular economy aims to keep products, equipment and infrastructure in use for longer, thus improving the productivity of these resources. All "waste" should become "food" for another process: either a by-product or recovered resource for another industrial process or as regenerative resources for nature (e.g., compost). This regenerative approach is in contrast to the traditional linear economy, which has a "take, make, dispose" model of production.
Intuitively, the circular economy would appear to be more sustainable than the current linear economic system. Reducing the resources used, and the waste and leakage created, conserves resources and helps to reduce environmental pollution. However, it is argued by some that these assumptions are simplistic; that they disregard the complexity of existing systems and their potential trade-offs. For example, the social dimension of sustainability seems to be only marginally addressed in many publications on the circular economy. There are cases that might require different or additional strategies, like purchasing new, more energy-efficient equipment. By reviewing the literature, a team of researchers from Cambridge and TU Delft could show that there are at least eight different relationship types between sustainability and the circular economy. In addition, it is important to underline the innovation aspect in the heart of sustained development based on circular economy components.
The circular economy can cover a broad scope. Findings from the literature show that researchers have focused on different areas such as industrial applications with both product-oriented, natural resources and services, practice and policies to better understand the limitations that the CE currently faces, strategic management for details of the circular economy and different outcomes such as potential re-use applications and waste management.The circular economy includes products, infrastructure, equipment and services, and applies to every industry sector. It includes 'technical' resources (metals, minerals, fossil resources) and 'biological' resources (food, fibres, timber, etc.). Most schools of thought advocate a shift from fossil fuels to the use of renewable energy, and emphasize the role of diversity as a characteristic of resilient and sustainable systems. The circular economy includes discussion of the role of money and finance as part of the wider debate, and some of its pioneers have called for a revamp of economic performance measurement tools.. One study points out how modularisation could become a cornerstone to enable circular economy and enhance the sustainability of energy infrastructure. One example of a circular economy model is the implementation of renting models in traditional ownership areas (e.g. electronics, clothes, furniture, transportation). Through renting the same product to several clients, manufacturers can increase revenues per unit, thus decreasing the need to produce more to increase revenues. Recycling initiatives are often described as a circular economy and are likely to be the most widespread models.
As early as 1966 Kenneth Boulding raised awareness of an "open economy" with unlimited input resources and output sinks, in contrast with a "closed economy", in which resources and sinks are tied and remain as long as possible a part of the economy. Boulding's essay "The Economics of the Coming Spaceship Earth" is often cited as the first expression of the "circular economy", although Boulding does not use that phrase. The circular economy is grounded in the study of feedback-rich (non-linear) systems, particularly living systems. The contemporary understanding of the Circular Economy and its practical applications to economic systems evolved incorporating different features and contributions from a variety of concepts sharing the idea of closed loops. Some of the relevant theoretical influences are cradle to cradle, laws of ecology (e.g., Barry Commoner § The Closing Circle), looped and performance economy (Walter R. Stahel), regenerative design, industrial ecology, biomimicry and blue economy.The circular economy was further modelled by British environmental economists David W. Pearce and R. Kerry Turner in 1989. In Economics of Natural Resources and the Environment, they pointed out that a traditional open-ended economy was developed with no built-in tendency to recycle, which was reflected by treating the environment as a waste reservoir.In the early 1990s, Tim Jackson began to pull together the scientific basis for this new approach to industrial production in his edited collection Clean Production Strategies, including chapters from pre-eminent writers in the field, such as Walter R Stahel, Bill Rees and Robert Constanza. At the time still called 'preventive environmental management', his follow-on book Material Concerns: Pollution, Profit and Quality of Life synthesised these findings into a manifesto for change, moving industrial production away from an extractive linear system towards a more circular economy.
In their 1976 research report to the European Commission, "The Potential for Substituting Manpower for Energy", Walter Stahel and Genevieve Reday sketched the vision of an economy in loops (or circular economy) and its impact on job creation, economic competitiveness, resource savings and waste prevention. The report was published in 1982 as the book Jobs for Tomorrow: The Potential for Substituting Manpower for Energy.In 1982, Walter Stahel was awarded third prize in the Mitchell Prize competition on sustainable business models with a paper The Product-Life Factor. First prize went to the then US Secretary of Agriculture, second prize to Amory and Hunter Lovins, fourth prize to Peter Senge. Considered as one of the first pragmatic and credible sustainability think tanks, the main goals of Stahel's institute are to extend the working life of products, to make goods last longer, to re-use existing goods and ultimately to prevent waste. This model emphasizes the importance of selling services rather than products, an idea referred to as the "functional service economy" and sometimes put under the wider notion of "performance economy". This model also advocates "more localization of economic activity".Promoting a circular economy was identified as national policy in China's 11th five-year plan starting in 2006. The Ellen MacArthur Foundation has more recently outlined the economic opportunity of a circular economy, bringing together complementary schools of thought in an attempt to create a coherent framework, thus giving the concept a wide exposure and appeal.Most frequently described as a framework for thinking, its supporters claim it is a coherent model that has value as part of a response to the end of the era of cheap oil and materials, moreover contributing to the transition for a low carbon economy. In line with this, a circular economy can contribute to meeting the COP 21 Paris Agreement. The emissions reduction commitments made by 195 countries at the COP 21 Paris Agreement, are not sufficient to limit global warming to 1.5 °C. To reach the 1.5 °C ambition it is estimated that additional emissions reductions of 15 billion tonnes CO2 per year need to be achieved by 2030. Circle Economy and Ecofys estimated that circular economy strategies may deliver emissions reductions that could basically bridge the gap by half.
Linear "take, make, dispose" industrial processes, and the lifestyles dependent on them, use up finite reserves to create products with a finite lifespan, which end up in landfills or in incinerators. The circular approach, by contrast, takes insights from living systems. It considers that our systems should work like organisms, processing nutrients that can be fed back into the cycle — whether biological or technical — hence the "closed loop" or "regenerative" terms usually associated with it. The generic circular economy label can be applied to or claimed by several different schools of thought, but all of them gravitate around the same basic principles. One prominent thinker on the topic is Walter R. Stahel, an architect, economist, and a founding father of industrial sustainability. Credited with having coined the expression "Cradle to Cradle" (in contrast with "Cradle to Grave", illustrating our "Resource to Waste" way of functioning), in the late 1970s, Stahel worked on developing a "closed loop" approach to production processes, co-founding the Product-Life Institute in Geneva. In the UK, Steve D. Parker researched waste as a resource in the UK agricultural sector in 1982, developing novel closed-loop production systems. These systems mimicked and worked with the biological ecosystems they exploited.
In 2013, a report was released entitled Towards the Circular Economy: Economic and Business Rationale for an Accelerated Transition. The report, commissioned by the Ellen MacArthur Foundation and developed by McKinsey & Company, was the first of its kind to consider the economic and business opportunity for the transition to a restorative, circular model. Using product case studies and economy-wide analysis, the report details the potential for significant benefits across the EU. It argues that a subset of the EU manufacturing sector could realize net materials cost savings worth up to $630 billion annually towards 2025—stimulating economic activity in the areas of product development, remanufacturing and refurbishment. Towards the Circular Economy also identified the key building blocks in making the transition to a circular economy, namely in skills in circular design and production, new business models, skills in building cascades and reverse cycles, and cross-cycle/cross-sector collaboration.Another report by WRAP and the Green Alliance (called "Employment and the circular economy: job creation in a more resource efficient Britain"), done in 2015 has examined different public policy scenarios to 2030. It estimates that, with no policy change, 200,000 new jobs will be created, reducing unemployment by 54,000. A more aggressive policy scenario could create 500,000 new jobs and permanently reduce unemployment by 102,000.On the other hand, implementing a circular economy in the United States has been presented by Ranta et al. who analyzed the institutional drivers and barriers for the circular economy in different regions worldwide, by following the framework developed by Scott R. In the article, different worldwide environment-friendly institutions were selected, and two types of manufacturing processes were chosen for the analysis (1) a product-oriented, and (2) a waste management. Specifically, in the U.S., the product-oriented company case in the study was Dell, a US manufacturing company for computer technology, which was the first company to offer free recycling to customers and to launch to the market a computer made from recycling materials from a verified third-party source. Moreover, the waste management case that includes many stages such as collection, disposal, recycling in study was Republic Services, the second-largest waste management company in the US. The approach to measuring the drivers and barriers was to first identify indicators for their cases in study and then to categorize these indicators into drivers when the indicator was in favor of the circular economy model or a barrier when it was not.
While the initial focus of academic, industry, and policy activities was mainly focused on the development of re-X (recycling, remanufacturing, reuse, etc.) technology, it soon became clear that the technological capabilities increasingly exceed their implementation. To leverage this technology for the transition towards a circular economy, various stakeholders have to work together. This shifted attention towards business-model innovation as a key leverage for 'circular' technology adaption. Rheaply, a platform that aims to scale reuse within and between organizations, is an example of a technology that focuses on asset management & disposition to support organizations transitioning to circular business models.Circular business models can be defined as business models that are closing, narrowing, slowing, intensifying and dematerializing loops, to minimize the resource inputs into and the waste and emission leakage out of the organizational system. This comprises recycling measures (closing), efficiency improvements (narrowing), use phase extensions (slowing), a more intense use phase (intensifying), and the substitution of products by service and software solutions (dematerializing). These strategies can be achieved through the purposeful design of material recovery processes and related circular supply chains. As illustrated in the Figure, these five approaches to resource loops can also be seen as generic strategies or archetypes of circular business model innovation. Circular business models, as the economic model more broadly, can have different emphases and various objectives, for example: extend the life of materials and products, where possible over multiple 'use cycles'; use a 'waste = food' approach to help recover materials, and ensure those biological materials returned to earth are benign, not toxic; retain the embedded energy, water and other process inputs in the product and the material for as long as possible; Use systems-thinking approaches in designing solutions; regenerate or at least conserve nature and living systems; push for policies, taxes and market mechanisms that encourage product stewardship, for example 'polluter pays' regulations.
Building on circular business model innovation, digitalization and digital technologies (e.g., Internet of Things, Big Data, Artificial Intelligence, Blockchain) are seen as a key enabler for upscaling the circular economy. Also referred to as the data economy, the central role of digital technologies for accelerating the circular economy transition is emphasized within the Circular Economy Action Plan of the European Green deal. The smart circular economy framework illustrates this by establishing a link between digital technologies and sustainable resource management. This allows assessment of different digital circular economy strategies with their associated level of maturity, providing guidance on how to leverage data and analytics to maximize circularity (i.e., optimizing functionality and resource intensity). Supporting this, a Strategic Research and Innovation Agenda for circular economy has been recently published in the framework of the Horizon 2020 project CICERONE that puts digital technologies at the core of many key innovation fields (waste management, industrial symbiosis, products traceability).
In 2018, the World Economic Forum, World Resources Institute, Philips, Ellen MacArthur Foundation, United Nations Environment Programme, and over 40 other partners launched the Platform for Accelerating the Circular Economy (PACE). PACE follows on the legacy of WEF's CEO-led initiative, Project MainStream, which sought to scale up circular economy innovations. PACE's original intent has three focal areas: (1) developing models of blended finance for circular economy projects, especially in developing and emerging economies; (2) creating policy frameworks to address specific barriers to advancing the circular economy; and (3) promoting public–private partnership for these purposes.PACE members include global corporations like IKEA, Coca-Cola, Alphabet Inc., and DSM (company), along with governmental partners and development institutions from Denmark, The Netherlands, Finland, Rwanda, UAE, China, and beyond. Initiatives currently managed under PACE include the Capital Equipment Coalition with Philips and numerous other partners and the Global Battery Alliance with over 70 partners. In January 2019, PACE released a report entitled "A New Circular Vision for Electronics: Time for a Global Reboot" (in support of the United Nations E-waste Coalition.
To provide authoritative guidance to organizations implementing circular economy (CE) strategies, in 2017, the British Standards Institution (BSI) developed and launched the first circular economy standard "BS 8001:2017 Framework for implementing the principles of the circular economy in organizations". The circular economy standard BS 8001:2017 tries to align the far-reaching ambitions of the CE with established business routines at the organizational level. It contains a comprehensive list of CE terms and definitions, describes the core CE principles, and presents a flexible management framework for implementing CE strategies in organizations. Little concrete guidance on circular economy monitoring and assessment is given, however, as there is no consensus yet on a set of central circular economy performance indicators applicable to organizations and individual products.
In 2018, the International Organization for Standardization (ISO) established a technical committee, TC 323, in the field of circular economy to develop frameworks, guidance, supporting tools, and requirements for the implementation of activities of all involved organizations, to maximize the contribution to Sustainable Development. Four new ISO standards are under development and in the direct responsibility of the committee (consisting of 70 participating members and 11 observing members).
There is some criticism of the idea of the circular economy. As Corvellec (2015) put it, the circular economy privileges continued economic growth with soft "anti-programs", and the circular economy is far from the most radical "anti-program". Corvellec (2019) raised the issue of multi-species and stresses "impossibility for waste producers to dissociate themselves from their waste and emphasizes the contingent, multiple, and transient value of waste". "Scatolic engagement draws on Reno's analogy of waste as scats and of scats as signs for enabling interspecies communication. This analogy stresses the impossibility for waste producers to dissociate themselves from their waste and emphasizes the contingent, multiple, and transient value of waste". A key tenet of a scatolic approach to waste is to consider waste as unavoidable and worthy of interest. Whereas total quality sees in waste a sign of failure, a scatolic understanding sees a sign of life. Likewise, whereas the Circular Economy analogy of a circle evokes endless perfection, the analogy of scats evokes disorienting messiness. A scatolic approach features waste as a lively matter open for interpretation, within organizations as well as across organizational species. Corvellec and Stål (2019) are mildly critical of apparel manufacturing circular economy take-back systems as ways to anticipate and head off more severe waste reduction programs: Apparel retailers exploit that the circular economy is evocative but still sufficiently vague to create any concrete policies (Lüdeke‐Freund, Gold, & Bocken, 2019) that might hinder their freedom of action (Corvellec & Stål, 2017). Their business-centered qualification of take-back systems amounts to an engagement in "market action (...) as leverage to push policymakers to create or repeal particular rules," as Funk and Hirschman (2017:33) put it. Research by Zink and Geyer (2017: 593) questioned the circular economy's engineering-centric assumptions: "However, proponents of the circular economy have tended to look at the world purely as an engineering system and have overlooked the economic part of the circular economy. Recent research has started to question the core of the circular economy—namely, whether closing material and product loops do, in fact, prevent primary production."There are other critiques of the circular economy (CE). For example, Allwood (2014) discussed the limits of CE 'material circularity', and questioned the desirability of the CE in a reality with growing demand. Do CE secondary production activities (reuse, repair, & remake) actually reduce, or instead displace, primary production (natural resource extraction)? The problem CE overlooks, its untold story, is how displacement is governed mainly by market forces, according to McMillan et al. (2012). It's the tired old narrative, that the invisible hand of market forces will conspire to create full displacement of virgin material of the same kind, said Zink & Geyer (2017). Korhonen, Nuur, Feldmann, and Birkie (2018) argued that "the basic assumptions concerning the values, societal structures, cultures, underlying world-views and the paradigmatic potential of CE remain largely unexplored".It is also often pointed out that there are fundamental limits to the concept, which are based, among other things, on the laws of thermodynamics. According to the second law of thermodynamics, all spontaneous processes are irreversible and associated with an increase in entropy. It follows that in a real implementation of the concept, one would either have to deviate from the perfect reversibility in order to generate an entropy increase by generating waste, which would ultimately amount to still having parts of the economy which follow a linear scheme, or enormous amounts of energy would be required (from which a significant part would be dissipated in order to for the total entropy to increase). In its comment to concept of the circular economy the European Academies' Science Advisory Council (EASAC) came to a similar conclusion: Recovery and recycling of materials that have been dispersed through pollution, waste and end-of-life product disposal require energy and resources, which increase in a nonlinear manner as the percentage of recycled material rises (owing to the second law of thermodynamics: entropy causing dispersion). Recovery can never be 100% (Faber et al., 1987). The level of recycling that is appropriate may differ between materials.
A circular economy within the textiles industry refers to the practice of clothes and fibers continually being recycled, to re-enter the economy as much as possible rather than ending up as waste. A circular textiles economy is in response to the current linear model of the fashion industry, "in which raw materials are extracted, manufactured into commercial goods and then bought, used and eventually discarded by consumers" (Business of Fashion, 2017). 'Fast fashion 'companies have fueled the high rates of consumption which further magnify the issues of a linear system. "The take-make-dispose model not only leads to an economic value loss of over $500 billion per year but also has numerous negative environmental and societal impacts" (Business of Fashion, 2018). Such environmental effects include tons of clothing ending up in landfills and incineration, while the societal effects put human rights at risk. A documentary about the world of fashion, The True Cost (2015), explained that in fast fashion, "wages, unsafe conditions, and factory disasters are all excused because of the needed jobs they create for people with no alternatives." This shows that fast fashion is harming the planet in more ways than one by running on a linear system. It is argued that by following a circular economy, the textile industry can be transformed into a sustainable business. A 2017 report, "A New Textiles Economy", stated the four key ambitions needed to establish a circular economy: "phasing out substances of concern and microfiber release; transforming the way clothes are designed, sold and used to break free from their increasingly disposable nature; radically improving recycling by transforming clothing design, collection, and reprocessing; and making effective use of resources and moving to renewable input." While it may sound like a simple task, only a handful of designers in the fashion industry have taken charge, including Patagonia, Eileen Fisher, and Stella McCartney. An example of a circular economy within a fashion brand is Eileen Fisher's Tiny Factory, in which customers are encouraged to bring their worn clothing to be manufactured and resold. In a 2018 interview, Fisher explained, "A big part of the problem with fashion is overconsumption. We need to make less and sell less ... you get to use your creativity but you also get to sell more but not create more stuff." Circular initiatives, such as clothing rental startups, are also getting more and more highlight in the EU and in the USA as well. Operating with circular business model, rental services offer everyday fashion, baby wear, maternity wear for rent. The companies either offer flexible pricing in a 'pay as you rent' model like PALANTA.CO does, or offer fixed monthly subscriptions such as Rent The Runway or Le Tote. Another circular initiative is offering a take-back program. A company located in Colorado Circular Threads repurposes post-consumer waste materials such as old denim jeans, retired climbing rope, and discarded sails into new products, rather than letting them go to a landfill. Their take back program allows the consumer to return any product at any time so that it can be recycled again. Both China and Europe have taken the lead in pushing a circular economy. The Journal of Industrial Ecology (2017) stated that the "Chinese perspective on the circular economy is broad, incorporating pollution and other issues alongside waste and resource concerns, [while] Europe's conception of the circular economy has a narrower environmental scope, focusing on waste and resources and opportunities for business".
The construction sector is one of the world's largest waste generators. The circular economy appears as a helpful solution to diminish the environmental impact of the industry. Construction is very important to the economy of the European Union and its state members. It provides 18 million direct jobs and contributes to about 9% of the EU's GDP. The main causes of the construction's environmental impact are found in the consumption of non-renewable resources and the generation of contaminant residues, both of which are increasing at an accelerating pace.Decision making about the circular economy can be performed on the operational (connected with particular parts of the production process), tactical (connected with whole processes) and strategic (connected with the whole organization) levels. It may concern both construction companies as well as construction projects (where a construction company is one of the stakeholders). As a good case that fits the idea of circular economy in the construction sector on the operational level, there can be pointed walnut husks, that belong to hard, light and natural abrasives used for example in cleaning brick surfaces. Abrasive grains are produced from crushed, cleaned and selected walnut shells. They are classified as reusable abrasives. A first attempt to measure the success of circular economy implementation was done in a construction company. The circular economy can contribute to creating new posts and economic growth. According to Gorecki, one of such posts may be the Circular economy manager employed for construction projects.
The circular economy is beginning to catch on inside the automotive industry. There are also incentives for carmakers to do so as a 2016 report by Accenture stated that the circular economy could redefine competitiveness in the automotive sector in terms of price, quality, and convenience and could double revenue by 2030 and lower the cost base by up to fourteen percent. So far, it has typically translated itself into using parts made from recycled materials, remanufacturing of car parts and looking at the design of new cars. With the vehicle recycling industry (in the EU) only being able to recycle just 75% of the vehicle, meaning 25% isn't recycled and may even end up in landfills, there is much to improve here. In the electric vehicle industry, disassembly robots are used to help disassemble the vehicle. In the EU's ETN-Demeter project (European Training Network for the Design and Recycling of Rare-Earth Permanent Magnet Motors and Generators in Hybrid and Full Electric Vehicles) they are looking at the sustainable design issue. They are for example making designs of electric motors of which the magnets can be easily removed for recycling the rare earth metals. Some car manufacturers such as Volvo are also looking at alternative ownership models (leasing from the automotive company; "Care by Volvo").
The logistics industry plays an important role in the Dutch economy because the Netherlands is located in a specific area where the transit of commodities takes place on a daily basis. The Netherlands is an example of a country from the EU that has increasingly moved towards incorporating a circular economy given the vulnerability of the Dutch economy (as well as other EU countries) to be highly dependable on raw materials imports from countries such as China, which makes the country susceptible to the unpredictable importation costs for such primary goods.Research related to the Dutch industry shows that 25% of the Dutch companies are knowledgeable and interested in a circular economy; furthermore, this number increases to 57% for companies with more than 500 employees. Some of the areas are chemical industries, wholesale trade, industry and agriculture, forestry and fisheries because they see a potential reduction of costs when reusing, recycling and reducing raw materials imports. In addition, logistic companies can enable a connection to a circular economy by providing customers incentives to reduce costs through shipment and route optimization, as well as, offering services such as prepaid shipping labels, smart packaging, and take-back options. The shift from linear flows of packaging to circular flows as encouraged by the circular economy is critical for the sustainable performance and reputation of the packaging industry. The government-wide program for a circular economy is aimed at developing a circular economy in the Netherlands by 2050.Several statistics have indicated that there will be an increase in freight transport worldwide, which will affect the environmental impacts of the global warming potential causing a challenge to the logistics industry, however, the Dutch council for the Environment and Infrastructure (Dutch acronym: Rli) provided a new framework in which it suggests that the logistics industry can provide other ways to add value to the different activities in the Dutch economy, such as, an exchange of resources (either waste or water flows) for production from the different industries, in addition, to change the transit port concept to a transit hub. Moreover, the Rli studied the role of the logistics industry for three sectors, agriculture and food, chemical industries and high tech industries.
The Netherlands, aiming to have a completely circular economy by 2050, has also foreseen a shift to circular agriculture (kringlooplandbouw) as part of this plan. This shift foresees having a "sustainable and strong agriculture" by as early as 2030. Changes in the Dutch laws and regulations will be introduced. Some key points in this plant include: closing the fodder-manure cycle reusing as much waste streams as possible (a team Reststromen will be appointed) reducing the use of artificial fertilizers in favor of natural manure providing the chance for farms within experimentation areas to deviate from law and regulations implementing uniform methods to measure the soil quality providing the opportunity to agricultural entrepreneurs to sign an agreement with the Staatsbosbeheer ("State forest management") to have it use the lands they lease for natuurinclusieve landbouw ("nature-inclusive management") providing initiatives to increase the earnings of farmers
When it comes to the furniture industry, most of the products are passive durable products, and accordingly implementing strategies and business models that extend the lifetime of the products (like repairing and remanufacturing) would usually have lower environmental impacts and lower costs. Companies such as GGMS are supporting a circular approach to furniture by refurbishing and reupholstering items for reuse. The EU has seen a huge potential for implementing a circular economy in the furniture sector. Currently, out of 10,000,000 tonnes of annually discarded furniture in the EU, most of it ends up in landfills or is incinerated. There is a potential increase of €4.9 billion in Gross Value Added by switching to a circular model by 2030, and 163,300 jobs could be created.A study about the status of Danish furniture companies' efforts on a circular economy states that 44% of the companies included maintenance in their business models, 22% had take-back schemes, and 56% designed furniture for recycling. The authors of the study concluded that although a circular furniture economy in Denmark is gaining momentum, furniture companies lack knowledge on how to effectively transition, and the need to change the business model could be another barrier.Another report in the UK saw a huge potential for reuse and recycling in the furniture sector. The study concluded that around 42% of the bulk waste sent to landfills annually (1.6 million tonnes) is furniture. They also found that 80% of the raw material in the production phase is waste.
The uptake to reuse within the oil and gas industry is very poor, the opportunity to reuse is never more evident, or possible, as when the equipment is being decommissioned. Hundreds of thousands of tons of waste are being brought back onshore to be recycled. Unfortunately, what this equates to; is equipment, which is perfectly suitable for continued use, being disposed of.In the next 30–40 years, the oil and gas sector will have to decommission 600 installations in the UK alone. Over the next decade around 840,000 tonnes of materials will have to be recovered at an estimated cost of £25Bn. In 2017 North Sea oil and gas decommissioning became a net drain on the public purse. With UK taxpayers covering 50%–70% of the bill, there is an urgent need to discuss the most economic, social and environmentally beneficial decommissioning solutions for the general public.Organizations such as Zero Waste Scotland have conducted studies to identify areas with reuse potential, allowing equipment to continue life in other industries, or be redeployed for oil and gas .
The CE does not aim at changing the profit maximization paradigm of businesses. Rather, it suggests an alternative way of thinking how to attain a sustained competitive advantage (SCA), while concurrently addressing the environmental and socio-economic concerns of the 21st century. Indeed, stepping away from linear forms of production most often leads to the development of new core competencies along the value chain and ultimately superior performance that cuts costs, improves efficiency, meets advanced government regulations and the expectations of green consumers. But despite the multiple examples of companies successfully embracing circular solutions across industries, and notwithstanding the wealth of opportunities that exist when a firm has clarity over what circular actions fit its unique profile and goals, CE decision-making remains a highly complex exercise with no one-size-fits-all solution. The intricacy and fuzziness of the topic is still felt by most companies (especially SMEs), which perceive circular strategies as something not applicable to them or too costly and risky to implement. This concern is today confirmed by the results of ongoing monitoring studies like the Circular Readiness Assessment.Strategic management is the field of management that comes to the rescue allowing companies to carefully evaluate CE-inspired ideas, but also to take a firm apart and investigate if/how/where seeds of circularity can be found or implanted. The book Strategic Management and the Circular Economy defined for the first time a CE strategic decision-making process, covering the phases of analysis, formulation, and planning. Each phase is supported by frameworks and concepts popular in management consulting – like idea tree, value chain, VRIE, Porter's five forces, PEST, SWOT, strategic clock, or the internationalization matrix – all adapted through a CE lens, hence revealing new sets of questions and considerations. Although yet to be verified, it is argued that all standard tools for strategic management can and should be calibrated and applied to a CE. A specific argument has already been made for the strategy direction matrix of product vs market and the 3 × 3 GE-McKinsey matrix to assess business strength vs industry attractiveness, the BCG matrix of market share vs industry growth rate, and Kraljic's portfolio matrix.
On 17 December 2012, the European Commission published a document entitled "Manifesto for a Resource Efficient Europe". This manifesto clearly stated that "In a world with growing pressures on resources and the environment, the EU has no choice but to go for the transition to a resource-efficient and ultimately regenerative circular economy." Furthermore, the document highlighted the importance of "a systemic change in the use and recovery of resources in the economy" in ensuring future jobs and competitiveness, and outlined potential pathways to a circular economy, in innovation and investment, regulation, tackling harmful subsidies, increasing opportunities for new business models, and setting clear targets. The European environmental research and innovation policy aims at supporting the transition to a circular economy in Europe, defining and driving the implementation of a transformative agenda to green the economy and the society as a whole, to achieve a truly sustainable development. Research and innovation in Europe are financially supported by the program Horizon 2020, which is also open to participation worldwide. Circular economy is found to play an important role to economic growth of European Countries, highlighting the crucial role of sustainability, innovation, and investment in no-waste initiatives to promote wealth.The European Union plans for a circular economy are spearheaded by its 2018 Circular Economy Package. Historically, the policy debate in Brussels mainly focused on waste management which is the second half of the cycle, and very little is said about the first half: eco-design. To draw the attention of policymakers and other stakeholders to this loophole, the Ecothis, an EU campaign was launched raising awareness about the economic and environmental consequences of not including eco-design as part of the circular economy package. In 2020, the European Union released its Circular Economy Action Plan.
The various approaches to 'circular' business and economic models share several common principles with other conceptual frameworks:
Janine Benyus, author of "Biomimicry: Innovation Inspired by Nature", defined Biomimicry as "a new discipline that studies nature's best ideas and then imitates these designs and processes to solve human problems. Studying a leaf to invent a better solar cell is an example. I think of it as 'innovation' inspired by nature".
Initiated by former Ecover CEO and Belgian entrepreneur Gunter Pauli, derived from the study of natural biological production processes the official manifesto states, "using the resources available...the waste of one product becomes the input to create a new cash flow".
Created by Walter R. Stahel and similar theorists, in which industry adopts the reuse and service-life extension of goods as a strategy of waste prevention, regional job creation, and resource efficiency in order to decouple wealth from resource consumption.
Industrial Ecology is the study of material and energy flows through industrial systems. Focusing on connections between operators within the "industrial ecosystem", this approach aims at creating closed-loop processes in which waste is seen as input, thus eliminating the notion of undesirable by-product.
Resource recovery is using wastes as an input material to create valuable products as new outputs. The aim is to reduce the amount of waste generated, therefore reducing the need for landfill space and also extracting maximum value from waste.
The ability to understand how things influence one another within a whole. Elements are considered as 'fitting in' their infrastructure, environment and social context.
The Biosphere Rules is a framework for implementing closed-loop production processes. They derived from nature systems and translated for industrial production systems. The five principles are Materials Parsimony, Value Cycling, Power Autonomy, Sustainable Product Platforms and Function Over Form.
The economy of the Philippines is the world's 31st largest economy by nominal GDP according to the International Monetary Fund 2020 and the 13th largest economy in Asia. The Philippines is one of the emerging markets and the 3rd highest in Southeast Asia by GDP nominal after Thailand and Indonesia. The Philippines is primarily considered a newly industrialized country, which has an economy in transition from one based on agriculture to one based more on services and manufacturing. As of 2020, GDP by purchasing power parity was estimated to be at $934 billion.Primary exports include semiconductors and electronic products, transport equipment, garments, copper products, petroleum products, coconut oil, and fruits. Major trading partners include Japan, China, the United States, Singapore, South Korea, the Netherlands, Hong Kong, Germany, Taiwan and Thailand. The Philippines has been named as one of the Tiger Cub Economies together with Indonesia, Malaysia, Vietnam, and Thailand. It is currently one of Asia's fastest growing economies. However, major problems remain, mainly having to do with alleviating the wide income and growth disparities between the country's different regions and socioeconomic classes, reducing corruption, and investing in the infrastructure necessary to ensure future growth. The Philippine economy is projected to be the 5th largest in Asia and 16th biggest in the world by 2050. According to the PricewaterhouseCoopers, it estimates that it will be the 12th to 14th richest economy in the world by 2060. While this opposes other reports from HSBC Holdings PLC, that by the year 2050, the Philippines will have been stated in 2050 maybe due to its yearly higher GDP growth rate of 6.5% (Second, after China). However, the economic statistics may still vary depending on the performance of the government every year, which are consistently plagued by corruption.
The economic history of the Philippine Islands had been traced back to the pre-colonial times. The country which was then composed of different kingdoms and thalassocracies oversaw the large number of merchants coming to the islands for trade. Indian, Arab, Chinese and Japanese merchants were welcomed by these kingdoms, which were mostly located by riverbanks, coastal ports and central plains. The merchants traded for goods such as gold, rice, pots and other products. The barter system was implemented at that time and the pre-colonial people enjoyed a life filled with imported goods which reflected their fashion and lifestyle. From the 12th century, a huge industry centred around the manufacture and trade of burnay clay pots, used for the storage of tea and other perishables, was set up in the northern Philippines with Japanese and Okinawan traders. These pots were known as 'Ruson-tsukuri' (Luzon-made) in Japanese, and were considered among the best storage vessels used for the purpose of keeping tea leaves and rice wine fresh. Hence, Ruson-Tsukuri pots became sought after in Northeast Asia. Each Philippine kiln had its own branding symbol, marked on the bottom of the Ruson-tsukuri by a single baybayin letter. The people also were great agriculturists and the islands especifically Luzon has great abundance of rice, fowls, wine as well as great numbers of carabaos, deer, wild boar and goats. In addition, there were also great quantities of cotton and colored clothes, wax, honey and date palms produced by the natives. The precolonial state of Caboloan in Pangasinan often exported deer-skins to Japan and Okinawa. The Nation of Ma-i produced beeswax, cotton, true pearls, tortoise shell, medicinal betel nuts and yuta cloth in their trade with East Asia. By the early sixteenth century, the two largest polities of the Pasig River delta, Maynila and Tondo, established a shared monopoly on the trade of Chinese goods throughout the rest of the Philippine archipelago.The Visayas islands which is home to the Kedatuan of Madja-as, the Kedatuan of Dapitan and the Rajahnate of Cebu on the other hand were abundant in rice, fish, cotton, swine, fowls, wax and honey. Leyte was said to produce two rice crops a year, and Pedro Chirino commented on the great rice and cotton harvests that were sufficient to feed and clothe the people. In Mindanao, the Rajahnate of Butuan specialized in the mining of gold and the manufacture of jewelry. The Sultanate of Maguindanao was known for the raising and harvesting of cinnamon. The Sultanate of Lanao had a fishing industry by lake Lanao and the Sultanate of Sulu had lively pearl-diving operations. The kingdoms of ancient Philippines were active in international trade, and they used the ocean as natural highways. Ancient peoples were engaged in long-range trading with their Asian neighbors as far as west as Maldives and as far as north as Japan. Some historians even proposed that they also had regular contacts with other Austronesian people in Western Micronesia because it was the only area in the Oceania that had rice crops, tuba (fermented coconut sap), and tradition of betel nut chewing when the first Europeans arrived there. The uncanny resemblance of complex body tattoos among the Visayans and those of Borneo also proved some interesting connection between Borneo and ancient Philippines. Magellan's chronicler, Antonio Pigafetta, mentioned that merchants and ambassadors from all surrounding areas came to pay tribute to the rajah of Sugbu (Cebu) for the purpose of trade. While Magellan's crew were with the rajah, a representative from Siam was paying tribute to the rajah. Miguel López de Legazpi also wrote how merchants from Luzon and Mindoro had come to Cebu for trade, and he also mentioned how the Chinese merchants regularly came to Luzon for the same purpose. The Visayan Islands had earlier encounters with Greek traders in 21 AD. Its people enjoyed extensive trade contacts with other cultures. Indians, Japanese, Arabs, Vietnamese, Cambodians, Thais, Malays and Indonesians as traders or immigrants.Aside from trade relations, the natives were also involved in aquaculture and fishing. The natives make use of the salambao, which is a type of raft that utilizes a large fishing net which is lowered into the water via a type of lever made of two criss-crossed poles. Night fishing was accomplished with the help of candles made from a particular type of resin similar to the copal of Mexico. Use of safe pens for incubation and protection of the small fry from predators was also observed, and this method astonished the Spaniards at that time. During fishing, large mesh nets were also used by the natives to protect the young and ensure future good catches. From the early 1500s to as late as the 1560s, people from Luzon, Philippines; were referred to in Portuguese Malacca as Luções, and they set up many overseas communities across Southeast Asia where they participated in trading ventures and military campaigns in Burma, Malacca and Eastern Timor as traders and mercenaries. One prominent Luções was Regimo de Raja, who was a spice magnate and a Temenggung (Jawi: تمڠݢوڠ) (Governor and Chief General) in Portuguese Malacca. He was also the head of an armada which traded and protected commerce between the Indian Ocean, the Strait of Malacca, the South China Sea, and the medieval maritime principalities of the Philippines.
The natives were slavered among them by other tribes like Lapu-Lapu which forced other islands to pay taxes. The arrival of the Spanish removed this slavering system. Miguel Lopez de Legazpi with Tlaxcaltecs from Mexico conquered and unified the islands. The conquered was possible thanks to the discovery of the trip back to Mexico coast by Agustino Urdaneta. The administration of Islas Filipinas was done through the Capitania General and depended on Mexico Capital which formed the New Spain Viceroyalty. The economy of Islas Filipinas grew further when the Spanish government inaugurated the Manila Galleon trade system. Trading ships, settlers and military reinforcements made voyages once or twice per year across the Pacific Ocean from the port of Acapulco in Mexico to Manila in the Philippines. Both cities were part of the then Province of New Spain. This trade made the city of Manila one of the major global cities in the world, improving the growth of the Philippine economy in the succeeding years. Trade also introduced foodstuffs such as maize, tomatoes, potatoes, chili peppers, chocolate and pineapples from Mexico and Peru. Tobacco, first domesticated in Latin-America, and then introduced to the Philippines, became an important cash crop for Filipinos. The Philippines also became the distribution center of silver mined in the Americas, which was in high demand in Asia, during the period. In exchange for this silver, Manila gathered Indonesian spices, Chinese silks and Indian gems to be exported to Mexico.The Manila Galleon system operated until 1815, when Mexico got its independence. Nevertheless, it didn't affect the economy of the islands. On March 10, 1785, King Charles III of Spain confirmed the establishment of the Royal Philippine Company with a 25-year charter. The Basque-based company was granted a monopoly on the importation of Chinese and Indian goods into the Philippines, as well as the shipping of the goods directly to Spain via the Cape of Good Hope.
After Spain lost Mexico as a territory, New Spain was dissolved making the Philippines and other Pacific islands to form the Spanish East Indies. This resulted in the Philippines being governed directly by the King of Spain and the Captaincy General of the Philippines while the Pacific islands of Northern Mariana Islands, Guam, Micronesia and Palau was governed by the Real Audiencia of Manila and was part of the Philippine territorial governance. It made the economy of the Philippines grow further as people saw the rise of opportunities. Agriculture remained the largest contributor to economy, being the largest producer of coffee in Asia as well as a large produce of tobacco. In Europe, the Industrial Revolution spread from Great Britain during the period known as the Victorian Age. The industrialization of Europe created great demands for raw materials from the colonies, bringing with it investment and wealth, although this was very unevenly distributed. Governor-General Basco had opened the Philippines to this trade. Previously, the Philippines was seen as a trading post for international trade but in the nineteenth century it was developed both as a source of raw materials and as a market for manufactured goods. The economy of the Philippines rose rapidly and its local industries developed to satisfy the rising demands of an industrializing Europe. A small flow of European immigrants came with the opening of the Suez Canal, which cut the travel time between Europe and the Philippines by half. New ideas about government and society, which the friars and colonial authorities found dangerous, quickly found their way into the Philippines, notably through the Freemasons, who along with others, spread the ideals of the American, French and other revolutions, including Spanish liberalism. In 1834 the Royal Company of the Philippines was abolished, and free trade was formally recognized. With its excellent harbor, Manila became an open port for Asian, European, and North American traders. European merchants alongside the Chinese immigrants opened stores selling goods from all parts of the world. The El Banco Español Filipino de Isabel II (now the Bank of the Philippine Islands) was the first bank opened in the Philippines in 1851. In 1873 additional ports were opened to foreign commerce, and by the late nineteenth century three crops—tobacco, abaca, and sugar—dominated Philippine exports.
The results of the economy under the Americans were mixed. An initial high growth phase occurred during the 1910s due to the recovery from the wars with Spain and the US, and investment in agriculture. The Philippines would at first briefly outpace its neighbors. This would not last as growth fell behind in the later years. Stagnation in the late 1920s and beyond took place as access to US markets became restricted by protectionist quotas and fiscal restraints forestalled any further development in agriculture.The growth period can be attributed to the results of a crash program in agricultural modernization undertaken in 1910–1920. This in turn was done in order to address the growing shortfall in the supply of rice. The Philippines once a net exporter became an importer of rice as a result of the wars with the Spanish and later the Americans and by the reallocation of labour to export crops.The 1930s would mark the end to this period of relative prosperity. The Sugar Act of 1934 capped Philippines sugar exports to the US at 921,000 tons per year. Expenditure on public infrastructure for agriculture was reduced as the Payne–Aldridge Act stripped the government of customs revenue. Manila hemp was now competing against the newly invented Nylon. Although the area of land cultivated for agriculture was still increasing, the rate was reduced to 1% per annum. The policy with the most far reaching consequences of this period was the peg between the peso and dollar. This was enforced by law until 1975. It provided monetary stability for foreign investment inflows, which lead to 40% of all capital invested in manufacturing and commercial enterprises to be owned by foreign entities by 1938. On the other hand, this overvaluation of the peso would have a negative impact with foreign trade with the rest of Asia. Economic policy leading to independence would have necessitated loosening trade links with the US. In order to achieve an internationally competitive exchange rate, the peso dollar link would have to be broken. The much belated move to a true floating exchange rate led to uncompetitive exports as such an import substitution strategy remained until significant currency devaluation opened up the opportunity for reorienting towards exports.
Due to the Japanese invasion establishing the unofficial Second Philippine Republic, the economic growth receded and food shortages occurred. Prioritizing the shortages of food, Jose Laurel, the appointed President, organized an agency to distribute rice, even though most of the rice was confiscated by Japanese soldiers. Manila was one of the many places in the country that suffered from severe shortages, due mainly to a typhoon that struck the country in November 1943. The people were forced to cultivate private plots which produced root crops like kangkong. The Japanese, in order to raise rice production in the country, brought a quick-maturing horai rice, which was first used in Taiwan. Horai rice was expected to make the Philippines self-sufficient in rice by 1943, but rains during 1942 prevented this. Also during World War II in the Philippines, the occupying Japanese government issued fiat currency in several denominations; this is known as the Japanese government-issued Philippine fiat peso. The first issue in 1942 consisted of denominations of 1, 5, 10 and 50 centavos and 1, 5, and 10 Pesos. The next year brought "replacement notes" of the 1, 5 and 10 Pesos while 1944 ushered in a 100 Peso note and soon after an inflationary 500 Pesos note. In 1945, the Japanese issued a 1,000 Pesos note. This set of new money, which was printed even before the war, became known in the Philippines as Mickey Mouse money due to its very low value caused by severe inflation. Anti-Japanese newspapers portrayed stories of going to the market laden with suitcases or "bayong" (native bags made of woven coconut or buri leaf strips) overflowing with the Japanese-issued bills. In 1944, a box of matches cost more than 100 Mickey Mouse pesos. In 1945, a kilogram of camote cost around 1000 Mickey Mouse pesos. Inflation plagued the country with the devaluation of the Japanese money, evidenced by a 60% inflation experienced in January 1944.
After the re-establishment of the Commonwealth in 1945, the country was left with a devastated city, food crisis and financial crisis. A year later in 1946, the Philippines got its independence in America, creating the Third Philippine Republic. In an effort to solve the massive socio-economic problems of the period, newly elected President Manuel Roxas reorganized the government, and proposed a wide-sweeping legislative program. Among the undertakings of the Third Republic's initial year were: The establishment of the Rehabilitation Finance Corporation (which would be reorganized in 1958 as the Development Bank of the Philippines); the creation of the Department of Foreign Affairs and the organization of the foreign service through Executive Order No. 18; the GI Bill of Rights for Filipino veterans; and the revision of taxation laws to increase government revenues.President Roxas moved to strengthen sovereignty by proposing a Central Bank for the Philippines to administer the Philippine banking system which was established by Republic Act No. 265. In leading a "cash-starved government" that needed to attend a battered nation, President Roxas campaigned for the parity amendment to the 1935 Constitution. This amendment, demanded by the Philippine Trade Relations Act or the Bell Trade Act, would give American citizens and industries the right to utilize the country's natural resources in return for rehabilitation support from the United States. The President, with the approval of Congress, proposed this move to the nation through a plebiscite. The Roxas administration also pioneered the foreign policy of the Republic. Vice President Elpidio Quirino was appointed Secretary of Foreign Affairs. General Carlos P. Romulo, as permanent representative of the Philippines to the United Nations, helped shape the country's international identity in the newly established stage for international diplomacy and relations. During the Roxas administration, the Philippines established diplomatic ties with foreign countries and gained membership to international entities, such as the United Nations General Assembly, the United Nations Educational, Scientific and Cultural Organization (UNESCO), the World Health Organization (WHO), the International Labor Organization (ILO), etc. When President Carlos P. Garcia won the elections, his administration promoted the "Filipino First" policy, whose focal point was to regain economic independence; a national effort by Filipinos to "obtain major and dominant participation in their economy." The administration campaigned for the citizens' support in patronizing Filipino products and services, and implemented import and currency controls favorable for Filipino industries. In connection with the government's goal of self-sufficiency was the "Austerity Program," which President Garcia described in his first State of the NatIon Address as "more work, more thrift, more productive investment, and more efficiency" that aimed to mobilize national savings. The Anti Graft and Corrupt Practices Act, through Republic Act No. 301, aimed to prevent corruption, and promote honesty and public trust. Another achievement of the Garcia administration was the Bohlen–Serrano Agreement of 1959, which shortened the term of lease of the US military bases in the country from the previous 99 to 25 years.President Diosdado Macapagal, during his inaugural address on December 30, 1961, emphasized the responsibilities and goals to be attained in the "new era" that was the Macapagal administration. He reiterated his resolve to eradicate corruption, and assured the public that honesty would prevail in his presidency. President Macapagal, too, aimed at self-sufficiency and the promotion of every citizen's welfare, through the partnership of the government and private sector, and to alleviate poverty by providing solutions for unemployment. Among the laws passed during the Macapagal administration were: Republic Act No. 3844 or the Agricultural Land Reform Code (an act that established the Land Bank of the Philippines); Republic Act No. 3466, which established the Emergency Employment Administration; Republic Act No. 3518, which established the Philippine Veterans Bank; Republic Act No. 3470, which established the National Cottage Industries Development Authority (NACIDA) to organize, revive, and promote the establishment of local cottage industries; and Republic Act No. 4156, which established the Philippine National Railways (PNR) to operate the national railroad and tramways. The administration lifted foreign exchange controls as part of the decontrol program in an attempt to promote national economic stability and growth.
President Ferdinand E. Marcos declared martial law in the midst of rising student movements and an increasing number communist and socialist groups lobbying for reforms in their respective sectors. Leftists held rallies to express their frustrations to the government, this restiveness culminating in the First Quarter Storm, where activists stormed Malacañang Palace only to be turned back by the Philippine Constabulary. This event in particular left four people dead and many injured after heavy exchanges of gunfire. There was further unrest, and in the middle of the disorder on September 21, 1972, Marcos issued Proclamation No. 1081, effectively installing martial law in the Philippines, a declaration that suspended civil rights and imposed military rule in the country. The GDP of the Philippines rose during the martial law, rising from P55 million to P193 million in about 8 years. This growth was spurred by massive lending from commercial banks, accounting for about 62% percent of external debt. As a developing country, the Philippines during the martial law was one of the heaviest borrowers. These aggressive moves were seen by critics as a means of legitimizing martial law by purportedly enhancing the chances of the country in the global market. Much of the money was spent on pump-priming to improve infrastructure and promote tourism. However, despite the aggressive borrowing and spending policies, the Philippines lagged behind its Southeast Asia counterparts in GDP growth rate per capita. The country, in 1970–1980, only registered an average 5.73 percent growth, while its counterparts like Thailand, Malaysia, Singapore, and Indonesia garnered a mean growth of 7.97 percent. This lag, which became very apparent at the end of the Marcos Regime, can be attributed to the failures of economic management that was brought upon by State-run monopolies, mismanaged exchange rates, imprudent monetary policy and debt management, all underpinned by rampant corruption and cronyism. As said by Emannuel de Dios “[…]main characteristics distinguishing the Marcos years from other periods of our history has been the trend towards the concentration of power in the hands of the government, and the use of governmental functions to dispense economic privileges to some small factions in the private sector.”There are few more palpable and glaring examples of the economic mismanagement of the time than the Bataan Nuclear Power Plant (BNPP) located in Morong, Bataan. Started in the 1970s, the BNPP was supposed to boost the country's competitiveness by providing affordable electricity to fuel industrialization and job creation in the country. Far from this, the US$2.3 billion nuclear plant suffered from cost over-runs and engineering and structural issues which eventually led to its mothballing—without generating a single watt of electricity. Income inequality grew during the era of martial law, as the poorest 60 percent of the nation were able to contribute only 22.5 percent of the income at 1980, down from 25.0 percent in 1970. The richest 10 percent, meanwhile, took a larger share of the income at 41.7 percent at 1980, up from 37.1 percent at 1970. These trends coincided with accusations of cronyism in the Marcos administration, as the administration faced questions of favoring certain companies that were close to the ruling family. According to the FIES (Family Income and Expenditure Survey) conducted from 1965 to 1985, poverty incidence in the Philippines rose from 41 percent in 1965 to 58.9 percent in 1985. This can be attributed to lower real agricultural wages and lesser real wages for unskilled and skilled laborers. Real agricultural wages fell about 25 percent from their 1962 level, while real wages for unskilled and skilled laborers decreased by about one-third of their 1962 level. It was observed that higher labor force participation and higher incomes of the rich helped cushion the blow of the mentioned problems.
The Aquino administration took over an economy that had gone through socio-political disasters during the People Power revolution, where there was financial and commodity collapse caused by an overall consumer cynicism, a result of the propaganda against cronies, social economic unrest resulting from numerous global shortages, massive protests, lack of government transparency, the opposition's speculations, and various assassination attempts and failed coups. At that point in time, the country's incurred debt from the Marcos Era's debt-driven development began crippling the country, which slowly made the Philippines the "Latin-American in East Asia" as it started to experience the worst recession since the post-war era. Most of the immediate efforts of the Aquino administration was directed in reforming the image of the country and paying off all debts, including those that some governments were ready to write-off, as possible. This resulted in budget cuts and further aggravated the plight of the lower class because the jobs offered to them by the government was now gone. Infrastructure projects, including repairs, were halted in secluded provinces turning concrete roads into asphalt. Privatization of many government corporations, most catering utilities, was the priority of the Aquino administration which led to massive lay-offs and inflation. The Aquino administration was persistent in its belief that the problems that arose from the removal of the previous administration can be solved by the decentralization of power. Growth gradually began in the next few years of the administration. Somehow, there was still a short-lived, patchy, and erratic recovery from 1987 to 1991 as the political situation stabilized a bit. With this, the peso became more competitive, confidence of investors was gradually regained, positive movements in terms of trade were realized, and regional growth gradually strengthened.
The Ramos administration basically served its role as the carrier of the momentum of reform and as an important vehicle in "hastening the pace of liberalization and openness in the country". The administration was a proponent of capital account liberalization, which made the country more open to foreign trade, investments, and relations. It was during the term of the administration when the Bangko Sentral ng Pilipinas was established, and the Philippines joined the World Trade Organization and other free trade associations such as the APEC. Also, debt reduction was considered and as such, the issuance of certain government bonds called Brady Bonds also came to fruition in 1992. Key negotiations with conflicting forces in Mindanao actually became more successful during the administration, with Jose Almonte as one of the key adviser of the administration. By the time Ramos succeeded Corazon Aquino in 1992, the Philippine economy was already burdened with a heavy budget deficit. This was largely the result of austerity measures imposed by a standard credit arrangement with the International Monetary Fund and the destruction caused by natural disasters such as the eruption of Mt. Pinatubo. Hence, according to Canlas, pump priming through government spending was immediately ruled out due to the deficit. Ramos therefore resorted to institutional changes through structural policy reforms, of which included privatization and deregulation. He sanctioned the formation of the Legislative-Executive Development Advisory Council (LEDAC), which served as a forum for consensus building, on the part of the Executive and the Legislative branches, on important bills on economic policy reform measures (4). The daily brownouts that plagued the economy were also addressed through the enactment of policies that placed guaranteed rates. The economy during the first year of Ramos administration suffered from severe power shortage, with frequent brownouts, each lasting from 8 to 12 hours. To resolve this problem, the Electric Power Crisis Act was made into law together with the Build-Operate-Transfer Law. Twenty power plants were built because of these, and in effect, the administration was able to eliminate the power shortage problems in December 1993 and sustained economic growth for some time.The economy seemed to be all set for long-run growth, as shown by sustainable and promising growth rates from 1994 to 1997. However, the Asian Crisis contagion which started from Thailand and Korea started affecting the Philippines. This prompted the Philippine economy to plunge into continuous devaluation and very risky ventures, resulting in property busts and a negative growth rate. The remarkable feat of the administration, however, was that it was able to withstand the contagion effect of the Asian Crisis better than anybody else in the neighboring countries. Most important in the administration was that it made clear the important tenets of reform, which included economic liberalization, stronger institutional foundations for development, redistribution, and political reform.Perhaps some of the most important policies and breakthroughs of the administration are the Capital Account Liberalization and the subsequent commitments to free trade associations such as APEC, AFTA, GATT, and WTO. The liberalization and opening of the capital opening culminated in full-peso convertibility in 1992. And then another breakthrough is again, the establishment of the Bangko Sentral ng Pilipinas, which also involved the reduction of debts in that the debts of the old central bank were taken off its books.
Although Estrada's administration had to endure the continued shocks of the Asian Crisis contagion, the administration was also characterized by the administration's economic mismanagement and "midnight cabinets." As if the pro-poor rhetoric, promises and drama were not really appalling enough, the administration also had "midnight cabinets composed of 'drinking buddies' influencing the decisions of the "daytime cabinet'". Cronyism and other big issues caused the country's image of economic stability to change towards the worse. And instead of adjustments happening, further deterioration of the economy occurred. Targeted revenues were not reached, implementation of policies became very slow, and fiscal adjustments were not efficiently conceptualized and implemented. All those disasters caused by numerous mistakes were made worse by the sudden entrance of the Jueteng controversy, which gave rise to the succeeding EDSA Revolutions. Despite all these controversies, the administration still had some meaningful and profound policies to applaud. The administration presents a reprise of the population policy, which involved the assisting of married couples to achieve their fertility goals, reduce unwanted fertility and match their unmet need for contraception. The administration also pushed for budget appropriations for family planning and contraceptives, an effort that was eventually stopped due to the fact that the church condemned it. The administration was also able to implement a piece of its overall Poverty Alleviation Plan, which involved the delivery of social services, basic needs, and assistance to the poor families. The Estrada administration also had limited contributions to Agrarian Reform, perhaps spurred by the acknowledgement that indeed, Agrarian Reform can also address poverty and inequitable control over resources. In that regard, the administration establishes the program "Sustainable Agrarian Reform Communities-Technical Support to Agrarian and Rural Development". As for regional development, however, the administration had no notable contributions or breakthroughs.
The Arroyo administration, in an economical standpoint, was a period of good growth rates simultaneous with the US, due perhaps to the emergence of the Overseas Filipino workers (OFW) and the Business Process Outsourcing (BPO). The emergence of the OFW and the BPO improved the contributions of OFW remittances and investments to growth. In 2004, however, fiscal deficits grew and grew as tax collections fell, perhaps due to rampant and wide scale tax avoidance and tax evasion incidences. Fearing that a doomsday prophecy featuring the [Argentina default] in 2002 might come to fruition, perhaps due to the same sort of fiscal crisis, the administration pushed for the enactment of the 12% VAT and the E-VAT to increase tax revenue and address the large fiscal deficits. This boosted fiscal policy confidence and brought the economy back on track once again. Soon afterwards, political instability afflicted the country and the economy anew with Abu Sayyaf terrors intensifying. The administration's Legitimacy Crisis also became a hot issue and threat to the authority of the Arroyo administration. Moreover, the Arroyo administration went through many raps and charges because of some controversial deals such as the NBN-ZTE Broadband Deal. Due however to the support of local leaders and the majority of the House of Representatives, political stability was restored and threats to the administration were quelled and subdued. Towards the end of the administration, high inflation rates for rice and oil in 2008 started to plague the country anew, and this led to another fiscal crisis, which actually came along with the major recession that the United States and the rest of the world were actually experiencing. The important policies of the Arroyo administration highlighted the importance of regional development, tourism, and foreign investments into the country. Therefore, apart from the enactment and establishment of the E-VAT policy to address the worsening fiscal deficits, the administration also pushed for regional development studies in order to address certain regional issues such as disparities in regional per capita income and the effects of commercial communities on rural growth. The administration also advocated for investments to improve tourism, especially in other unexplored regions that actually need development touches as well. To further improve tourism, the administration launched the policy touching on Holiday Economics, which involves the changing of days in which we would celebrate certain holidays. Indeed, through the Holiday Economics approach, investments and tourism really improved. As for investment, the Arroyo administration would frequently visit other countries to encourage foreign investment for the betterment of the Philippine economy and its development.
The Philippines consistently coined as one of the Newly Industrialized Countries has had a fair gain during the latter years under the Arroyo Presidency to the current administration. The government managed foreign debts falling from 58% in 2008 to 47% of total government borrowings. According to the 2012 World Wealth Report, the Philippines was the fastest growing economy in the world in 2010 with a GDP growth of 7.3% driven by the growing business process outsourcing and overseas remittances.The country markedly slipped to 3.6% in 2011 after the government placed less emphasis on exports, as well as spending less on infrastructure. In addition, the disruption of the flow of imports for raw materials as a result from floods in Thailand and the tsunami in Japan affected the manufacturing sector in the same year. "The Philippines contributed more than $125 million as of end-2011 to the pool of money disbursed by the International Monetary Fund to help address the financial crisis confronting economies in Europe. This was according to the Bangko Sentral ng Pilipinas, which reported Tuesday that the Philippines, which enjoys growing foreign exchange reserves, has made available about $251.5 million to the IMF to finance the assistance program—the Financial Transactions Plan (FTP)—for crisis-stricken countries."The economy saw continuous real GDP growth of at least 5% since 2012. The Philippine Stock Exchange index ended 2012 with 5,812.73 points a 32.95% growth from the 4,371.96-finish in 2011.
The Philippine economy has been growing steadily over decades and the International Monetary Fund in 2014 reported it as the 39th largest economy in the world. However its growth has been behind that of many of its Asian neighbors, the so-called Asian Tigers, and it is not a part of the Group of 20 nations. Instead it is grouped in a second tier for emerging markets or newly industrialized countries. Depending on the analyst, this second tier can go by the name the Next Eleven or the Tiger Cub Economies. In the years 2012 and 2013, the Philippines posted high GDP growth rates, reaching 6.8% in 2012 and 7.2% in 2013, the highest GDP growth rates in Asia for the first two quarters of 2013, followed by China and Indonesia.A chart of selected statistics showing trends in the gross domestic product of the Philippines using data taken from the International Monetary Fund.
As a newly industrialized country, the Philippines is still an economy with a large agricultural sector; however, services have come to dominate the economy. Much of the industrial sector is based on processing and assembly operations in the manufacturing of electronics and other high-tech components, usually from foreign multinational corporations. Filipinos who go abroad to work–-known as Overseas Filipino Workers or OFWs—are a significant contributor to the economy but are not reflected in the below sectoral discussion of the domestic economy. OFW remittances is also credited for the Philippines' recent economic growth resulting in investment status upgrades from credit ratings agencies such as the Fitch Group and Standard & Poor's. In 1994, more than $2 billion USD worth of remittance from Overseas Filipinos were sent to the Philippines. In 2012, Filipino Americans sent 43% of all remittances sent to the Philippines, totaling to US$10.6 billion.
Agriculture employs 30% of the Filipino workforce as of 2014. Agriculture accounts for 11% of Philippines GDP as of 2014. The type of activity ranges from small subsistence farming and fishing to large commercial ventures with significant export focus. The Philippines is the world's largest producer of coconuts producing 19,500,000 tons in 2009. Coconut production in the Philippines is generally concentrated in medium-sized farms. The Philippines is also the world's second largest producer of pineapples, producing 2,730,000 metric tons in 2018. Rice production in the Philippines is important to the food supply in the country and economy. The Philippines is the 8th largest rice producer in the world, accounting for 2.8% of global rice production. The Philippines was also the world's largest rice importer in 2010. Rice is the most important food crop, a staple food in most of the country. It is produced extensively in Luzon (especially Central Luzon), Western Visayas, Southern Mindanao and Central Mindanao. The Philippines is one of the largest producers of sugar in the world. At least 17 provinces located in eight regions of the nation have grown sugarcane crops, of which the Negros Island Region accounts for half of the country's total production. As of Crop Year 2012–2013, 29 mills are operational divided as follows: 13 mills in Negros, 6 mills in Luzon, 4 mills in Panay, 3 mills in Eastern Visayas and 3 mills in Mindanao. A range from 360,000 to 390,000 hectares are devoted to sugarcane production. The largest sugarcane areas are found in the Negros Island Region, which accounts for 51% of sugarcane areas planted. This is followed by Mindanao which accounts for 20%; Luzon by 17%; Panay by 07% and Eastern Visayas by 04%.
The Philippines is a major player in the global shipbuilding industry with shipyards in Subic, Cebu, General Santos City and Batangas. It became the fourth largest shipbuilding nation in 2010. Subic-made cargo vessels are now exported to countries where shipping operators are based. South Korea's Hanjin started production in Subic in 2007 of the 20 ships ordered by German and Greek shipping operators. The country's shipyards are now building ships like bulk carriers, container ships and big passenger ferries. General Santos' shipyard is mainly for ship repair and maintenance.Being surrounded by waters, the country has abundant natural deep-sea ports ideal for development as production, construction and repair sites. On top of the current operating shipyards, two additional shipyards in Misamis Oriental and Cagayan province are being expanded to support future locators. It has a vast manpower pool of 60,000 certified welders that comprise the bulk of workers in shipbuilding. In the ship repair sector, the Navotas complex in Metro Manila is expected to accommodate 96 vessels for repair.
The ABS used in Mercedes-Benz, BMW, and Volvo cars are made in the Philippines. Toyota, Mitsubishi, Nissan and Honda are the most prominent automakers manufacturing cars in the country. Kia and Suzuki produce small cars in the country. Isuzu also produces SUVs in the country. Honda and Suzuki produce motorcycles in the country. A 2003 Canadian market research report predicted that further investments in this sector were expected to grow in the following years. Toyota sells the most vehicles in the country. By 2011, China's Chery Automobile company is going to build their assembly plant in Laguna, that will serve and export cars to other countries in the region if monthly sales would reach 1,000 units. Automotive sales in the Philippines moved up from 165,056 units in 2011 to over 180,000 in 2012. Japan's automotive manufacturing giant Mitsubishi Motors has announced that it will be expanding its operations in the Philippines.
Aerospace products in the Philippines are mainly for the export market and include manufacturing parts for aircraft built by both Boeing and Airbus. Moog is the biggest aerospace manufacturer with base in Baguio in the Cordillera region. The company produces aircraft actuators in their manufacturing facility. In 2011, the total export output of aerospace products in the Philippines reached US$3 billion.
A Texas Instruments plant in Baguio has been operating for 20 years and is the largest producer of DSP chips in the world. Texas Instruments' Baguio plant produces all the chips used in Nokia cell phones and 80% of chips used in Ericsson cell phones in the world. Until 2005, Toshiba laptops were produced in Santa Rosa, Laguna. Presently the Philippine plant's focus is in the production of hard disk drives. Printer manufacturer Lexmark has a factory in Mactan in the Cebu region. Electronics and other light industries are concentrated in Laguna, Cavite, Batangas and other CALABARZON provinces with sizable numbers found in Southern Philippines that account for most of the country's export.
The country is rich in mineral and geothermal energy resources. In 2003, it produced 1931 MW of electricity from geothermal sources (27% of total electricity production), second only to the United States, and a recent discovery of natural gas reserves in the Malampaya oil fields off the island of Palawan is already being used to generate electricity in three gas-powered plants. Philippine gold, nickel, copper, palladium and chromite deposits are among the largest in the world. Other important minerals include silver, coal, gypsum, and sulphur. Significant deposits of clay, limestone, marble, silica, and phosphate exist. About 60% of total mining production are accounted for by non-metallic minerals, which contributed substantially to the industry's steady output growth between 1993 and 1998, with the value of production growing 58%. In 1999, however, mineral production declined 16% to $793 million. Mineral exports have generally slowed since 1996. Led by copper cathodes, Philippine mineral exports amounted to $650 million in 2000, barely up from 1999 levels. Low metal prices, high production costs, lack of investment in infrastructure, and a challenge to the new mining law have contributed to the mining industry's overall decline.The industry rebounded starting in late 2004 when the Supreme Court upheld the constitutionality of an important law permitting foreign ownership of Philippines mining companies. However, the DENR has yet to approve the revised Department Administrative Order (DAO) that will provide the Implementing Rules and Regulations of the Financial and Technical Assistance Agreement (FTAA), the specific part of the 1994 Mining Act that allows 100% foreign ownership of Philippines mines.
In 2008, the Philippines has surpassed India as the world leader in business process outsourcing. The majority of the top ten BPO firms of the United States operate in the Philippines. The industry generated 100,000 jobs, and total revenues were placed at $960 million for 2005. In 2012, BPO sector employment ballooned to over 700,000 people and is contributing to a growing middle class. BPO facilities are concentrated in IT parks and centers in Economic Zones across the Philippines. BPO facilities are located mainly in Metro Manila and Cebu City although other regional areas such as Baguio, Bacolod, Cagayan de Oro, Clark Freeport Zone, Dagupan, Davao City, Dumaguete, Lipa, Iloilo City, and Naga City, Camarines Sur are now being promoted and developed for BPO operations. Call centers began in the Philippines as plain providers of email response and managing services and is now a major source of employment. Call center services include customer relations, ranging from travel services, technical support, education, customer care, financial services, online business to customer support, and online business-to-business support. Business process outsourcing (BPO) is regarded as one of the fastest growing industries in the world. The Philippines is also considered as a location of choice due to its many outsourcing benefits such as less expensive operational and labor costs, the high proficiency in spoken English of a significant number of its people, and a highly educated labor pool. In 2011, the business process outsourcing industry in the Philippines generated 700 thousand jobs and some US$11 billion in revenue, 24 percent higher than 2010. By 2016, the industry is projected to reach US$27.4 billion in revenue with employment generation to almost double at 1.3 million workers.BPOs and the call center industry in general are also credited for the Philippines' recent economic growth resulting in investment status upgrades from credit ratings agencies such as Fitch and S&P.With the Philippines being the 34th largest economy in the world, the country continues to be a promising prospect for the BPO Industry. Just in August 2014, the Philippines hit an all-time high for employment in the BPO industry. From 101,000 workers in 2004, the labor force in the industry has grown to over 930,000 in just the first quarter of 2014.Growth in the BPO industry continues to show significant improvements with an average annual expansion rate of 20%. Figures have shown that from $1.3 Billion in 2004, export revenues from the BPO sector has increased to over $13.1 Billion in 2013. The IT and Business Process Association of the Philippines (IBPAP) also projects that the sector will have an expected total revenue of $25 Billion in 2016. IBPAP projects that the industry will employ 1.8 million workers and generate US$38.9 billion of revenue by 2022.This growth in the industry is further promoted by the Philippine government. The industry is highlighted by the Philippines Development Plan as among the 10 high potential and priority development areas. To further entice investors, government programs include different incentives such as tax holidays, tax exemptions, and simplified export and import procedures. Additionally, training is also available for BPO applicants.
Tourism is an important sector for the Philippine economy, contributing 7.8% to the Philippine gross domestic product (GDP) in 2014.The tourism industry employed 3.8 million Filipinos, or 10.2 per cent of national employment in 2011, according to data gathered by the National Statistical Coordination Board. In a greater thrust by the Aquino administration to pump billion to employ 7.4 million people by 2016, or about 18.8 per cent of the total workforce, contributing 8 per cent to 9 per cent to the nation's GDP.In 2014, the tourism sector contributed 1.4 trillion pesos to the country's economy.
According to PSA, Gross Regional Domestic Product (GRDP) is GDP measured at regional levels. Figures below are for the year 2018: Note: Green-colored cells indicate higher value or best performance in index, while yellow-colored cells indicate the opposite. Numbers may not add up to totals due to rounding.
The national government budget for 2016 has set the following budget allocations:
Bamboo network Economy of Asia Emerging markets List of companies of the Philippines Newly Industrialized countries Philippine Statistics Authority Tiger Cub Economies
Bangko Sentral ng Pilipinas (Central Bank of the Philippines) National Statistical Coordination Board Department of Trade and Industry Department of Finance Philippine Stock Exchange National Federation of Sugarcane Planters Department of Tourism Philippines Business Brokers Philippine Economic Zone AuthorityTradeWorld Bank Summary Trade Statistics Philippines 2012 Tariffs applied by the Philippines as provided by ITC's Market Access Map, an online database of customs tariffs and market requirements
In microeconomics, economies of scale are the cost advantages that enterprises obtain due to their scale of operation (typically measured by the amount of output produced), with cost per unit of output decreasing with increasing scale. At the basis of economies of scale there may be technical, statistical, organizational or related factors to the degree of market control. Economies of scale apply to a variety of organizational and business situations and at various levels, such as a production, plant or an entire enterprise. When average costs start falling as output increases, then economies of scale occur. Some economies of scale, such as capital cost of manufacturing facilities and friction loss of transportation and industrial equipment, have a physical or engineering basis. Another source of scale economies is the possibility of purchasing inputs at a lower per-unit cost when they are purchased in large quantities. The economic concept dates back to Adam Smith and the idea of obtaining larger production returns through the use of division of labor. Diseconomies of scale are the opposite. Economies of scale often have limits, such as passing the optimum design point where costs per additional unit begin to increase. Common limits include exceeding the nearby raw material supply, such as wood in the lumber, pulp and paper industry. A common limit for a low cost per unit weight commodities is saturating the regional market, thus having to ship product uneconomic distances. Other limits include using energy less efficiently or having a higher defect rate. Large producers are usually efficient at long runs of a product grade (a commodity) and find it costly to switch grades frequently. They will, therefore, avoid specialty grades even though they have higher margins. Often smaller (usually older) manufacturing facilities remain viable by changing from commodity-grade production to specialty products.Economies of scale must be distinguished from economies stemming from an increase in the production of a given plant. When a plant is used below its optimal production capacity, increases in its degree of utilization bring about decreases in the total average cost of production. As noticed, among the others, by Nicholas Georgescu-Roegen (1966) and Nicholas Kaldor (1972) these economies are not economies of scale.
The simple meaning of economies of scale is doing things more efficiently with increasing size. Common sources of economies of scale are purchasing (bulk buying of materials through long-term contracts), managerial (increasing the specialization of managers), financial (obtaining lower-interest charges when borrowing from banks and having access to a greater range of financial instruments), marketing (spreading the cost of advertising over a greater range of output in media markets), and technological (taking advantage of returns to scale in the production function). Each of these factors reduces the long run average costs (LRAC) of production by shifting the short-run average total cost (SRATC) curve down and to the right. Economies of scale is a concept that may explain real-world phenomena such as patterns of international trade or the number of firms in a market. The exploitation of economies of scale helps explain why companies grow large in some industries. It is also a justification for free trade policies, since some economies of scale may require a larger market than is possible within a particular country—for example, it would not be efficient for Liechtenstein to have its own carmaker if they only sold to their local market. A lone carmaker may be profitable, but even more so if they exported cars to global markets in addition to selling to the local market. Economies of scale also play a role in a "natural monopoly". There is a distinction between two types of economies of scale: internal and external. An industry that exhibits an internal economy of scale is one where the costs of production fall when the number of firms in the industry drops, but the remaining firms increase their production to match previous levels. Conversely, an industry exhibits an external economy of scale when costs drop due to the introduction of more firms, thus allowing for more efficient use of specialized services and machinery.
Some of the economies of scale recognized in engineering have a physical basis, such as the square-cube law, by which the surface of a vessel increases by the square of the dimensions while the volume increases by the cube. This law has a direct effect on the capital cost of such things as buildings, factories, pipelines, ships and airplanes.In structural engineering, the strength of beams increases with the cube of the thickness. Drag loss of vehicles like aircraft or ships generally increases less than proportional with increasing cargo volume, although the physical details can be quite complicated. Therefore, making them larger usually results in less fuel consumption per ton of cargo at a given speed. Heat loss from industrial processes vary per unit of volume for pipes, tanks and other vessels in a relationship somewhat similar to the square-cube law. In some productions, an increase in the size of the plant reduces the average variable cost, thanks to the energy savings resulting from the lower dispersion of heat. Economies of increased dimension are often misinterpreted because of the confusion between indivisibility and three-dimensionality of space. This confusion arises from the fact that three-dimensional production elements, such as pipes and ovens, once installed and operating, are always technically indivisible. However, the economies of scale due to the increase in size do not depend on indivisibility but exclusively on the three-dimensionality of space. Indeed, indivisibility only entails the existence of economies of scale produced by the balancing of productive capacities, considered above; or of increasing returns in the utilisation of a single plant, due to its more efficient use as the quantity produced increases. However, this latter phenomenon has nothing to do with the economies of scale which, by definition, are linked to the use of a larger plant.
At the base of economies of scale there are also returns to scale linked to statistical factors. In fact, the greater of the number of resources involved, the smaller, in proportion, is the quantity of reserves necessary to cope with unforeseen contingencies (for instance, machine spare parts, inventories, circulating capital, etc.).
A larger scale generally determines greater bargaining power over input prices and therefore benefits from pecuniary economies in terms of purchasing raw materials and intermediate goods compared to companies that make orders for smaller amounts. In this case, we speak of pecuniary economies, to highlight the fact that nothing changes from the "physical" point of view of the returns to scale. Furthermore, supply contracts entail fixed costs which lead to decreasing average costs if the scale of production increases.
Economies of productive capacity balancing derives from the possibility that a larger scale of production involves a more efficient use of the production capacities of the individual phases of the production process. If the inputs are indivisible and complementary, a small scale may be subject to idle times or to the underutilization of the productive capacity of some sub-processes. A higher production scale can make the different production capacities compatible. The reduction in machinery idle times is crucial in the case of a high cost of machinery. === Economies resulting from the division of labour and the use of superior techniques === A larger scale allows for a more efficient division of labour. The economies of division of labour derive from the increase in production speed, from the possibility of using specialized personnel and adopting more efficient techniques. An increase in the division of labour inevitably leads to changes in the quality of inputs and outputs.
Many administrative and organizational activities are mostly cognitive and, therefore, largely independent of the scale of production. When the size of the company and the division of labour increase, there are a number of advantages due to the possibility of making organizational management more effective and perfecting accounting and control techniques. Furthermore, the procedures and routines that turned out to be the best can be reproduced by managers at different times and places.
Learning and growth economies are at the base of dynamic economies of scale, associated with the process of growth of the scale dimension and not to the dimension of scale per se. Learning by doing implies improvements in the ability to perform and promotes the introduction of incremental innovations with a progressive lowering of average costs. Learning economies are directly proportional to the cumulative production (experience curve). Growth economies occur when a company acquires an advantage by increasing its size. These economies are due to the presence of some resource or competence that is not fully utilized, or to the existence of specific market positions that create a differential advantage in expanding the size of the firms. That growth economies disappear once the scale size expansion process is completed. For example, a company that owns a supermarket chain benefits from an economy of growth if, opening a new supermarket, it gets an increase in the price of the land it owns around the new supermarket. The sale of these lands to economic operators, who wish to open shops near the supermarket, allows the company in question to make a profit, making a profit on the revaluation of the value of building land.
Overall costs of capital projects are known to be subject to economies of scale. A crude estimate is that if the capital cost for a given sized piece of equipment is known, changing the size will change the capital cost by the 0.6 power of the capacity ratio (the point six to the power rule).In estimating capital cost, it typically requires an insignificant amount of labor, and possibly not much more in materials, to install a larger capacity electrical wire or pipe having significantly greater capacity.The cost of a unit of capacity of many types of equipment, such as electric motors, centrifugal pumps, diesel and gasoline engines, decreases as size increases. Also, the efficiency increases with size.
Operating crew size for ships, airplanes, trains, etc., does not increase in direct proportion to capacity. (Operating crew consists of pilots, co-pilots, navigators, etc. and does not include passenger service personnel.) Many aircraft models were significantly lengthened or "stretched" to increase payload.Many manufacturing facilities, especially those making bulk materials like chemicals, refined petroleum products, cement and paper, have labor requirements that are not greatly influenced by changes in plant capacity. This is because labor requirements of automated processes tend to be based on the complexity of the operation rather than production rate, and many manufacturing facilities have nearly the same basic number of processing steps and pieces of equipment, regardless of production capacity.
Karl Marx noted that large scale manufacturing allowed economical use of products that would otherwise be waste. Marx cited the chemical industry as an example, which today along with petrochemicals, remains highly dependent on turning various residual reactant streams into salable products. In the pulp and paper industry it is economical to burn bark and fine wood particles to produce process steam and to recover the spent pulping chemicals for conversion back to a usable form.
Large and more productive firms typically generate enough net revenues abroad to cover the fixed costs associated with exporting. However, the in the event of trade liberalization, resources will have to be reallocated toward the more productive firm, which raises the average productivity within the industry.Firms differ in their labor productivity and the quality of their goods produced. It is because of this, more efficient firms are more likely to generate more net income abroad and thus become exporters of their goods or services. There is a correlating relationship between a firms' total sales and underlying efficiency. Firms with higher productivity will always outperform a firm with lower productivity which will lead to lower sales. Through trade liberalization, organizations are able to drop their trade costs due to export growth. However, trade liberalization does not account for any tariff reduction or shipping logistics improvement. However, total economies of scale is based on the exporters individual frequency and size. So large-scale companies are more likely to have a lower cost per unit as opposed to small-scale companies. Likewise, high trade frequency companies are able to reduce their overall cost attributed per unit when compared to those of low-trade frequency companies.
Economies of scale is related to and can easily be confused with the theoretical economic notion of returns to scale. Where economies of scale refer to a firm's costs, returns to scale describe the relationship between inputs and outputs in a long-run (all inputs variable) production function. A production function has constant returns to scale if increasing all inputs by some proportion results in output increasing by that same proportion. Returns are decreasing if, say, doubling inputs results in less than double the output, and increasing if more than double the output. If a mathematical function is used to represent the production function, and if that production function is homogeneous, returns to scale are represented by the degree of homogeneity of the function. Homogeneous production functions with constant returns to scale are first degree homogeneous, increasing returns to scale are represented by degrees of homogeneity greater than one, and decreasing returns to scale by degrees of homogeneity less than one. If the firm is a perfect competitor in all input markets, and thus the per-unit prices of all its inputs are unaffected by how much of the inputs the firm purchases, then it can be shown that at a particular level of output, the firm has economies of scale if and only if it has increasing returns to scale, has diseconomies of scale if and only if it has decreasing returns to scale, and has neither economies nor diseconomies of scale if it has constant returns to scale. In this case, with perfect competition in the output market the long-run equilibrium will involve all firms operating at the minimum point of their long-run average cost curves (i.e., at the borderline between economies and diseconomies of scale). If, however, the firm is not a perfect competitor in the input markets, then the above conclusions are modified. For example, if there are increasing returns to scale in some range of output levels, but the firm is so big in one or more input markets that increasing its purchases of an input drives up the input's per-unit cost, then the firm could have diseconomies of scale in that range of output levels. Conversely, if the firm is able to get bulk discounts of an input, then it could have economies of scale in some range of output levels even if it has decreasing returns in production in that output range. In essence, returns to scale refer to the variation in the relationship between inputs and output. This relationship is therefore expressed in "physical" terms. But when talking about economies of scale, the relation taken into consideration is that between the average production cost and the dimension of scale. Economies of scale therefore are affected by variations in input prices. If input prices remain the same as their quantities purchased by the firm increase, the notions of increasing returns to scale and economies of scale can be considered equivalent. However, if input prices vary in relation to their quantities purchased by the company, it is necessary to distinguish between returns to scale and economies of scale. The concept of economies of scale is more general than that of returns to scale since it includes the possibility of changes in the price of inputs when the quantity purchased of inputs varies with changes in the scale of production.The literature assumed that due to the competitive nature of reverse auctions, and in order to compensate for lower prices and lower margins, suppliers seek higher volumes to maintain or increase the total revenue. Buyers, in turn, benefit from the lower transaction costs and economies of scale that result from larger volumes. In part as a result, numerous studies have indicated that the procurement volume must be sufficiently high to provide sufficient profits to attract enough suppliers, and provide buyers with enough savings to cover their additional costs.However, surprisingly enough, Shalev and Asbjornse found, in their research based on 139 reverse auctions conducted in the public sector by public sector buyers, that the higher auction volume, or economies of scale, did not lead to better success of the auction. They found that auction volume did not correlate with competition, nor with the number of bidders, suggesting that auction volume does not promote additional competition. They noted, however, that their data included a wide range of products, and the degree of competition in each market varied significantly, and offer that further research on this issue should be conducted to determine whether these findings remain the same when purchasing the same product for both small and high volumes. Keeping competitive factors constant, increasing auction volume may further increase competition.
The first systematic analysis of the advantages of the division of labour capable of generating economies of scale, both in a static and dynamic sense, was that contained in the famous First Book of Wealth of Nations (1776) by Adam Smith, generally considered the founder of political economy as an autonomous discipline. John Stuart Mill, in Chapter IX of the First Book of his Principles, referring to the work of Charles Babbage (On the economics of machines and manufactories), widely analyses the relationships between increasing returns and scale of production all inside the production unit.
In “Das Kapital” (1867), Karl Marx, referring to Charles Babbage, extensively analyses economies of scale and concludes that they are one of the factors underlying the ever-increasing concentration of capital. Marx observes that in the capitalist system the technical conditions of the work process are continuously revolutionized in order to increase the surplus by improving the productive force of work. According to Marx, with the cooperation of many workers brings about an economy in the use of the means of production and an increase in productivity due to the increase in the division of labour. Furthermore, the increase in the size of the machinery allows significant savings in construction, installation and operation costs. The tendency to exploit economies of scale entails a continuous increase in the volume of production which, in turn, requires a constant expansion of the size of the market. However, if the market does not expand at the same rate as production increases, overproduction crises can occur. According to Marx the capitalist system is therefore characterized by two tendencies, connected to economies of scale: towards a growing concentration and towards economic crises due to overproduction.In his 1844 Economic and Philosophic Manuscripts, Karl Marx observes that economies of scale have historically been associated with an increasing concentration of private wealth and have been used to justify such concentration. Marx points out that concentrated private ownership of large-scale economic enterprises is a historically contingent fact, and not essential to the nature of such enterprises. In the case of agriculture, for example, Marx calls attention to the sophistical nature of the arguments used to justify the system of concentrated ownership of land: As for large landed property, its defenders have always sophistically identified the economic advantages offered by large-scale agriculture with large-scale landed property, as if it were not precisely as a result of the abolition of property that this advantage, for one thing, received its greatest possible extension, and, for another, only then would be of social benefit.Instead of concentrated private ownership of land, Marx recommends that economies of scale should instead be realized by associations: Association, applied to land, shares the economic advantage of large-scale landed property, and first brings to realization the original tendency inherent in land-division, namely, equality. In the same way association re-establishes, now on a rational basis, no longer mediated by serfdom, overlordship and the silly mysticism of property, the intimate ties of man with the earth, for the earth ceases to be an object of huckstering, and through free labor and free enjoyment becomes once more a true personal property of man.
Alfred Marshall notes that "some, among whom Cournot himself", have considered "the internal economies [...] apparently without noticing that their premises lead inevitably to the conclusion that, whatever firm first gets a good start will obtain a monopoly of the whole business of its trade … ". Marshall believes that there are factors that limit this trend toward monopoly, and in particular: the death of the founder of the firm and the difficulty that the successors may have inherited his/her entrepreneurial skills; the difficulty of reaching new markets for one's goods; the growing difficulty of being able to adapt to changes in demand and to new techniques of production; The effects of external economies, that is the particular type of economies of scale connected not to the production scale of an individual production unit, but to that of an entire sector.
Piero Sraffa observes that Marshall, in order to justify the operation of the law of increasing returns without it coming into conflict with the hypothesis of free competition, tended to highlight the advantages of external economies linked to an increase in the production of an entire sector of activity. However, “those economies which are external from the point of view of the individual firm, but internal as regards the industry in its aggregate, constitute precisely the class which is most seldom to be met with”. “In any case - Sraffa notes – in so far as external economies of the kind in question exist, they are not linked to be called forth by small increases in production”, as required by the marginalist theory of price. Sraffa points out that, in the equilibrium theory of the individual industries, the presence of external economies cannot play an important role because this theory is based on marginal changes in the quantities produced. Sraffa concludes that, if the hypothesis of perfect competition is maintained, economies of scale should be excluded. He then suggests the possibility of abandoning the assumption of free competition to address the study of firms that have their own particular market. This stimulated a whole series of studies on the cases of imperfect competition in Cambridge. However, in the succeeding years Sraffa will follow a different path of research that will bring him to write and publish his main work Production of commodities by means of commodities (Sraffa, 1960). In this book, Sraffa determines relative prices assuming no changes in output, so that no question arises as to the variation or constancy of returns.
It has been noted that in many industrial sectors there are numerous companies with different sizes and organizational structures, despite the presence of significant economies of scale. This contradiction, between the empirical evidence and the logical incompatibility between economies of scale and competition, has been called the ‘Cournot dilemma’. As Mario Morroni observes, Cournot's dilemma appears to be unsolvable if we only consider the effects of economies of scale on the dimension of scale. If, on the other hand, the analysis is expanded, including the aspects concerning the development of knowledge and the organization of transactions, it is possible to conclude that economies of scale do not always lead to monopoly. In fact, the competitive advantages deriving from the development of the firm's capabilities and from the management of transactions with suppliers and customers can counterbalance those provided by the scale, thus counteracting the tendency towards a monopoly inherent in economies of scale. In other words, the heterogeneity of the organizational forms and of the size of the companies operating in a sector of activity can be determined by factors regarding the quality of the products, the production flexibility, the contractual methods, the learning opportunities, the heterogeneity of preferences of customers who express a differentiated demand with respect to the quality of the product, and assistance before and after the sale. Very different organizational forms can therefore co-exist in the same sector of activity, even in the presence of economies of scale, such as, for example, flexible production on a large scale, small-scale flexible production, mass production, industrial production based on rigid technologies associated with flexible organizational systems and traditional artisan production. The considerations regarding economies of scale are therefore important, but not sufficient to explain the size of the company and the market structure. It is also necessary to take into account the factors linked to the development of capabilities and the management of transaction costs.
External economies of scale tend to be a lot more prevalent than internal economies of scale. Through the external economies of scale, the entry of new firms benefits all existing competitors as it creates greater competition and also reduces the average cost for all firms as opposed to internal economies of scale which only allows benefits to the individual firm. Advantages that arise from external economies of scale include; Expansion of the industry. Benefits most or all of the firms within the industry. Can lead to rapid growth of local governments.
Firms are able to lower their average costs by buying their inputs required for the production process in bulk or from special wholesalers.
Firms might be able to lower their average costs by improving their management structure within the firm. This can range from hiring better skilled or more experienced managers from the industry.
Technological advancements will change the production process which will subsequently reduce the overall cost per unit.
Economies of Scale Definition by The Linux Information Project (LINFO) Economies of Scale by Economics Online
A beacon is an intentionally conspicuous device designed to attract attention to a specific location. A common example is the lighthouse, which provides a fixed location that can be used to navigate around obstacles or into port. More modern examples include a variety of radio beacons that can be read on radio direction finders in all weather, and radar transponders that appear on radar displays. Beacons can also be combined with semaphoric or other indicators to provide important information, such as the status of an airport, by the colour and rotational pattern of its airport beacon, or of pending weather as indicated on a weather beacon mounted at the top of a tall building or similar site. When used in such fashion, beacons can be considered a form of optical telegraphy.
Beacons help guide navigators to their destinations. Types of navigational beacons include radar reflectors, radio beacons, sonic and visual signals. Visual beacons range from small, single-pile structures to large lighthouses or light stations and can be located on land or on water. Lighted beacons are called lights; unlighted beacons are called daybeacons. Aerodrome beacons are used to indicate locations of airports and helipads. Handheld beacons are also employed in aircraft marshalling, and are used by the marshal to deliver instructions to the crew of aircraft as they move around an active airport, heliport or aircraft carrier.
Classically, beacons were fires lit at well-known locations on hills or high places, used either as lighthouses for navigation at sea, or for signalling over land that enemy troops were approaching, in order to alert defenses. As signals, beacons are an ancient form of optical telegraph and were part of a relay league. Systems of this kind have existed for centuries over much of the world. The ancient Greeks called them phryctoriae, while beacons figure on several occasions on the column of Trajan. In ancient China, sentinels on and near the Great Wall of China used a sophisticated system of daytime smoke and nighttime flame to send signals along long chains of beacon towers.Legend has it that Zhōu Yōu Wáng, king of the Western Zhou dynasty, played a trick multiple times in order to amuse his often melancholy concubine, ordering beacon towers lit to fool his Marquess and soldiers. But when enemies, led by Marquess of Shen really arrived at the wall, although the towers were lit, no defenders came, leading to Yōu's death and the collapse of the Western Zhou dynasty.In the 10th century, during the Arab–Byzantine wars, the Byzantine Empire used a beacon system to transmit messages from the border with the Abbasid Caliphate, across Anatolia to the imperial palace in the Byzantine capital, Constantinople. It was devised by Leo the Mathematician for Emperor Theophilos, but either abolished or radically curtailed by Theophilos' son and successor, Michael III. Beacons were later used in Greece as well, while the surviving parts of the beacon system in Anatolia seem to have been reactivated in the 12th century by Emperor Manuel I Komnenos.In Scandinavia many hill forts were part of beacon networks to warn against invading pillagers. In Finland, these beacons were called vainovalkeat, "persecution fires", or vartiotulet, "guard fires", and were used to warn Finn settlements of imminent raids by the Vikings. In Wales, the Brecon Beacons were named for beacons used to warn of approaching English raiders. In England, the most famous examples are the beacons used in Elizabethan England to warn of the approaching Spanish Armada. Many hills in England were named Beacon Hill after such beacons. In England the authority to erect beacons originally lay with the King and later was deligated to the Lord High Admiral. The money due for the maintenance of beacons was called Beaconagium and was levied by the sheriff of each county. In the Scottish borders country, a system of beacon fires was at one time established to warn of incursions by the English. Hume and Eggerstone castles and Soltra Edge were part of this network.In Spain, the border of Granada in the territory of the Crown of Castile had a complex beacon network to warn against Moorish raiders and military campaigns.
Vehicular beacons are rotating or flashing lights affixed to the top of a vehicle to attract the attention of surrounding vehicles and pedestrians. Emergency vehicles such as fire engines, ambulances, police cars, tow trucks, construction vehicles, and snow-removal vehicles carry beacon lights. The color of the lamps varies by jurisdiction; typical colors are blue and/or red for police, fire, and medical-emergency vehicles; amber for hazards (slow-moving vehicles, wide loads, tow trucks, security personnel, construction vehicles, etc.); green for volunteer firefighters or for medical personnel, and violet for funerary vehicles. Beacons may be constructed with halogen bulbs similar to those used in vehicle headlamps, xenon flashtubes, or LEDs. Incandescent and xenon light sources require the vehicle's engine to continue running to ensure that the battery is not depleted when the lights are used for a prolonged period. The low power consumption of LEDs allows the vehicle's engine to remain turned off while the lights operate nodes.
Beacons and bonfires are also used to mark occasions and celebrate events. Beacons have also allegedly been abused by shipwreckers. An illicit fire at a wrong position would be used to direct a ship against shoals or beaches, so that its cargo could be looted after the ship sank or ran aground. There are, however, no historically substantiated occurrences of such intentional shipwrecking. In wireless networks, a beacon is a type of frame which is sent by the access point (or WiFi router) to indicate that it is on. Bluetooth based beacons periodically send out a data packet and this could be used by software to identify the beacon location. This is typically used by indoor navigation and positioning applications.Beaconing is the process that allows a network to self-repair network problems. The stations on the network notify the other stations on the ring when they are not receiving the transmissions. Beaconing is used in Token ring and FDDI networks.
In Aeschylus' tragedy Agamemnon, a chain of eight beacons manned by so-called lampadóphoroi inform Clytemnestra in Argos, within a single night's time, that Troy has just fallen under her husband king Agamemnon's control, after a famous ten years siege. In J. R. R. Tolkien's high fantasy novel, The Lord of the Rings, a series of beacons alerts the entire realm of Gondor when the kingdom is under attack. These beacon posts were manned by messengers who would carry word of their lighting to either Rohan or Belfalas. In Peter Jackson's film adaptation of the novel, the beacons serve as a connection between the two realms of Rohan and Gondor, alerting one another directly when they require military aid, as opposed to relying on messengers as in the novel.
Beacons are sometimes used in retail to send digital coupons or invites to customers passing by.
An infrared beacon (IR beacon) transmits a modulated light beam in the infrared spectrum, which can be identified easily and positively. A line of sight clear of obstacles between the transmitter and the receiver is essential. IR beacons have a number of applications in robotics and in Combat Identification (CID). Infrared beacons are the key infrastructure for the Universal Traffic Management System (UTMS) in Japan. They perform two-way communication with travelling vehicles based on highly directional infrared communication technology and have a vehicle detecting capability to provide more accurate traffic information.A contemporary military use of an Infrared beacon is reported in Operation Acid Gambit.
A sonar beacon is an underwater device which transmits sonic or ultrasonic signals for the purpose of providing bearing information. The most common type is that of a rugged watertight sonar transmitter attached to a submarine and capable of operating independently of the electrical system of the boat. It can be used in cases of emergencies to guide salvage vessels to the location of a disabled submarine.
Aerodrome beacon Beacon mode service Beacon School Belisha beacon Emergency locator beacon Emergency position-indicating radiobeacon station (ELTs, PLBs & EPIRBs) iBeacon Lantern Leading lights Lighthouse of Alexandria Milestone/Kilometric point Polaris Strobe beacon Time ball Trail blazing Warning light (disambiguation) Weather beacon Web beacon
Francis Bacon, 1st Viscount St Alban, (; 22 January 1561 – 9 April 1626), also known as Lord Verulam, was an English philosopher and statesman who served as Attorney General and as Lord Chancellor of England. His works are credited with developing the scientific method and remained influential through the scientific revolution.Bacon has been called the father of empiricism. His works argued for the possibility of scientific knowledge based only upon inductive reasoning and careful observation of events in nature. Most importantly, he argued science could be achieved by use of a sceptical and methodical approach whereby scientists aim to avoid misleading themselves. Although his most specific proposals about such a method, the Baconian method, did not have a long-lasting influence, the general idea of the importance and possibility of a sceptical methodology makes Bacon the father of the scientific method. This method was a new rhetorical and theoretical framework for science, the practical details of which are still central in debates about science and methodology. Francis Bacon was a patron of libraries and developed a functional system for the cataloguing of books by dividing them into three categories—history, poetry, and philosophy—which could further be divided into more specific subjects and subheadings. Bacon was educated at Trinity College, Cambridge, where he rigorously followed the medieval curriculum, largely in Latin. Bacon was the first recipient of the Queen's counsel designation, which was conferred in 1597 when Elizabeth I of England reserved Bacon as her legal advisor. After the accession of James VI and I in 1603, Bacon was knighted. He was later created Baron Verulam in 1618 and Viscount St. Alban in 1621.Because he had no heirs, both titles became extinct upon his death in 1626, at 65 years. Bacon died of pneumonia, with one account by John Aubrey stating that he had contracted the condition while studying the effects of freezing on the preservation of meat. He is buried at St Michael's Church, St Albans, Hertfordshire.
Francis Bacon was born on 22 January 1561 at York House near the Strand in London, the son of Sir Nicholas Bacon (Lord Keeper of the Great Seal) by his second wife, Anne (Cooke) Bacon, the daughter of the noted Renaissance humanist Anthony Cooke. His mother's sister was married to William Cecil, 1st Baron Burghley, making Burghley Bacon's uncle.Biographers believe that Bacon was educated at home in his early years owing to poor health, which would plague him throughout his life. He received tuition from John Walsall, a graduate of Oxford with a strong leaning toward Puritanism. He went up to Trinity College at the University of Cambridge on 5 April 1573 at the age of 12, living for three years there, together with his older brother Anthony Bacon under the personal tutelage of Dr John Whitgift, future Archbishop of Canterbury. Bacon's education was conducted largely in Latin and followed the medieval curriculum. It was at Cambridge that Bacon first met Queen Elizabeth, who was impressed by his precocious intellect, and was accustomed to calling him "The young lord keeper".His studies brought him to the belief that the methods and results of science as then practised were erroneous. His reverence for Aristotle conflicted with his rejection of Aristotelian philosophy, which seemed to him barren, disputatious and wrong in its objectives. On 27 June 1576, he and Anthony entered de societate magistrorum at Gray's Inn. A few months later, Francis went abroad with Sir Amias Paulet, the English ambassador at Paris, while Anthony continued his studies at home. The state of government and society in France under Henry III afforded him valuable political instruction. For the next three years he visited Blois, Poitiers, Tours, Italy, and Spain. There is no evidence that he studied at the University of Poitiers. During his travels, Bacon studied language, statecraft, and civil law while performing routine diplomatic tasks. On at least one occasion he delivered diplomatic letters to England for Walsingham, Burghley, and Leicester, as well as for the queen.The sudden death of his father in February 1579 prompted Bacon to return to England. Sir Nicholas had laid up a considerable sum of money to purchase an estate for his youngest son, but he died before doing so, and Francis was left with only a fifth of that money. Having borrowed money, Bacon got into debt. To support himself, he took up his residence in law at Gray's Inn in 1579, his income being supplemented by a grant from his mother Lady Anne of the manor of Marks near Romford in Essex, which generated a rent of £46.
Bacon stated that he had three goals: to uncover truth, to serve his country, and to serve his church. He sought to further these ends by seeking a prestigious post. In 1580, through his uncle, Lord Burghley, he applied for a post at court that might enable him to pursue a life of learning, but his application failed. For two years he worked quietly at Gray's Inn, until he was admitted as an outer barrister in 1582.His parliamentary career began when he was elected MP for Bossiney, Cornwall, in a by-election in 1581. In 1584 he took his seat in Parliament for Melcombe in Dorset, and in 1586 for Taunton. At this time, he began to write on the condition of parties in the church, as well as on the topic of philosophical reform in the lost tract Temporis Partus Maximus. Yet he failed to gain a position that he thought would lead him to success. He showed signs of sympathy to Puritanism, attending the sermons of the Puritan chaplain of Gray's Inn and accompanying his mother to the Temple Church to hear Walter Travers. This led to the publication of his earliest surviving tract, which criticized the English church's suppression of the Puritan clergy. In the Parliament of 1586, he openly urged execution for the Catholic Mary, Queen of Scots. About this time, he again approached his powerful uncle for help; this move was followed by his rapid progress at the bar. He became a bencher in 1586 and was elected a Reader in 1587, delivering his first set of lectures in Lent the following year. In 1589, he received the valuable appointment of reversion to the Clerkship of the Star Chamber, although he did not formally take office until 1608; the post was worth £1,600 a year.In 1588 he became MP for Liverpool and then for Middlesex in 1593. He later sat three times for Ipswich (1597, 1601, 1604) and once for Cambridge University (1614).He became known as a liberal-minded reformer, eager to amend and simplify the law. Though a friend of the crown, he opposed feudal privileges and dictatorial powers. He spoke against religious persecution. He struck at the House of Lords in its usurpation of the Money Bills. He advocated for the union of England and Scotland, which made him a significant influence toward the consolidation of the United Kingdom; and he later would advocate for the integration of Ireland into the Union. Closer constitutional ties, he believed, would bring greater peace and strength to these countries.
Bacon soon became acquainted with Robert Devereux, 2nd Earl of Essex, Queen Elizabeth's favorite. By 1591 he acted as the earl's confidential adviser.In 1592 he was commissioned to write a tract in response to the Jesuit Robert Parson's anti-government polemic, which he titled Certain observations made upon a libel, identifying England with the ideals of democratic Athens against the belligerence of Spain.Bacon took his third parliamentary seat for Middlesex when in February 1593 Elizabeth summoned Parliament to investigate a Roman Catholic plot against her. Bacon's opposition to a bill that would levy triple subsidies in half the usual time offended the Queen: opponents accused him of seeking popularity, and for a time the Court excluded him from favour. When the office of Attorney General fell vacant in 1594, Lord Essex's influence was not enough to secure the position for Bacon and it was given to Sir Edward Coke. Likewise, Bacon failed to secure the lesser office of Solicitor General in 1595, the Queen pointedly snubbing him by appointing Sir Thomas Fleming instead. To console him for these disappointments, Essex presented him with a property at Twickenham, which Bacon subsequently sold for £1,800.In 1597 Bacon became the first Queen's Counsel designate, when Queen Elizabeth reserved him as her legal counsel. In 1597, he was also given a patent, giving him precedence at the Bar. Despite his designations, he was unable to gain the status and notoriety of others. In a plan to revive his position he unsuccessfully courted the wealthy young widow Lady Elizabeth Hatton. His courtship failed after she broke off their relationship upon accepting marriage to Sir Edward Coke, a further spark of enmity between the men. In 1598 Bacon was arrested for debt. Afterward, however, his standing in the Queen's eyes improved. Gradually, Bacon earned the standing of one of the learned counsels. His relationship with the Queen further improved when he severed ties with Essex—a shrewd move, as Essex would be executed for treason in 1601.With others, Bacon was appointed to investigate the charges against Essex. A number of Essex's followers confessed that Essex had planned a rebellion against the Queen. Bacon was subsequently a part of the legal team headed by the Attorney General Sir Edward Coke at Essex's treason trial. After the execution, the Queen ordered Bacon to write the official government account of the trial, which was later published as A DECLARATION of the Practices and Treasons attempted and committed by Robert late Earle of Essex and his Complices, against her Majestie and her Kingdoms ... after Bacon's first draft was heavily edited by the Queen and her ministers.According to his personal secretary and chaplain, William Rawley, as a judge Bacon was always tender-hearted, "looking upon the examples with the eye of severity, but upon the person with the eye of pity and compassion". And also that "he was free from malice", "no revenger of injuries", and "no defamer of any man".
The succession of James I brought Bacon into greater favour. He was knighted in 1603. In another shrewd move, Bacon wrote his Apologies in defense of his proceedings in the case of Essex, as Essex had favoured James to succeed to the throne. The following year, during the course of the uneventful first parliament session, Bacon married Alice Barnham. In June 1607 he was at last rewarded with the office of solicitor general. The following year, he began working as the Clerkship of the Star Chamber. Despite a generous income, old debts still could not be paid. He sought further promotion and wealth by supporting King James and his arbitrary policies. In 1610 the fourth session of James's first parliament met. Despite Bacon's advice to him, James and the Commons found themselves at odds over royal prerogatives and the king's embarrassing extravagance. The House was finally dissolved in February 1611. Throughout this period Bacon managed to stay in the favor of the king while retaining the confidence of the Commons. In 1613 Bacon was finally appointed attorney general, after advising the king to shuffle judicial appointments. As attorney general, Bacon, by his zealous efforts—which included torture—to obtain the conviction of Edmund Peacham for treason, raised legal controversies of high constitutional importance; and successfully prosecuted Robert Carr, 1st Earl of Somerset, and his wife, Frances Howard, Countess of Somerset, for murder in 1616. The so-called Prince's Parliament of April 1614 objected to Bacon's presence in the seat for Cambridge and to the various royal plans that Bacon had supported. Although he was allowed to stay, parliament passed a law that forbade the attorney general to sit in parliament. His influence over the king had evidently inspired resentment or apprehension in many of his peers. Bacon, however, continued to receive the King's favour, which led to his appointment in March 1617 as temporary Regent of England (for a period of a month), and in 1618 as Lord Chancellor. On 12 July 1618 the king created Bacon Baron Verulam, of Verulam, in the Peerage of England; he then became known as Francis, Lord Verulam.Bacon continued to use his influence with the king to mediate between the throne and Parliament, and in this capacity he was further elevated in the same peerage, as Viscount St Alban, on 27 January 1621.
Bacon was a devout Anglican. He believed that philosophy and the natural world must be studied inductively, but argued that we can only study arguments for the existence of God. Information on his attributes (such as nature, action, and purposes) can only come from special revelation. But Bacon also held that knowledge was cumulative, that study encompassed more than a simple preservation of the past. "Knowledge is the rich storehouse for the glory of the Creator and the relief of man's estate," he wrote. In his Essays, he affirms that "a little philosophy inclineth man's mind to atheism, but depth in philosophy bringeth men's minds about to religion."Bacon's idea of idols of the mind may have self-consciously represented an attempt to Christianize science at the same time as developing a new, reliable scientific method; Bacon gave worship of Neptune as an example of the idola tribus fallacy, hinting at the religious dimensions of his critique of the idols.
When he was 36, Bacon courted Elizabeth Hatton, a young widow of 20. Reportedly, she broke off their relationship upon accepting marriage to a wealthier man, Bacon's rival, Sir Edward Coke. Years later, Bacon still wrote of his regret that the marriage to Hatton had not taken place.At the age of 45, Bacon married Alice Barnham, the 14-year-old daughter of a well-connected London alderman and MP. Bacon wrote two sonnets proclaiming his love for Alice. The first was written during his courtship and the second on his wedding day, 10 May 1606. When Bacon was appointed lord chancellor, "by special Warrant of the King", Lady Bacon was given precedence over all other Court ladies. Bacon's personal secretary and chaplain, William Rawley, wrote in his biography of Bacon that his marriage was one of "much conjugal love and respect", mentioning a robe of honor that he gave to Alice and which "she wore unto her dying day, being twenty years and more after his death". However, an increasing number of reports circulated about friction in the marriage, with speculation that this may have been due to Alice's making do with less money than she had once been accustomed to. It was said that she was strongly interested in fame and fortune, and when household finances dwindled, she complained bitterly. Bunten wrote in her Life of Alice Barnham that, upon their descent into debt, she went on trips to ask for financial favours and assistance from their circle of friends. Bacon disinherited her upon discovering her secret romantic relationship with Sir John Underhill. He subsequently rewrote his will, which had previously been very generous—leaving her lands, goods, and income—and instead revoked it all.
Several authors believe that, despite his marriage, Bacon was primarily attracted to men. Forker, for example, has explored the "historically documentable sexual preferences" of both Francis Bacon and King James I and concluded they were both orientated to "masculine love", a contemporary term that "seems to have been used exclusively to refer to the sexual preference of men for members of their own gender."The well-connected antiquary John Aubrey noted in his Brief Lives concerning Bacon, "He was a Pederast. His Ganimeds and Favourites tooke Bribes". ("Pederast" in Renaissance diction meant generally "homosexual" rather than specifically a lover of minors; "ganimed" derives from the mythical prince abducted by Zeus to be his cup-bearer and bed warmer.) The Jacobean antiquarian, Sir Simonds D'Ewes (Bacon's fellow Member of Parliament) implied there had been a question of bringing him to trial for buggery, which his brother Anthony Bacon had also been charged with.In his Autobiography and Correspondence, in the diary entry for 3 May 1621, the date of Bacon's censure by Parliament, D'Ewes describes Bacon's love for his Welsh serving-men, in particular Godrick, a "very effeminate-faced youth" whom he calls "his catamite and bedfellow".This conclusion has been disputed by others, who point to lack of consistent evidence, and consider the sources to be more open to interpretation. Publicly, at least, Bacon distanced himself from the idea of homosexuality. In his New Atlantis, he described his utopian island as being "the chastest nation under heaven", and "as for masculine love, they have no touch of it".
Francis Bacon's philosophy is displayed in the vast and varied writings he left, which might be divided into three great branches: Scientific works – in which his ideas for a universal reform of knowledge into scientific methodology and the improvement of mankind's state using the Scientific method are presented. Religious and literary works – in which he presents his moral philosophy and theological meditations. Juridical works – in which his reforms in English Law are proposed.
Bacon's seminal work Novum Organum was influential in the 1630s and 1650s among scholars, in particular Sir Thomas Browne, who in his encyclopedia Pseudodoxia Epidemica (1646–72) frequently adheres to a Baconian approach to his scientific enquiries. This book entails the basis of the Scientific Method as a means of observation and induction. According to Francis Bacon, learning and knowledge all derive from the basis of inductive reasoning. Through his belief of experimental encounters, he theorized that all the knowledge that was necessary to fully understand a concept could be attainable because of induction. In order to get to the point of an inductive conclusion, one must consider the importance of observing the particulars (specific parts of nature). "Once these particulars have been gathered together, the interpretation of Nature proceeds by sorting them into a formal arrangement so that they may be presented to the understanding." Experimentation is essential to discovering the truths of Nature. When an experiment happens, parts of the tested hypothesis are started to be pieced together, forming a result and conclusion. Through this conclusion of particulars, an understanding of Nature can be formed. Now that an understanding of Nature has been arrived at, an inductive conclusion can be drawn. "For no one successfully investigates the nature of a thing in the thing itself; the inquiry must be enlarged to things that have more in common with it."Francis Bacon explains how we come to this understanding and knowledge because of this process in comprehending the complexities of nature. "Bacon sees nature as an extremely subtle complexity, which affords all the energy of the natural philosopher to disclose her secrets." Bacon described the evidence and proof revealed through taking a specific example from nature and expanding that example into a general, substantial claim of nature. Once we understand the particulars in nature, we can learn more about it and become surer of things occurring in nature, gaining knowledge and obtaining new information all the while. "It is nothing less than a revival of Bacon’s supremely confident belief that inductive methods can provide us with ultimate and infallible answers concerning the laws and nature of the universe." Bacon states that when we come to understand parts of nature, we can eventually understand nature better as a whole because of induction. Because of this, Bacon concludes that all learning and knowledge must be drawn from inductive reasoning. During the Restoration, Bacon was commonly invoked as a guiding spirit of the Royal Society founded under Charles II in 1660. During the 18th-century French Enlightenment, Bacon's non-metaphysical approach to science became more influential than the dualism of his French contemporary Descartes, and was associated with criticism of the Ancien Régime. In 1733 Voltaire introduced him to a French audience as the "father" of the scientific method, an understanding which had become widespread by the 1750s. In the 19th century his emphasis on induction was revived and developed by William Whewell, among others. He has been reputed as the "Father of Experimental Philosophy".He also wrote a long treatise on Medicine, History of Life and Death, with natural and experimental observations for the prolongation of life. One of his biographers, the historian William Hepworth Dixon, states: "Bacon's influence in the modern world is so great that every man who rides in a train, sends a telegram, follows a steam plough, sits in an easy chair, crosses the channel or the Atlantic, eats a good dinner, enjoys a beautiful garden, or undergoes a painless surgical operation, owes him something."In 1902 Hugo von Hofmannsthal published a fictional letter, known as The Lord Chandos Letter, addressed to Bacon and dated 1603, about a writer who is experiencing a crisis of language. Although Bacon’s works are extremely instrumental, his argument falls short because observation and the scientific method are not completely necessary for everything. Bacon takes the inductive method too far, as seen through one of his aphorisms which says, "Man, being the servant and interpreter of Nature, can do and understand so much only as he has observed in fact or in thought of the course of nature: Beyond this he neither knows anything nor can do anything." As humans, we are capable of more than pure observation and can use deduction to form theories. In fact, we must use deduction because Bacon’s pure inductive method is incomplete. Thus, it is not Bacon’s ideas alone that form the scientific method we use today. If that were the case, we would not be able to fully understand the observations we make and deduce new theories. Author Ernst Mayr states, "Inductivism had a great vogue in the eighteenth and early nineteenth centuries, but it is now clear that a purely inductive approach is quite sterile." Mayr points out that an inductive approach on its own just doesn’t work. One could observe an experiment multiple times, but still be unable to make generalizations and correctly understand the knowledge. Bacon’s inductive method is beneficial, but incomplete and leaves gaps. However, when combined with the ideas of Descartes, the gaps are filled in Bacon’s inductive method. The "anticipation of nature" as Bacon puts it, connects the information gained from observation, enabling hypotheses and theories to become more effective. Bacon’s inductive ideas now have more value. Jurgen Klein, who researched Bacon and analyzed his works, says, "The inductive method helps the human mind to find a way to ascertain truthful knowledge." Klein shows the value that Bacon’s method truly brings. It is not a value that stands on its own, for it has holes, but it is a value that supports and strengthens. The inductive method can be seen as a tool used alongside other ideas, such as deduction, which now creates a method which is most effective and used today: the scientific method. The inductive method is more prominent in the scientific method than other ideas, which leads to misconception, but the takeaway is that it has supporting ideas. Francis Bacon’s scientific method is extremely influential, but has been developed for its own good, as all great ideas are.
Bacon played a leading role in establishing the British colonies in North America, especially in Virginia, the Carolinas and Newfoundland in northeastern Canada. His government report on "The Virginia Colony" was submitted in 1609. In 1610 Bacon and his associates received a charter from the king to form the Tresurer and the Companye of Adventurers and planter of the Cittye of London and Bristoll for the Collonye or plantacon in Newfoundland, and sent John Guy to found a colony there. Thomas Jefferson, the third President of the United States, wrote: "Bacon, Locke and Newton. I consider them as the three greatest men that have ever lived, without any exception, and as having laid the foundation of those superstructures which have been raised in the Physical and Moral sciences".In 1910 Newfoundland issued a postage stamp to commemorate Bacon's role in establishing the colony. The stamp describes Bacon as "the guiding spirit in Colonization Schemes in 1610". Moreover, some scholars believe he was largely responsible for the drafting, in 1609 and 1612, of two charters of government for the Virginia Colony. William Hepworth Dixon considered that Bacon's name could be included in the list of Founders of the United States.
Although few of his proposals for law reform were adopted during his lifetime, Bacon's legal legacy was considered by the magazine New Scientist in 1961 as having influenced the drafting of the Napoleonic Code as well as the law reforms introduced by 19th-century British Prime Minister Sir Robert Peel. The historian William Hepworth Dixon referred to the Napoleonic Code as "the sole embodiment of Bacon's thought", saying that Bacon's legal work "has had more success abroad than it has found at home", and that in France "it has blossomed and come into fruit".Harvey Wheeler attributed to Bacon, in Francis Bacon's Verulamium—the Common Law Template of The Modern in English Science and Culture, the creation of these distinguishing features of the modern common law system: using cases as repositories of evidence about the "unwritten law"; determining the relevance of precedents by exclusionary principles of evidence and logic; treating opposing legal briefs as adversarial hypotheses about the application of the "unwritten law" to a new set of facts. As late as the 18th century some juries still declared the law rather than the facts, but already before the end of the 17th century Sir Matthew Hale explained modern common law adjudication procedure and acknowledged Bacon as the inventor of the process of discovering unwritten laws from the evidences of their applications. The method combined empiricism and inductivism in a new way that was to imprint its signature on many of the distinctive features of modern English society. Paul H. Kocher writes that Bacon is considered by some jurists to be the father of modern Jurisprudence.Bacon is commemorated with a statue in Gray's Inn, South Square in London where he received his legal training, and where he was elected Treasurer of the Inn in 1608.More recent scholarship on Bacon's jurisprudence has focused on his advocating torture as a legal recourse for the crown. Bacon himself was not a stranger to the torture chamber; in his various legal capacities in both Elizabeth I's and James I's reigns, Bacon was listed as a commissioner on five torture warrants. In 1613(?), in a letter addressed to King James I on the question of torture's place within English law, Bacon identifies the scope of torture as a means to further the investigation of threats to the state: "In the cases of treasons, torture is used for discovery, and not for evidence." For Bacon, torture was not a punitive measure, an intended form of state repression, but instead offered a modus operandi for the government agent tasked with uncovering acts of treason.
Francis Bacon developed the idea that a classification of knowledge must be universal while handling all possible resources. In his progressive view, humanity would be better if the access to educational resources were provided to the public, hence the need to organise it. His approach to learning reshaped the Western view of knowledge theory from an individual to a social interest. The original classification proposed by Bacon organised all types of knowledge in three general groups: history, poetry, and philosophy. He did that based on his understanding of how information is processed: memory, imagination, and reason, respectively. His methodical approach to the categorization of knowledge goes hand-in-hand with his principles of scientific methods. Bacon’s writings were the starting point for William Torrey Harris classification system for libraries in the United States by the second half of the 1800s.
The Baconian hypothesis of Shakespearean authorship, first proposed in the mid-19th century, contends that Francis Bacon wrote some or even all of the plays conventionally attributed to William Shakespeare.
Francis Bacon often gathered with the men at Gray's Inn to discuss politics and philosophy, and to try out various theatrical scenes that he admitted writing. Bacon's alleged connection to the Rosicrucians and the Freemasons has been widely discussed by authors and scholars in many books. However, others, including Daphne du Maurier in her biography of Bacon, have argued that there is no substantive evidence to support claims of involvement with the Rosicrucians. Frances Yates does not make the claim that Bacon was a Rosicrucian, but presents evidence that he was nevertheless involved in some of the more closed intellectual movements of his day. She argues that Bacon's movement for the advancement of learning was closely connected with the German Rosicrucian movement, while Bacon's New Atlantis portrays a land ruled by Rosicrucians. He apparently saw his own movement for the advancement of learning to be in conformity with Rosicrucian ideals. The link between Bacon's work and the Rosicrucians' ideals which Yates allegedly found was the conformity of the purposes expressed by the Rosicrucian Manifestos and Bacon's plan of a "Great Instauration", for the two were calling for a reformation of both "divine and human understanding", as well as both had in view the purpose of mankind's return to the "state before the Fall".Another major link is said to be the resemblance between Bacon's New Atlantis and the German Rosicrucian Johann Valentin Andreae's Description of the Republic of Christianopolis (1619). Andreae describes a utopic island in which Christian theosophy and applied science ruled, and in which the spiritual fulfilment and intellectual activity constituted the primary goals of each individual, the scientific pursuits being the highest intellectual calling—linked to the achievement of spiritual perfection. Andreae's island also depicts a great advancement in technology, with many industries separated in different zones which supplied the population's needs—which shows great resemblance to Bacon's scientific methods and purposes.While rejecting occult conspiracy theories surrounding Bacon and the claim Bacon personally identified as a Rosicrucian, intellectual historian Paolo Rossi has argued for an occult influence on Bacon's scientific and religious writing. He argues that Bacon was familiar with early modern alchemical texts and that Bacon's ideas about the application of science had roots in Renaissance magical ideas about science and magic facilitating humanity's domination of nature. Rossi further interprets Bacon's search for hidden meanings in myth and fables in such texts as The Wisdom of the Ancients as succeeding earlier occultist and Neoplatonic attempts to locate hidden wisdom in pre-Christian myths. As indicated by the title of his study, however, Rossi claims Bacon ultimately rejected the philosophical foundations of occultism as he came to develop a form of modern science.Rossi's analysis and claims have been extended by Jason Josephson-Storm in his study, The Myth of Disenchantment. Josephson-Storm also rejects conspiracy theories surrounding Bacon and does not make the claim that Bacon was an active Rosicrucian. However, he argues that Bacon's "rejection" of magic actually constituted an attempt to purify magic of Catholic, demonic, and esoteric influences and to establish magic as a field of study and application paralleling Bacon's vision of science. Furthermore, Josephson-Storm argues that Bacon drew on magical ideas when developing his experimental method. Josephson-Storm finds evidence that Bacon considered nature a living entity, populated by spirits, and argues Bacon's views on the human domination and application of nature actually depend on his spiritualism and personification of nature.The Rosicrucian organization AMORC claims that Bacon was the "Imperator" (leader) of the Rosicrucian Order in both England and the European continent, and would have directed it during his lifetime.Bacon's influence can also be seen on a variety of religious and spiritual authors, and on groups that have utilized his writings in their own belief systems.
Some of the more notable works by Bacon are: Essays 1st edition with 10 essays (1597) 2nd edition with 38 essays (1612) 3rd/final edition with 58 essays (1625) The Advancement and Proficience of Learning Divine and Human (1605) Instauratio magna (The Great Instauration) (1620): a multi-part work including Distributio operis (Plan of the Work); Novum Organum (New Engine); Parasceve ad historiam naturalem (Preparatory for Natural History) and Catalogus historiarum particularium (Catalogue of Particular Histories) De augmentis scientiarum (1623) – an enlargement of The Advancement of Learning translated into Latin New Atlantis (1626)
Cestui que (defence and comment on Chudleigh's Case) Romanticism and Bacon
Klein, Juergen. "Francis Bacon". In Zalta, Edward N. (ed.). Stanford Encyclopedia of Philosophy. "Francis Bacon". Internet Encyclopedia of Philosophy. Works by Francis Bacon at Project Gutenberg Works by or about Francis Bacon at Internet Archive Works by Francis Bacon at LibriVox (public domain audiobooks) "Archival material relating to Francis Bacon". UK National Archives. Contains the New Organon, slightly modified for easier reading Lord Macaulay's essay Lord Bacon (Edinburgh Review, 1837) [1] Francis Bacon of Verulam. Realistic Philosophy and its Age by Kuno Fischer, translated from the German by John Oxenford London 1857 Bacon by Thomas Fowler (1881) public domain at Internet Archive The Francis Bacon Society English translation of Hugo von Hofmannsthal's fictional The Lord Chandos Letter, addressed to Bacon The George Fabyan Collection at the Library of Congress is rich in the works of Francis Bacon. Francis Bacon Research Trust Sir Francis Bacon's New Advancement of Learning Montmorency, James E. G. (1913). "FRANCIS BACON". In Macdonell, John; Manson, Edward William Donoghue (eds.). Great Jurists of the World. London: John Murray. pp. 144–168. Retrieved 11 March 2019 – via Internet Archive. Letterbook and correspondence by Sir Francis Bacon at Columbia University. Rare Book & Manuscript Library.
Six Degrees of Kevin Bacon or Bacon's Law is a parlor game where players challenge each other to find the shortest path between an arbitrary actor and prolific actor Kevin Bacon, linked by films they have appeared in together. It rests on the assumption that anyone involved in the Hollywood film industry can be linked through their film roles to Bacon within six steps. The game's name is a reference to "six degrees of separation", a concept which posits that any two people on Earth are six or fewer acquaintance links apart. In 2007, Bacon started a charitable organization called SixDegrees.org. In 2020, Bacon started a podcast called The Last Degree of Kevin Bacon.
While at the University of Virginia, Brett Tjaden created the Oracle of Bacon. A previous version of this computer program used information on some 800,000 people from the Internet Movie Database (IMDb), while the current implementation uses data drawn from Wikipedia. The algorithm calculates "how good a center" an individual IMDb personality is, i.e. a weighted average of the degree of separation of all the people that link to that particular person. The site returns an average personality number, e.g. for Clint Eastwood, it returns an average "Clint Eastwood Number." From there the Oracle site posits "The Center of the Hollywood Universe" as being the person with the lowest average personality number. Kevin Bacon, as it turns out, is not the "Center of the Hollywood Universe" (i.e. the most linkable actor). In fact, Bacon does not even make the top 100 list of average personality numbers. While he is not the most linkable actor, this still signifies being a better center than more than 99% of the people who have ever appeared in a film. Since each actor's average personality number can change with each new film made, the center can and does shift. "Centers" have included Rod Steiger, Donald Sutherland, Eric Roberts, Dennis Hopper, Christopher Lee and Harvey Keitel. A 2020 study from Cardiff University also used further measures to determine the center of the movie universe. These included degree centrality, closeness centrality and betweenness centrality, which were implemented using the NetworkX Python Library. The study also looked at movies released in different decades and found that, between 2000 and 2020, the most central actors included Angelina Jolie, Brahmanandam, Samuel L. Jackson, and Ben Kingsley.
Inspired by the game, the British photographer Andy Gotts tried to reach Kevin Bacon through photographic links instead of film links. Gotts wrote to 300 actors asking to take their pictures, and received permission only from Joss Ackland. Ackland then suggested that Gotts photograph Greta Scacchi, with whom he had appeared in the film White Mischief. Gotts proceeded from there, asking each actor to refer him to one or more friends or colleagues. Eventually, Christian Slater referred him to Bacon. Gotts' photograph of Bacon completed the project, eight years after it began. Gotts published the photos in a book, Degrees (ISBN 0-9546843-6-2), with text by Alan Bates, Pierce Brosnan, and Bacon.
Small-world experiment Morphy Number, connections via chess games to Paul Morphy Shusaku number, equivalent in the Go world with Honinbo Shusaku
The Oracle of Bacon computes the Bacon number of any actor or actress from Wikipedia data. A previous implementation used IMDB data. Six Degrees of James A. Conrad A how-to demonstration for those wishing to compile their own "degrees of" list by a Hollywood author who is three degrees of Bacon. Cinema FreeNet Movie Connector finds links between stars, but can also use directors and producers. Filmlovr.com browse the extensive film library to find films to connect via their actors aka determine the Bacon number. Six Degrees of Lois Weisberg suggests that Bacon connects to many actors because he acts in many different kinds of roles and films.
Sosie Ruth Bacon (born March 15, 1992) is an American actress. Her first role was playing 10-year-old Emily in the movie Loverboy (2005), which was directed by her father, Kevin Bacon. James Duff, producer of The Closer, was compelled by Bacon's performance in Loverboy to suggest that she play the role of Deputy Chief Brenda Leigh Johnson's niece Charlie in the fifth season of the show. Although her parents were opposed to her being involved in acting, Bacon accepted the role and appeared in four episodes alongside her mother, who played the role of Chief Johnson. Bacon portrayed the character Skye Miller in the TV series 13 Reasons Why.
Sosie Bacon was born on March 15, 1992, to married actors Kevin Bacon and Kyra Sedgwick. Sedgwick gave birth shortly after filming Miss Rose White, and named her newborn after the movie's art director, Sosie Hublitz. Despite her parents' successful acting careers, Bacon was provided with a "fairly ordinary" upbringing, according to producer James Duff, and her parents were determined that she not follow them into acting. During the filming of The Closer, Bacon's mother would spend half of the year in Los Angeles, while Bacon would stay in Manhattan with her father and brother. Sedgwick credited this as leading to a closer bond between Bacon and her father.
Sosie Bacon on IMDb
Francis Bacon (28 October 1909 – 28 April 1992) was an Irish-born English figurative painter known for his raw, unsettling imagery. Focusing on the human form, his subjects included crucifixions, portraits of popes, self-portraits, and portraits of close friends, with abstracted figures sometimes isolated in geometrical structures. Rejecting various classifications of his work, Bacon claimed that he strove to render "the brutality of fact."Bacon said that he saw images "in series", and his work, which numbers c. 590 extant paintings along with many others he destroyed, typically focused on a single subject for sustained periods, often in triptych or diptych formats. His output can be broadly described as sequences or variations on single motifs; including the 1930s Picasso-influenced bio-morphs and Furies, the 1940s male heads isolated in rooms or geometric structures, the 1950s "screaming popes," the mid-to-late 1950s animals and lone figures, the early 1960s crucifixions, the mid-to-late 1960s portraits of friends, the 1970s self-portraits, and the cooler, more technical 1980s paintings. Bacon did not begin to paint until his late twenties, having drifted in the late 1920s and early 1930s as an interior decorator, bon vivant, and gambler. He said that his artistic career was delayed because he spent too long looking for subject matter that could sustain his interest. His breakthrough came with the 1944 triptych Three Studies for Figures at the Base of a Crucifixion, which sealed his reputation as a uniquely bleak chronicler of the human condition. From the mid-1960s he mainly produced portraits of friends and drinking companions, either as single, diptych or triptych panels. Following the suicide of his lover George Dyer in 1971 (memorialized in his Black Triptychs, and a number of posthumous portraits) his art became more sombre, inward-looking and preoccupied with the passage of time and death. The climax of his later period is marked the masterpieces Study for Self-Portrait (1982) and Study for a Self-Portrait—Triptych, 1985–86. Despite his existentialist and bleak outlook, Bacon was charismatic, articulate and well-read. A bon vivant, he spent his middle age eating, drinking and gambling in London's Soho with like-minded friends including Lucian Freud (although they fell out in the mid-1970s, for reasons neither ever explained), John Deakin, Muriel Belcher, Henrietta Moraes, Daniel Farson, Tom Baker and Jeffrey Bernard. After Dyer's suicide he largely distanced himself from this circle, and while still socially active and his passion for gambling and drinking continued, he settled into a platonic and somewhat fatherly relationship with his eventual heir, John Edwards. Since his death, Bacon's reputation has grown steadily, and his work is among the most acclaimed, expensive and sought-after on the art market. In the late 1990s a number of major works, previously assumed destroyed, including early 1950s popes and 1960s portraits, re-emerged to set record prices at auction.
Francis Bacon was born on 28 October 1909 in 63 Lower Baggot Street, Dublin. His father, Captain Anthony Edward Mortimer Bacon, known as Eddy, was born in Adelaide, South Australia, to an English father and an Australian mother. Eddie was a veteran of the Boer War, a racehorse trainer, and grandson of Anthony Bacon, who claimed descent from Sir Nicholas Bacon, elder half-brother of Sir Francis Bacon, the Elizabethan statesman, philosopher and essayist. Francis' mother, Christina Winifred Firth, known as Winnie, was heiress to a Sheffield steel business and coal mine. Bacon had an older brother, Harley, two younger sisters, Ianthe and Winifred, and a younger brother, Edward. Francis was raised by the family nanny, Jessie Lightfoot, from Cornwall, known as 'Nanny Lightfoot', a maternal figure who remained close to him until her death. During the early 1940s, he rented the ground floor of 7 Cromwell Place, South Kensington, John Everett Millais's old studio, and Nanny Lightfoot helped him install an illicit roulette wheel there, organised by Bacon and his friends, for their financial benefit. The family moved house often, moving between Ireland and England several times, leading to a sense of displacement which remained with Francis throughout his life. The family lived in Cannycourt House in County Kildare from 1911, later moving to Westbourne Terrace in London, close to where Bacon's father worked at the Territorial Force Records Office. They returned to Ireland after the First World War. Bacon lived with his maternal grandmother and step-grandfather, Winifred and Kerry Supple, at Farmleigh, Abbeyleix, County Laois, although the rest of the family again moved to Straffan Lodge near Naas, County Kildare. Bacon was shy as a child, and enjoyed dressing up. This, and his effeminate manner, angered his father. A story emerged in 1992 of his father having had Francis horsewhipped by their grooms. In 1924 his parents moved to Gloucestershire, first to Prescott House in Gotherington, then Linton Hall near the border with Herefordshire. At a fancy-dress party at the Firth family home, Cavendish Hall in Suffolk, Francis dressed as a flapper with an Eton crop, beaded dress, lipstick, high heels, and a long cigarette holder. In 1926, the family moved back to Straffan Lodge. His sister, Ianthe, twelve years his junior, recalled that Bacon made drawings of ladies with cloche hats and long cigarette holders. Later that year, Francis was thrown out of Straffan Lodge following an incident in which his father found him admiring himself in front of a large mirror wearing his mother's underwear.
The 1933 Crucifixion was his first painting to attract public attention, and was in part based on Pablo Picasso's The Three Dancers of 1925. It was not well received; disillusioned, he abandoned painting for nearly a decade, and suppressed his earlier works. He visited Paris in 1935 where he bought a secondhand book on anatomical diseases of the mouth containing high quality hand-coloured plates of both open mouths and oral interiors, which haunted and obsessed him for the remainder of his life. These and the scene with the nurse screaming on the Odessa steps from the Battleship Potemkin later became recurrent parts of Bacon's iconography, with the angularity of Eisenstein's images often combined with the thick red palette of his recently purchased medical tome. In the winter of 1935–36, Roland Penrose and Herbert Read, making a first selection for the International Surrealist Exhibition, visited his studio at 71 Royal Hospital Road, Chelsea saw "three or four large canvases including one with a grandfather clock", but found his work "insufficiently surreal to be included in the show". Bacon claimed Penrose told him "Mr. Bacon, don't you realise a lot has happened in painting since the Impressionists?" In 1936 or 1937 Bacon moved from 71 Royal Hospital Road to the top floor of 1 Glebe Place, Chelsea, which Eric Hall had rented. The following year, Patrick White moved to the top two floors of the building where De Maistre had his studio, on Eccleston Street and commissioned from Bacon, by now a friend, a writing desk (with wide drawers and a red linoleum top). Expressing one of his basic concerns from the late 1930s, Bacon said that his artistic career was delayed because he spent too long looking for subject matter that could sustain his interest.In January 1937, at Thomas Agnew and Sons, 43 Old Bond Street, London, Bacon exhibited in a group show, Young British Painters, which included Graham Sutherland and Roy De Maistre. Eric Hall organised the show. Four works by Bacon were shown: Figures in a Garden (1936), purchased by Diana Watson; Abstraction, and Abstraction from the Human Form, known from magazine photographs. They prefigure Three Studies for Figures at the Base of a Crucifixion (1944) in alternatively representing a tripod structure (Abstraction), bared teeth (Abstraction from the Human Form), and both being biomorphic in form. Seated Figure is lost. On 1 June 1940 Bacon's father died. Bacon was named sole Trustee/Executor of his father's will, which requested the funeral be as "private and simple as possible". Unfit for active wartime service, Francis volunteered for civil defence and worked full-time in the Air Raid Precautions (ARP) rescue service; the fine dust of bombed London worsened his asthma and he was discharged. At the height of the Blitz, Eric Hall rented a cottage for Bacon and himself at Bedales Lodge in Steep, near Petersfield, Hampshire. Figure Getting Out of a Car (ca. 1939/1940) was painted here but is known only from an early 1946 photograph taken by Peter Rose Pulham. The photograph was taken shortly before the canvas was painted over by Bacon and retitled Landscape with Car. An ancestor to the biomorphic form of the central panel of Three Studies for Figures at the Base of a Crucifixion (1944), the composition was suggested by a photograph of Hitler getting out of a car at one of the Nuremberg rallies. Bacon claims to have "copied the car and not much else".Bacon and Hall in 1943 took the ground floor of 7 Cromwell Place, South Kensington, formerly the house and studio of John Everett Millais. High-vaulted and north-lit, its roof was recently bombed – Bacon was able to adapt a large old billiard room at the back as his studio. Lightfoot, lacking an alternative location, slept on the kitchen table. They held illicit roulette parties, organised by Bacon with the assistance of Hall.
By 1944 Bacon had gained confidence. His Three Studies for Figures at the Base of a Crucifixion had summarised themes explored in his earlier paintings, including his examination of Picasso's biomorphs, his interpretations of the Crucifixion, and the Greek Furies. It is generally considered his first mature piece; he regarded his works before the triptych as irrelevant. The painting caused a sensation when exhibited in 1945 and established him as a foremost post-war painter. Remarking on the cultural significance of Three Studies, John Russell observed in 1971 that "there was painting in England before the Three Studies, and painting after them, and no one ... can confuse the two."Painting (1946) was shown in several group shows including in the British section of Exposition internationale d'art moderne (18 November – 28 December 1946) at the Musée National d'Art Moderne, for which Bacon travelled to Paris. Within a fortnight of the sale of Painting (1946) to the Hanover Gallery Bacon used the proceeds to decamp from London to Monte Carlo. After staying at a succession of hotels and flats, including the Hôtel de Ré, Bacon settled in a large villa, La Frontalière, in the hills above the town. Hall and Lightfoot would come to stay. Bacon spent much of the next few years in Monte Carlo apart from short visits to London. From Monte Carlo, Bacon wrote to Sutherland and Erica Brausen. His letters to Brausen show he painted there, but no paintings are known to survive. Bacon said he became "obsessed" with the Casino de Monte Carlo, where he would "spend whole days". Falling in debt from gambling here, he was unable to afford a new canvas. This compelled him to paint on the raw, unprimed side of his previous work, a practice he kept throughout his life.In 1948, Painting (1946) sold to Alfred Barr for the Museum of Modern Art (MoMA) in New York for £240. Bacon wrote to Sutherland asking that he apply fixative to the patches of pastel on Painting (1946) before it was shipped to New York. (The work is now too fragile to be moved from MoMA for exhibition elsewhere.) At least one visit to Paris in 1946 brought Bacon into more immediate contact with French postwar painting and with Left Bank ideas such as Existentialism. He had, by this time, embarked on his lifelong friendship with Isabel Rawsthorne, a painter closely involved with Giacometti and the Left Bank set. They shared many interests, including ethnography and classical literature.
In 1947, Sutherland introduced Bacon to Brausen, who represented Bacon for twelve years. Despite this, Bacon did not mount a one-man show in Brausen's Hanover Gallery until 1949. Bacon returned to London and Cromwell Place late in 1948. The following year Bacon exhibited his "Heads" series, most notable for Head VI, Bacon's first surviving engagement with Velázquez's Portrait of Pope Innocent X (three 'popes' were painted in Monte Carlo in 1946 but were destroyed). He kept an extensive inventory of images for source material, but preferred not to confront the major works in person; he viewed Portrait of Innocent X only once, much later in his life.
Bacon met George Dyer in 1963 at a pub, although a much-repeated myth claims their acquaintance started during the younger man's burglary into the artist's apartment. Dyer was about 30 years old, from London's East End. He came from a family steeped in crime, and had till then spent his life drifting between theft, detention and jail. Bacon's earlier relationships had been with older and tumultuous men. His first lover, Peter Lacy, tore up the artist's paintings, beat him in drunken rages, at times leaving him on streets half-conscious. Bacon was now the dominating personality; attracted to Dyer's vulnerability and trusting nature. Dyer was impressed by Bacon's self-confidence and success, and Bacon acted as a protector and father figure to the insecure younger man.Dyer was, like Bacon, a borderline alcoholic and similarly took obsessive care with his appearance. Pale-faced and a chain-smoker, Dyer typically confronted his daily hangovers by drinking again. His compact and athletic build belied a docile and inwardly tortured personality, although the art critic Michael Peppiatt describes him as having the air of a man who could "land a decisive punch". Their behaviours eventually overwhelmed their affair, and by 1970 Bacon was merely providing Dyer with enough money to stay more or less permanently drunk.As Bacon's work moved from the extreme subject matter of his early paintings to portraits of friends in the mid-1960s, Dyer became a dominating presence. Bacon's paintings emphasises Dyer's physicality, yet are uncharacteristically tender. More than any other of Bacon's close friends, Dyer came to feel inseparable from his portraits. The paintings gave him stature, a raison d'etre, and offered meaning to what Bacon described as Dyer's "brief interlude between life and death". Many critics have described Dyer's portraits as favourites, including Michel Leiris and Lawrence Gowing. Yet as Dyer's novelty diminished within Bacon's circle of sophisticated intellectuals, the younger man became increasingly bitter and ill at ease. Although Dyer welcomed the attention the paintings brought him, he did not pretend to understand or even like them. "All that money an' I fink they're reely 'orrible," he observed with choked pride.Dyer abandoned crime but descended into alcoholism. Bacon's money attracted hangers-on for benders around London's Soho. Withdrawn and reserved when sober, Dyer was highly animated and aggressive when drunk, and often attempted to "pull a Bacon" by buying large rounds and paying for expensive dinners for his wide circle. Dyer's erratic behaviour inevitably wore thin with his cronies, with Bacon, and with Bacon's friends. Most of Bacon's art world associates regarded Dyer as a nuisance – an intrusion into the world of high culture to which their Bacon belonged. Dyer reacted by becoming increasingly needy and dependent. By 1971, he was drinking alone and only in occasional contact with his former lover. In October 1971, Dyer joined Bacon in Paris for the opening of the artist's retrospective at the Grand Palais. The show was the high point of Bacon's career to date, and he was now described as Britain's "greatest living painter". Dyer was a desperate man, and although he was "allowed" to attend, he was well aware that he was slipping out of the picture. To draw Bacon's attention, he planted cannabis in his flat and phoned the police, and attempted suicide on a number of occasions. On the eve of the Paris exhibition, Bacon and Dyer shared a hotel room, but Bacon was forced to escape in disgust to the room of gallery employee Terry Danziger-Miles, as Dyer was entertaining an Arab rent boy with "smelly feet". When Bacon returned to his room the next morning, together with Danziger-Miles and Valerie Beston, they discovered Dyer in the bathroom dead, sat on the toilet. With the agreement of the hotel manager, the party agreed not to announce the death for two days.Bacon spent the following day surrounded by people eager to meet him. In mid-evening of the following day he was "informed" that Dyer had taken an overdose of barbiturates and was dead. Bacon continued with the retrospective and displayed powers of self-control "to which few of us could aspire", according to Russell. Bacon was deeply affected by the loss of Dyer, and had recently lost four other friends and his nanny. From this point, death haunted his life and work. Though outwardly stoic at the time, he was inwardly broken. He did not express his feelings to critics, but later admitted to friends that "daemons, disaster and loss" now stalked him as if his own version of the Eumenides (Greek for The Furies). Bacon spent the remainder of his stay in Paris attending to promotional activities and funeral arrangements. He returned to London later that week to comfort Dyer's family. During the funeral many of Dyer's friends, including hardened East-End criminals, broke down in tears. As the coffin was lowered into the grave one friend was overcome and screamed "you bloody fool!" Bacon remained stoic during the proceedings, but in the following months suffered an emotional and physical breakdown. Deeply affected, over the following two years he painted a number of single canvas portraits of Dyer, and the three highly regarded "Black Triptychs", each of which details moments immediately before and after Dyer's suicide.
While holidaying in Madrid in 1992, Bacon was admitted to the Handmaids of Maria, a private clinic, where he was cared for by Sister Mercedes. His chronic asthma, which had plagued him all his life, had developed into a more severe respiratory condition and he could not talk or breathe very well. He died of a heart attack on 28 April 1992. He bequeathed his estate (then valued at £11 million) to his heir and sole legatee John Edwards, and to Brian Clarke, a friend of Bacon and Edwards who was installed as sole executor of the estate by the High Court. In 1998 the director of the Hugh Lane Gallery in Dublin secured the donation of the contents of Bacon's studio at 7 Reece Mews, South Kensington. The contents of his studio were moved and reconstructed in the gallery.A collection of drawings, some little more than scribbles, given by Bacon to his driver and handyman Barry Joule, surfaced in 1998, when Joule handed them over to the Tate Gallery. According to Joule the items were given as a gift, although they were probably to be destroyed. Their artistic and commercial value proved negligible but they provided some insight into Bacon's imagination and his thinking, in the early stages of conceiving a finished work. Today most of the works are in the Hugh Lane Gallery in Dublin.
The imagery of the crucifixion weighs heavily in the work of Francis Bacon. Critic John Russell wrote that the crucifixion in Bacon's work is a "generic name for an environment in which bodily harm is done to one or more persons and one or more other persons gather to watch". Bacon admitted that he saw the scene as "a magnificent armature on which you can hang all types of feeling and sensation". He believed the imagery of the crucifixion allowed him to examine "certain areas of human behaviour" in a unique way, as the armature of the theme had been accumulated by so many old masters.Though he came to painting relatively late in life – he did not begin to paint seriously until his late 30s – crucifixion scenes can be found in his earliest works. In 1933, his patron Eric Hall commissioned a series of three paintings based on the subject. The early paintings were influenced by such old masters as Matthias Grünewald, Diego Velázquez and Rembrandt, but also by Picasso's late 1920s/early 1930s biomorphs and the early work of the Surrealists.
Bacon's series of Popes, largely quoting Velázquez's famous portrait Pope Innocent X (1650, Galeria Doria Pamphili, Rome) are striking images which further develop motifs already found in his earlier works, like the "Study for Three Figures at the Base of a Crucifixion", such as the screaming open mouth. The figures of the popes, pictorially isolated by partly curved parallel lines indicating psychological forces and symbolizing inner energy like strength of feeling, are alienated from their original representation and, stripped of their representation of power to allegories of suffering humanity.
Many of Bacon's paintings are "inhabited" by reclining figures. Single, or, as in triptychs, repeated with variations, they can be commented by symbolic indexes (like circular arrows as signs for rotation), turning painted images to blueprints for moving images of the type of contemporary GIFs. The composition of especially the nude figures is influenced by the sculptural work of Michelangelo. The multi-phasing of his rendition of the figures, which often is also applied to the sitters in the portraits, is also a reference to Eadweard Muybridge's chronophotography.
The inspiration for the recurring motif of screaming mouths in many Bacons of the late 1940s and early 1950s was drawn from a number of sources, including medical text books, the works of Matthias Grünewald and photographic stills of the nurse in the Odessa Steps scene in Eisenstein's 1925 silent film Battleship Potemkin. Bacon saw the film in 1935, and viewed it frequently thereafter. He kept in his studio a photographic still of the scene, showing a close-up of the nurse's head screaming in panic and terror and with broken pince-nez spectacles hanging from her blood-stained face. He referred to the image throughout his career, using it as a source of inspiration.Bacon described the screaming mouth as a catalyst for his work, and incorporated its shape when painting the chimera. His use of the motif can be seen in one of his first surviving works, Abstraction from the Human Form. By the early 1950s it became an obsessive concern, to the point, according to art critic and Bacon biographer Michael Peppiatt, "it would be no exaggeration to say that, if one could really explain the origins and implications of this scream, one would be far closer to understanding the whole art of Francis Bacon."
In 1999, England's High Court ruled that Marlborough Fine Art had to be replaced by a new independent representative for the Bacon estate. The estate moved its business to Faggionato Fine Arts in Europe and Tony Shafrazi in New York. That same year, the estate sued Marlborough UK and Marlborough International, Vaduz, charging them with wrongfully exploiting Bacon in a relationship that was manifestly disadvantageous to him until his death in 1992, and to his estate. The suit alleged Marlborough in London grossly underpaid Bacon for his works and resold them through its Liechtenstein branch at much higher prices. It contended that Marlborough never supplied a complete accounting of Bacon's works and sales and that Marlborough handled some works it has never accounted for. The suit was dropped in early 2002 when both sides agreed to pay their own costs and Marlborough released all its documents about Bacon. In 2003, the estate was handed to a four-person trust based in Jersey.
A first, incomplete catalogue raisonné was compiled by curator Ronald Alley in 1964. In 2016, a five-volume Francis Bacon: Catalogue Raisonné, documenting 584 paintings by Bacon, was released by Martin Harrison and others.Harrison's Catalogue Raisonné summarized the artist's motivations when he gifted the second version of his triptych of the Crucifixion to the Tate in 1991. "This reprise of the painting Bacon considered his 'Opus I' is both larger and more monumental than the original, the sumptuous, dusky crimson backgrounds and deep space evoking a subverted Baroque altarpiece. Conscious no doubt of his artistic legacy, Bacon intended a posthumous bequest of the painting to the Tate; he was persuaded to bring forward the donation and, on condition there should be no ceremony, gifted it to the gallery in 1991".
Official website Francis Bacon image licensing at Artimage 44 paintings by or after Francis Bacon at the Art UK site Francis Bacon in the Tate Collection Francis Bacon in the Guggenheim Collection Francis Bacon at the Museum of Modern Art Tate Channel Francis Bacon: A Brush with Violence (2017) BBC on YouTube
List of smoked foods
Media related to Back bacon at Wikimedia Commons "A Guide to Bacon Styles, and How to Make Proper British Rashers" at thepauperedchef.com weblog
Bacon, the youngest of six children, was born and raised in a close-knit family in Philadelphia. His mother, Ruth Hilda (née Holmes; 1916–1991), taught at an elementary school and was a liberal activist, while his father, Edmund Norwood Bacon (1910–2005), was an architect who served for many years as executive director of the Philadelphia City Planning Commission.Bacon attended Julia R. Masterman High School for both middle and high school. At age 16, in 1975, Bacon won a full scholarship to and attended the Pennsylvania Governor's School for the Arts at Bucknell University, a state-funded five-week arts program at which he studied theater under Glory Van Scott. The experience solidified Bacon's passion for the arts.
In 1980, he appeared in the slasher film Friday the 13th. Some of his early stage work included Getting Out, performed at New York's Phoenix Theater, and Flux, at Second Stage Theatre during their 1981–1982 season.In 1982, he won an Obie Award for his role in Forty Deuce, and soon afterward he made his Broadway debut in Slab Boys, with then-unknowns Sean Penn and Val Kilmer. However, it was not until he portrayed Timothy Fenwick that same year in Barry Levinson's film Diner – costarring Steve Guttenberg, Daniel Stern, Mickey Rourke, Tim Daly, and Ellen Barkin – that he made an indelible impression on film critics and moviegoers alike.Bolstered by the attention garnered by his performance in Diner, Bacon starred in Footloose (1984). Richard Corliss of TIME likened Footloose to the James Dean classic Rebel Without a Cause and the old Mickey Rooney/Judy Garland musicals, commenting that the film includes "motifs on book burning, mid-life crisis, AWOL parents, fatal car crashes, drug enforcement, and Bible Belt vigilantism." To prepare for the role, Bacon enrolled at a high school as a transfer student named "Ren McCormick" and studied teenagers before leaving in the middle of the day. Bacon earned strong reviews for Footloose. Bacon's critical and box office success led to a period of typecasting in roles similar to the two he portrayed in Diner and Footloose, and he had difficulty shaking this on-screen image. For the next several years he chose films that cast him against either type and experienced, by his own estimation, a career slump. In 1988, he starred in John Hughes' comedy She's Having a Baby, and the following year he was in another comedy called The Big Picture.
In 2000, he appeared in Paul Verhoeven's Hollow Man. Bacon, Colin Firth and Rachel Blanchard depict a ménage à trois in their film, Where the Truth Lies. Bacon and director Atom Egoyan have condemned the MPAA ratings board decision to rate the film "NC-17" rather than the preferable "R". Bacon commented: "I don't get it, when I see films (that) are extremely violent, extremely objectionable sometimes in terms of the roles that women play, slide by with an R, no problem, because the people happen to have more of their clothes on."In 2003 he acted with Sean Penn and Tim Robbins in Clint Eastwood's movie Mystic River. Bacon was again acclaimed for a dark starring role playing an offending pedophile on parole in The Woodsman (2004), for which he was nominated for best actor and received the Independent Spirit Award. He appeared in the HBO Films production of Taking Chance, based on an eponymous story written by Lieutenant Colonel Michael Strobl, an American Desert Storm war veteran. The film premiered on HBO on February 21, 2009. Bacon won a Golden Globe Award and a Screen Actors Guild Award for Outstanding Performance by a Male Actor in a Miniseries or Television Movie for his role.
Beginning in 2012, Bacon has appeared in a major advertising campaign for the EE mobile network in the United Kingdom, based on the Six Degrees concept and his various film roles. In 2015, he became a commercial spokesperson for the U.S. egg industry.
Bacon is the subject of the trivia game titled "Six Degrees of Kevin Bacon," based on the idea that, due to his prolific screen career covering a diverse range of genres, any Hollywood actor can be linked to another in a handful of steps based on their association with Bacon. The name of the game derives from the idea of six degrees of separation. Although he was initially dismayed by the game, the meme stuck, and Bacon eventually embraced it, forming the charitable initiative SixDegrees.org, a social networking site intended to link people and charities to each other.The measure of proximity to Bacon has been mathematically formalized as the Bacon number and can be referenced at websites including Oracle of Bacon, which is in turn based upon Internet Movie Database data. In 2012, Google added a feature to their search engine, whereby searching for an actor's name followed by the words "Bacon Number" will show the ways in which that actor is connected to Kevin Bacon. This feature is no longer active. A similar measurement exists in the mathematics community, where one measures how far one is removed from co-writing a mathematical paper with the famous mathematician Paul Erdős. This is done by means of the Erdős number, which is 0 for Paul Erdős himself, 1 for someone who co-wrote an article with him, 2 for someone who co-wrote with someone who co-wrote with him, etc. People have combined the Bacon number and the Erdős number to form the Erdős–Bacon number, which is the sum of the two.
Kevin formed a band called The Bacon Brothers with his brother, Michael. The duo has released six albums.
2003, September 30: Inducted into Hollywood Walk of Fame with a star for his contribution to Motion Picture presented to him by the Chamber of Commerce. 2004: Received the John Cassavetes Award during the Denver International Film Festival. 2005: Received the Copper Wing Tribute Award during the Phoenix Film Festival. 2005: Received the American Rivera Award during the Santa Barbara International Film Festival. 2010: Honored with the Joel Siegel Award by the Broadcast Film Critics Association. 2015: Honored with the Career Achievement in Acting Award by the Seattle International Film Festival.
List of actors with Hollywood Walk of Fame motion picture stars
Kevin Bacon on IMDb Kevin Bacon at the Internet Broadway Database Kevin Bacon at the Internet Off-Broadway Database Kevin Bacon at AllMovie Oracle of Bacon
A bacon sandwich (also known in parts of the United Kingdom and New Zealand as a bacon butty, bacon bap or bacon sarnie, in Ireland as a rasher sandwich and as a bacon sanger in Australia) is a sandwich of cooked bacon between bread that is optionally spread with butter, and may be seasoned with ketchup or brown sauce. It is generally served hot. In some establishments the sandwich will be made from bread toasted on only one side, while other establishments serve it on the same roll as is used for hamburgers. Bacon sandwiches are an all-day favourite throughout the United Kingdom. They are often served in British cafes, and are anecdotally recommended as a hangover cure.
The New York Times-London Journal reported on a study conducted at Leeds University which consisted of testing 700 variants of the sandwich. They experimented with different cooking styles, types of bacon, breads, oils, and special additions. Each variant was then ranked by 50 tasters. In conclusion, the best bacon sandwiches are made with "crispy, fried, and not-too-fat bacon between thick slices of white bread." Another study by the Direct Line for Business listed the top additions to the traditional bacon butty in England. Although the original was still the preferred sandwich, the next top contender was the "breggy" which adds an egg. The next popular accessory was mushrooms, followed by cheese. For sauces, brown sauce was slightly favoured over ketchup. The BLT is a popular variant of the bacon sandwich with the additional ingredients of lettuce and tomato, but served cold.In Canada, Peameal bacon § Sandwiches are a common variation, and are even considered the unofficial dish of Toronto.
Numerous studies have showed a connection between processed meats and an increased risk of serious health conditions such as type 2 diabetes, various cancers, and cardiovascular disease.A study in 2007 conducted by the World Cancer Research Fund found that there was "convincing evidence" of a link between processed meats and an increased chance of cancer. Although no numerical value was provided for the risk, they did state that "people should not eat more than 500g of red meat a week." The World Health Organization released a warning concerning the sodium content in bacon. For 100g of bacon, there are approximately 1,500 mg of sodium. Currently, the FDA reports that the average American adult should consume less than 2,300 mg per day. Too much sodium in the diet can lead to a higher probability of high blood pressure, which is a major cause of heart disease and stroke.
"Scientists 'perfect' bacon butty: Scientists have created a mathematical formula of how to make the perfect bacon butty". BBC. British Broadcasting Corporation. 9 April 2007.
Peameal bacon (also known as cornmeal bacon) is a wet-cured, unsmoked back bacon made from trimmed lean boneless pork loin rolled in cornmeal. It is found mainly in Southern Ontario. Toronto pork packer William Davies, who came to Canada from England in 1854, is credited with its development.The name "peameal bacon" derives from the historic practice of rolling the cured and trimmed boneless loin in dried and ground yellow peas to extend shelf life. Since the end of World War I, it has been rolled in ground yellow cornmeal. Peameal bacon sandwiches, consisting of cooked peameal bacon on a kaiser roll and sometimes topped with mustard or other toppings, are often considered a signature dish of Toronto, particularly from Toronto's St. Lawrence Market.
The origins of peameal bacon have not been firmly established. Curing pork with brine has been practiced for centuries, in many parts of the world. Peameal bacon has been linked to pork-packer William Davies and the Toronto-based William Davies Company, though it is uncertain if the process was invented by Davies, an employee, or if it was otherwise acquired by the company. Davies immigrated to Canada from Britain in 1854, and set up a shop in Toronto's St. Lawrence Market.According to Toronto's oral history, Davies sent a side of brine-cured pork loins to relatives in England. To help preserve this shipment, he packed it in ground yellow peas. This was well received and Davies continued rolling cured loins in pea meal to extend shelf life. The William Davies Company expanded, forming Canada's first major chain of food stores, and becoming the largest pork exporter in the British Empire. By the early 1900s, the company's Front Street plant processed nearly half a million hogs per year. This contributed to Toronto's longstanding nickname of "Hogtown". Following World War I, cornmeal replaced the pea meal crust, due to the former's availability and improved refrigeration practices.
In the 1960s, customers of Joe Hoiner's St. Lawrence Market butcher shop opted for the centre cut of cured peameal loins, leaving him with the ends. He partnered with Elso Biancolin, who ran a bakery shop at the market, and they sliced and fried the bacon ends and sold them on buns. Biancolin's sons, Robert and Maurice, expanded the family's Carousel Bakery during the market's 1977 renovation, and their featured peameal bacon sandwich on a fresh kaiser roll received national and international attention from food critics and TV chefs. It is noted in many tourist guides and visiting chefs often seek it out. The Carousel Bakery's peameal bacon sandwich is simple, without complicated sauces, toppings or layers. It is composed of 1⁄8-inch (3.2 mm) slices of peameal bacon cooked on a griddle long enough to crisp, drizzled with honey mustard, served on a soft fresh roll. There are options to add an egg or side bacon.It was served at the inaugural Canadian Comedy Awards in 2000. In 2016, the peameal bacon sandwich was named Toronto's signature dish. This was announced by Mayor John Tory at a local food festival with several versions offered. Peameal bacon sandwiches were included in a wager between Tory and Oakland Mayor Libby Schaaf during the 2019 NBA Finals.
Cuisine of Toronto
Make your own Canadian peameal bacon at Pittsburgh Post-Gazette
Canadian Bacon is a 1995 American-Canadian comedy film written, produced, and directed by Michael Moore which satirizes Canada–United States relations along the Canada–United States border. The film stars an ensemble cast featuring Alan Alda, John Candy, Bill Nunn, Kevin J. O'Connor, Rhea Perlman, Kevin Pollak, G. D. Spradlin, and Rip Torn. The film was screened in the Un Certain Regard section at the 1995 Cannes Film Festival, and was the final film released starring John Candy, though it was shot before the earlier-released Wagons East. It is also Moore's only non-documentary film to date.
Thousands of former employees are outraged with military businessman R. J. Hacker (G. D. Spradlin), who had closed down his weapons manufacturing plant, Hacker Dynamics. At a conference held at the former plant, he pins the blame for the shutdown of his business on the current President of the United States (Alan Alda), who has just arrived. The president defends his own belief that the future of the children is more important than war, which has caused major decline in his approval rating. After the conference, he expresses to confidantes General Dick Panzer (Rip Torn) and National Security Advisor Stuart Smiley (Kevin Pollak), revealed to have ties with Hacker, his discontent about not having an enemy to engage in war. An attempted negotiation with Russian President Vladimir Kruschkin (Richard E. Council) to start a new cold war with Russia fails, and the president's suggestion of a war on international terrorism is deemed too absurd. Serendipitously, American sheriff Bud Boomer (John Candy) offensively criticizes Canadian beer while attending a hockey game between the neighboring nations in Niagara Falls, Ontario. The ensuing brawl ends up on the news and catches Stuart's attention; Stuart, in turn, collects more information about Canada from a CIA agent named Gus (Brad Sullivan), and suggests Canada as their new enemy during a cabinet meeting. Before long, television channels are littered with anti-Canada propaganda, which Boomer believes wholeheartedly. He prepares for war by distributing guns to his fellow sheriffs, including his girlfriend Honey (Rhea Perlman) and their friends Roy Boy (Kevin J. O'Connor) and Kabral Jabar (Bill Nunn). After they apprehend a group of Americans "dressed as Canadians" attempting to destroy a hydroelectric plant, despite Gus's protests that they are just Americans, they sneak across the border to litter on Canadian lands, which leads to Honey being arrested by the Royal Canadian Mounted Police. In a rescue attempt, Boomer, Roy Boy and Kabral sneak into a Canadian power plant and cause a countrywide blackout. When the president learns of this, he orders Boomer's immediate removal from Canada before it is too late. Hacker, seeking revenge on the president for shutting down his business, uses a software program ("Hacker Hellstorm") to activate missile silos across the country. The president learns that the signal causing the activation of the silos originated from Canada, and summons Hacker. Hacker offers to sell a program to the president that can cancel out the Hellstorm—for $1 trillion. Stuart, fed up with the president being too busy to give Hacker the money, realizes that Hacker, getting up to leave, is the one controlling the silos, not Canada, and, after storming up, takes the operating codes from him required to stop the Hellstorm (accidentally killing Hacker in the process). The president orders Stuart's arrest, despite his protests that he is now able to give the codes to the president so they could deactivate the missiles, which are aimed at Moscow. As the launch time approaches the president pleads with Canadian Prime Minister Clark MacDonald (Wallace Shawn) over the phone to stop the launch. Meanwhile, Honey was taken to a mental hospital upon her capture and escaped all the way to the CN Tower. She discovers the central computer for the Hellstorm located at the top and destroys it with a machine gun, aborting the launch sequence. She then reunites with Boomer, who had tracked her to the Tower, and they return to the United States via a speedboat. An ending montage reveals the characters' fates: Boomer realizes his dream of appearing on Cops; Honey has been named "Humanitarian of the Year" by the National Rifle Association; the president was defeated in the next election by a large landslide and now hosts Get Up, Cleveland; Stuart served eight months in prison, but was pardoned by the new president; Panzer committed suicide after learning that Hogan's Heroes was fictional; Gus was last spotted heading to Mexico in a tank; Hacker's body has been viewed daily at Republican National Headquarters; Kabral has become a hockey star, winning the Hart Memorial Trophy three years in a row; Roy Boy's whereabouts become unknown; and MacDonald is "still ruling with an iron fist".
The film was shot in fall 1993, in Toronto, Hamilton, and Niagara Falls, Ontario; and Buffalo and Niagara Falls, New York. Scenes depicting the rapids of the Niagara River were actually filmed at Twelve Mile Creek in St. Catharines. Parkwood Estate in Oshawa was the site for the White House, and Dofasco in Hamilton was the site for Hacker Dynamics. The scene where the American characters look longingly home at the US across the putative Niagara River is them looking across Burlington Bay at Stelco steelworks in Hamilton, Ontario.The hockey game/riot were shot at the Niagara Falls Memorial Arena in Niagara Falls, Ontario, and the actors portraying the police officers (who eventually join in the riot upon hearing "Canadian beer sucks") are wearing authentic Niagara Regional Police uniforms.The film has numerous cameos by Canadian actors, including Dan Aykroyd, who appears uncredited as an Ontario Provincial Police officer who pulls Candy over (ticketing him not for the crude anti-Canadian graffiti on his truck, but its lack of a French translation). Moore himself appears as an American gun nut.
Canadian Bacon has a 14% from Rotten Tomatoes.Nathan Rabin in a 2009 review concluded, "After generating solid laughs during its first hour, Canadian Bacon falls apart in its third act," lamenting the film "was perceived as too lowbrow for the highbrows, and too highbrow for the lowbrows."
Canadian Bacon on IMDb Canadian Bacon at AllMovie Canadian Bacon at the TCM Movie Database Canadian Bacon at Box Office Mojo Canadian Bacon at Rotten Tomatoes
Several major groups of animals typically have readily distinguishable eggs.
The most common reproductive strategy for fish is known as oviparity, in which the female lays undeveloped eggs that are externally fertilized by a male. Typically large numbers of eggs are laid at one time (an adult female cod can produce 4–6 million eggs in one spawning) and the eggs are then left to develop without parental care. When the larvae hatch from the egg, they often carry the remains of the yolk in a yolk sac which continues to nourish the larvae for a few days as they learn how to swim. Once the yolk is consumed, there is a critical point after which they must learn how to hunt and feed or they will die. A few fish, notably the rays and most sharks use ovoviviparity in which the eggs are fertilized and develop internally. However, the larvae still grow inside the egg consuming the egg's yolk and without any direct nourishment from the mother. The mother then gives birth to relatively mature young. In certain instances, the physically most developed offspring will devour its smaller siblings for further nutrition while still within the mother's body. This is known as intrauterine cannibalism. In certain scenarios, some fish such as the hammerhead shark and reef shark are viviparous, with the egg being fertilized and developed internally, but with the mother also providing direct nourishment. The eggs of fish and amphibians are jellylike. Cartilaginous fish (sharks, skates, rays, chimaeras) eggs are fertilized internally and exhibit a wide variety of both internal and external embryonic development. Most fish species spawn eggs that are fertilized externally, typically with the male inseminating the eggs after the female lays them. These eggs do not have a shell and would dry out in the air. Even air-breathing amphibians lay their eggs in water, or in protective foam as with the Coast foam-nest treefrog, Chiromantis xerampelina.
The default color of vertebrate eggs is the white of the calcium carbonate from which the shells are made, but some birds, mainly passerines, produce colored eggs. The pigment biliverdin and its zinc chelate give a green or blue ground color, and protoporphyrin produces reds and browns as a ground color or as spotting. Non-passerines typically have white eggs, except in some ground-nesting groups such as the Charadriiformes, sandgrouse and nightjars, where camouflage is necessary, and some parasitic cuckoos which have to match the passerine host's egg. Most passerines, in contrast, lay colored eggs, even if there is no need of cryptic colors. However some have suggested that the protoporphyrin markings on passerine eggs actually act to reduce brittleness by acting as a solid-state lubricant. If there is insufficient calcium available in the local soil, the egg shell may be thin, especially in a circle around the broad end. Protoporphyrin speckling compensates for this, and increases inversely to the amount of calcium in the soil.For the same reason, later eggs in a clutch are more spotted than early ones as the female's store of calcium is depleted. The color of individual eggs is also genetically influenced, and appears to be inherited through the mother only, suggesting that the gene responsible for pigmentation is on the sex-determining W chromosome (female birds are WZ, males ZZ). It used to be thought that color was applied to the shell immediately before laying, but subsequent research shows that coloration is an integral part of the development of the shell, with the same protein responsible for depositing calcium carbonate, or protoporphyrins when there is a lack of that mineral. In species such as the common guillemot, which nest in large groups, each female's eggs have very different markings, making it easier for females to identify their own eggs on the crowded cliff ledges on which they breed.
Bird eggshells are diverse. For example: cormorant eggs are rough and chalky tinamou eggs are shiny duck eggs are oily and waterproof cassowary eggs are heavily pittedTiny pores in bird eggshells allow the embryo to breathe. The domestic hen's egg has around 7000 pores.Some bird eggshells have a coating of vaterite spherules, which is a rare polymorph of calcium carbonate. In Greater Ani Crotophaga major this vaterite coating is thought to act as a shock absorber, protecting the calcite shell from fracture during incubation, such as colliding with other eggs in the nest.
Most bird eggs have an oval shape, with one end rounded and the other more pointed. This shape results from the egg being forced through the oviduct. Muscles contract the oviduct behind the egg, pushing it forward. The egg's wall is still shapeable, and the pointed end develops at the back. Long, pointy eggs are an incidental consequence of having a streamlined body typical of birds with strong flying abilities; flight narrows the oviduct, which changes the type of egg a bird can lay. Cliff-nesting birds often have highly conical eggs. They are less likely to roll off, tending instead to roll around in a tight circle; this trait is likely to have arisen due to evolution via natural selection. In contrast, many hole-nesting birds have nearly spherical eggs.
Like amphibians, amniotes are air-breathing vertebrates, but they have complex eggs or embryos, including an amniotic membrane. Amniotes include reptiles (including dinosaurs and their descendants, birds) and mammals. Reptile eggs are often rubbery and are always initially white. They are able to survive in the air. Often the sex of the developing embryo is determined by the temperature of the surroundings, with cooler temperatures favouring males. Not all reptiles lay eggs; some are viviparous ("live birth"). Dinosaurs laid eggs, some of which have been preserved as petrified fossils. Among mammals, early extinct species laid eggs, as do platypuses and echidnas (spiny anteaters). Platypuses and two genera of echidna are Australian monotremes. Marsupial and placental mammals do not lay eggs, but their unborn young do have the complex tissues that identify amniotes.
The eggs of the egg-laying mammals (the platypus and the echidnas) are macrolecithal eggs very much like those of reptiles. The eggs of marsupials are likewise macrolecithal, but rather small, and develop inside the body of the female, but do not form a placenta. The young are born at a very early stage, and can be classified as a "larva" in the biological sense.In placental mammals, the egg itself is void of yolk, but develops an umbilical cord from structures that in reptiles would form the yolk sac. Receiving nutrients from the mother, the fetus completes the development while inside the uterus.
Eggs are common among invertebrates, including insects, spiders, mollusks, and crustaceans.
All sexually reproducing life, including both plants and animals, produces gametes. The male gamete cell, sperm, is usually motile whereas the female gamete cell, the ovum, is generally larger and sessile. The male and female gametes combine to produce the zygote cell. In multicellular organisms the zygote subsequently divides in an organised manner into smaller more specialised cells, so that this new individual develops into an embryo. In most animals the embryo is the sessile initial stage of the individual life cycle, and is followed by the emergence (that is, the hatching) of a motile stage. The zygote or the ovum itself or the sessile organic vessel containing the developing embryo may be called the egg. A recent proposal suggests that the phylotypic animal body plans originated in cell aggregates before the existence of an egg stage of development. Eggs, in this view, were later evolutionary innovations, selected for their role in ensuring genetic uniformity among the cells of incipient multicellular organisms.
Scientists often classify animal reproduction according to the degree of development that occurs before the new individuals are expelled from the adult body, and by the yolk which the egg provides to nourish the embryo.
Vertebrate eggs can be classified by the relative amount of yolk. Simple eggs with little yolk are called microlecithal, medium-sized eggs with some yolk are called mesolecithal, and large eggs with a large concentrated yolk are called macrolecithal. This classification of eggs is based on the eggs of chordates, though the basic principle extends to the whole animal kingdom.
Small eggs with little yolk are called microlecithal. The yolk is evenly distributed, so the cleavage of the egg cell cuts through and divides the egg into cells of fairly similar sizes. In sponges and cnidarians the dividing eggs develop directly into a simple larva, rather like a morula with cilia. In cnidarians, this stage is called the planula, and either develops directly into the adult animals or forms new adult individuals through a process of budding.Microlecithal eggs require minimal yolk mass. Such eggs are found in flatworms, roundworms, annelids, bivalves, echinoderms, the lancelet and in most marine arthropods. In anatomically simple animals, such as cnidarians and flatworms, the fetal development can be quite short, and even microlecithal eggs can undergo direct development. These small eggs can be produced in large numbers. In animals with high egg mortality, microlecithal eggs are the norm, as in bivalves and marine arthropods. However, the latter are more complex anatomically than e.g. flatworms, and the small microlecithal eggs do not allow full development. Instead, the eggs hatch into larvae, which may be markedly different from the adult animal. In placental mammals, where the embryo is nourished by the mother throughout the whole fetal period, the egg is reduced in size to essentially a naked egg cell.
Mesolecithal eggs have comparatively more yolk than the microlecithal eggs. The yolk is concentrated in one part of the egg (the vegetal pole), with the cell nucleus and most of the cytoplasm in the other (the animal pole). The cell cleavage is uneven, and mainly concentrated in the cytoplasma-rich animal pole.The larger yolk content of the mesolecithal eggs allows for a longer fetal development. Comparatively anatomically simple animals will be able to go through the full development and leave the egg in a form reminiscent of the adult animal. This is the situation found in hagfish and some snails. Animals with smaller size eggs or more advanced anatomy will still have a distinct larval stage, though the larva will be basically similar to the adult animal, as in lampreys, coelacanth and the salamanders.
Eggs with a large yolk are called macrolecithal. The eggs are usually few in number, and the embryos have enough food to go through full fetal development in most groups. Macrolecithal eggs are only found in selected representatives of two groups: Cephalopods and vertebrates.Macrolecithal eggs go through a different type of development than other eggs. Due to the large size of the yolk, the cell division can not split up the yolk mass. The fetus instead develops as a plate-like structure on top of the yolk mass, and only envelopes it at a later stage. A portion of the yolk mass is still present as an external or semi-external yolk sac at hatching in many groups. This form of fetal development is common in bony fish, even though their eggs can be quite small. Despite their macrolecithal structure, the small size of the eggs does not allow for direct development, and the eggs hatch to a larval stage ("fry"). In terrestrial animals with macrolecithal eggs, the large volume to surface ratio necessitates structures to aid in transport of oxygen and carbon dioxide, and for storage of waste products so that the embryo does not suffocate or get poisoned from its own waste while inside the egg, see amniote.In addition to bony fish and cephalopods, macrolecithal eggs are found in cartilaginous fish, reptiles, birds and monotreme mammals. The eggs of the coelacanths can reach a size of 9 cm (3.5 in) in diameter, and the young go through full development while in the uterus, living on the copious yolk.
Animals are commonly classified by their manner of reproduction, at the most general level distinguishing egg-laying (Latin. oviparous) from live-bearing (Latin. viviparous). These classifications are divided into more detail according to the development that occurs before the offspring are expelled from the adult's body. Traditionally: Ovuliparity means the female spawns unfertilized eggs (ova), which must then be externally fertilised. Ovuliparity is typical of bony fish, anurans, echinoderms, bivalves and cnidarians. Most aquatic organisms are ovuliparous. The term is derived from the diminutive meaning "little egg". Oviparity is where fertilisation occurs internally and so the eggs laid by the female are zygotes (or newly developing embryos), often with important outer tissues added (for example, in a chicken egg, no part outside of the yolk originates with the zygote). Oviparity is typical of birds, reptiles, some cartilaginous fish and most arthropods. Terrestrial organisms are typically oviparous, with egg-casings that resist evaporation of moisture. Ovo-viviparity is where the zygote is retained in the adult's body but there are no trophic (feeding) interactions. That is, the embryo still obtains all of its nutrients from inside the egg. Most live-bearing fish, amphibians or reptiles are actually ovoviviparous. Examples include the reptile Anguis fragilis, the sea horse (where zygotes are retained in the male's ventral "marsupium"), and the frogs Rhinoderma darwinii (where the eggs develop in the vocal sac) and Rheobatrachus (where the eggs develop in the stomach). Histotrophic viviparity means embryos develop in the female's oviducts but obtain nutrients by consuming other ova, zygotes or sibling embryos (oophagy or adelphophagy). This intra-uterine cannibalism occurs in some sharks and in the black salamander Salamandra atra. Marsupials excrete a "uterine milk" supplementing the nourishment from the yolk sac. Hemotrophic viviparity is where nutrients are provided from the female's blood through a designated organ. This most commonly occurs through a placenta, found in most mammals. Similar structures are found in some sharks and in the lizard Pseudomoia pagenstecheri. In some hylid frogs, the embryo is fed by the mother through specialized gills.The term hemotropic derives from the Latin for blood-feeding, contrasted with histotrophic for tissue-feeding.
Eggs laid by many different species, including birds, reptiles, amphibians, and fish, have probably been eaten by mankind for millennia. Popular choices for egg consumption are chicken, duck, roe, and caviar, but by a wide margin the egg most often humanly consumed is the chicken egg, typically unfertilized.
According to the Kashrut, that is the set of Jewish dietary laws, kosher food may be consumed according to halakha (Jewish law). Kosher meat and milk (or derivatives) cannot be mixed (Deuteronomy 14:21) or stored together. Eggs are considered pareve (neither meat nor dairy) despite being an animal product and can be mixed with either milk or kosher meat. Mayonnaise, for instance, is usually marked "pareve" despite by definition containing egg.
Many vaccines for infectious diseases are produced in fertile chicken eggs. The basis of this technology was the discovery in 1931 by Alice Miles Woodruff and Ernest William Goodpasture at Vanderbilt University that the rickettsia and viruses that cause a variety of diseases will grow in chicken embryos. This enabled the development of vaccines against influenza, chicken pox, smallpox, yellow fever, typhus, Rocky mountain spotted fever and other diseases.
The egg is a symbol of new life and rebirth in many cultures around the world. Christians view Easter eggs as symbolic of the resurrection of Jesus Christ. A popular Easter tradition in some parts of the world is the decoration of hard-boiled eggs (usually by dyeing, but often by hand-painting or spray-painting). Adults often hide the eggs for children to find, an activity known as an Easter egg hunt. A similar tradition of egg painting exists in areas of the world influenced by the culture of Persia. Before the spring equinox in the Persian New Year tradition (called Norouz), each family member decorates a hard-boiled egg and sets them together in a bowl. The tradition of a dancing egg is held during the feast of Corpus Christi in Barcelona and other Catalan cities since the 16th century. It consists of an emptied egg, positioned over the water jet from a fountain, which starts turning without falling.Although a food item, raw eggs are sometimes thrown at houses, cars, or people. This act, known commonly as "egging" in the various English-speaking countries, is a minor form of vandalism and, therefore, usually a criminal offense and is capable of damaging property (egg whites can degrade certain types of vehicle paint) as well as potentially causing serious eye injury. On Halloween, for example, trick or treaters have been known to throw eggs (and sometimes flour) at property or people from whom they received nothing. Eggs are also often thrown in protests, as they are inexpensive and nonlethal, yet very messy when broken.
Egg collecting was a popular hobby in some cultures, including among the first Australians. Traditionally, the embryo would be removed before a collector stored the egg shell.Collecting eggs of wild birds is now banned by many jurisdictions, as the practice can threaten rare species. In the United Kingdom, the practice is prohibited by the Protection of Birds Act 1954 and Wildlife and Countryside Act 1981. On the other hand, ongoing underground trading is becoming a serious issue.Since the protection of wild bird eggs was regulated, early collections have come to the museums as curiosities. For example, the Australian Museum hosts a collection of about 20,000 registered clutches of eggs, and the collection in Western Australia Museum has been archived in a gallery. Scientists regard egg collections as a good natural-history data, as the details recorded in the collectors' notes have helped them to understand birds' nesting behaviors.
Dinosaurs are a diverse group of reptiles of the clade Dinosauria. They first appeared during the Triassic period, between 243 and 233.23 million years ago, although the exact origin and timing of the evolution of dinosaurs is the subject of active research. They became the dominant terrestrial vertebrates after the Triassic–Jurassic extinction event 201.3 million years ago; their dominance continued throughout the Jurassic and Cretaceous periods. The fossil record shows that birds are modern feathered dinosaurs, having evolved from earlier theropods during the Late Jurassic epoch, and are the only dinosaur lineage to survive the Cretaceous–Paleogene extinction event approximately 66 million years ago. Dinosaurs can therefore be divided into avian dinosaurs, or birds; and non-avian dinosaurs, which are all dinosaurs other than birds. Dinosaurs are a varied group of animals from taxonomic, morphological and ecological standpoints. Birds, at over 10,000 living species, are the most diverse group of vertebrates besides perciform fish. Using fossil evidence, paleontologists have identified over 500 distinct genera and more than 1,000 different species of non-avian dinosaurs. Dinosaurs are represented on every continent by both extant species (birds) and fossil remains. Through the first half of the 20th century, before birds were recognized to be dinosaurs, most of the scientific community believed dinosaurs to have been sluggish and cold-blooded. Most research conducted since the 1970s, however, has indicated that all dinosaurs were active animals with elevated metabolisms and numerous adaptations for social interaction. Some were herbivorous, others carnivorous. Evidence suggests that all dinosaurs were egg-laying; and that nest-building was a trait shared by many dinosaurs, both avian and non-avian. While dinosaurs were ancestrally bipedal, many extinct groups included quadrupedal species, and some were able to shift between these stances. Elaborate display structures such as horns or crests are common to all dinosaur groups, and some extinct groups developed skeletal modifications such as bony armor and spines. While the dinosaurs' modern-day surviving avian lineage (birds) are generally small due to the constraints of flight, many prehistoric dinosaurs (non-avian and avian) were large-bodied—the largest sauropod dinosaurs are estimated to have reached lengths of 39.7 meters (130 feet) and heights of 18 m (59 ft) and were the largest land animals of all time. Still, the idea that non-avian dinosaurs were uniformly gigantic is a misconception based in part on preservation bias, as large, sturdy bones are more likely to last until they are fossilized. Many dinosaurs were quite small: Xixianykus, for example, was only about 50 centimeters (20 inches) long. Since the first dinosaur fossils were recognized in the early 19th century, mounted fossil dinosaur skeletons have been major attractions at museums around the world, and dinosaurs have become an enduring part of world culture. The large sizes of some dinosaur groups, as well as their seemingly monstrous and fantastic nature, have ensured dinosaurs' regular appearance in best-selling books and films, such as Jurassic Park. Persistent public enthusiasm for the animals has resulted in significant funding for dinosaur science, and new discoveries are regularly covered by the media.
The taxon 'Dinosauria' was formally named in 1841 by paleontologist Sir Richard Owen, who used it to refer to the "distinct tribe or sub-order of Saurian Reptiles" that were then being recognized in England and around the world. The term is derived from Ancient Greek δεινός (deinos) 'terrible, potent or fearfully great', and σαῦρος (sauros) 'lizard or reptile'. Though the taxonomic name has often been interpreted as a reference to dinosaurs' teeth, claws, and other fearsome characteristics, Owen intended it to also evoke their size and majesty.Other prehistoric animals, including pterosaurs, mosasaurs, ichthyosaurs, plesiosaurs, and Dimetrodon, while often popularly conceived of as dinosaurs, are not taxonomically classified as dinosaurs. Pterosaurs are distantly related to dinosaurs, being members of the clade Ornithodira. The other groups mentioned are, like dinosaurs and pterosaurs, members of Sauropsida (the reptile and bird clade), except Dimetrodon (which is a synapsid).
Under phylogenetic nomenclature, dinosaurs are usually defined as the group consisting of the most recent common ancestor (MRCA) of Triceratops and modern birds (Neornithes), and all its descendants. It has also been suggested that Dinosauria be defined with respect to the MRCA of Megalosaurus and Iguanodon, because these were two of the three genera cited by Richard Owen when he recognized the Dinosauria. Both definitions result in the same set of animals being defined as dinosaurs: "Dinosauria = Ornithischia + Saurischia", encompassing ankylosaurians (armored herbivorous quadrupeds), stegosaurians (plated herbivorous quadrupeds), ceratopsians (herbivorous quadrupeds with horns and frills), ornithopods (bipedal or quadrupedal herbivores including "duck-bills"), theropods (mostly bipedal carnivores and birds), and sauropodomorphs (mostly large herbivorous quadrupeds with long necks and tails).Birds are now recognized as being the sole surviving lineage of theropod dinosaurs. In traditional taxonomy, birds were considered a separate class that had evolved from dinosaurs, a distinct superorder. However, a majority of contemporary paleontologists concerned with dinosaurs reject the traditional style of classification in favor of phylogenetic taxonomy; this approach requires that, for a group to be natural, all descendants of members of the group must be included in the group as well. Birds are thus considered to be dinosaurs and dinosaurs are, therefore, not extinct. Birds are classified as belonging to the subgroup Maniraptora, which are coelurosaurs, which are theropods, which are saurischians, which are dinosaurs.Research by Matthew G. Baron, David B. Norman, and Paul M. Barrett in 2017 suggested a radical revision of dinosaurian systematics. Phylogenetic analysis by Baron et al. recovered the Ornithischia as being closer to the Theropoda than the Sauropodomorpha, as opposed to the traditional union of theropods with sauropodomorphs. They resurrected the clade Ornithoscelida to refer to the group containing Ornithischia and Theropoda. Dinosauria itself was re-defined as the last common ancestor of Triceratops horridus, Passer domesticus and Diplodocus carnegii, and all of its descendants, to ensure that sauropods and kin remain included as dinosaurs.
Using one of the above definitions, dinosaurs can be generally described as archosaurs with hind limbs held erect beneath the body. Many prehistoric animal groups are popularly conceived of as dinosaurs, such as ichthyosaurs, mosasaurs, plesiosaurs, pterosaurs, and pelycosaurs (especially Dimetrodon), but are not classified scientifically as dinosaurs, and none had the erect hind limb posture characteristic of true dinosaurs. Dinosaurs were the dominant terrestrial vertebrates of the Mesozoic Era, especially the Jurassic and Cretaceous periods. Other groups of animals were restricted in size and niches; mammals, for example, rarely exceeded the size of a domestic cat, and were generally rodent-sized carnivores of small prey.Dinosaurs have always been an extremely varied group of animals; according to a 2006 study, over 500 non-avian dinosaur genera have been identified with certainty so far, and the total number of genera preserved in the fossil record has been estimated at around 1850, nearly 75% of which remain to be discovered. An earlier study predicted that about 3,400 dinosaur genera existed, including many that would not have been preserved in the fossil record. By September 17, 2008, 1,047 different species of dinosaurs had been named.In 2016, the estimated number of dinosaur species that existed in the Mesozoic was estimated to be 1,543–2,468. Some are herbivorous, others carnivorous, including seed-eaters, fish-eaters, insectivores, and omnivores. While dinosaurs were ancestrally bipedal (as are all modern birds), some prehistoric species were quadrupeds, and others, such as Anchisaurus and Iguanodon, could walk just as easily on two or four legs. Cranial modifications like horns and crests are common dinosaurian traits, and some extinct species had bony armor. Although known for large size, many Mesozoic dinosaurs were human-sized or smaller, and modern birds are generally small in size. Dinosaurs today inhabit every continent, and fossils show that they had achieved global distribution by at least the Early Jurassic epoch. Modern birds inhabit most available habitats, from terrestrial to marine, and there is evidence that some non-avian dinosaurs (such as Microraptor) could fly or at least glide, and others, such as spinosaurids, had semiaquatic habits.
While recent discoveries have made it more difficult to present a universally agreed-upon list of dinosaurs' distinguishing features, nearly all dinosaurs discovered so far share certain modifications to the ancestral archosaurian skeleton, or are clear descendants of older dinosaurs showing these modifications. Although some later groups of dinosaurs featured further modified versions of these traits, they are considered typical for Dinosauria; the earliest dinosaurs had them and passed them on to their descendants. Such modifications, originating in the most recent common ancestor of a certain taxonomic group, are called the synapomorphies of such a group.A detailed assessment of archosaur interrelations by Sterling Nesbitt confirmed or found the following twelve unambiguous synapomorphies, some previously known: in the skull, a supratemporal fossa (excavation) is present in front of the supratemporal fenestra, the main opening in the rear skull roof epipophyses, obliquely backward-pointing processes on the rear top corners, present in the anterior (front) neck vertebrae behind the atlas and axis, the first two neck vertebrae apex of a deltopectoral crest (a projection on which the deltopectoral muscles attach) located at or more than 30% down the length of the humerus (upper arm bone) radius, a lower arm bone, shorter than 80% of humerus length fourth trochanter (projection where the caudofemoralis muscle attaches on the inner rear shaft) on the femur (thigh bone) is a sharp flange fourth trochanter asymmetrical, with distal, lower, margin forming a steeper angle to the shaft on the astragalus and calcaneum, upper ankle bones, the proximal articular facet, the top connecting surface, for the fibula occupies less than 30% of the transverse width of the element exoccipitals (bones at the back of the skull) do not meet along the midline on the floor of the endocranial cavity, the inner space of the braincase in the pelvis, the proximal articular surfaces of the ischium with the ilium and the pubis are separated by a large concave surface (on the upper side of the ischium a part of the open hip joint is located between the contacts with the pubic bone and the ilium) cnemial crest on the tibia (protruding part of the top surface of the shinbone) arcs anterolaterally (curves to the front and the outer side) distinct proximodistally oriented (vertical) ridge present on the posterior face of the distal end of the tibia (the rear surface of the lower end of the shinbone) concave articular surface for the fibula of the calcaneum (the top surface of the calcaneum, where it touches the fibula, has a hollow profile)Nesbitt found a number of further potential synapomorphies and discounted a number of synapomorphies previously suggested. Some of these are also present in silesaurids, which Nesbitt recovered as a sister group to Dinosauria, including a large anterior trochanter, metatarsals II and IV of subequal length, reduced contact between ischium and pubis, the presence of a cnemial crest on the tibia and of an ascending process on the astragalus, and many others. A variety of other skeletal features are shared by dinosaurs. However, because they are either common to other groups of archosaurs or were not present in all early dinosaurs, these features are not considered to be synapomorphies. For example, as diapsids, dinosaurs ancestrally had two pairs of Infratemporal fenestrae (openings in the skull behind the eyes), and as members of the diapsid group Archosauria, had additional openings in the snout and lower jaw. Additionally, several characteristics once thought to be synapomorphies are now known to have appeared before dinosaurs, or were absent in the earliest dinosaurs and independently evolved by different dinosaur groups. These include an elongated scapula, or shoulder blade; a sacrum composed of three or more fused vertebrae (three are found in some other archosaurs, but only two are found in Herrerasaurus); and a perforate acetabulum, or hip socket, with a hole at the center of its inside surface (closed in Saturnalia tupiniquim, for example). Another difficulty of determining distinctly dinosaurian features is that early dinosaurs and other archosaurs from the Late Triassic epoch are often poorly known and were similar in many ways; these animals have sometimes been misidentified in the literature. Dinosaurs stand with their hind limbs erect in a manner similar to most modern mammals, but distinct from most other reptiles, whose limbs sprawl out to either side. This posture is due to the development of a laterally facing recess in the pelvis (usually an open socket) and a corresponding inwardly facing distinct head on the femur. Their erect posture enabled early dinosaurs to breathe easily while moving, which likely permitted stamina and activity levels that surpassed those of "sprawling" reptiles. Erect limbs probably also helped support the evolution of large size by reducing bending stresses on limbs. Some non-dinosaurian archosaurs, including rauisuchians, also had erect limbs but achieved this by a "pillar-erect" configuration of the hip joint, where instead of having a projection from the femur insert on a socket on the hip, the upper pelvic bone was rotated to form an overhanging shelf.
Dinosaurs diverged from their archosaur ancestors during the Middle to Late Triassic epochs, roughly 20 million years after the devastating Permian–Triassic extinction event wiped out an estimated 96% of all marine species and 70% of terrestrial vertebrate species approximately 252 million years ago. Radiometric dating of the rock formation that contained fossils from the early dinosaur genus Eoraptor at 231.4 million years old establishes its presence in the fossil record at this time. Paleontologists think that Eoraptor resembles the common ancestor of all dinosaurs; if this is true, its traits suggest that the first dinosaurs were small, bipedal predators. The discovery of primitive, dinosaur-like ornithodirans such as Marasuchus and Lagerpeton in Argentinian Middle Triassic strata supports this view; analysis of recovered fossils suggests that these animals were indeed small, bipedal predators. Dinosaurs may have appeared as early as 243 million years ago, as evidenced by remains of the genus Nyasasaurus from that period, though known fossils of these animals are too fragmentary to tell if they are dinosaurs or very close dinosaurian relatives. Paleontologist Max C. Langer et al. (2018) determined that Staurikosaurus from the Santa Maria Formation dates to 233.23 million years ago, making it older in geologic age than Eoraptor.When dinosaurs appeared, they were not the dominant terrestrial animals. The terrestrial habitats were occupied by various types of archosauromorphs and therapsids, like cynodonts and rhynchosaurs. Their main competitors were the pseudosuchia, such as aetosaurs, ornithosuchids and rauisuchians, which were more successful than the dinosaurs. Most of these other animals became extinct in the Triassic, in one of two events. First, at about 215 million years ago, a variety of basal archosauromorphs, including the protorosaurs, became extinct. This was followed by the Triassic–Jurassic extinction event (about 201 million years ago), that saw the end of most of the other groups of early archosaurs, like aetosaurs, ornithosuchids, phytosaurs, and rauisuchians. Rhynchosaurs and dicynodonts survived (at least in some areas) at least as late as early –mid Norian and late Norian or earliest Rhaetian stages, respectively, and the exact date of their extinction is uncertain. These losses left behind a land fauna of crocodylomorphs, dinosaurs, mammals, pterosaurians, and turtles. The first few lines of early dinosaurs diversified through the Carnian and Norian stages of the Triassic, possibly by occupying the niches of the groups that became extinct. Also notably, there was a heightened rate of extinction during the Carnian Pluvial Event.
Dinosaur evolution after the Triassic follows changes in vegetation and the location of continents. In the Late Triassic and Early Jurassic, the continents were connected as the single landmass Pangaea, and there were a worldwide dinosaur fauna mostly composed of coelophysoid carnivores and early sauropodomorph herbivores. Gymnosperm plants (particularly conifers), a potential food source, radiated in the Late Triassic. Early sauropodomorphs did not have sophisticated mechanisms for processing food in the mouth, and so must have employed other means of breaking down food farther along the digestive tract. The general homogeneity of dinosaurian faunas continued into the Middle and Late Jurassic, where most localities had predators consisting of ceratosaurians, spinosauroids, and carnosaurians, and herbivores consisting of stegosaurian ornithischians and large sauropods. Examples of this include the Morrison Formation of North America and Tendaguru Beds of Tanzania. Dinosaurs in China show some differences, with specialized sinraptorid theropods and unusual, long-necked sauropods like Mamenchisaurus. Ankylosaurians and ornithopods were also becoming more common, but prosauropods had become extinct. Conifers and pteridophytes were the most common plants. Sauropods, like the earlier prosauropods, were not oral processors, but ornithischians were evolving various means of dealing with food in the mouth, including potential cheek-like organs to keep food in the mouth, and jaw motions to grind food. Another notable evolutionary event of the Jurassic was the appearance of true birds, descended from maniraptoran coelurosaurians. By the Early Cretaceous and the ongoing breakup of Pangaea, dinosaurs were becoming strongly differentiated by landmass. The earliest part of this time saw the spread of ankylosaurians, iguanodontians, and brachiosaurids through Europe, North America, and northern Africa. These were later supplemented or replaced in Africa by large spinosaurid and carcharodontosaurid theropods, and rebbachisaurid and titanosaurian sauropods, also found in South America. In Asia, maniraptoran coelurosaurians like dromaeosaurids, troodontids, and oviraptorosaurians became the common theropods, and ankylosaurids and early ceratopsians like Psittacosaurus became important herbivores. Meanwhile, Australia was home to a fauna of basal ankylosaurians, hypsilophodonts, and iguanodontians. The stegosaurians appear to have gone extinct at some point in the late Early Cretaceous or early Late Cretaceous. A major change in the Early Cretaceous, which would be amplified in the Late Cretaceous, was the evolution of flowering plants. At the same time, several groups of dinosaurian herbivores evolved more sophisticated ways to orally process food. Ceratopsians developed a method of slicing with teeth stacked on each other in batteries, and iguanodontians refined a method of grinding with dental batteries, taken to its extreme in hadrosaurids. Some sauropods also evolved tooth batteries, best exemplified by the rebbachisaurid Nigersaurus. There were three general dinosaur faunas in the Late Cretaceous. In the northern continents of North America and Asia, the major theropods were tyrannosaurids and various types of smaller maniraptoran theropods, with a predominantly ornithischian herbivore assemblage of hadrosaurids, ceratopsians, ankylosaurids, and pachycephalosaurians. In the southern continents that had made up the now-splitting Gondwana, abelisaurids were the common theropods, and titanosaurian sauropods the common herbivores. Finally, in Europe, dromaeosaurids, rhabdodontid iguanodontians, nodosaurid ankylosaurians, and titanosaurian sauropods were prevalent. Flowering plants were greatly radiating, with the first grasses appearing by the end of the Cretaceous. Grinding hadrosaurids and shearing ceratopsians became extremely diverse across North America and Asia. Theropods were also radiating as herbivores or omnivores, with therizinosaurians and ornithomimosaurians becoming common.The Cretaceous–Paleogene extinction event, which occurred approximately 66 million years ago at the end of the Cretaceous, caused the extinction of all dinosaur groups except for the neornithine birds. Some other diapsid groups, such as crocodilians, sebecosuchians, turtles, lizards, snakes, sphenodontians, and choristoderans, also survived the event.The surviving lineages of neornithine birds, including the ancestors of modern ratites, ducks and chickens, and a variety of waterbirds, diversified rapidly at the beginning of the Paleogene period, entering ecological niches left vacant by the extinction of Mesozoic dinosaur groups such as the arboreal enantiornithines, aquatic hesperornithines, and even the larger terrestrial theropods (in the form of Gastornis, eogruiids, bathornithids, ratites, geranoidids, mihirungs, and "terror birds"). It is often cited that mammals out-competed the neornithines for dominance of most terrestrial niches but many of these groups co-existed with rich mammalian faunas for most of the Cenozoic Era. Terror birds and bathornithids occupied carnivorous guilds alongside predatory mammals, and ratites are still fairly successful as mid-sized herbivores; eogruiids similarly lasted from the Eocene to Pliocene, only becoming extinct very recently after over 20 million years of co-existence with many mammal groups.
Dinosaurs belong to a group known as archosaurs, which also includes modern crocodilians. Within the archosaur group, dinosaurs are differentiated most noticeably by their gait. Dinosaur legs extend directly beneath the body, whereas the legs of lizards and crocodilians sprawl out to either side.Collectively, dinosaurs as a clade are divided into two primary branches, Saurischia and Ornithischia. Saurischia includes those taxa sharing a more recent common ancestor with birds than with Ornithischia, while Ornithischia includes all taxa sharing a more recent common ancestor with Triceratops than with Saurischia. Anatomically, these two groups can be distinguished most noticeably by their pelvic structure. Early saurischians—"lizard-hipped", from the Greek sauros (σαῦρος) meaning "lizard" and ischion (ἰσχίον) meaning "hip joint"—retained the hip structure of their ancestors, with a pubis bone directed cranially, or forward. This basic form was modified by rotating the pubis backward to varying degrees in several groups (Herrerasaurus, therizinosauroids, dromaeosaurids, and birds). Saurischia includes the theropods (exclusively bipedal and with a wide variety of diets) and sauropodomorphs (long-necked herbivores which include advanced, quadrupedal groups).By contrast, ornithischians—"bird-hipped", from the Greek ornitheios (ὀρνίθειος) meaning "of a bird" and ischion (ἰσχίον) meaning "hip joint"—had a pelvis that superficially resembled a bird's pelvis: the pubic bone was oriented caudally (rear-pointing). Unlike birds, the ornithischian pubis also usually had an additional forward-pointing process. Ornithischia includes a variety of species that were primarily herbivores. Despite the terms "bird hip" and "lizard hip", birds are not part of Ornithischia, but rather Saurischia—birds evolved from earlier dinosaurs with "lizard hips".
The following is a simplified classification of dinosaur groups based on their evolutionary relationships, and organized based on the list of Mesozoic dinosaur species provided by Holtz (2007). A more detailed version can be found at Dinosaur classification. The dagger (†) is used to signify groups with no living members. DinosauriaSaurischia ("lizard-hipped"; includes Theropoda and Sauropodomorpha)†Herrerasauria (early bipedal carnivores) Theropoda (all bipedal; most were carnivorous) †Coelophysoidea (small, early theropods; includes Coelophysis and close relatives) †Dilophosauridae (early crested and carnivorous theropods) †Ceratosauria (generally elaborately horned, the dominant southern carnivores of the Cretaceous) Tetanurae ("stiff tails"; includes most theropods)†Megalosauroidea (early group of large carnivores including the semiaquatic spinosaurids) †Carnosauria (Allosaurus and close relatives, like Carcharodontosaurus) Coelurosauria (feathered theropods, with a range of body sizes and niches)†Compsognathidae (common early coelurosaurs with reduced forelimbs) †Tyrannosauridae (Tyrannosaurus and close relatives; had reduced forelimbs) †Ornithomimosauria ("ostrich-mimics"; mostly toothless; carnivores to possible herbivores) †Alvarezsauroidea (small insectivores with reduced forelimbs each bearing one enlarged claw) Maniraptora ("hand snatchers"; had long, slender arms and fingers)†Therizinosauria (bipedal herbivores with large hand claws and small heads) †Oviraptorosauria (mostly toothless; their diet and lifestyle are uncertain) †Archaeopterygidae (small, winged theropods or primitive birds) †Deinonychosauria (small- to medium-sized; bird-like, with a distinctive toe claw) Avialae (modern birds and extinct relatives)†Scansoriopterygidae (small primitive avialans with long third fingers) †Omnivoropterygidae (large, early short-tailed avialans) †Confuciusornithidae (small toothless avialans) †Enantiornithes (primitive tree-dwelling, flying avialans) Euornithes (advanced flying birds)†Yanornithiformes (toothed Cretaceous Chinese birds) †Hesperornithes (specialized aquatic diving birds) Aves (modern, beaked birds and their extinct relatives) †Sauropodomorpha (herbivores with small heads, long necks, long tails)†Guaibasauridae (small, primitive, omnivorous sauropodomorphs) †Plateosauridae (primitive, strictly bipedal "prosauropods") †Riojasauridae (small, primitive sauropodomorphs) †Massospondylidae (small, primitive sauropodomorphs) †Sauropoda (very large and heavy, usually over 15 m (49 ft) long; quadrupedal)†Vulcanodontidae (primitive sauropods with pillar-like limbs) †Eusauropoda ("true sauropods")†Cetiosauridae ("whale reptiles") †Turiasauria (European group of Jurassic and Cretaceous sauropods) †Neosauropoda ("new sauropods")†Diplodocoidea (skulls and tails elongated; teeth typically narrow and pencil-like) †Macronaria (boxy skulls; spoon- or pencil-shaped teeth)†Brachiosauridae (long-necked, long-armed macronarians) †Titanosauria (diverse; stocky, with wide hips; most common in the Late Cretaceous of southern continents) †Ornithischia ("bird-hipped"; diverse bipedal and quadrupedal herbivores)†Heterodontosauridae (small basal ornithopod herbivores/omnivores with prominent canine-like teeth) †Thyreophora (armored dinosaurs; mostly quadrupeds)†Ankylosauria (scutes as primary armor; some had club-like tails) †Stegosauria (spikes and plates as primary armor)†Neornithischia ("new ornithischians")†Ornithopoda (various sizes; bipeds and quadrupeds; evolved a method of chewing using skull flexibility and numerous teeth) †Marginocephalia (characterized by a cranial growth)†Pachycephalosauria (bipeds with domed or knobby growth on skulls) †Ceratopsia (quadrupeds with frills; many also had horns)
Knowledge about dinosaurs is derived from a variety of fossil and non-fossil records, including fossilized bones, feces, trackways, gastroliths, feathers, impressions of skin, internal organs and soft tissues. Many fields of study contribute to our understanding of dinosaurs, including physics (especially biomechanics), chemistry, biology, and the Earth sciences (of which paleontology is a sub-discipline). Two topics of particular interest and study have been dinosaur size and behavior.
Current evidence suggests that dinosaur average size varied through the Triassic, Early Jurassic, Late Jurassic and Cretaceous. Predatory theropod dinosaurs, which occupied most terrestrial carnivore niches during the Mesozoic, most often fall into the 100 to 1000 kg (220 to 2200 lb) category when sorted by estimated weight into categories based on order of magnitude, whereas recent predatory carnivoran mammals peak in the 10 to 100 kg (22 to 220 lb) category. The mode of Mesozoic dinosaur body masses is between 1 to 10 metric tons (1.1 to 11.0 short tons). This contrasts sharply with the average size of Cenozoic mammals, estimated by the National Museum of Natural History as about 2 to 5 kg (4.4 to 11.0 lb).The sauropods were the largest and heaviest dinosaurs. For much of the dinosaur era, the smallest sauropods were larger than anything else in their habitat, and the largest was an order of magnitude more massive than anything else that has since walked the Earth. Giant prehistoric mammals such as Paraceratherium (the largest land mammal ever) were dwarfed by the giant sauropods, and only modern whales approach or surpass them in size. There are several proposed advantages for the large size of sauropods, including protection from predation, reduction of energy use, and longevity, but it may be that the most important advantage was dietary. Large animals are more efficient at digestion than small animals, because food spends more time in their digestive systems. This also permits them to subsist on food with lower nutritive value than smaller animals. Sauropod remains are mostly found in rock formations interpreted as dry or seasonally dry, and the ability to eat large quantities of low-nutrient browse would have been advantageous in such environments.
Scientists will probably never be certain of the largest and smallest dinosaurs to have ever existed. This is because only a tiny percentage of animals were ever fossilized and most of these remain buried in the earth. Few of the specimens that are recovered are complete skeletons, and impressions of skin and other soft tissues are rare. Rebuilding a complete skeleton by comparing the size and morphology of bones to those of similar, better-known species is an inexact art, and reconstructing the muscles and other organs of the living animal is, at best, a process of educated guesswork.The tallest and heaviest dinosaur known from good skeletons is Giraffatitan brancai (previously classified as a species of Brachiosaurus). Its remains were discovered in Tanzania between 1907 and 1912. Bones from several similar-sized individuals were incorporated into the skeleton now mounted and on display at the Museum für Naturkunde in Berlin; this mount is 12 meters (39 ft) tall and 21.8 to 22.5 meters (72 to 74 ft) long, and would have belonged to an animal that weighed between 30000 and 60000 kilograms (70000 and 130000 lb). The longest complete dinosaur is the 27 meters (89 ft) long Diplodocus, which was discovered in Wyoming in the United States and displayed in Pittsburgh's Carnegie Museum of Natural History in 1907. The longest dinosaur known from good fossil material is the Patagotitan: the skeleton mount in the American Museum of Natural History in New York is 37 meters (121 ft) long. The Museo Municipal Carmen Funes in Plaza Huincul, Argentina, has an Argentinosaurus reconstructed skeleton mount 39.7 meters (130 ft) long. There were larger dinosaurs, but knowledge of them is based entirely on a small number of fragmentary fossils. Most of the largest herbivorous specimens on record were discovered in the 1970s or later, and include the massive Argentinosaurus, which may have weighed 80000 to 100000 kilograms (90 to 110 short tons) and reached length of 30 to 40 meters (98 to 131 ft); some of the longest were the 33.5 meters (110 ft) long Diplodocus hallorum (formerly Seismosaurus), the 33 to 34 meters (108 to 112 ft) long Supersaurus and 37 meters (121 ft) long Patagotitan; and the tallest, the 18 meters (59 ft) tall Sauroposeidon, which could have reached a sixth-floor window. The heaviest and longest dinosaur may have been Maraapunisaurus, known only from a now lost partial vertebral neural arch described in 1878. Extrapolating from the illustration of this bone, the animal may have been 58 meters (190 ft) long and weighed 122400 kg (270000 lb). However, as no further evidence of sauropods of this size has been found, and the discoverer, Edward Drinker Cope, had made typographic errors before, it is likely to have been an extreme overestimation.The largest carnivorous dinosaur was Spinosaurus, reaching a length of 12.6 to 18 meters (41 to 59 ft), and weighing 7 to 20.9 metric tons (7.7 to 23.0 short tons). Other large carnivorous theropods included Giganotosaurus, Carcharodontosaurus and Tyrannosaurus. Therizinosaurus and Deinocheirus were among the tallest of the theropods. The largest ornithischian dinosaur was probably the hadrosaurid Shantungosaurus giganteus which measured 16.6 meters (54 ft). The largest individuals may have weighed as much as 16 metric tons (18 short tons).The smallest dinosaur known is the bee hummingbird, with a length of only 5 centimeters (2.0 in) and mass of around 1.8 g (0.063 oz). The smallest known non-avialan dinosaurs were about the size of pigeons and were those theropods most closely related to birds. For example, Anchiornis huxleyi is currently the smallest non-avialan dinosaur described from an adult specimen, with an estimated weight of 110 g (3.9 oz) and a total skeletal length of 34 centimeters (1.12 ft). The smallest herbivorous non-avialan dinosaurs included Microceratus and Wannanosaurus, at about 60 centimeters (2.0 ft) long each.
Many modern birds are highly social, often found living in flocks. There is general agreement that some behaviors that are common in birds, as well as in crocodiles (birds' closest living relatives), were also common among extinct dinosaur groups. Interpretations of behavior in fossil species are generally based on the pose of skeletons and their habitat, computer simulations of their biomechanics, and comparisons with modern animals in similar ecological niches.The first potential evidence for herding or flocking as a widespread behavior common to many dinosaur groups in addition to birds was the 1878 discovery of 31 Iguanodon bernissartensis, ornithischians that were then thought to have perished together in Bernissart, Belgium, after they fell into a deep, flooded sinkhole and drowned. Other mass-death sites have been discovered subsequently. Those, along with multiple trackways, suggest that gregarious behavior was common in many early dinosaur species. Trackways of hundreds or even thousands of herbivores indicate that duck-billed (hadrosaurids) may have moved in great herds, like the American bison or the African Springbok. Sauropod tracks document that these animals traveled in groups composed of several different species, at least in Oxfordshire, England, although there is no evidence for specific herd structures. Congregating into herds may have evolved for defense, for migratory purposes, or to provide protection for young. There is evidence that many types of slow-growing dinosaurs, including various theropods, sauropods, ankylosaurians, ornithopods, and ceratopsians, formed aggregations of immature individuals. One example is a site in Inner Mongolia that has yielded the remains of over 20 Sinornithomimus, from one to seven years old. This assemblage is interpreted as a social group that was trapped in mud. The interpretation of dinosaurs as gregarious has also extended to depicting carnivorous theropods as pack hunters working together to bring down large prey. However, this lifestyle is uncommon among modern birds, crocodiles, and other reptiles, and the taphonomic evidence suggesting mammal-like pack hunting in such theropods as Deinonychus and Allosaurus can also be interpreted as the results of fatal disputes between feeding animals, as is seen in many modern diapsid predators. The crests and frills of some dinosaurs, like the marginocephalians, theropods and lambeosaurines, may have been too fragile to be used for active defense, and so they were likely used for sexual or aggressive displays, though little is known about dinosaur mating and territorialism. Head wounds from bites suggest that theropods, at least, engaged in active aggressive confrontations.From a behavioral standpoint, one of the most valuable dinosaur fossils was discovered in the Gobi Desert in 1971. It included a Velociraptor attacking a Protoceratops, providing evidence that dinosaurs did indeed attack each other. Additional evidence for attacking live prey is the partially healed tail of an Edmontosaurus, a hadrosaurid dinosaur; the tail is damaged in such a way that shows the animal was bitten by a tyrannosaur but survived. Cannibalism amongst some species of dinosaurs was confirmed by tooth marks found in Madagascar in 2003, involving the theropod Majungasaurus.Comparisons between the scleral rings of dinosaurs and modern birds and reptiles have been used to infer daily activity patterns of dinosaurs. Although it has been suggested that most dinosaurs were active during the day, these comparisons have shown that small predatory dinosaurs such as dromaeosaurids, Juravenator, and Megapnosaurus were likely nocturnal. Large and medium-sized herbivorous and omnivorous dinosaurs such as ceratopsians, sauropodomorphs, hadrosaurids, ornithomimosaurs may have been cathemeral, active during short intervals throughout the day, although the small ornithischian Agilisaurus was inferred to be diurnal.Based on current fossil evidence from dinosaurs such as Oryctodromeus, some ornithischian species seem to have led a partially fossorial (burrowing) lifestyle. Many modern birds are arboreal (tree climbing), and this was also true of many Mesozoic birds, especially the enantiornithines. While some early bird-like species may have already been arboreal as well (including dromaeosaurids such as Microraptor) most non-avialan dinosaurs seem to have relied on land-based locomotion. A good understanding of how dinosaurs moved on the ground is key to models of dinosaur behavior; the science of biomechanics, pioneered by Robert McNeill Alexander, has provided significant insight in this area. For example, studies of the forces exerted by muscles and gravity on dinosaurs' skeletal structure have investigated how fast dinosaurs could run, whether diplodocids could create sonic booms via whip-like tail snapping, and whether sauropods could float.
Modern birds are known to communicate using visual and auditory signals, and the wide diversity of visual display structures among fossil dinosaur groups, such as horns, frills, crests, sails, and feathers, suggests that visual communication has always been important in dinosaur biology. Reconstruction of the plumage color of Anchiornis huxleyi, suggest the importance of color in visual communication in non-avian dinosaurs. The evolution of dinosaur vocalization is less certain. Paleontologist Phil Senter suggests that non-avian dinosaurs relied mostly on visual displays and possibly non-vocal acoustic sounds like hissing, jaw grinding or clapping, splashing and wing beating (possible in winged maniraptoran dinosaurs). He states they were unlikely to have been capable of vocalizing since their closest relatives, crocodilians and birds, use different means to vocalize, the former via the larynx and the latter through the unique syrinx, suggesting they evolved independently and their common ancestor was mute.The earliest remains of a syrinx, which has enough mineral content for fossilization, was found in a specimen of the duck-like Vegavis iaai dated 69 –66 million years ago, and this organ is unlikely to have existed in non-avian dinosaurs. However, in contrast to Senter, the researchers have suggested that dinosaurs could vocalize and that the syrinx-based vocal system of birds evolved from a larynx-based one, rather than the two systems evolving independently. A 2016 study suggests that dinosaurs produced closed mouth vocalizations like cooing, which occur in both crocodilians and birds as well as other reptiles. Such vocalizations evolved independently in extant archosaurs numerous times, following increases in body size. The crests of the Lambeosaurini and nasal chambers of ankylosaurids have been suggested to function in vocal resonance, though Senter states that the presence of resonance chambers in some dinosaurs is not necessarily evidence of vocalization as modern snakes have such chambers which intensify their hisses.
All dinosaurs laid amniotic eggs with hard shells made mostly of calcium carbonate. Dinosaur eggs were usually laid in a nest. Most species create somewhat elaborate nests which can be cups, domes, plates, beds scrapes, mounds, or burrows. Some species of modern bird have no nests; the cliff-nesting common guillemot lays its eggs on bare rock, and male emperor penguins keep eggs between their body and feet. Primitive birds and many non-avialan dinosaurs often lay eggs in communal nests, with males primarily incubating the eggs. While modern birds have only one functional oviduct and lay one egg at a time, more primitive birds and dinosaurs had two oviducts, like crocodiles. Some non-avialan dinosaurs, such as Troodon, exhibited iterative laying, where the adult might lay a pair of eggs every one or two days, and then ensured simultaneous hatching by delaying brooding until all eggs were laid.When laying eggs, females grow a special type of bone between the hard outer bone and the marrow of their limbs. This medullary bone, which is rich in calcium, is used to make eggshells. A discovery of features in a Tyrannosaurus rex skeleton provided evidence of medullary bone in extinct dinosaurs and, for the first time, allowed paleontologists to establish the sex of a fossil dinosaur specimen. Further research has found medullary bone in the carnosaur Allosaurus and the ornithopod Tenontosaurus. Because the line of dinosaurs that includes Allosaurus and Tyrannosaurus diverged from the line that led to Tenontosaurus very early in the evolution of dinosaurs, this suggests that the production of medullary tissue is a general characteristic of all dinosaurs. Another widespread trait among modern birds (but see below in regards to fossil groups and extant megapodes) is parental care for young after hatching. Jack Horner's 1978 discovery of a Maiasaura ("good mother lizard") nesting ground in Montana demonstrated that parental care continued long after birth among ornithopods. A specimen of the Mongolian oviraptorid Citipati osmolskae was discovered in a chicken-like brooding position in 1993, which may indicate that they had begun using an insulating layer of feathers to keep the eggs warm. A dinosaur embryo (pertaining to the prosauropod Massospondylus) was found without teeth, indicating that some parental care was required to feed the young dinosaurs. Trackways have also confirmed parental behavior among ornithopods from the Isle of Skye in northwestern Scotland.However, there is ample evidence of precociality or superprecociality among many dinosaur species, particularly theropods. For instance, non-ornithuromorph birds have been abundantly demonstrated to have had slow growth rates, megapode-like egg burying behavior and the ability to fly soon after birth. Both Tyrannosaurus rex and Troodon formosus display juveniles with clear superprecociality and likely occupying different ecological niches than the adults. Superprecociality has been inferred for sauropods.
Because both modern crocodilians and birds have four-chambered hearts (albeit modified in crocodilians), it is likely that this is a trait shared by all archosaurs, including all dinosaurs. While all modern birds have high metabolisms and are "warm-blooded" (endothermic), a vigorous debate has been ongoing since the 1960s regarding how far back in the dinosaur lineage this trait extends. Scientists disagree as to whether non-avian dinosaurs were endothermic, ectothermic, or some combination of both.After non-avian dinosaurs were discovered, paleontologists first posited that they were ectothermic. This supposed "cold-bloodedness" was used to imply that the ancient dinosaurs were relatively slow, sluggish organisms, even though many modern reptiles are fast and light-footed despite relying on external sources of heat to regulate their body temperature. The idea of dinosaurs as ectothermic remained a prevalent view until Robert T. "Bob" Bakker, an early proponent of dinosaur endothermy, published an influential paper on the topic in 1968.Modern evidence indicates that some non-avian dinosaurs thrived in cooler temperate climates and that some early species must have regulated their body temperature by internal biological means (aided by the animals' bulk in large species and feathers or other body coverings in smaller species). Evidence of endothermy in Mesozoic dinosaurs includes the discovery of polar dinosaurs in Australia and Antarctica as well as analysis of blood-vessel structures within fossil bones that are typical of endotherms. Scientific debate continues regarding the specific ways in which dinosaur temperature regulation evolved. In saurischian dinosaurs, higher metabolisms were supported by the evolution of the avian respiratory system, characterized by an extensive system of air sacs that extended the lungs and invaded many of the bones in the skeleton, making them hollow. Early avian-style respiratory systems with air sacs may have been capable of sustaining higher activity levels than those of mammals of similar size and build. In addition to providing a very efficient supply of oxygen, the rapid airflow would have been an effective cooling mechanism, which is essential for animals that are active but too large to get rid of all the excess heat through their skin.Like other reptiles, dinosaurs are primarily uricotelic, that is, their kidneys extract nitrogenous wastes from their bloodstream and excrete it as uric acid instead of urea or ammonia via the ureters into the intestine. In most living species, uric acid is excreted along with feces as a semisolid waste. However, at least some modern birds (such as hummingbirds) can be facultatively ammonotelic, excreting most of the nitrogenous wastes as ammonia. This material, as well as the output of the intestines, emerges from the cloaca. In addition, many species regurgitate pellets, and fossil pellets that may have come from dinosaurs are known from as long ago as the Cretaceous.
The possibility that dinosaurs were the ancestors of birds was first suggested in 1868 by Thomas Henry Huxley. After the work of Gerhard Heilmann in the early 20th century, the theory of birds as dinosaur descendants was abandoned in favor of the idea of their being descendants of generalized thecodonts, with the key piece of evidence being the supposed lack of clavicles in dinosaurs. However, as later discoveries showed, clavicles (or a single fused wishbone, which derived from separate clavicles) were not actually absent; they had been found as early as 1924 in Oviraptor, but misidentified as an interclavicle. In the 1970s, John Ostrom revived the dinosaur–bird theory, which gained momentum in the coming decades with the advent of cladistic analysis, and a great increase in the discovery of small theropods and early birds. Of particular note have been the fossils of the Yixian Formation, where a variety of theropods and early birds have been found, often with feathers of some type. Birds share over a hundred distinct anatomical features with theropod dinosaurs, which are now generally accepted to have been their closest ancient relatives. They are most closely allied with maniraptoran coelurosaurs. A minority of scientists, most notably Alan Feduccia and Larry Martin, have proposed other evolutionary paths, including revised versions of Heilmann's basal archosaur proposal, or that maniraptoran theropods are the ancestors of birds but themselves are not dinosaurs, only convergent with dinosaurs.
Feathers are one of the most recognizable characteristics of modern birds, and a trait that was shared by all other dinosaur groups. Based on the current distribution of fossil evidence, it appears that feathers were an ancestral dinosaurian trait, though one that may have been selectively lost in some species. Direct fossil evidence of feathers or feather-like structures has been discovered in a diverse array of species in many non-avian dinosaur groups, both among saurischians and ornithischians. Simple, branched, feather-like structures are known from heterodontosaurids, primitive neornithischians and theropods, and primitive ceratopsians. Evidence for true, vaned feathers similar to the flight feathers of modern birds has been found only in the theropod subgroup Maniraptora, which includes oviraptorosaurs, troodontids, dromaeosaurids, and birds. Feather-like structures known as pycnofibres have also been found in pterosaurs, suggesting the possibility that feather-like filaments may have been common in the bird lineage and evolved before the appearance of dinosaurs themselves. Research into the genetics of American alligators has also revealed that crocodylian scutes do possess feather-keratins during embryonic development, but these keratins are not expressed by the animals before hatching.Archaeopteryx was the first fossil found that revealed a potential connection between dinosaurs and birds. It is considered a transitional fossil, in that it displays features of both groups. Brought to light just two years after Charles Darwin's seminal On the Origin of Species (1859), its discovery spurred the nascent debate between proponents of evolutionary biology and creationism. This early bird is so dinosaur-like that, without a clear impression of feathers in the surrounding rock, at least one specimen was mistaken for Compsognathus. Since the 1990s, a number of additional feathered dinosaurs have been found, providing even stronger evidence of the close relationship between dinosaurs and modern birds. Most of these specimens were unearthed in the lagerstätte of the Yixian Formation, Liaoning, northeastern China, which was part of an island continent during the Cretaceous. Though feathers have been found in only a few locations, it is possible that non-avian dinosaurs elsewhere in the world were also feathered. The lack of widespread fossil evidence for feathered non-avian dinosaurs may be because delicate features like skin and feathers are not often preserved by fossilization and thus are absent from the fossil record.The description of feathered dinosaurs has not been without controversy; perhaps the most vocal critics have been Alan Feduccia and Theagarten Lingham-Soliar, who have proposed that some purported feather-like fossils are the result of the decomposition of collagenous fiber that underlaid the dinosaurs' skin, and that maniraptoran dinosaurs with vaned feathers were not actually dinosaurs, but convergent with dinosaurs. However, their views have for the most part not been accepted by other researchers, to the point that the scientific nature of Feduccia's proposals has been questioned.In 2016, it was reported that a dinosaur tail with feathers had been found enclosed in amber. The fossil is about 99 million years old.
Because feathers are often associated with birds, feathered dinosaurs are often touted as the missing link between birds and dinosaurs. However, the multiple skeletal features also shared by the two groups represent another important line of evidence for paleontologists. Areas of the skeleton with important similarities include the neck, pubis, wrist (semi-lunate carpal), arm and pectoral girdle, furcula (wishbone), and breast bone. Comparison of bird and dinosaur skeletons through cladistic analysis strengthens the case for the link.
Large meat-eating dinosaurs had a complex system of air sacs similar to those found in modern birds, according to a 2005 investigation led by Patrick M. O'Connor. The lungs of theropod dinosaurs (carnivores that walked on two legs and had bird-like feet) likely pumped air into hollow sacs in their skeletons, as is the case in birds. "What was once formally considered unique to birds was present in some form in the ancestors of birds", O'Connor said. In 2008, scientists described Aerosteon riocoloradensis, the skeleton of which supplies the strongest evidence to date of a dinosaur with a bird-like breathing system. CT scanning of Aerosteon's fossil bones revealed evidence for the existence of air sacs within the animal's body cavity.
Fossils of the troodonts Mei and Sinornithoides demonstrate that some dinosaurs slept with their heads tucked under their arms. This behavior, which may have helped to keep the head warm, is also characteristic of modern birds. Several deinonychosaur and oviraptorosaur specimens have also been found preserved on top of their nests, likely brooding in a bird-like manner. The ratio between egg volume and body mass of adults among these dinosaurs suggest that the eggs were primarily brooded by the male, and that the young were highly precocial, similar to many modern ground-dwelling birds.Some dinosaurs are known to have used gizzard stones like modern birds. These stones are swallowed by animals to aid digestion and break down food and hard fibers once they enter the stomach. When found in association with fossils, gizzard stones are called gastroliths.
The discovery that birds are a type of dinosaur showed that dinosaurs in general are not, in fact, extinct as is commonly stated. However, all non-avian dinosaurs, estimated to have been 628-1078 species, as well as many groups of birds did suddenly become extinct approximately 66 million years ago. It has been suggested that because small mammals, squamata and birds occupied the ecological niches suited for small body size, non-avian dinosaurs never evolved a diverse fauna of small-bodied species, which led to their downfall when large-bodied terrestrial tetrapods were hit by the mass extinction event. Many other groups of animals also became extinct at this time, including ammonites (nautilus-like mollusks), mosasaurs, plesiosaurs, pterosaurs, and many groups of mammals. Significantly, the insects suffered no discernible population loss, which left them available as food for other survivors. This mass extinction is known as the Cretaceous–Paleogene extinction event. The nature of the event that caused this mass extinction has been extensively studied since the 1970s; at present, several related theories are supported by paleontologists. Though the consensus is that an impact event was the primary cause of dinosaur extinction, some scientists cite other possible causes, or support the idea that a confluence of several factors was responsible for the sudden disappearance of dinosaurs from the fossil record.
The asteroid impact hypothesis, which was brought to wide attention in 1980 by Walter Alvarez and colleagues, links the extinction event at the end of the Cretaceous to a bolide impact approximately 66 million years ago. Alvarez et al. proposed that a sudden increase in iridium levels, recorded around the world in the period's rock stratum, was direct evidence of the impact. The bulk of the evidence now suggests that a bolide 5 to 15 kilometers (3.1 to 9.3 miles) wide hit in the vicinity of the Yucatán Peninsula (in southeastern Mexico), creating the approximately 180 km (110 mi) Chicxulub crater and triggering the mass extinction.Scientists are not certain whether dinosaurs were thriving or declining before the impact event. Some scientists propose that the meteorite impact caused a long and unnatural drop in Earth's atmospheric temperature, while others claim that it would have instead created an unusual heat wave. The consensus among scientists who support this hypothesis is that the impact caused extinctions both directly (by heat from the meteorite impact) and also indirectly (via a worldwide cooling brought about when matter ejected from the impact crater reflected thermal radiation from the sun). Although the speed of extinction cannot be deduced from the fossil record alone, various models suggest that the extinction was extremely rapid, being down to hours rather than years.In 2019, scientists drilling into the seafloor off Mexico extracted a unique geologic record of what they believe to be the day a city-sized asteroid smashed into the planet.
Before 2000, arguments that the Deccan Traps flood basalts caused the extinction were usually linked to the view that the extinction was gradual, as the flood basalt events were thought to have started around 68 million years ago and lasted for over 2 million years. However, there is evidence that two-thirds of the Deccan Traps were created in only 1 million years about 66 million years ago, and so these eruptions would have caused a fairly rapid extinction, possibly over a period of thousands of years, but still longer than would be expected from a single impact event.The Deccan Traps in India could have caused extinction through several mechanisms, including the release into the air of dust and sulfuric aerosols, which might have blocked sunlight and thereby reduced photosynthesis in plants. In addition, Deccan Trap volcanism might have resulted in carbon dioxide emissions, which would have increased the greenhouse effect when the dust and aerosols cleared from the atmosphere. Before the mass extinction of the dinosaurs, the release of volcanic gases during the formation of the Deccan Traps "contributed to an apparently massive global warming. Some data point to an average rise in temperature of [8 °C (14 °F)] in the last half-million years before the impact [at Chicxulub]."In the years when the Deccan Traps hypothesis was linked to a slower extinction, Luis Alvarez (who died in 1988) replied that paleontologists were being misled by sparse data. While his assertion was not initially well-received, later intensive field studies of fossil beds lent weight to his claim. Eventually, most paleontologists began to accept the idea that the mass extinctions at the end of the Cretaceous were largely or at least partly due to a massive Earth impact. However, even Walter Alvarez has acknowledged that there were other major changes on Earth even before the impact, such as a drop in sea level and massive volcanic eruptions that produced the Indian Deccan Traps, and these may have contributed to the extinctions.
Non-avian dinosaur remains are occasionally found above the Cretaceous–Paleogene boundary. In 2000, paleontologist Spencer G. Lucas et al. reported the discovery of a single hadrosaur right femur in the San Juan Basin, New Mexico, and described it as evidence of Paleocene dinosaurs. The formation in which the bone was discovered has been dated to the early Paleocene epoch, approximately 64.5 million years ago. If the bone was not re-deposited into that stratum by weathering action, it would provide evidence that some dinosaur populations may have survived at least a half-million years into the Cenozoic. Other evidence includes the finding of dinosaur remains in the Hell Creek Formation up to 1.3 m (51 in) above the Cretaceous–Paleogene boundary, representing 40000 years of elapsed time. Similar reports have come from other parts of the world, including China. Many scientists, however, dismissed the supposed Paleocene dinosaurs as re-worked, that is, washed out of their original locations and then re-buried in much later sediments. Direct dating of the bones themselves has supported the later date, with uranium–lead dating methods resulting in a precise age of 64.8 ± 0.9 million years ago. If correct, the presence of a handful of dinosaurs in the early Paleocene would not change the underlying facts of the extinction.
Dinosaur fossils have been known for millennia, although their true nature was not recognized. The Chinese considered them to be dragon bones and documented them as such. For example, Huayang Guo Zhi (華陽國志), a gazetteer compiled by Chang Qu (常璩) during the Western Jin Dynasty (265–316), reported the discovery of dragon bones at Wucheng in Sichuan Province. Villagers in central China have long unearthed fossilized "dragon bones" for use in traditional medicines. In Europe, dinosaur fossils were generally believed to be the remains of giants and other biblical creatures.Scholarly descriptions of what would now be recognized as dinosaur bones first appeared in the late 17th century in England. Part of a bone, now known to have been the femur of a Megalosaurus, was recovered from a limestone quarry at Cornwell near Chipping Norton, Oxfordshire, in 1676. The fragment was sent to Robert Plot, Professor of Chemistry at the University of Oxford and first curator of the Ashmolean Museum, who published a description in his The Natural History of Oxford-shire (1677). He correctly identified the bone as the lower extremity of the femur of a large animal, and recognized that it was too large to belong to any known species. He, therefore, concluded it to be the femur of a huge human, perhaps a Titan or another type of giant featured in legends. Edward Lhuyd, a friend of Sir Isaac Newton, published Lithophylacii Britannici ichnographia (1699), the first scientific treatment of what would now be recognized as a dinosaur when he described and named a sauropod tooth, Rutellum impicatum, that had been found in Caswell, near Witney, Oxfordshire. Between 1815 and 1824, the Rev William Buckland, the first Reader of Geology at the University of Oxford, collected more fossilized bones of Megalosaurus and became the first person to describe a dinosaur in a scientific journal. The second dinosaur genus to be identified, Iguanodon, was discovered in 1822 by Mary Ann Mantell – the wife of English geologist Gideon Mantell. Gideon Mantell recognized similarities between his fossils and the bones of modern iguanas. He published his findings in 1825.The study of these "great fossil lizards" soon became of great interest to European and American scientists, and in 1842 the English paleontologist Richard Owen coined the term "dinosaur". He recognized that the remains that had been found so far, Iguanodon, Megalosaurus and Hylaeosaurus, shared a number of distinctive features, and so decided to present them as a distinct taxonomic group. With the backing of Prince Albert, the husband of Queen Victoria, Owen established the Natural History Museum, London, to display the national collection of dinosaur fossils and other biological and geological exhibits.In 1858, William Parker Foulke discovered the first known American dinosaur, in marl pits in the small town of Haddonfield, New Jersey. (Although fossils had been found before, their nature had not been correctly discerned.) The creature was named Hadrosaurus foulkii. It was an extremely important find: Hadrosaurus was one of the first nearly complete dinosaur skeletons found (the first was in 1834, in Maidstone, England), and it was clearly a bipedal creature. This was a revolutionary discovery as, until that point, most scientists had believed dinosaurs walked on four feet, like other lizards. Foulke's discoveries sparked a wave of interests in dinosaurs in the United States, known as dinosaur mania. Dinosaur mania was exemplified by the fierce rivalry between Edward Drinker Cope and Othniel Charles Marsh, both of whom raced to be the first to find new dinosaurs in what came to be known as the Bone Wars. The feud probably originated when Marsh publicly pointed out that Cope's reconstruction of an Elasmosaurus skeleton was flawed: Cope had inadvertently placed the plesiosaur's head at what should have been the animal's tail end. The fight between the two scientists lasted for over 30 years, ending in 1897 when Cope died after spending his entire fortune on the dinosaur hunt. Unfortunately, many valuable dinosaur specimens were damaged or destroyed due to the pair's rough methods: for example, their diggers often used dynamite to unearth bones. Modern paleontologists would find such methods crude and unacceptable, since blasting easily destroys fossil and stratigraphic evidence. Despite their unrefined methods, the contributions of Cope and Marsh to paleontology were vast: Marsh unearthed 86 new species of dinosaur and Cope discovered 56, a total of 142 new species. Cope's collection is now at the American Museum of Natural History, while Marsh's is on display at the Peabody Museum of Natural History at Yale University.After 1897, the search for dinosaur fossils extended to every continent, including Antarctica. The first Antarctic dinosaur to be discovered, the ankylosaurid Antarctopelta oliveroi, was found on James Ross Island in 1986, although it was 1994 before an Antarctic species, the theropod Cryolophosaurus ellioti, was formally named and described in a scientific journal.Current dinosaur "hot spots" include southern South America (especially Argentina) and China. China, in particular, has produced many exceptional feathered dinosaur specimens due to the unique geology of its dinosaur beds, as well as an ancient arid climate particularly conducive to fossilization.
The field of dinosaur research has enjoyed a surge in activity that began in the 1970s and is ongoing. This was triggered, in part, by John Ostrom's discovery of Deinonychus, an active predator that may have been warm-blooded, in marked contrast to the then-prevailing image of dinosaurs as sluggish and cold-blooded. Vertebrate paleontology has become a global science. Major new dinosaur discoveries have been made by paleontologists working in previously unexploited regions, including India, South America, Madagascar, Antarctica, and most significantly China (the well-preserved feathered dinosaurs in China have further consolidated the link between dinosaurs and their living descendants, modern birds). The widespread application of cladistics, which rigorously analyzes the relationships between biological organisms, has also proved tremendously useful in classifying dinosaurs. Cladistic analysis, among other modern techniques, helps to compensate for an often incomplete and fragmentary fossil record.
One of the best examples of soft-tissue impressions in a fossil dinosaur was discovered in the Pietraroia Plattenkalk in southern Italy. The discovery was reported in 1998, and described the specimen of a small, very young coelurosaur, Scipionyx samniticus. The fossil includes portions of the intestines, colon, liver, muscles, and windpipe of this immature dinosaur.In the March 2005 issue of Science, the paleontologist Mary Higby Schweitzer and her team announced the discovery of flexible material resembling actual soft tissue inside a 68-million-year-old Tyrannosaurus rex leg bone from the Hell Creek Formation in Montana. After recovery, the tissue was rehydrated by the science team. When the fossilized bone was treated over several weeks to remove mineral content from the fossilized bone-marrow cavity (a process called demineralization), Schweitzer found evidence of intact structures such as blood vessels, bone matrix, and connective tissue (bone fibers). Scrutiny under the microscope further revealed that the putative dinosaur soft tissue had retained fine structures (microstructures) even at the cellular level. The exact nature and composition of this material, and the implications of Schweitzer's discovery, are not yet clear.In 2009, a team including Schweitzer announced that, using even more careful methodology, they had duplicated their results by finding similar soft tissue in a duck-billed dinosaur, Brachylophosaurus canadensis, found in the Judith River Formation of Montana. This included even more detailed tissue, down to preserved bone cells that seem even to have visible remnants of nuclei and what seem to be red blood cells. Among other materials found in the bone was collagen, as in the Tyrannosaurus bone. The type of collagen an animal has in its bones varies according to its DNA and, in both cases, this collagen was of the same type found in modern chickens and ostriches.The extraction of ancient DNA from dinosaur fossils has been reported on two separate occasions; upon further inspection and peer review, however, neither of these reports could be confirmed. However, a functional peptide involved in the vision of a theoretical dinosaur has been inferred using analytical phylogenetic reconstruction methods on gene sequences of related modern species such as reptiles and birds. In addition, several proteins, including hemoglobin, have putatively been detected in dinosaur fossils.In 2015, researchers reported finding structures similar to blood cells and collagen fibers, preserved in the bone fossils of six Cretaceous dinosaur specimens, which are approximately 75 million years old.
By human standards, dinosaurs were creatures of fantastic appearance and often enormous size. As such, they have captured the popular imagination and become an enduring part of human culture. The entry of the word "dinosaur" into the common vernacular reflects the animals' cultural importance: in English, "dinosaur" is commonly used to describe anything that is impractically large, obsolete, or bound for extinction.Public enthusiasm for dinosaurs first developed in Victorian England, where in 1854, three decades after the first scientific descriptions of dinosaur remains, a menagerie of lifelike dinosaur sculptures was unveiled in London's Crystal Palace Park. The Crystal Palace dinosaurs proved so popular that a strong market in smaller replicas soon developed. In subsequent decades, dinosaur exhibits opened at parks and museums around the world, ensuring that successive generations would be introduced to the animals in an immersive and exciting way. Dinosaurs' enduring popularity, in its turn, has resulted in significant public funding for dinosaur science, and has frequently spurred new discoveries. In the United States, for example, the competition between museums for public attention led directly to the Bone Wars of the 1880s and 1890s, during which a pair of feuding paleontologists made enormous scientific contributions.The popular preoccupation with dinosaurs has ensured their appearance in literature, film, and other media. Beginning in 1852 with a passing mention in Charles Dickens' Bleak House, dinosaurs have been featured in large numbers of fictional works. Jules Verne's 1864 novel Journey to the Center of the Earth, Sir Arthur Conan Doyle's 1912 book The Lost World, the iconic 1933 film King Kong, the 1954 Godzilla and its many sequels, the best-selling 1990 novel Jurassic Park by Michael Crichton and its 1993 film adaptation are just a few notable examples of dinosaur appearances in fiction. Authors of general-interest non-fiction works about dinosaurs, including some prominent paleontologists, who have often sought to use the animals as a way to educate readers about science in general. Dinosaurs are ubiquitous in advertising; numerous companies have referenced dinosaurs in printed or televised advertisements, either in order to sell their own products or in order to characterize their rivals as slow-moving, dim-witted, or obsolete.
Animal track Dinosaur diet and feeding Evolutionary history of life Lists of dinosaur-bearing stratigraphic units List of dinosaur genera List of informally named dinosaurs
DinoDatabase.com – Hundreds of dinosaurs and dinosaur related topics.
The Dino Directory – An illustrated dinosaur directory from the Natural History Museum in London. Dinosaurnews – Dinosaur-related headlines from around the world, including finds and discoveries, and many links. "The Dinosauria – From University of California Museum of Paleontology. Zoom Dinosaurs – From Enchanted Learning. Kids' site, info pages and stats, theories, history. Dinosaur genus list contains data tables on nearly every published Mesozoic dinosaur genus as of January 2011.
Palaeontologia Electronica From Coquina Press. Online technical journal.
Rabbits are small mammals in the family Leporidae of the order Lagomorpha (along with the hare and the pika). Oryctolagus cuniculus includes the European rabbit species and its descendants, the world's 305 breeds of domestic rabbit. Sylvilagus includes 13 wild rabbit species, among them the seven types of cottontail. The European rabbit, which has been introduced on every continent except Antarctica, is familiar throughout the world as a wild prey animal and as a domesticated form of livestock and pet. With its widespread effect on ecologies and cultures, the rabbit (or bunny) is, in many areas of the world, a part of daily life—as food, clothing, a companion, and a source of artistic inspiration. Although once considered rodents, lagomorphs like rabbits have been discovered to have diverged separately and earlier than their rodent cousins, and have a number of traits rodents lack, like two extra incisors.
Hares are precocial, born relatively mature and mobile with hair and good vision, while rabbits are altricial, born hairless and blind, and requiring closer care. Hares (and cottontail rabbits) live a relatively solitary life in a simple nest above the ground, while most rabbits live in social groups in burrows or warrens. Hares are generally larger than rabbits, with ears that are more elongated, and with hind legs that are larger and longer. Hares have not been domesticated, while descendants of the European rabbit are commonly bred as livestock and kept as pets.
Rabbits have long been domesticated. Beginning in the Middle Ages, the European rabbit has been widely kept as livestock, starting in ancient Rome. Selective breeding has generated a wide variety of rabbit breeds, many of which (since the early 19th century) are also kept as pets. Some strains of rabbit have been bred specifically as research subjects. As livestock, rabbits are bred for their meat and fur. The earliest breeds were important sources of meat, and so became larger than wild rabbits, but domestic rabbits in modern times range in size from dwarf to giant. Rabbit fur, prized for its softness, can be found in a broad range of coat colors and patterns, as well as lengths. The Angora rabbit breed, for example, was developed for its long, silky fur, which is often hand-spun into yarn. Other domestic rabbit breeds have been developed primarily for the commercial fur trade, including the Rex, which has a short plush coat.
Because the rabbit's epiglottis is engaged over the soft palate except when swallowing, the rabbit is an obligate nasal breather. Rabbits have two sets of incisor teeth, one behind the other. This way they can be distinguished from rodents, with which they are often confused. Carl Linnaeus originally grouped rabbits and rodents under the class Glires; later, they were separated as the scientific consensus is that many of their similarities were a result of convergent evolution. However, recent DNA analysis and the discovery of a common ancestor has supported the view that they do share a common lineage, and thus rabbits and rodents are now often referred to together as members of the superorder Glires.
The anatomy of rabbits' hind limbs are structurally similar to that of other land mammals and contribute to their specialized form of locomotion. The bones of the hind limbs consist of long bones (the femur, tibia, fibula, and phalanges) as well as short bones (the tarsals). These bones are created through endochondral ossification during development. Like most land mammals, the round head of the femur articulates with the acetabulum of the ox coxae. The femur articulates with the tibia, but not the fibula, which is fused to the tibia. The tibia and fibula articulate with the tarsals of the pes, commonly called the foot. The hind limbs of the rabbit are longer than the front limbs. This allows them to produce their hopping form of locomotion. Longer hind limbs are more capable of producing faster speeds. Hares, which have longer legs than cottontail rabbits, are able to move considerably faster. Rabbits stay just on their toes when moving this is called Digitigrade locomotion. The hind feet have four long toes that allow for this and are webbed to prevent them from spreading when hopping. Rabbits do not have paw pads on their feet like most other animals that use digitigrade locomotion. Instead, they have coarse compressed hair that offers protection.
Rabbits have muscled hind legs that allow for maximum force, maneuverability, and acceleration that is divided into three main parts; foot, thigh, and leg. The hind limbs of a rabbit are an exaggerated feature, that are much longer than the forelimbs providing more force. Rabbits run on their toes to gain the optimal stride during locomotion. The force put out by the hind limbs is contributed to both the structural anatomy of the fusion tibia and fibula, and muscular features. Bone formation and removal, from a cellular standpoint, is directly correlated to hind limb muscles. Action pressure from muscles creates force that is then distributed through the skeletal structures. Rabbits that generate less force, putting less stress on bones are more prone to osteoporosis due to bone rarefaction. In rabbits, the more fibers in a muscle, the more resistant to fatigue. For example, hares have a greater resistance to fatigue than cottontails. The muscles of rabbit's hind limbs can be classified into four main categories: hamstrings, quadriceps, dorsiflexors, or plantar flexors. The quadriceps muscles are in charge of force production when jumping. Complementing these muscles are the hamstrings which aid in short bursts of action. These muscles play off of one another in the same way as the plantar flexors and doriflexors, contributing to the generation and actions associated with force.
Within the order lagomorphs, the ears are utilized to detect and avoid predators. In the family leporidae, the ears are typically longer than they are wide. For example, in black tailed jack rabbits, their long ears cover a greater surface area relative to their body size that allow them to detect predators from far away. Contrasted to cotton tailed rabbits, their ears are smaller and shorter, requiring predators to be closer to detect them before they can flee. Evolution has favored rabbits to have shorter ears so the larger surface area does not cause them to lose heat in more temperate regions. The opposite can be seen in rabbits that live in hotter climates, mainly because they possess longer ears that have a larger surface area that help with dispersion of heat as well as the theory that sound does not travel well in more arid air, opposed to cooler air. Therefore, longer ears are meant to aid the organism in detecting predators sooner rather than later in warmer temperatures. The rabbit is characterized by its shorter ears while hares are characterized by their longer ears. Rabbits' ears are an important structure to aid thermoregulation and detect predators due to how the outer, middle, and inner ear muscles coordinate with one another. The ear muscles also aid in maintaining balance and movement when fleeing predators. Outer ear The Auricle (anatomy), also known as the pinna is a rabbit's outer ear. The rabbit's pinnae represent a fair part of the body surface area. It is theorized that the ears aid in dispersion of heat at temperatures above 30 °C with rabbits in warmer climates having longer pinnae due to this. Another theory is that the ears function as shock absorbers that could aid and stabilize rabbit's vision when fleeing predators, but this has typically only been seen in hares. The rest of the outer ear has bent canals that lead to the eardrum or tympanic membrane.Middle ear The middle ear is filled with three bones called ossicles and is separated by the outer eardrum in the back of the rabbit's skull. The three ossicles are called hammer, anvil, and stirrup and act to decrease sound before it hits the inner ear. In general, the ossicles act as a barrier to the inner ear for sound energy.Inner ear Inner ear fluid called endolymph receives the sound energy. After receiving the energy, later within the inner ear there are two parts: the cochlea that utilizes sound waves from the ossicles and the vestibular apparatus that manages the rabbit's position in regards to movement. Within the cochlea there is a basilar membrane that contains sensory hair structures utilized to send nerve signals to the brain so it can recognize different sound frequencies. Within the vestibular apparatus the rabbit possesses three semicircular canals to help detect angular motion.
Thermoregulation is the process that an organism utilizes to maintain an optimal body temperature independent of external conditions. This process is carried out by the pinnae which takes up most of the rabbit's body surface and contain a vascular network and arteriovenous shunts. In a rabbit, the optimal body temperature is around 38.5–40℃. If their body temperature exceeds or does not meet this optimal temperature, the rabbit must return to homeostasis. Homeostasis of body temperature is maintained by the use of their large, highly vascularized ears that are able to change the amount of blood flow that passes through the ears. Constriction and dilation of blood vessels in the ears are used to control the core body temperature of a rabbit. If the core temperature exceeds its optimal temperature greatly, blood flow is constricted to limit the amount of blood going through the vessels. With this constriction, there is only a limited amount of blood that is passing through the ears where ambient heat would be able to heat the blood that is flowing through the ears and therefore, increasing the body temperature. Constriction is also used when the ambient temperature is much lower than that of the rabbit's core body temperature. When the ears are constricted it again limits blood flow through the ears to conserve the optimal body temperature of the rabbit. If the ambient temperature is either 15 degrees above or below the optimal body temperature, the blood vessels will dilate. With the blood vessels being enlarged, the blood is able to pass through the large surface area which causes it to either heat or cool down. During the summer, the rabbit has the capability to stretch its pinnae which allows for greater surface area and increase heat dissipation. In the winter, the rabbit does the opposite and folds its ears in order to decrease its surface area to the ambient air which would decrease their body temperature. The jackrabbit has the largest ears within the Oryctolagus cuniculus group. Their ears contribute to 17% of their total body surface area. Their large pinna were evolved to maintain homeostasis while in the extreme temperatures of the desert.
The rabbit's nasal cavity lies dorsal to the oral cavity, and the two compartments are separated by the hard and soft palate. The nasal cavity itself is separated into a left and right side by a cartilage barrier, and it is covered in fine hairs that trap dust before it can enter the respiratory tract. As the rabbit breathes, air flows in through the nostrils along the alar folds. From there, the air moves into the nasal cavity, also known as the nasopharynx, down through the trachea, through the larynx, and into the lungs. The larynx functions as the rabbit's voice box, which enables it to produce a wide variety of sounds. The trachea is a long tube embedded with cartilaginous rings that prevent the tube from collapsing as air moves in and out of the lungs. The trachea then splits into a left and right bronchus, which meet the lungs at a structure called the hilum. From there, the bronchi split into progressively more narrow and numerous branches. The bronchi branch into bronchioles, into respiratory bronchioles, and ultimately terminate at the alveolar ducts. The branching that is typically found in rabbit lungs is a clear example of monopodial branching, in which smaller branches divide out laterally from a larger central branch.Rabbits breathe primarily through their noses due to the fact that the epiglottis is fixed to the backmost portion of the soft palate. Within the oral cavity, a layer of tissue sits over the opening of the glottis, which blocks airflow from the oral cavity to the trachea. The epiglottis functions to prevent the rabbit from aspirating on its food. Further, the presence of a soft and hard palate allow the rabbit to breathe through its nose while it feeds. Rabbits lungs are divided into four lobes: the cranial, middle, caudal, and accessory lobes. The right lung is made up of all four lobes, while the left lung only has two: the cranial and caudal lobes. In order to provide space for the heart, the left cranial lobe of the lungs is significantly smaller than that of the right. The diaphragm is a muscular structure that lies caudal to the lungs and contracts to facilitate respiration.
Rabbits are herbivores that feed by grazing on grass, forbs, and leafy weeds. In consequence, their diet contains large amounts of cellulose, which is hard to digest. Rabbits solve this problem via a form of hindgut fermentation. They pass two distinct types of feces: hard droppings and soft black viscous pellets, the latter of which are known as caecotrophs or "night droppings" and are immediately eaten (a behaviour known as coprophagy). Rabbits reingest their own droppings (rather than chewing the cud as do cows and numerous other herbivores) to digest their food further and extract sufficient nutrients.Rabbits graze heavily and rapidly for roughly the first half-hour of a grazing period (usually in the late afternoon), followed by about half an hour of more selective feeding. In this time, the rabbit will also excrete many hard fecal pellets, being waste pellets that will not be reingested. If the environment is relatively non-threatening, the rabbit will remain outdoors for many hours, grazing at intervals. While out of the burrow, the rabbit will occasionally reingest its soft, partially digested pellets; this is rarely observed, since the pellets are reingested as they are produced. Hard pellets are made up of hay-like fragments of plant cuticle and stalk, being the final waste product after redigestion of soft pellets. These are only released outside the burrow and are not reingested. Soft pellets are usually produced several hours after grazing, after the hard pellets have all been excreted. They are made up of micro-organisms and undigested plant cell walls.Rabbits are hindgut digesters. This means that most of their digestion takes place in their large intestine and cecum. In rabbits, the cecum is about 10 times bigger than the stomach and it along with the large intestine makes up roughly 40% of the rabbit's digestive tract. The unique musculature of the cecum allows the intestinal tract of the rabbit to separate fibrous material from more digestible material; the fibrous material is passed as feces, while the more nutritious material is encased in a mucous lining as a cecotrope. Cecotropes, sometimes called "night feces", are high in minerals, vitamins and proteins that are necessary to the rabbit's health. Rabbits eat these to meet their nutritional requirements; the mucous coating allows the nutrients to pass through the acidic stomach for digestion in the intestines. This process allows rabbits to extract the necessary nutrients from their food.The chewed plant material collects in the large cecum, a secondary chamber between the large and small intestine containing large quantities of symbiotic bacteria that help with the digestion of cellulose and also produce certain B vitamins. The pellets are about 56% bacteria by dry weight, largely accounting for the pellets being 24.4% protein on average. The soft feces form here and contain up to five times the vitamins of hard feces. After being excreted, they are eaten whole by the rabbit and redigested in a special part of the stomach. The pellets remain intact for up to six hours in the stomach; the bacteria within continue to digest the plant carbohydrates. This double-digestion process enables rabbits to use nutrients that they may have missed during the first passage through the gut, as well as the nutrients formed by the microbial activity and thus ensures that maximum nutrition is derived from the food they eat. This process serves the same purpose in the rabbit as rumination does in cattle and sheep. Rabbits are incapable of vomiting. Because rabbits cannot vomit, if buildup occurs within the intestines (due often to a diet with insufficient fiber), intestinal blockage can occur.
The adult male reproductive system forms the same as most mammals with the seminiferous tubular compartment containing the Sertoli cells and an adluminal compartment that contains the Leydig cells. The Leydig cells produce testosterone, which maintains libido and creates secondary sex characteristics such as the genital tubercle and penis. The Sertoli cells triggers the production of Anti-Müllerian duct hormone, which absorbs the Müllerian duct. In an adult male rabbit, the sheath of the penis is cylinder-like and can be extruded as early as two months of age. The scrotal sacs lay lateral to the penis and contain epididymal fat pads which protect the testes. Between 10–14 weeks, the testes descend and are able to retract into the pelvic cavity in order to thermoregulate. Furthermore, the secondary sex characteristics, such as the testes, are complex and secrete many compounds. These compounds includes fructose, citric acid, minerals, and a uniquely high amount of catalase. The adult female reproductive tract is bipartite, which prevents an embryo from translocating between uteri. The two uterine horns communicate to two cervixes and forms one vaginal canal. Along with being bipartite, the female rabbit does not go through an estrus cycle, which causes mating induced ovulation.The average female rabbit becomes sexually mature at 3 to 8 months of age and can conceive at any time of the year for the duration of her life. However, egg and sperm production can begin to decline after three years. During mating, the male rabbit will mount the female rabbit from behind and insert his penis into the female and make rapid pelvic hip thrusts. The encounter lasts only 20–40 seconds and after, the male will throw himself backwards off the female.The rabbit gestation period is short and ranges from 28 to 36 days with an average period of 31 days. A longer gestation period will generally yield a smaller litter while shorter gestation periods will give birth to a larger litter. The size of a single litter can range from four to 12 kits allowing a female to deliver up to 60 new kits a year. After birth, the female can become pregnant again as early as the next day.The mortality rates of embryos are high in rabbits and can be due to infection, trauma, poor nutrition and environmental stress so a high fertility rate is necessary to counter this.
Rabbits may appear to be crepuscular, but their natural inclination is toward nocturnal activity. In 2011, the average sleep time of a rabbit in captivity was calculated at 8.4 hours per day. As with other prey animals, rabbits often sleep with their eyes open, so that sudden movements will awaken the rabbit to respond to potential danger.
In addition to being at risk of disease from common pathogens such as Bordetella bronchiseptica and Escherichia coli, rabbits can contract the virulent, species-specific viruses RHD ("rabbit hemorrhagic disease", a form of calicivirus) or myxomatosis. Among the parasites that infect rabbits are tapeworms (such as Taenia serialis), external parasites (including fleas and mites), coccidia species, and Toxoplasma gondii. Domesticated rabbits with a diet lacking in high fiber sources, such as hay and grass, are susceptible to potentially lethal gastrointestinal stasis. Rabbits and hares are almost never found to be infected with rabies and have not been known to transmit rabies to humans.Encephalitozoon cuniculi, an obligate intracellular parasite is also capable of infecting many mammals including rabbits.
Rabbits are prey animals and are therefore constantly aware of their surroundings. For instance, in Mediterranean Europe, rabbits are the main prey of red foxes, badgers, and Iberian lynxes. If confronted by a potential threat, a rabbit may freeze and observe then warn others in the warren with powerful thumps on the ground. Rabbits have a remarkably wide field of vision, and a good deal of it is devoted to overhead scanning. They survive predation by burrowing, hopping away in a zig-zag motion, and, if captured, delivering powerful kicks with their hind legs. Their strong teeth allow them to eat and to bite in order to escape a struggle. The longest-lived rabbit on record, a domesticated European rabbit living in Tasmania, died at age 18. The lifespan of wild rabbits is much shorter; the average longevity of an eastern cottontail, for instance, is less than one year.
Rabbit habitats include meadows, woods, forests, grasslands, deserts and wetlands. Rabbits live in groups, and the best known species, the European rabbit, lives in burrows, or rabbit holes. A group of burrows is called a warren.More than half the world's rabbit population resides in North America. They are also native to southwestern Europe, Southeast Asia, Sumatra, some islands of Japan, and in parts of Africa and South America. They are not naturally found in most of Eurasia, where a number of species of hares are present. Rabbits first entered South America relatively recently, as part of the Great American Interchange. Much of the continent has just one species of rabbit, the tapeti, while most of South America's southern cone is without rabbits. The European rabbit has been introduced to many places around the world.
Rabbits have been a source of environmental problems when introduced into the wild by humans. As a result of their appetites, and the rate at which they breed, feral rabbit depredation can be problematic for agriculture. Gassing, barriers (fences), shooting, snaring, and ferreting have been used to control rabbit populations, but the most effective measures are diseases such as myxomatosis (myxo or mixi, colloquially) and calicivirus. In Europe, where rabbits are farmed on a large scale, they are protected against myxomatosis and calicivirus with a genetically modified virus. The virus was developed in Spain, and is beneficial to rabbit farmers. If it were to make its way into wild populations in areas such as Australia, it could create a population boom, as those diseases are the most serious threats to rabbit survival. Rabbits in Australia and New Zealand are considered to be such a pest that land owners are legally obliged to control them.
In some areas, wild rabbits and hares are hunted for their meat, a lean source of high quality protein. In the wild, such hunting is accomplished with the aid of trained falcons, ferrets, or dogs, as well as with snares or other traps, and rifles. A caught rabbit may be dispatched with a sharp blow to the back of its head, a practice from which the term rabbit punch is derived. Wild leporids comprise a small portion of global rabbit-meat consumption. Domesticated descendants of the European rabbit (Oryctolagus cuniculus) that are bred and kept as livestock (a practice called cuniculture) account for the estimated 200 million tons of rabbit meat produced annually. Approximately 1.2 billion rabbits are slaughtered each year for meat worldwide. In 1994, the countries with the highest consumption per capita of rabbit meat were Malta with 8.89 kg (19 lb 10 oz), Italy with 5.71 kg (12 lb 9 oz), and Cyprus with 4.37 kg (9 lb 10 oz), falling to 0.03 kg (1 oz) in Japan. The figure for the United States was 0.14 kg (5 oz) per capita. The largest producers of rabbit meat in 1994 were China, Russia, Italy, France, and Spain. Rabbit meat was once a common commodity in Sydney, Australia, but declined after the myxomatosis virus was intentionally introduced to control the exploding population of feral rabbits in the area. In the United Kingdom, fresh rabbit is sold in butcher shops and markets, and some supermarkets sell frozen rabbit meat. At farmers markets there, including the famous Borough Market in London, rabbit carcasses are sometimes displayed hanging, unbutchered (in the traditional style), next to braces of pheasant or other small game. Rabbit meat is a feature of Moroccan cuisine, where it is cooked in a tajine with "raisins and grilled almonds added a few minutes before serving". In China, rabbit meat is particularly popular in Sichuan cuisine, with its stewed rabbit, spicy diced rabbit, BBQ-style rabbit, and even spicy rabbit heads, which have been compared to spicy duck neck. Rabbit meat is comparatively unpopular elsewhere in the Asia-Pacific. An extremely rare infection associated with rabbits-as-food is tularemia (also known as rabbit fever), which may be contracted from an infected rabbit. Hunters are at higher risk for tularemia because of the potential for inhaling the bacteria during the skinning process. In addition to their meat, rabbits are used for their wool, fur, and pelts, as well as their nitrogen-rich manure and their high-protein milk. Production industries have developed domesticated rabbit breeds (such as the well-known Angora rabbit) to efficiently fill these needs.
Rabbits are often used as a symbol of fertility or rebirth, and have long been associated with spring and Easter as the Easter Bunny. The species' role as a prey animal with few defenses evokes vulnerability and innocence, and in folklore and modern children's stories, rabbits often appear as sympathetic characters, able to connect easily with youth of all kinds (for example, the Velveteen Rabbit, or Thumper in Bambi). With its reputation as a prolific breeder, the rabbit juxtaposes sexuality with innocence, as in the Playboy Bunny. The rabbit (as a swift prey animal) is also known for its speed, agility, and endurance, symbolized (for example) by the marketing icons the Energizer Bunny and the Duracell Bunny.
The rabbit often appears in folklore as the trickster archetype, as he uses his cunning to outwit his enemies. In Aztec mythology, a pantheon of four hundred rabbit gods known as Centzon Totochtin, led by Ometotchtli or Two Rabbit, represented fertility, parties, and drunkenness. In Central Africa, the common hare (Kalulu), is "inevitably described" as a trickster figure. In Chinese folklore, rabbits accompany Chang'e on the Moon. In the Chinese New Year, the zodiacal rabbit is one of the twelve celestial animals in the Chinese zodiac. Note that the Vietnamese zodiac includes a zodiacal cat in place of the rabbit, possibly because rabbits did not inhabit Vietnam. The most common explanation, however, is that the ancient Vietnamese word for "rabbit" (mao) sounds like the Chinese word for "cat" (卯, mao). In Japanese tradition, rabbits live on the Moon where they make mochi, the popular snack of mashed sticky rice. This comes from interpreting the pattern of dark patches on the moon as a rabbit standing on tiptoes on the left pounding on an usu, a Japanese mortar. In Jewish folklore, rabbits (shfanim שפנים) are associated with cowardice, a usage still current in contemporary Israeli spoken Hebrew (similar to the English colloquial use of "chicken" to denote cowardice). In Korean mythology, as in Japanese, rabbits live on the moon making rice cakes ("Tteok" in Korean). In Anishinaabe traditional beliefs, held by the Ojibwe and some other Native American peoples, Nanabozho, or Great Rabbit, is an important deity related to the creation of the world. A Vietnamese mythological story portrays the rabbit of innocence and youthfulness. The Gods of the myth are shown to be hunting and killing rabbits to show off their power. Buddhism, Christianity, and Judaism have associations with an ancient circular motif called the three rabbits (or "three hares"). Its meaning ranges from "peace and tranquility", to purity or the Holy Trinity, to Kabbalistic levels of the soul or to the Jewish diaspora. The tripartite symbol also appears in heraldry and even tattoos.The rabbit as trickster is a part of American popular culture, as Br'er Rabbit (from African-American folktales and, later, Disney animation) and Bugs Bunny (the cartoon character from Warner Bros.), for example. Anthropomorphized rabbits have appeared in film and literature, in Alice's Adventures in Wonderland (the White Rabbit and the March Hare characters), in Watership Down (including the film and television adaptations), in Rabbit Hill (by Robert Lawson), and in the Peter Rabbit stories (by Beatrix Potter). In the 1920s, Oswald the Lucky Rabbit, was a popular cartoon character.
Windling, Terri. The Symbolism of Rabbits and Hares
Rabbit at the Encyclopædia Britannica American Rabbit Breeders Association organization which promotes all phases of rabbit keeping House Rabbit Society an activist organization which promotes keeping rabbits indoors. RabbitShows.com an informational site on the hobby of showing rabbits. The (mostly) silent language of rabbits World Rabbit Science Association an international rabbit-health science-based organization The Year of the Rabbit – slideshow by Life magazine House Rabbit Society- FAQ: Aggression
Rabbiting (also rabbit hunting and cottontail hunting) is the sport of hunting rabbits. It often involves using ferrets or dogs to track or chase the prey. There are various methods used in capturing the rabbit, including trapping and shooting. Depending on where the hunting occurs, there may be licenses required and other rules in regards to methods being used.
Most rabbit hunters try to locate rabbit holes, which are usually found in wooded areas with higher grounds soft enough for the rabbits to burrow in. Hunters without hounds have the following options. A hunter, alone or with a partner, walks through the possible locations of rabbit hiding places, kicking or stomping possible covers to chase the rabbit out. In winter, an advantage is visible rabbit tracks after a fresh snow. Unraveling tracks allows the hunter to locate the hiding place: if no tracks lead out of a suspected location, then the quarry is located. After this, hunters with short-range arms (archers or the ones with small calibre) may scrutinize the location to find the rabbit and shoot it immobile. Alternatively, one may just as well scare the animal out and shoot it while it is on the run.
Spotlighting or lamping can refer to any form of rabbit hunting performed at night with the aid of powerful hand-held, rifle mounted or vehicle mounted search lights. The light is often used in conjunction with a dog such as a sighthound, (or lurcher) alongside an air rifle, or some other firearm such as a .22LR.17 HMR The rabbit is illuminated by the light and then shot, or a dog will chase and capture it. Most often lurchers are used to catch the prey, the most popular crosses involve greyhounds, border collies and salukis.Using a vehicle is a very popular method of spotlighting. Pick ups and 4×4 are preferred modes of transport. ATVS are also popular vehicles for rabbiting. They provide rapid acceleration making it easy to chase down rabbits.
There is a large variety of different traps that are used to capture rabbits and can be divided into categories. A lot of traps are typically used for pest control. When hunting for sport, long netting is the most common method of trapping. Many traps are illegal.
These traps have a high rate of success and are very easy to set up. The springs inside the trap are triggered by the weight of the rabbit, causing them to shorten and the door to shut behind the animal, leaving it safely enclosed. Homemade traps such as these do not have a great success rate, as the effectiveness depends entirely on the trap's quality. The way the traps operate vary, but ultimately the rabbit's movement is what triggers them to close.
These traps are quite advanced because they are able to capture a large number of rabbits and automatically reset themselves. They are buried into the ground and usually have a type of tunnel that lures the animal to a spring-loaded trap, which will then drop the rabbit into an enclosure once it is triggered by weight.
In medieval times, a hawk or falcon would have been used to catch the rabbit as it exited the warren burrow. For this type of hunt, an albino ferret would typically be used, allowing the bird-of-prey to more easily recognize it. While this hunting style is still occasionally used, especially in the UK where it remains popular (see Falconry), the methods above have almost entirely replaced it. Also around this time, the popularity of hare coursing sport was growing. Back then, two greyhounds would be released at the same time in pursuit of the rabbit and the one that kills it is declared the winner of the game; people typically placed bets on which dog would be the victor.In sixteenth-century Britain, hunting rabbits typically involved two hunters either on foot or horseback, a group of hounds, and a horn. The hunter leading the hounds used the horn to encourage them to chase after the rabbit, while the other stayed at the back of the group to motivate any dogs that fell behind. When the rabbit was caught, its death would be marked by a ritual dissection of its body following a blow of the horn. After the actual hunt, the meat would be taken home by the hunters, and the leftovers were given to the dogs as a reward. The rabbit's meat was not highly rated during this time period; huntsman still collected the meat, but the hunt was ultimately a form of entertainment.
In the United Kingdom it is not required for a hunter to have a game license to kill and take rabbits. The hunting season for rabbits runs through the entire year from January 1 to December 31. On the Isle of Man, a game license is required to shoot rabbits and a dealer's license is required for dealing any type of game; they can be obtained from the Treasury Office. The hunting season also runs throughout the entire year. In Jersey, no license is required because there is no hunting season for rabbits. Guernsey's laws require a shotgun or firearm certificate rather than a hunting license. In most of these places, it is considered an offence to kill any type of game on a Sunday.
In the United States, every person wishing to hunt must have a state hunting license (few states have exceptions to this). Some national wildlife refuges may have separate permits required. Each state has different hunting seasons for rabbits. In Virginia, the season lasts throughout November, December, and January.
Phoenician sailors visiting the coast of Spain c. 12th century BC, mistaking the European rabbit for a species from their homeland (the rock hyrax Procavia capensis), gave it the name i-shepan-ham (land or island of hyraxes). The captivity of rabbits as a food source is recorded as early as the 1st century BC, when the Roman writer Pliny the Elder described the use of rabbit hutches, along with enclosures called leporaria. A controversial theory is that a corruption of the rabbit's name used by the Romans became the Latin name for the peninsula, Hispania. In Rome, rabbits were raised in large walled colonies with walls extended underground. According to Pliny, the consumption of unborn and newborn rabbits, called laurices, was considered a delicacy. Evidence for the domestic rabbit is rather late. In the Middle Ages, wild rabbits were often kept for the hunt. Monks in southern France were crossbreeding rabbits at least by the 12th century AD. Domestication was probably a slow process that took place from the Roman period (or earlier) until the 1500s.In the 19th century, as animal fancy in general began to emerge, rabbit fanciers began to sponsor rabbit exhibitions and fairs in Western Europe and the United States. Breeds of various domesticated animals were created and modified for the added purpose of exhibition, a departure from the breeds that had been created solely for food, fur, or wool. The rabbit's emergence as a household pet began during the Victorian era.The keeping of the rabbit as a pet commencing from the 1800s coincides with the first observable skeletal differences between the wild and domestic populations, even though captive rabbits had been exploited for over 2,000 years. Domestic rabbits have been popular in the United States since the late 19th century. What became known as the "Belgian Hare Boom" began with the importation of the first Belgian Hares from England in 1888 and, soon after, the founding of the American Belgian Hare Association, the first rabbit club in America. From 1898 to 1901, many thousands of Belgian Hares were imported to America. Today, the Belgian Hare is one of the rarest breeds, with only 132 specimens found in the United States in a 2015 census. The American Rabbit Breeders Association (ARBA) was founded in 1910 and is the national authority on rabbit raising and rabbit breeds having a uniform Standard of Perfection, registration and judging system. The domestic rabbit continues to be popular as a show animal and pet. Many thousand rabbit shows occur each year and are sanctioned in Canada and the United States by the ARBA. Today, the domesticated rabbit is the third most popular mammalian pet in Britain after dogs and cats.
Rabbits have been, and continue to be, used in laboratory work such as the production of antibodies for vaccines and research of human male reproductive system toxicology. The Environmental Health Perspective, published by the National Institute of Health, states, "The rabbit [is] an extremely valuable model for studying the effects of chemicals or other stimuli on the male reproductive system." According to the Humane Society of the United States, rabbits are also used extensively in the study of bronchial asthma, stroke prevention treatments, cystic fibrosis, diabetes, and cancer. Animal rights activists have opposed animal experimentation for non-medical purposes, such as the testing of cosmetic and cleaning products, which has resulted in decreased use of rabbits in these areas.
As a refinement of the diet of the wild rabbit, the diet of the domestic rabbit is often a function of its purpose. Show rabbits are fed for vibrant health, strong musculoskeletal systems, and—like rabbits intended for the fur trade—optimal coat production and condition. Rabbits intended for the meat trade are fed for swift and efficient production of flesh, while rabbits in research settings have closely controlled diets for specific goals. Nutritional needs of the domestic rabbit may also be focused on developing a physique that allows for the safe delivery of larger litters of healthy kits. Optimizing costs and producing feces that meet local waste regulations may also be factors. The diet of a pet rabbit, too, is geared toward its purpose—as a healthy and long-lived companion. Hay is an essential part of the diet of all rabbits and it is a major component of the commercial food pellets that are formulated for domestic rabbits and available in many areas. Pellets are typically fed to adult rabbits in limited quantities once or twice a day, to mimic their natural behavior and to prevent obesity. It is recommended only a teaspoon to an egg cup full of pellets is fed to adult rabbits each day. Most rabbit pellets are alfalfa-based for protein and fiber, with other grains completing the carbohydrate requirements. "Muesli" style rabbit foods are also available; these contain separate components—e.g., dried carrot, pea flakes and hay pellets as opposed to a uniform pellet. These are not recommended as rabbits will choose favored parts and leave the rest. Muesli style feeds are often lower in fiber than pelleted versions of rabbit food. Additionally numerous studies have found they increase the risk of obesity and dental disease. Minerals and vitamins are added during production of rabbit pellets to meet the nutritional requirements of the domestic rabbit. Along with pellets, many commercial rabbit raisers also feed one or more types of loose hay, for its freshness and important cellulose components. Alfalfa in particular is recommended for the growth needs of young rabbits.
Rabbits are hindgut fermenters and therefore have an enlarged cecum. This allows a rabbit to digest, via fermentation, what it otherwise would not be able to metabolically process. After a rabbit ingests food, the food travels down the esophagus and through a small valve called the cardia. In rabbits, this valve is very well pronounced and makes the rabbit incapable of vomiting. The food enters the stomach after passing through the cardia. Food then moves to the stomach and small intestine, where a majority of nutrient extraction and absorption takes place. Food then passes into the colon and eventually into the cecum. Peristaltic muscle contractions (waves of motion) help to separate fibrous and non-fibrous particles. The non-fibrous particles are then moved backwards up the colon, through the illeo-cecal valve, and into the cecum. Symbiotic bacteria in the cecum help to further digest the non-fibrous particles into a more metabolically manageable substance. After as little as three hours, a soft, fecal "pellet," called a cecotrope, is expelled from the rabbit's anus. The rabbit instinctively eats these grape-like pellets, without chewing, in exchange keeping the mucous coating intact. This coating protects the vitamin- and nutrient-rich bacteria from stomach acid, until it reaches the small intestine, where the nutrients from the cecotrope can be absorbed.The soft pellets contain a sufficiently large portion of nutrients that are critical to the rabbit's health. This soft fecal matter is rich in vitamin B and other nutrients. The process of coprophagy is important to the stability of a rabbit's digestive health because it is one important way that which a rabbit receives vitamin B in a form that is useful to its digestive wellness. Occasionally, the rabbit may leave these pellets lying about its cage; this behavior is harmless and usually related to an ample food supply. When caecal pellets are wet and runny (semi-liquid) and stick to the rabbit and surrounding objects, they are called intermittent soft cecotropes (ISCs). This is different from ordinary diarrhea and is usually caused by a diet too high in carbohydrates or too low in fiber. Soft fruit or salad items such as lettuce, cucumbers and tomatoes are possible causes.
Disease is rare when rabbits are raised in sanitary conditions and provided with adequate care. Rabbits have fragile bones, especially in their spines, and need support on the belly or bottom when they are picked up. Spayed or neutered rabbits kept indoors with proper care may have a lifespan of 8 to 12 years, with mixed-breed rabbits typically living longer than purebred specimens, and dwarf breeds having longer average lifespans than larger breeds. The world record for longest-lived rabbit is 18 years.Rabbits will gnaw on almost anything, including electrical cords (possibly leading to electrocution), potentially poisonous plants, and material like carpet and fabric that may cause life-threatening intestinal blockages, so areas to which they have access need to be pet-proofed.
Rabbit fancier organizations and veterinarians recommend that pet rabbits be made infertile by spaying or neutering by a rabbit-experienced veterinarian. Health advantages of surgically altering a rabbit include increased longevity and (for females) a reduced risk of ovarian and uterine cancers or of endometritis. For both rabbit sexes, spaying or neutering reduces aggression toward other rabbits, as well as territorial marking (especially in males). Rabbits are at high risk for complications from anesthesia and infection of the surgical site is another top concern. Since un-altered animals are not as likely to form agreeable social bonds, spaying and neutering promotes less stressful interactions.
In most jurisdictions, including the United States (except where required by local animal control ordinances), rabbits do not require vaccination. Vaccinations exist for both rabbit hemorrhagic disease and myxomatosis. These vaccinations are usually given annually, two weeks apart. If there is an outbreak of myxomatosis locally, this vaccine can be administered every six months for extra protection. Myxomatosis immunizations are not available in all countries, including Australia, due to fears that immunity will pass on to feral rabbits. However, they are recommended by some veterinarians as prophylactics, where they are legally available. In the UK a combined vaccination exists for myxomatosis and VHD1 made by Nobivac called Myxo-RHD, this is given yearly. Due to increasing cases of VHD2 it is now recommended rabbits receive an additional vaccination for RHD2 one brand for this is filovac, the vaccination is given yearly 2 weeks apart from other vaccinations, it may be given 6 monthly at rabbit believed to be at higher risk.
A rabbit cannot be declawed. Lacking pads on the bottoms of its feet, a rabbit requires its claws for traction. Removing its claws would render it unable to stand.
Coping with stress is a key aspect of rabbit behavior, and this can be traced to part of the brain known as ventral tegmental area (VTA). Dopaminergic neurons in this part of the brain release the hormone dopamine. In rabbits, it is released as part of a coping mechanism while in a heightened state of fear or stress, and has a calming effect. Dopamine has also been found in the rabbit's medial prefrontal cortex, the nucleus accumbens, and the amygdala. Physiological and behavioral responses to human-induced tonic immobility (TI, sometimes termed "trancing" or "playing dead") have been found to be indicative of a fear-motivated stress state, confirming that the promotion of TI to try to increase a bond between rabbits and their owners—thinking the rabbits enjoy it—is misplaced. However, some researchers conclude that inducing TI in rabbits is appropriate for certain procedures, as it holds less risk than anesthesia.
The formation of open sores on the rabbit's hocks, commonly called sore hocks, is a problem that commonly afflicts mostly heavy-weight rabbits kept in cages with wire flooring or soiled solid flooring. The problem is most prevalent in rex-furred rabbits and heavy-weight rabbits (over 9 pounds (4.1 kg)), as well as those with thin foot bristles. The condition results when, over the course of time, the protective bristle-like fur on the rabbit's hocks thins down. Standing urine or other unsanitary cage conditions can exacerbate the problem by irritating the sensitive skin. The exposed skin in turn can result in tender areas or, in severe cases, open sores, which may then become infected and abscessed if not properly cared for.
Gastrointestinal stasis (GI stasis) is a serious and potentially fatal condition that occurs in some rabbits in which gut motility is severely reduced and possibly completely stopped. When untreated or improperly treated, GI stasis can be fatal in as little as 24 hours. GI stasis is the condition of food not moving through the gut as quickly as normal. The gut contents may dehydrate and compact into a hard, immobile mass (impacted gut), blocking the digestive tract of the rabbit. Food in an immobile gut may also ferment, causing significant gas buildup and resultant gas pain for the rabbit. The first noticeable symptom of GI stasis may be that the rabbit suddenly stops eating. Treatment frequently includes intravenous or subcutaneous fluid therapy (rehydration through injection of a balanced electrolyte solution), pain control, possible careful massage to promote gas expulsion and comfort, drugs to promote gut motility, and careful monitoring of all inputs and outputs. The rabbit's diet may also be changed as part of treatment, to include force-feeding to ensure adequate nutrition. Surgery to remove the blockage is not generally recommended and comes with a poor prognosis.Some rabbits are more prone to GI stasis than others. The causes of GI stasis are not completely understood, but common contributing factors are thought to include stress, reduced food intake, low fiber in the diet, dehydration, reduction in exercise or blockage caused by excess fur or carpet ingestion. Stress factors can include changes in housing, transportation, or medical procedures under anesthesia. As many of these factors may occur together (poor dental structure leading to decreased food intake, followed by a stressful veterinary dental procedure to correct the dental problem) establishing a root cause may be difficult.GI stasis is sometimes misdiagnosed as "hair balls" by veterinarians or rabbit keepers not familiar with the condition. While fur is commonly found in the stomach following a fatal case of GI stasis, it is also found in healthy rabbits. Molting and chewing fur can be a predisposing factor in the occurrence of GI stasis, however, the primary cause is the change in motility of the gut.
Dental disease has several causes, namely genetics, inappropriate diet, injury to the jaw, infection, or cancer. Malocclusion: Rabbit teeth are open-rooted and continue to grow throughout their lives. In some rabbits, the teeth are not properly aligned, a condition called malocclusion. Because of the misaligned nature of the rabbit's teeth, there is no normal wear to control the length to which the teeth grow. There are three main causes of malocclusion, most commonly genetic predisposition, injury, or bacterial infection. In the case of congenital malocclusion, treatment usually involves veterinary visits in which the teeth are treated with a dental burr (a procedure called crown reduction or, more commonly, teeth clipping) or, in some cases, permanently removed. In cases of simple malocclusion, a block of wood for the rabbit to chew on can rectify this problem. Molar spurs: These are spurs that can dig into the rabbit's tongue and/or cheek causing pain. These should be filed down by an experienced exotic veterinarian specialised in rabbit care, using a dental burr, for example. Osteoporosis: Rabbits, especially neutered females and those that are kept indoors without adequate natural sunlight, can suffer from osteoporosis, in which holes appear in the skull by X-Ray imaging. This reflects the general thinning of the bone, and teeth will start to become looser in the sockets, making it uncomfortable and painful for the animal to chew hay. The inability to properly chew hay can result in molar spurs, as described above, and weight loss, leading into a downward spiral if not treated promptly. This can be reversible and treatable. A veterinary formulated liquid calcium supplement with vitamin D3 and magnesium can be given mixed with the rabbit's drinking water, once or twice per week, according to the veterinarian's instructions. The molar spurs should also be trimmed down by an experienced exotic veterinarian specialised in rabbit care, once per 1–2 months depending on the case.Signs of dental difficulty include difficulty eating, weight loss and small stools and visibly overgrown teeth. However, there are many other causes of ptyalism, including pain due to other causes.
An over-diagnosed ailment amongst rabbits is respiratory infection, known colloquially as "snuffles". Pasteurella, a bacterium, is usually misdiagnosed and this is known to be a factor in the overuse of antibiotics among rabbits. A runny nose, for instance, can have several causes, among those being high temperature or humidity, extreme stress, environmental pollution (like perfume or incense), or a sinus infection. Options for treating this is removing the pollutant, lowering or raising the temperature accordingly, and medical treatment for sinus infections. Pasteurella does live naturally in a rabbit's respiratory tract, and it can flourish out of control in some cases. In the rare event that happens, antibiotic treatment is necessary. Sneezing can be a sign of environmental pollution (such as too much dust) or a food allergy. Runny eyes and other conjunctival problems can be caused by dental disease or a blockage of the tear duct. Environmental pollution, corneal disease, entropion, distichiasis, or inflammation of the eyes are also causes. This is easy to diagnose as well as treat.
Rabbits are subject to infection by a variety of viruses. Some have had deadly and widespread impact.
Myxomatosis is a virulent threat to all rabbits but not to humans. The intentional introduction of myxomatosis in rabbit-ravaged Australia killed an estimated 500 million feral rabbits between 1950 and 1952. The Australian government will not allow veterinarians to purchase and use the myxomatosis vaccine that would protect domestic rabbits, for fear that this immunity would be spread into the wild via escaped livestock and pets. This potential consequence is also one motivation for the pet-rabbit ban in Queensland.In Australia, rabbits caged outdoors in areas with high numbers of mosquitoes are vulnerable to myxomatosis. In Europe, fleas are the carriers of myxomatosis. In some countries, annual vaccinations against myxomatosis are available.
Rabbit hemorrhagic disease (RHD), also known as viral hemorrhagic disease (VHD) or rabbit calicivirus disease (RCD), is caused by a rabbit-specific calicivirus known as RHDV or RCV. Discovered in 1983, RHD is highly infectious and usually fatal. Initial signs of the disease may be limited to fever and lethargy, until significant internal organ damage results in labored breathing, squealing, bloody mucus, and eventual coma and death. Internally, the infection causes necrosis of the liver and damages other organs, especially the spleen, kidneys, and small intestine. RHD, like myxomatosis, has been intentionally introduced to control feral rabbit populations in Australia and (illegally) in New Zealand, and RHD has, in some areas, escaped quarantine. The disease has killed tens of millions of rabbits in China (unintentionally) as well as Australia, with other epidemics reported in Bolivia, Mexico, South Korea, and continental Europe. Rabbit populations in New Zealand have bounced back after developing a genetic immunity to RHD, and the disease has, so far, had no effect on the genetically divergent native wild rabbits and hares in the Americas. In the United States, an October 2013 USDA document stated:RHD has been found in the United States as recently as 2010, and was detected in Canada in 2011. Thus far, outbreaks have been controlled quickly through quarantine, depopulation, disease tracing, and cleaning and disinfection; however, rabbit losses have been in the thousands. An RHD vaccine exists, but it is not recommended for use where the disease is not widespread in wildlife, as it may hide signs of disease and is not considered a practical response for such a rapidly spreading disease. In the UK, reports of RHD (as recently as February 2018) have been submitted to the British Rabbit Council's online "Notice Board". Vaccines for RHD are available—and mandatory—in the UK.
West Nile virus is another threat to domestic as well as wild rabbits. It is a fatal disease, and while vaccines are available for other species, there are none yet specifically indicated for rabbits.
Wry neck (or head tilt) is a condition in rabbits that can be fatal, due to the resulting disorientation that causes the animal to stop eating and drinking. Inner ear infections or ear mites, as well as diseases or injuries affecting the brain (including stroke) can lead to wry neck. The most common cause, however, is a parasitic microscopic fungus called Encephalitozoon cuniculi (E. cuniculi). Note that: "despite approximately half of all pet rabbits carrying the infection, only a small proportion of these cases ever show any illness". Some vets now recommend treating rabbits against E. cuniculi. The usual drugs for treatment and prevention are the benzimidazole anthelmintics, particularly fenbendazole (also used as a deworming agent in other animal species). In the UK, fenbendazole (under the brand name Panacur Rabbit), is sold over-the-counter in oral paste form as a nine-day treatment. Fenbendazole is particularly recommended for rabbits kept in colonies and as a preventive before mixing new rabbits with each other.
Fly strike, or blowfly strike, (Lucilia sericata) is a condition that occurs when flies (particularly botflies) lay their eggs in a rabbit's damp or soiled fur, or in an open wound. Within 12 hours, the eggs hatch into the larval stage of the fly, known as maggots. Initially small but quickly growing to 15 millimetres (0.59 in) long, maggots can burrow into skin and feed on an animal's tissue, leading to shock and death. The most susceptible rabbits are those in unsanitary conditions, sedentary ones, and those unable to clean their excretory areas. Rabbits with diarrhea should be inspected for fly strike, especially during the summer months. The topical treatment Rearguard® (from Novartis) is approved in the United Kingdom for 10-week-per-application prevention of fly strike.
As of 2017, there were at least 305 breeds of domestic rabbit in 70 countries around the world. The American Rabbit Breeders Association currently recognizes 49 rabbit breeds and the British Rabbit Council recognizes 106. Selective breeding has produced rabbits ranging in size from dwarf to giant. Across the world, rabbits are raised as livestock (in cuniculture) for their meat, pelts, and wool, and also by fanciers and hobbyists as pets. Rabbits have been selectively bred since ancient times to achieve certain desired characteristics. Variations include size and body shape, coat type (including hair length and texture), coat color, ear carriage (erect or lop), and even ear length. As with any animal, domesticated rabbits' temperaments vary in such factors as energy level and novelty seeking. Most genetic defects in the domestic rabbit (such as dental problems in the Holland Lop breed) are due to recessive genes. Genetics are carefully tracked by fanciers who show rabbits, to breed out defects.
Rabbits have been kept as pets in Western nations since the 19th century, but because of the destructive history of feral rabbits in Australia, domestic rabbits are illegal as pets in Queensland. Depending upon its size, a rabbit may be considered a type of pocket pet. Rabbits can bond with humans, can learn to follow simple voice commands and to come when called, and are curious and playful. Rabbits do not make good pets for small children because rabbits are fragile and easily injured by rough handling, can bite when hurt or frightened, and are easily frightened by loud noises and sudden motions. With the right guidance, rabbits can be trained to live indoors perfectly.Rabbits are especially popular as pets in the United States during the Easter season, due to their association with the holiday. However, animal shelters that accept rabbits often complain that during the weeks and months following Easter, there is a rise in unwanted and neglected rabbits that were bought as Easter gifts, especially for children. Similar problems arise in rural areas after county fairs and the like, in jurisdictions where rabbits are legal prizes in fairground games. Thus, there are many humane societies, animal shelters, and rescue groups that have rabbits available for pet adoption. Fancy rabbit breeds are often purchased from pet stores, private breeders, and fanciers.
Rabbits may be kept as small house pets and "rabbit-proofed" spaces reduce the risks associated with their intrinsic need to chew. Rabbits are easily litter box trained and a rabbit that lives indoors may be less exposed to the dangers of predators, parasites, diseases, adverse weather, and pesticides, which in turn increases their lifespan. Rabbits are often compatible with others of their kind, or with birds or guinea pigs, but opinion differs regarding the dangers of housing different species together. For example, while rabbits can synthesize their own Vitamin C, guinea pigs cannot, so the two species should not be fed the same diet. Also, most rabbits tend to be stronger than guinea pigs, so this may cause deliberate or inadvertent injury. Some people consider rabbits a pocket pet even though they are rather large. Keeping a rabbit as a house companion was popularised by Sandy Crook in her 1981 book Your French Lop. In 1983, at the American Family Pet Show in Anaheim, California (attended by 35,000), Crook presented her personal experiences living with an indoor rabbit as evidence of a human-rabbit bond. In the late 1980s, it became more common to litter box train a rabbit and keep it indoors, after the publication of Marinell Harriman's House Rabbit Handbook: How to Live with an Urban Rabbit in 1985. (The book's fifth edition was published in 2013.)As the domestic descendants of wild prey animals, rabbits are alert, timid creatures that startle fairly easily, and many of their behaviors are triggered by the fight-or-flight response to perceived threats. According to the House Rabbit Society, the owner of a pet rabbit can use various behavioral approaches to gain the animal's trust and reduce aggression, though this can be a long and difficult process.In addition, there is evidence to suggest that young rabbits that occupy the periphery of the "litter huddle" obtain less milk from the mother and, as a result, have a lower weight. It has been suggested that this factor may contribute to behavioural differences in litter mates during adolescence.
Not all veterinarians will treat rabbits, and pet owners may have to seek out an Exotic Animal Veterinarian for their rabbit's care. Rabbits need regular checkups at the veterinarian because they may hide signs of illness or disease. Additionally, rabbits need regular maintenance in the form of being able to chew on something and having their nails trimmed regularly.
The advantages of keeping rabbits as pets is that they are clean, smart, cute, soft and have a low carbon footprint. They may or may not react favorably to handling and petting depending on their personality and how they were raised. There are also many different sizes and characteristics available, owing to a long history of breeding. Rabbits are friendly to each other and are often compatible with other pets. Rabbits are herbivores and their diet is relatively simple. Compared to other small animals kept as pets, rabbits are physically robust creatures with strong hind legs that enable them to run fast, and they have powerful teeth. Rabbits should never be picked up by the ears or the "scruff" on the back of their neck because "their skeletons are light compared to their bodies, and they susceptible to trauma from falling, twisting, and kicking". Rabbits breed rapidly and so it is often easy, and affordable, to find one to buy or adopt. Some disadvantages of keeping rabbits as pets is that they may chew many things in the house. Unneutered male rabbits may spray their territory with a strong-smelling urine, unspayed female urine is also pungent, and so the litter box may smell. Rabbits can bite and scratch, and may do so to communicate displeasure, or if ignored; it is a part of normal communication and cannot be stopped entirely. They have to be picked up and handled properly to avoid injury to the rabbit or the owner. They may leave faeces around the house and are not always that conscious of leaving their droppings in the litter box. Rabbits can potentially be aggressive and territorial. Some rabbits may also be unfriendly, and then would be unsuitable as pets for children. Rabbits have a different body language to the most common domestic pets: cats and dogs. If someone wants a rabbit and is only familiar with those pet animals, then they would have to learn a lot about caring for this species and the behaviour of rabbits. They are often compared to guinea pigs but they may be as similar, in care and behaviour, to guinea pigs as they are to cats. Like cats, they are smart and can be litterbox trained. They also use their teeth and claws as weapons of defense and they can jump like a cat. They are quiet like a cat and independent, but they are also quite curious. Another animal they might be compared to is a chinchilla.
Rabbits have been kept as livestock since ancient times for their meat, wool, and fur. In modern times, rabbits are also utilized in scientific research as laboratory animals.
Breeds such as the New Zealand and Californian are frequently utilized for meat in commercial rabbitries. These breeds have efficient metabolisms and grow quickly; they are ready for slaughter by approximately 14 to 16 weeks of age. Rabbit fryers are rabbits that are between 70 and 90 days of age, and weighing between 3 and 5 lb (1 to 2 kg) live weight. Rabbit roasters are rabbits from 90 days to 6 months of age weighing between 5 and 8 lb (2 to 3.5 kg) live weight. Rabbit stewers are rabbits from 6 months on weighing over 8 lb. Any type of rabbit can be slaughtered for meat, but those exhibiting the "commercial" body type are most commonly raised for meat purposes. Dark fryers (any other color but albino whites) are sometimes lower in price than albino fryers because of the slightly darker tinge of the fryer (purely pink carcasses are preferred by consumers) and because the dark hairs are easier to see than if there are residual white hairs on the carcass. There is no difference in skinability.
Rabbits such as the Angora, American Fuzzy Lop, and Jersey Wooly produce wool. However, since the American Fuzzy Lop and Jersey Wooly are both dwarf breeds, only the much larger Angora breeds such as the English Angora, Satin Angora, Giant Angora, and French Angoras are used for commercial wool production. Their long fur is sheared, combed, or plucked (gently pulling loose hairs from the body during molting) and then spun into yarn used to make a variety of products. Angora sweaters can be purchased in many clothing stores and is generally mixed with other types of wool. Rabbit wool, called Angora, is 2.5 times warmer than sheep's wool.
Rabbit breeds that were developed for their fur qualities include the Rex with its plush texture, the Satin with its lustrous color, and the Chinchilla for its exotic pattern. White rabbit fur may be dyed in an array of colors that aren't produced naturally. Rabbits in the fur industry are fed a diet focused for robust coat production and pelts are harvested after the rabbit reaches prime condition, which takes longer than in the meat industry. Rabbit fur is used in local and commercial textile industries throughout the world. China imports much of its rabbit fur from Scandinavia (80%) and some from North America (5%), according to the USDA Foreign Agricultural Service GAIN Report CH7607.
Rabbits have been and continue to be used in laboratory work such as production of antibodies for vaccines and research of human male reproductive system toxicology. In 1972, around 450,000 rabbits were used for experiments in the United States, decreasing to around 240,000 in 2006. The Environmental Health Perspective, published by the National Institute of Health, states, "The rabbit [is] an extremely valuable model for studying the effects of chemicals or other stimuli on the male reproductive system." According to the Humane Society of the United States, rabbits are also used extensively in the study of bronchial asthma, stroke prevention treatments, cystic fibrosis, diabetes, and cancer. The New Zealand White is one of the most commonly used breeds for research and testing. The use of rabbits for the Draize test, a method of testing cosmetics on animals, has been cited as an example of cruelty in animal research by animal rights activists. Albino rabbits are typically used in the Draize tests because they have less tear flow than other animals, and the lack of eye pigment makes the effects easier to visualize.
Rabbits can live outdoors in properly constructed, sheltered hutches, which provide protection from the elements in winter and keep rabbits cool in summer heat. To protect from predators, rabbit hutches are usually situated in a fenced yard, shed, barn, or other enclosed structure, which may also contain a larger pen for exercise. Rabbits in such an environment can alternatively be allowed to roam the secured area freely, and simply be provided with an adapted doghouse for shelter. A more elaborate setup is an artificial warren. However, because of stress related to being inside confined spaces too small for a rabbit, it is recommended that instead of a cage, domestic rabbits free-roam indoors.
Rabbit show jumping, a form of animal sport between rabbits, began in the 1970s and has since become popular in Europe, particularly Sweden and the United Kingdom. When rabbit jumping was first starting out, the rules of competition were the same as horse jumping rules. However, rules were later changed to reflect a rabbit's abilities. The first national championship for rabbit show jumping was held in Stockholm, Sweden in 1987. Any rabbit, regardless of breed, may participate in this kind of competition, as it is based on athletic skill.
Cuniculture Dwarf rabbit Lop rabbit
The American Rabbit Breeders Association – the oldest and largest rabbit specialist organization in the United States The Livestock Conservancy – a registry of the rarest breeds of domestic rabbits World Rabbit Science Association – an international science organization dedicated to rabbit health research The British Rabbit Council – recognized breeds with photographs and more MediRabbit – a site dedicated to spreading the knowledge of rabbit medicine and safe medication in rabbits, for the owner and the vet professional Rabbit Breeds - directory of ARBA-recognized breeds of rabbit Complete Guide of Rabbit Breeds - List of rabbit breeds approved by American Rabbit Breeders Association RabbitPedia.com - Source for information about rabbit care. House Rabbit Society – a US-based educational and advocacy organization for rabbit pet-keepers, founded in 1988 Domestic rabbit at Curlie
Rabbit Niiname-no-Matsuri
There is another folk tradition which may use a variation "Rabbit", "Bunny", "I hate/love Grey Rabbits" or "White Rabbit" to ward off smoke that the wind is directing into your face when gathered around a campfire. It is thought that this tradition may be related to the tradition of invoking the rabbit on the first of the month. Others conjecture that it may originate with a North American First Nation story about smoke resembling rabbit fur. This tradition may be more of a social tradition in a group setting than a genuine belief that certain words will change the wind direction, and may be more of a childhood tradition than an adult one. Children have sometimes adapted from Rabbit to "Pink Elephant" or other comical derivatives. Because of this more mutable usage, historical record of this is even more scarce than other more static meanings. As with all folklore, its truth is made evident even in its only occasional fulfillment: should the wind then appear to change direction, others will interpret the use of such an expression as evidence of its effectiveness and will then tend to adopt and repeat its use. That multiple instances of its ineffectiveness also exist is discounted in light of the "fact" that it appeared to work once.
Three hares Rabbit's foot Stamping (custom)
On the White Rabbit Theory – An attempt to catalogue different "rabbit rabbit" variations and determine their origins. The Psychic Well Superstitions About Rabbits
The United States of America (USA), commonly known as the United States (US or U.S.) or America, is a country primarily located in central North America, between Canada and Mexico. It consists of 50 states, a federal district, five self-governing territories, and several other island possessions. At 3.8 million square miles (9.8 million square kilometers), it is the world's third- or fourth-largest country by total area. With a population of over 328 million, it is the third most populous country in the world. The national capital is Washington, D.C., and the most populous city is New York City. Paleo-Indians migrated from Siberia to the North American mainland at least 12,000 years ago, and European colonization began in the 16th century. The United States emerged from the thirteen British colonies established along the East Coast. Disputes over taxation and political representation with Great Britain led to the American Revolutionary War (1775–1783), which established independence. In the late 18th century, the U.S. began vigorously expanding across North America, gradually acquiring new territories, conquering and displacing Native Americans, and admitting new states; by 1848, the United States spanned the continent. Slavery was legal in the southern United States until the second half of the 19th century when the American Civil War led to its abolition. The Spanish–American War and World War I established the U.S. as a world power, a status confirmed by the outcome of World War II. During the Cold War, the United States and the Soviet Union engaged in various proxy wars but avoided direct military conflict. They also competed in the Space Race, culminating in the 1969 spaceflight that first landed humans on the Moon. The Soviet Union's collapse in 1991 ended the Cold War and left the United States as the world's sole superpower, with immense power in global geopolitics. The United States is a federal republic and a representative democracy with three separate branches of government, including a bicameral legislature. It is a founding member of the United Nations, World Bank, International Monetary Fund, Organization of American States (OAS), NATO, and other international organizations. It is a permanent member of the United Nations Security Council. The U.S. ranks high in international measures of economic freedom, lack of government corruption, quality of life, and quality of higher education. Despite income and wealth disparities, the United States continuously ranks high in measures of socioeconomic performance. It is one of the most racially and ethnically diverse nations in the world. Considered a melting pot of cultures, religions, and ethnicities, its population has been profoundly shaped by centuries of immigration. A highly developed country, the United States accounts for approximately a quarter of global gross domestic product (GDP) and is the world's largest economy by nominal GDP. By value, the United States is the world's largest importer and the second-largest exporter of goods. Although its population is only 4.3% of the world total, it holds 29.4% of the total wealth in the world, the largest share held by any country. Making up more than a third of global military spending, it is the foremost military power in the world and is a leading political, cultural, and scientific force internationally.
It has been generally accepted that the first inhabitants of North America migrated from Siberia by way of the Bering land bridge and arrived at least 12,000 years ago; however, increasing evidence suggests an even earlier arrival. The Clovis culture, which appeared around 11,000 BC, is believed to represent the first wave of human settlement of the Americas. This was likely the first of three major waves of migration into North America; later waves brought the forerunners of present-day Athabaskans, Aleuts, and Eskimos.Over time, indigenous cultures in North America grew increasingly complex, and some, such as the pre-Columbian Mississippian culture in the southeast, developed advanced agriculture, grand architecture, and state-level societies. Its city-state Cahokia is the largest, most complex pre-Columbian archaeological site in the modern-day United States. In the Four Corners region, Ancestral Puebloan culture developed from centuries of agricultural experimentation. The Iroquois Confederacy, located in the southern Great Lakes region, was established at some point between the twelfth and fifteenth centuries. Most prominent along the Atlantic coast were the Algonquian tribes, who practiced hunting and trapping, along with limited cultivation. Estimating the native population of North America at the time of European contact is difficult. Douglas H. Ubelaker of the Smithsonian Institution estimated that there was a population of 92,916 in the south Atlantic states and a population of 473,616 in the Gulf states, but most academics regard this figure as too low. Anthropologist Henry F. Dobyns believed the populations were much higher, suggesting 1,100,000 along the shores of the Gulf of Mexico, 2,211,000 people living between Florida and Massachusetts, 5,250,000 in the Mississippi Valley and tributaries, and 697,000 people in the Florida peninsula.
The first Europeans to arrive in the contiguous United States were Spanish conquistadors such as Juan Ponce de León, who made his first visit to Florida in 1513. Even earlier, Christopher Columbus landed in Puerto Rico on his 1493 voyage. The Spanish set up the first settlements in Florida and New Mexico, such as Saint Augustine and Santa Fe. The French established their own as well along the Mississippi River, notably New Orleans. Successful English settlement of the eastern coast of North America began with the Virginia Colony in 1607 at Jamestown and with the Pilgrims' Plymouth Colony in 1620. Many English settlers were dissenting Christian groups who came seeking religious freedom. The continent's first elected legislative assembly, Virginia's House of Burgesses, was created in 1619. Documents such as the Mayflower Compact and the Fundamental Orders of Connecticut established precedents for representative self-government and constitutionalism that would develop throughout the American colonies. In the early days of colonization, many European settlers were subject to food shortages, disease, and attacks from Native Americans. Native Americans were also often at war with neighboring tribes and allied with Europeans in their colonial wars. In many cases, however, natives and settlers came to depend on each other. Settlers traded for food and animal pelts; natives for guns, ammunition, and other European goods. Natives taught many settlers to cultivate corn, beans, and squash. European missionaries and others felt it was important to "civilize" the Native Americans and urged them to adopt European agricultural techniques and lifestyles. However, with the advancement of European colonization in North America, the Native Americans were often conquered and displaced. The native population of America declined after European arrival for various reasons, primarily diseases such as smallpox and measles.A large-scale slave trade with English privateers began. Because of less disease and better food and treatment, the life expectancy of slaves was much higher in North America than further south, leading to a rapid increase in the numbers of slaves. Colonial society was largely divided over the religious and moral implications of slavery, and colonies passed acts for and against the practice. But by the turn of the 18th century, African slaves were replacing European indentured servants for cash crop labor, especially in the South.The Thirteen Colonies (New Hampshire, Massachusetts, Connecticut, Rhode Island, New York, New Jersey, Pennsylvania, Delaware, Maryland, Virginia, North Carolina, South Carolina, and Georgia) that would become the United States of America were administered by the British as overseas dependencies. All nonetheless had local governments with elections open to most free men. With extremely high birth rates, low death rates, and steady settlement, the colonial population grew rapidly, eclipsing Native American populations. The Christian revivalist movement of the 1730s and 1740s known as the Great Awakening fueled interest both in religion and in religious liberty.During the Seven Years' War (1756–63), known in the U.S. as the French and Indian War, British forces seized Canada from the French. With the creation of the Province of Quebec (1763–1791), Canada's francophone population would remain politically and culturally isolated from the English-speaking colonial dependencies of Nova Scotia, Newfoundland and the Thirteen Colonies. Excluding the Native Americans who lived there, the Thirteen Colonies had a population of over 2.1 million in 1770, about a third that of Britain. Despite continuing new arrivals, the rate of natural increase was such that by the 1770s only a small minority of Americans had been born overseas. The colonies' distance from Britain had allowed the development of self-government, but their unprecedented success motivated British monarchs to periodically seek to reassert royal authority.
The American Revolutionary War fought by the Thirteen Colonies against the British Empire was the first successful colonial war of independence against a European power. Americans had developed an ideology of "republicanism", asserting that government rested on the will of the people as expressed in their local legislatures. They demanded their rights as Englishmen and "no taxation without representation". The British insisted on administering the empire through Parliament, and the conflict escalated into war.The Second Continental Congress unanimously adopted the Declaration of Independence on July 4, 1776; this day is celebrated annually as Independence Day. In 1777, the Articles of Confederation established a decentralized government that operated until 1789.After its defeat at the Battle of Yorktown in 1781, Britain signed a peace treaty. American sovereignty became internationally recognized, and the country was granted all lands east of the Mississippi River. Tensions with Britain remained, however, leading to the War of 1812, which was fought to a draw. Nationalists led the Philadelphia Convention of 1787 in writing the United States Constitution, ratified in state conventions in 1788. The federal government was reorganized into three branches in 1789, on the principle of creating salutary checks and balances. George Washington, who had led the Continental Army to victory, was the first president elected under the new constitution. The Bill of Rights, forbidding federal restriction of personal freedoms and guaranteeing a range of legal protections, was adopted in 1791. Although the federal government criminalized the international slave trade in 1808, after 1820, cultivation of the highly profitable cotton crop exploded in the Deep South, and along with it, the slave population. The Second Great Awakening, especially in the period 1800–1840, converted millions to evangelical Protestantism. In the North, it energized multiple social reform movements, including abolitionism; in the South, Methodists and Baptists proselytized among slave populations.Beginning in the late 18th century, Americans began to expand westward, prompting a long series of American Indian Wars. The 1803 Louisiana Purchase almost doubled the nation's area, Spain ceded Florida and other Gulf Coast territory in 1819, the Republic of Texas was annexed in 1845 during a period of expansionism, and the 1846 Oregon Treaty with Britain led to U.S. control of the present-day American Northwest. Victory in the Mexican–American War resulted in the 1848 Mexican Cession of California and much of the present-day American Southwest, making the U.S. span the continent.The California Gold Rush of 1848–49 spurred migration to the Pacific coast, which led to the California Genocide and the creation of additional western states. After the Civil War, new transcontinental railways made relocation easier for settlers, expanded internal trade, and increased conflicts with Native Americans. In 1869, a new Peace Policy nominally promised to protect Native Americans from abuses, avoid further war, and secure their eventual U.S. citizenship. Nonetheless, large-scale conflicts continued throughout the West into the 1900s.
Irreconcilable sectional conflict regarding the slavery of Africans and African Americans ultimately led to the American Civil War. With the 1860 election of Republican Abraham Lincoln, conventions in thirteen slave states declared secession and formed the Confederate States of America (the "South" or the "Confederacy"), while the federal government (the "Union") maintained that secession was illegal. In order to bring about this secession, military action was initiated by the secessionists, and the Union responded in kind. The ensuing war would become the deadliest military conflict in American history, resulting in the deaths of approximately 618,000 soldiers as well as many civilians. The Union initially simply fought to keep the country united. Nevertheless, as casualties mounted after 1863 and Lincoln delivered his Emancipation Proclamation, the main purpose of the war from the Union's viewpoint became the abolition of slavery. Indeed, when the Union ultimately won the war in April 1865, each of the states in the defeated South was required to ratify the Thirteenth Amendment, which prohibited slavery. Two amendments ensuring citizenship and, in theory, voting rights for blacks were also ratified. Reconstruction began in earnest following the war. While President Lincoln attempted to foster friendship and forgiveness between the Union and the former Confederacy, his assassination on April 14, 1865 drove a wedge between North and South again. Republicans in the federal government made it their goal to oversee the rebuilding of the South and to ensure the rights of African Americans. They persisted until the Compromise of 1877 when the Republicans agreed to cease protecting the rights of African Americans in the South in order for Democrats to concede the presidential election of 1876. Southern white Democrats, calling themselves "Redeemers", took control of the South after the end of Reconstruction. From 1890 to 1910, the Redeemers established so-called Jim Crow laws, disenfranchising most blacks and some poor whites throughout the region. Blacks faced racial segregation, especially in the South. They also occasionally experienced vigilante violence, including lynching.
In the North, urbanization and an unprecedented influx of immigrants from Southern and Eastern Europe supplied a surplus of labor for the country's industrialization and transformed its culture. National infrastructure, including telegraph and transcontinental railroads, spurred economic growth and greater settlement and development of the American Old West. The later invention of electric light and the telephone would also affect communication and urban life.The United States fought Indian Wars west of the Mississippi River from 1810 to at least 1890. Most of these conflicts ended with the cession of Native American territory and their confinement to Indian reservations. Additionally, the Trail of Tears in the 1830s exemplified the Indian removal policy that forcibly resettled Indians. This further expanded acreage under mechanical cultivation, increasing surpluses for international markets. Mainland expansion also included the purchase of Alaska from Russia in 1867. In 1893, pro-American elements in Hawaii overthrew the monarchy and formed the Republic of Hawaii, which the U.S. annexed in 1898. Puerto Rico, Guam, and the Philippines were ceded by Spain in the same year, following the Spanish–American War. American Samoa was acquired by the United States in 1900 after the end of the Second Samoan Civil War. The U.S. Virgin Islands were purchased from Denmark in 1917.Rapid economic development during the late 19th and early 20th centuries fostered the rise of many prominent industrialists. Tycoons like Cornelius Vanderbilt, John D. Rockefeller, and Andrew Carnegie led the nation's progress in the railroad, petroleum, and steel industries. Banking became a major part of the economy, with J. P. Morgan playing a notable role. The American economy boomed, becoming the world's largest. These dramatic changes were accompanied by social unrest and the rise of populist, socialist, and anarchist movements. This period eventually ended with the advent of the Progressive Era, which saw significant reforms including women's suffrage, alcohol prohibition, regulation of consumer goods, greater antitrust measures to ensure competition and attention to worker conditions.
The United States remained neutral from the outbreak of World War I in 1914 until 1917 when it joined the war as an "associated power" alongside the formal Allies of World War I, helping to turn the tide against the Central Powers. In 1919, President Woodrow Wilson took a leading diplomatic role at the Paris Peace Conference and advocated strongly for the U.S. to join the League of Nations. However, the Senate refused to approve this and did not ratify the Treaty of Versailles that established the League of Nations.In 1920, the women's rights movement won passage of a constitutional amendment granting women's suffrage. The 1920s and 1930s saw the rise of radio for mass communication and the invention of early television. The prosperity of the Roaring Twenties ended with the Wall Street Crash of 1929 and the onset of the Great Depression. After his election as president in 1932, Franklin D. Roosevelt responded with the New Deal. The Great Migration of millions of African Americans out of the American South began before World War I and extended through the 1960s; whereas the Dust Bowl of the mid-1930s impoverished many farming communities and spurred a new wave of western migration.At first effectively neutral during World War II, the United States began supplying materiel to the Allies in March 1941 through the Lend-Lease program. On December 7, 1941, the Empire of Japan launched a surprise attack on Pearl Harbor, prompting the United States to join the Allies against the Axis powers and, the following year, to intern about 120,000 U.S. residents (including American citizens) of Japanese descent. Although Japan attacked the United States first, the U.S. nonetheless pursued a "Europe first" defense policy. The United States thus left its vast Asian colony, the Philippines, isolated and fighting a losing struggle against Japanese invasion and occupation. During the war, the United States was referred to as one of the "Four Policemen" of Allies power who met to plan the postwar world, along with Britain, the Soviet Union, and China. Although the nation lost around 400,000 military personnel, it emerged relatively undamaged from the war with even greater economic and military influence.The United States played a leading role in the Bretton Woods and Yalta conferences, which signed agreements on new international financial institutions and Europe's postwar reorganization. As an Allied victory was won in Europe, a 1945 international conference held in San Francisco produced the United Nations Charter, which became active after the war. The United States and Japan then fought each other in the largest naval battle in history, the Battle of Leyte Gulf. The United States eventually developed the first nuclear weapons and used them on Japan in the cities of Hiroshima and Nagasaki; the Japanese surrendered on September 2, ending World War II.
After World War II, the United States and the Soviet Union competed for power, influence, and prestige during what became known as the Cold War, driven by an ideological divide between capitalism and communism. They dominated the military affairs of Europe, with the U.S. and its NATO allies on one side and the Soviet Union and its Warsaw Pact allies on the other. The U.S. developed a policy of containment towards the expansion of communist influence. While the U.S. and Soviet Union engaged in proxy wars and developed powerful nuclear arsenals, the two countries avoided direct military conflict.The United States often opposed Third World movements that it viewed as Soviet-sponsored and occasionally pursued direct action for regime change against left-wing governments, even occasionally supporting authoritarian right-wing regimes. American troops fought communist Chinese and North Korean forces in the Korean War of 1950–53. The Soviet Union's 1957 launch of the first artificial satellite and its 1961 launch of the first crewed spaceflight initiated a "Space Race" in which the United States became the first nation to land a man on the Moon in 1969. A proxy war in Southeast Asia eventually evolved into the Vietnam War (1955–1975), with full American participation.At home, the U.S. had experienced sustained economic expansion and a rapid growth of its population and middle class following World War II. After a surge in female labor participation, especially in the 1970s, by 1985, the majority of women aged 16 and over were employed. Construction of an Interstate Highway System transformed the nation's infrastructure over the following decades. Millions moved from farms and inner cities to large suburban housing developments. In 1959 Hawaii became the 50th and last U.S. state added to the country. The growing Civil Rights Movement used nonviolence to confront segregation and discrimination, with Martin Luther King Jr. becoming a prominent leader and figurehead. A combination of court decisions and legislation, culminating in the Civil Rights Act of 1968, sought to end racial discrimination. Meanwhile, a counterculture movement grew, which was fueled by opposition to the Vietnam war, the Black Power movement, and the sexual revolution. The launch of a "War on Poverty" expanded entitlements and welfare spending, including the creation of Medicare and Medicaid, two programs that provide health coverage to the elderly and poor, respectively, and the means-tested Food Stamp Program and Aid to Families with Dependent Children.The 1970s and early 1980s saw the onset of stagflation. After his election in 1980, President Ronald Reagan responded to economic stagnation with free-market oriented reforms. Following the collapse of détente, he abandoned "containment" and initiated the more aggressive "rollback" strategy towards the Soviet Union. The late 1980s brought a "thaw" in relations with the Soviet Union, and its collapse in 1991 finally ended the Cold War. This brought about unipolarity with the U.S. unchallenged as the world's dominant superpower.
After the Cold War, the conflict in the Middle East triggered a crisis in 1990, when Iraq invaded and attempted to annex Kuwait, an ally of the United States. Fearing the spread of instability, in August, President George H. W. Bush launched and led the Gulf War against Iraq; waged until January 1991 by coalition forces from 34 nations, it ended in the expulsion of Iraqi forces from Kuwait and restoration of the monarchy.Originating within U.S. military defense networks, the Internet spread to international academic platforms and then to the public in the 1990s, greatly affecting the global economy, society, and culture. Due to the dot-com boom, stable monetary policy, and reduced social welfare spending, the 1990s saw the longest economic expansion in modern U.S. history. Beginning in 1994, the U.S. signed the North American Free Trade Agreement (NAFTA), causing trade among the U.S., Canada, and Mexico to soar.On September 11, 2001, Al-Qaeda terrorist hijackers flew passenger planes into the World Trade Center in New York City and the Pentagon near Washington, D.C., killing nearly 3,000 people. In response, the United States launched the War on Terror, which included a war in Afghanistan and the 2003–11 Iraq War. A 2011 military operation in Pakistan led to the death of the leader of Al-Qaeda.Government policy designed to promote affordable housing, widespread failures in corporate and regulatory governance, and historically low interest rates set by the Federal Reserve led to the mid-2000s housing bubble, which culminated with the 2008 financial crisis, the nation's largest economic contraction since the Great Depression. During the crisis, assets owned by Americans lost about a quarter of their value. Barack Obama, the first African-American and multiracial president, was elected in 2008 amid the crisis, and subsequently passed stimulus measures and the Dodd–Frank Act in an attempt to mitigate its negative effects and ensure there would not be a repeat of the crisis. In 2010, President Obama led efforts to pass the Affordable Care Act, the most sweeping reform to the nation's healthcare system in nearly five decades.In the presidential election of 2016, Republican Donald Trump was elected as the 45th president of the United States. On January 20, 2020, the first case of COVID-19 in the United States was confirmed. As of September 2020, the United States has over 6.2 million COVID-19 cases and over 180,000 deaths. The United States is by far the country with the most cases of COVID-19 since April 11, 2020.
The 48 contiguous states and the District of Columbia occupy a combined area of 3,119,885 square miles (8,080,470 km2). Of this area, 2,959,064 square miles (7,663,940 km2) is contiguous land, composing 83.65% of total U.S. land area. Hawaii, occupying an archipelago in the central Pacific, southwest of North America, is 10,931 square miles (28,311 km2) in area. The populated territories of Puerto Rico, American Samoa, Guam, Northern Mariana Islands, and U.S. Virgin Islands together cover 9,185 square miles (23,789 km2). Measured by only land area, the United States is third in size behind Russia and China, just ahead of Canada.The United States is the world's third- or fourth-largest nation by total area (land and water), ranking behind Russia and Canada and nearly equal to China. The ranking varies depending on how two territories disputed by China and India are counted, and how the total size of the United States is measured.The coastal plain of the Atlantic seaboard gives way further inland to deciduous forests and the rolling hills of the Piedmont. The Appalachian Mountains divide the eastern seaboard from the Great Lakes and the grasslands of the Midwest. The Mississippi–Missouri River, the world's fourth longest river system, runs mainly north–south through the heart of the country. The flat, fertile prairie of the Great Plains stretches to the west, interrupted by a highland region in the southeast.The Rocky Mountains, west of the Great Plains, extend north to south across the country, peaking around 14,000 feet (4,300 m) in Colorado. Farther west are the rocky Great Basin and deserts such as the Chihuahua and Mojave. The Sierra Nevada and Cascade mountain ranges run close to the Pacific coast, both ranges reaching altitudes higher than 14,000 feet (4,300 m). The lowest and highest points in the contiguous United States are in the state of California, and only about 84 miles (135 km) apart. At an elevation of 20,310 feet (6,190.5 m), Alaska's Denali is the highest peak in the country and in North America. Active volcanoes are common throughout Alaska's Alexander and Aleutian Islands, and Hawaii consists of volcanic islands. The supervolcano underlying Yellowstone National Park in the Rockies is the continent's largest volcanic feature.The United States, with its large size and geographic variety, includes most climate types. To the east of the 100th meridian, the climate ranges from humid continental in the north to humid subtropical in the south. The Great Plains west of the 100th meridian are semi-arid. Much of the Western mountains have an alpine climate. The climate is arid in the Great Basin, desert in the Southwest, Mediterranean in coastal California, and oceanic in coastal Oregon and Washington and southern Alaska. Most of Alaska is subarctic or polar. Hawaii and the southern tip of Florida are tropical, as well as its territories in the Caribbean and the Pacific. States bordering the Gulf of Mexico are prone to hurricanes, and most of the world's tornadoes occur in the country, mainly in Tornado Alley areas in the Midwest and South. Overall, the United States receives more high-impact extreme weather incidents than any other country in the world.
The U.S. ecology is megadiverse: about 17,000 species of vascular plants occur in the contiguous United States and Alaska, and more than 1,800 species of flowering plants are found in Hawaii, few of which occur on the mainland. The United States is home to 428 mammal species, 784 bird species, 311 reptile species, and 295 amphibian species, as well as about 91,000 insect species.There are 62 national parks and hundreds of other federally managed parks, forests, and wilderness areas. Altogether, the government owns about 28% of the country's land area, mostly in the western states. Most of this land is protected, though some is leased for oil and gas drilling, mining, logging, or cattle ranching, and about .86% is used for military purposes.Environmental issues include debates on oil and nuclear energy, dealing with air and water pollution, the economic costs of protecting wildlife, logging and deforestation, and international responses to global warming. The most prominent environmental agency is the Environmental Protection Agency (EPA), created by presidential order in 1970. The idea of wilderness has shaped the management of public lands since 1964, with the Wilderness Act. The Endangered Species Act of 1973 is intended to protect threatened and endangered species and their habitats, which are monitored by the United States Fish and Wildlife Service.
The First Amendment of the U.S. Constitution guarantees the free exercise of religion and forbids Congress from passing laws respecting its establishment. The United States has the world's largest Christian population. In a 2014 survey, 70.6% of adults in the United States identified themselves as Christians; Protestants accounted for 46.5%, while Roman Catholics, at 20.8%, formed the largest single Christian group. In 2014, 5.9% of the U.S. adult population claimed a non-Christian religion. These include Judaism (1.9%), Islam (0.9%), Hinduism (0.7%), and Buddhism (0.7%). The survey also reported that 22.8% of Americans described themselves as agnostic, atheist or simply having no religion—up from 8.2% in 1990.Protestantism is the largest Christian religious grouping in the United States, accounting for almost half of all Americans. Baptists collectively form the largest branch of Protestantism at 15.4%, and the Southern Baptist Convention is the largest individual Protestant denomination at 5.3% of the U.S. population. Apart from Baptists, other Protestant categories include nondenominational Protestants, Methodists, Pentecostals, unspecified Protestants, Lutherans, Presbyterians, Congregationalists, other Reformed, Episcopalians/Anglicans, Quakers, Adventists, Holiness, Christian fundamentalists, Anabaptists, Pietists, and multiple others.The Bible Belt is an informal term for a region in the Southern United States in which socially conservative evangelical Protestantism is a significant part of the culture and Christian church attendance across the denominations is generally higher than the nation's average. By contrast, religion plays the least important role in New England and in the Western United States.
The United States had a life expectancy of 78.6 years at birth in 2017, which was the third year of declines in life expectancy following decades of continuous increase. The recent decline, primarily among the age group 25 to 64, is largely due to record highs in the drug overdose and suicide rates; the country has one of the highest suicide rates among wealthy countries. From 1999 to 2019, more than 770,000 Americans died from drug overdoses. Life expectancy was highest among Asians and Hispanics and lowest among blacks.Increasing obesity in the United States and improvements in health and longevity outside the U.S. contributed to lowering the country's rank in life expectancy from 11th in the world in 1987 to 42nd in 2007. In 2017, the United States had the lowest life expectancy among Japan, Canada, Australia, the United Kingdom, and seven nations in western Europe. Obesity rates have more than doubled in the last 30 years and are the highest in the industrialized world. Approximately one-third of the adult population is obese and an additional third is overweight. Obesity-related type 2 diabetes is considered epidemic by health care professionals.In 2010, coronary artery disease, lung cancer, stroke, chronic obstructive pulmonary diseases, and traffic accidents caused the most years of life lost in the U.S. Low back pain, depression, musculoskeletal disorders, neck pain, and anxiety caused the most years lost to disability. The most harmful risk factors were poor diet, tobacco smoking, obesity, high blood pressure, high blood sugar, physical inactivity, and alcohol use. Alzheimer's disease, drug abuse, kidney disease, cancer, and falls caused the most additional years of life lost over their age-adjusted 1990 per-capita rates. U.S. teenage pregnancy and abortion rates are substantially higher than in other Western nations, especially among blacks and Hispanics.Health-care coverage in the United States is a combination of public and private efforts and is not universal. In 2017, 12.2% of the population did not carry health insurance. The subject of uninsured and underinsured Americans is a major political issue. The Affordable Care Act, passed in early 2010, roughly halved the uninsured share of the population, though the bill and its ultimate effect are issues of controversy. The U.S. health-care system far outspends any other nation, measured both in per capita spending and as percentage of GDP. However, the U.S. is a global leader in medical innovation.
American public education is operated by state and local governments and regulated by the United States Department of Education through restrictions on federal grants. In most states, children are required to attend school from the age of six or seven (generally, kindergarten or first grade) until they turn 18 (generally bringing them through twelfth grade, the end of high school); some states allow students to leave school at 16 or 17.About 12% of children are enrolled in parochial or nonsectarian private schools. Just over 2% of children are homeschooled. The U.S. spends more on education per student than any nation in the world, spending an average of $12,794 per year on public elementary and secondary school students in the 2016–2017 school year. Some 80% of U.S. college students attend public universities.Of Americans 25 and older, 84.6% graduated from high school, 52.6% attended some college, 27.2% earned a bachelor's degree, and 9.6% earned graduate degrees. The basic literacy rate is approximately 99%. The United Nations assigns the United States an Education Index of 0.97, tying it for 12th in the world.The United States has many private and public institutions of higher education. The majority of the world's top universities, as listed by various ranking organizations, are in the U.S. There are also local community colleges with generally more open admission policies, shorter academic programs, and lower tuition. In 2018, U21, a network of research-intensive universities, ranked the United States first in the world for breadth and quality of higher education, and 15th when GDP was a factor. As for public expenditures on higher education, the U.S. trails some other OECD (Organization for Cooperation and Development) nations but spends more per student than the OECD average, and more than all nations in combined public and private spending. As of 2018, student loan debt exceeded 1.5 trillion dollars.
The United States is a federal republic of 50 states, a federal district, five territories and several uninhabited island possessions. It is the world's oldest surviving federation. It is a federal republic and a representative democracy "in which majority rule is tempered by minority rights protected by law." The U.S. ranked 25th on the Democracy Index in 2018. On Transparency International's 2019 Corruption Perceptions Index, its public sector position deteriorated from a score of 76 in 2015 to 69 in 2019.In the American federalist system, citizens are usually subject to three levels of government: federal, state, and local. The local government's duties are commonly split between county and municipal governments. In almost all cases, executive and legislative officials are elected by a plurality vote of citizens by district. The government is regulated by a system of checks and balances defined by the U.S. Constitution, which serves as the country's supreme legal document. The original text of the Constitution establishes the structure and responsibilities of the federal government and its relationship with the individual states. Article One protects the right to the writ of habeas corpus. The Constitution has been amended 27 times; the first ten amendments, which make up the Bill of Rights, and the Fourteenth Amendment form the central basis of Americans' individual rights. All laws and governmental procedures are subject to judicial review and any law ruled by the courts to be in violation of the Constitution is voided. The principle of judicial review, not explicitly mentioned in the Constitution, was established by the Supreme Court in Marbury v. Madison (1803) in a decision handed down by Chief Justice John Marshall.The federal government comprises three branches: Legislative: The bicameral Congress, made up of the Senate and the House of Representatives, makes federal law, declares war, approves treaties, has the power of the purse, and has the power of impeachment, by which it can remove sitting members of the government. Executive: The president is the commander-in-chief of the military, can veto legislative bills before they become law (subject to congressional override), and appoints the members of the Cabinet (subject to Senate approval) and other officers, who administer and enforce federal laws and policies. Judicial: The Supreme Court and lower federal courts, whose judges are appointed by the president with Senate approval, interpret laws and overturn those they find unconstitutional.The House of Representatives has 435 voting members, each representing a congressional district for a two-year term. House seats are apportioned among the states by population. Each state then draws single-member districts to conform with the census apportionment. The District of Columbia and the five major U.S. territories each have one member of Congress—these members are not allowed to vote.The Senate has 100 members with each state having two senators, elected at-large to six-year terms; one-third of Senate seats are up for election every two years. The District of Columbia and the five major U.S. territories do not have senators. The president serves a four-year term and may be elected to the office no more than twice. The president is not elected by direct vote, but by an indirect electoral college system in which the determining votes are apportioned to the states and the District of Columbia. The Supreme Court, led by the chief justice of the United States, has nine members, who serve for life.
The 50 states are the principal administrative divisions in the country. These are subdivided into counties or county equivalents and further divided into municipalities. The District of Columbia is a federal district that contains the capital of the United States, Washington, D.C. The states and the District of Columbia choose the president of the United States. Each state has presidential electors equal to the number of their representatives and senators in Congress; the District of Columbia has three (because of the 23rd Amendment). Territories of the United States such as Puerto Rico do not have presidential electors, and so people in those territories cannot vote for the president.The United States also observes tribal sovereignty of the American Indian nations to a limited degree, as it does with the states' sovereignty. American Indians are U.S. citizens and tribal lands are subject to the jurisdiction of the U.S. Congress and the federal courts. Like the states they have a great deal of autonomy, but also like the states, tribes are not allowed to make war, engage in their own foreign relations, or print and issue currency.Citizenship is granted at birth in all states, the District of Columbia, and all major U.S. territories except American Samoa.
The United States has operated under a two-party system for most of its history. For elective offices at most levels, state-administered primary elections choose the major party nominees for subsequent general elections. Since the general election of 1856, the major parties have been the Democratic Party, founded in 1824, and the Republican Party, founded in 1854. Since the Civil War, only one third-party presidential candidate—former president Theodore Roosevelt, running as a Progressive in 1912—has won as much as 20% of the popular vote. The president and vice president are elected by the Electoral College.In American political culture, the center-right Republican Party is considered "conservative" and the center-left Democratic Party is considered "liberal". The states of the Northeast and West Coast and some of the Great Lakes states, known as "blue states", are relatively liberal. The "red states" of the South and parts of the Great Plains and Rocky Mountains are relatively conservative. Republican Donald Trump, the winner of the 2016 presidential election, is serving as the 45th president of the United States. Leadership in the Senate includes vice president Mike Pence, president pro tempore Chuck Grassley, Majority Leader Mitch McConnell, and Minority Leader Chuck Schumer. Leadership in the House includes Speaker of the House Nancy Pelosi, Majority Leader Steny Hoyer, and Minority Leader Kevin McCarthy. In the 116th United States Congress, the House of Representatives is controlled by the Democratic Party and the Senate is controlled by the Republican Party, giving the U.S. a split Congress. The Senate consists of 53 Republicans and 45 Democrats with two Independents who caucus with the Democrats; the House consists of 233 Democrats, 196 Republicans, and 1 Libertarian. Of state governors, there are 26 Republicans and 24 Democrats. Among the D.C. mayor and the five territorial governors, there are four Democrats, one Republican, and one New Progressive.
The United States has an established structure of foreign relations. It is a permanent member of the United Nations Security Council. New York City is home to the United Nations Headquarters. Almost all countries have embassies in Washington, D.C., and many have consulates around the country. Likewise, nearly all nations host American diplomatic missions. However, Iran, North Korea, Bhutan, and the Republic of China (Taiwan) do not have formal diplomatic relations with the United States (although the U.S. still maintains unofficial relations with Bhutan and Taiwan). It is a member of the G7, G20, and OECD. The United States has a "Special Relationship" with the United Kingdom and strong ties with India, Canada, Australia, New Zealand, the Philippines, Japan, South Korea, Israel, and several European Union countries, including France, Italy, Germany, Spain and Poland. It works closely with fellow NATO members on military and security issues and with its neighbors through the Organization of American States and free trade agreements such as the trilateral North American Free Trade Agreement with Canada and Mexico. Colombia is traditionally considered by the United States as its most loyal ally in South America.The U.S. exercises full international defense authority and responsibility for Micronesia, the Marshall Islands and Palau through the Compact of Free Association.
The president is the commander-in-chief of the United States Armed Forces and appoints its leaders, the secretary of defense and the Joint Chiefs of Staff. The Department of Defense administers five of the six service branches, which are made up of the Army, Marine Corps, Navy, Air Force, and Space Force. The Coast Guard, also a branch of the armed forces, is administered by the Department of Homeland Security in peacetime and by the Department of the Navy in wartime. In 2019, all six branches of the U.S. Armed Forces reported 1.4 million personnel on active duty. The Reserves and National Guard brought the total number of troops to 2.3 million. The Department of Defense also employed about 700,000 civilians, not including contractors.Military service in the United States is voluntary, although conscription may occur in wartime through the Selective Service System. From 1940 until 1973, conscription was mandatory even during peacetime. Today, American forces can be rapidly deployed by the Air Force's large fleet of transport aircraft, the Navy's 11 active aircraft carriers, and Marine expeditionary units at sea with the Navy's Atlantic and Pacific fleets. The military operates about 800 bases and facilities abroad, and maintains deployments greater than 100 active duty personnel in 25 foreign countries. The United States spent $649 billion on its military in 2019, 36% of global military spending. At 4.7% of GDP, the rate was the second-highest among the top 15 military spenders, after Saudi Arabia. Defense spending plays a major role in science and technology investment, with roughly half of U.S. federal research and development funded by the Department of Defense. Defense's share of the overall U.S. economy has generally declined in recent decades, from early Cold War peaks of 14.2% of GDP in 1953 and 69.5% of federal spending in 1954 to 4.7% of GDP and 18.8% of federal spending in 2011.The country is one of the five recognized nuclear weapons states and one of nine countries to possess nuclear weapons. The United States possesses the second-largest stockpile of nuclear weapons in the world. More than 40% of the world's 14,000 nuclear weapons are held by the United States.
Law enforcement in the United States is primarily the responsibility of local police departments and sheriff's offices, with state police providing broader services. Federal agencies such as the Federal Bureau of Investigation (FBI) and the U.S. Marshals Service have specialized duties, including protecting civil rights, national security and enforcing U.S. federal courts' rulings and federal laws. State courts conduct most criminal trials while federal courts handle certain designated crimes as well as certain appeals from the state criminal courts. A cross-sectional analysis of the World Health Organization Mortality Database from 2010 showed that United States homicide rates "were 7.0 times higher than in other high-income countries, driven by a gun homicide rate that was 25.2 times higher." In 2016, the U.S. murder rate was 5.4 per 100,000. The United States has the highest documented incarceration rate and largest prison population in the world. As of 2020, the Prison Policy Initiative reported that there were some 2.3 million people incarcerated. According to the Federal Bureau of Prisons, the majority of inmates held in federal prisons are convicted of drug offenses. The imprisonment rate for all prisoners sentenced to more than a year in state or federal facilities is 478 per 100,000 in 2013. About 9% of prisoners are held in privatized prisons, a practice beginning in the 1980s and a subject of contention.Capital punishment is sanctioned in the United States for certain federal and military crimes, and at the state level in 28 states, though three states have moratoriums on carrying out the penalty imposed by their governors. In 2019, the country had the sixth-highest number of executions in the world, following China, Iran, Saudi Arabia, Iraq, and Egypt. No executions took place from 1967 to 1977, owing in part to a U.S. Supreme Court ruling striking down the practice. Since the decision, however, there have been more than 1,500 executions. In recent years the number of executions and presence of capital punishment statute on whole has trended down nationally, with several states recently abolishing the penalty.
According to the International Monetary Fund, the U.S. GDP of $16.8 trillion constitutes 24% of the gross world product at market exchange rates and over 19% of the gross world product at purchasing power parity. The United States is the largest importer of goods and second-largest exporter, though exports per capita are relatively low. In 2010, the total U.S. trade deficit was $635 billion. Canada, China, Mexico, Japan, and Germany are its top trading partners. From 1983 to 2008, U.S. real compounded annual GDP growth was 3.3%, compared to a 2.3% weighted average for the rest of the G7. The country ranks ninth in the world in nominal GDP per capita and sixth in GDP per capita at PPP. The U.S. dollar is the world's primary reserve currency. In 2009, the private sector was estimated to constitute 86.4% of the economy. While its economy has reached a postindustrial level of development, the United States remains an industrial power. In August 2010, the American labor force consisted of 154.1 million people (50%). With 21.2 million people, government is the leading field of employment. The largest private employment sector is health care and social assistance, with 16.4 million people. It has a smaller welfare state and redistributes less income through government action than most European nations.The United States is the only advanced economy that does not guarantee its workers paid vacation and is one of a few countries in the world without paid family leave as a legal right. 74% of full-time American workers get paid sick leave, according to the Bureau of Labor Statistics, although only 24% of part-time workers get the same benefits. In 2009, the United States had the third-highest workforce productivity per person in the world, behind Luxembourg and Norway.
The United States has been a leader in technological innovation since the late 19th century and scientific research since the mid-20th century. Methods for producing interchangeable parts were developed by the U.S. War Department by the Federal Armories during the first half of the 19th century. This technology, along with the establishment of a machine tool industry, enabled the U.S. to have large-scale manufacturing of sewing machines, bicycles, and other items in the late 19th century and became known as the American system of manufacturing. Factory electrification in the early 20th century and introduction of the assembly line and other labor-saving techniques created the system of mass production. In the 21st century, approximately two-thirds of research and development funding comes from the private sector. The United States leads the world in scientific research papers and impact factor.In 1876, Alexander Graham Bell was awarded the first U.S. patent for the telephone. Thomas Edison's research laboratory, one of the first of its kind, developed the phonograph, the first long-lasting light bulb, and the first viable movie camera. The latter led to emergence of the worldwide entertainment industry. In the early 20th century, the automobile companies of Ransom E. Olds and Henry Ford popularized the assembly line. The Wright brothers, in 1903, made the first sustained and controlled heavier-than-air powered flight.The rise of fascism and Nazism in the 1920s and 30s led many European scientists, including Albert Einstein, Enrico Fermi, and John von Neumann, to immigrate to the United States. During World War II, the Manhattan Project developed nuclear weapons, ushering in the Atomic Age, while the Space Race produced rapid advances in rocketry, materials science, and aeronautics.The invention of the transistor in the 1950s, a key active component in practically all modern electronics, led to many technological developments and a significant expansion of the U.S. technology industry. This, in turn, led to the establishment of many new technology companies and regions around the country such as Silicon Valley in California. Advancements by American microprocessor companies such as Advanced Micro Devices (AMD) and Intel, along with both computer software and hardware companies such as Adobe Systems, Apple Inc., IBM, Microsoft, and Sun Microsystems, created and popularized the personal computer. The ARPANET was developed in the 1960s to meet Defense Department requirements, and became the first of a series of networks which evolved into the Internet.
Accounting for 4.24% of the global population, Americans collectively possess 29.4% of the world's total wealth, the largest percentage of any country. Americans also make up roughly half of the world's population of millionaires. The Global Food Security Index ranked the U.S. number one for food affordability and overall food security in March 2013. Americans on average have more than twice as much living space per dwelling and per person as EU residents. For 2017 the United Nations Development Programme ranked the United States 13th among 189 countries in its Human Development Index (HDI) and 25th among 151 countries in its inequality-adjusted HDI (IHDI).Wealth, like income and taxes, is highly concentrated; the richest 10% of the adult population possess 72% of the country's household wealth, while the bottom half possess only 2%. According to the Federal Reserve, the top 1% controlled 38.6% of the country's wealth in 2016. In 2017, Forbes found that just three individuals (Jeff Bezos, Warren Buffett and Bill Gates) held more money than the bottom half of the population. According to a 2018 study by the OECD, the United States has a larger percentage of low-income workers than almost any other developed nation, largely because of a weak collective bargaining system and lack of government support for at-risk workers. The top one percent of income-earners accounted for 52 percent of the income gains from 2009 to 2015, where income is defined as market income excluding government transfers. After years of stagnation, median household income reached a record high in 2016 following two consecutive years of record growth. Income inequality remains at record highs however, with the top fifth of earners taking home more than half of all overall income. The rise in the share of total annual income received by the top one percent, which has more than doubled from nine percent in 1976 to 20 percent in 2011, has significantly affected income inequality, leaving the United States with one of the widest income distributions among OECD nations. The extent and relevance of income inequality is a matter of debate.There were about 567,715 sheltered and unsheltered homeless persons in the U.S. in January 2019, with almost two-thirds staying in an emergency shelter or transitional housing program. In 2011, 16.7 million children lived in food-insecure households, about 35% more than 2007 levels, though only 845,000 U.S. children (1.1%) saw reduced food intake or disrupted eating patterns at some point during the year, and most cases were not chronic. As of June 2018, 40 million people, roughly 12.7% of the U.S. population, were living in poverty, including 13.3 million children. Of those impoverished, 18.5 million live in deep poverty (family income below one-half of the poverty threshold) and over five million live "in 'Third World' conditions". In 2017, the U.S. states or territories with the lowest and highest poverty rates were New Hampshire (7.6%) and American Samoa (65%), respectively. The economic impact and mass unemployment caused by the COVID-19 pandemic has raised fears of a mass eviction crisis, with an analysis by the Aspen Institute indicating that between 30 and 40 million people are at risk for eviction by the end of 2020.
Personal transportation is dominated by automobiles, which operate on a network of 4 million miles (6.4 million kilometers) of public roads. The United States has the world's second-largest automobile market, and has the highest vehicle ownership per capita in the world, with 816.4 vehicles per 1,000 Americans (2014). In 2017, there were 255,009,283 non-two wheel motor vehicles, or about 910 vehicles per 1,000 people.The civil airline industry is entirely privately owned and has been largely deregulated since 1978, while most major airports are publicly owned. The three largest airlines in the world by passengers carried are U.S.-based; American Airlines is number one after its 2013 acquisition by US Airways. Of the world's 50 busiest passenger airports, 16 are in the United States, including the busiest, Hartsfield–Jackson Atlanta International Airport.
The United States energy market is about 29,000 terawatt hours per year. In 2005, 40% of this energy came from petroleum, 23% from coal, and 22% from natural gas. The remainder was supplied by nuclear and renewable energy sources.Since 2007, the total greenhouse gas emissions by the United States are the second highest by country, exceeded only by China. The United States has historically been the world's largest producer of greenhouse gases, and greenhouse gas emissions per capita remain high.
The United States is home to many cultures and a wide variety of ethnic groups, traditions, and values. Aside from the Native American, Native Hawaiian, and Native Alaskan populations, nearly all Americans or their ancestors immigrated within the past five centuries. Mainstream American culture is a Western culture largely derived from the traditions of European immigrants with influences from many other sources, such as traditions brought by slaves from Africa. More recent immigration from Asia and especially Latin America has added to a cultural mix that has been described as both a homogenizing melting pot, and a heterogeneous salad bowl in which immigrants and their descendants retain distinctive cultural characteristics.Americans have traditionally been characterized by a strong work ethic, competitiveness, and individualism, as well as a unifying belief in an "American creed" emphasizing liberty, equality, private property, democracy, rule of law, and a preference for limited government. Americans are extremely charitable by global standards: according to a 2006 British study, Americans gave 1.67% of GDP to charity, more than any other nation studied.The American Dream, or the perception that Americans enjoy high social mobility, plays a key role in attracting immigrants. Whether this perception is accurate has been a topic of debate. While mainstream culture holds that the United States is a classless society, scholars identify significant differences between the country's social classes, affecting socialization, language, and values. Americans tend to greatly value socioeconomic achievement, but being ordinary or average is also generally seen as a positive attribute.
In the 18th and early 19th centuries, American art and literature took most of its cues from Europe. Writers such as Washington Irving, Nathaniel Hawthorne, Edgar Allan Poe, and Henry David Thoreau established a distinctive American literary voice by the middle of the 19th century. Mark Twain and poet Walt Whitman were major figures in the century's second half; Emily Dickinson, virtually unknown during her lifetime, is now recognized as an essential American poet. A work seen as capturing fundamental aspects of the national experience and character—such as Herman Melville's Moby-Dick (1851), Twain's The Adventures of Huckleberry Finn (1885), F. Scott Fitzgerald's The Great Gatsby (1925) and Harper Lee's To Kill a Mockingbird (1960)—may be dubbed the "Great American Novel."Thirteen U.S. citizens have won the Nobel Prize in Literature. William Faulkner, Ernest Hemingway and John Steinbeck are often named among the most influential writers of the 20th century. Popular literary genres such as the Western and hardboiled crime fiction developed in the United States. The Beat Generation writers opened up new literary approaches, as have postmodernist authors such as John Barth, Thomas Pynchon, and Don DeLillo.The transcendentalists, led by Thoreau and Ralph Waldo Emerson, established the first major American philosophical movement. After the Civil War, Charles Sanders Peirce and then William James and John Dewey were leaders in the development of pragmatism. In the 20th century, the work of W. V. O. Quine and Richard Rorty, and later Noam Chomsky, brought analytic philosophy to the fore of American philosophical academia. John Rawls and Robert Nozick also led a revival of political philosophy. In the visual arts, the Hudson River School was a mid-19th-century movement in the tradition of European naturalism. The 1913 Armory Show in New York City, an exhibition of European modernist art, shocked the public and transformed the U.S. art scene. Georgia O'Keeffe, Marsden Hartley, and others experimented with new, individualistic styles. Major artistic movements such as the abstract expressionism of Jackson Pollock and Willem de Kooning and the pop art of Andy Warhol and Roy Lichtenstein developed largely in the United States. The tide of modernism and then postmodernism has brought fame to American architects such as Frank Lloyd Wright, Philip Johnson, and Frank Gehry. Americans have long been important in the modern artistic medium of photography, with major photographers including Alfred Stieglitz, Edward Steichen, Edward Weston, and Ansel Adams.
Mainstream American cuisine is similar to that in other Western countries. Wheat is the primary cereal grain with about three-quarters of grain products made of wheat flour and many dishes use indigenous ingredients, such as turkey, venison, potatoes, sweet potatoes, corn, squash, and maple syrup which were consumed by Native Americans and early European settlers. These homegrown foods are part of a shared national menu on one of America's most popular holidays, Thanksgiving, when some Americans make traditional foods to celebrate the occasion.The American fast food industry, the world's largest, pioneered the drive-through format in the 1940s. Characteristic dishes such as apple pie, fried chicken, pizza, hamburgers, and hot dogs derive from the recipes of various immigrants. French fries, Mexican dishes such as burritos and tacos, and pasta dishes freely adapted from Italian sources are widely consumed. Americans drink three times as much coffee as tea. Marketing by U.S. industries is largely responsible for making orange juice and milk ubiquitous breakfast beverages.
Although little known at the time, Charles Ives's work of the 1910s established him as the first major U.S. composer in the classical tradition, while experimentalists such as Henry Cowell and John Cage created a distinctive American approach to classical composition. Aaron Copland and George Gershwin developed a new synthesis of popular and classical music. The rhythmic and lyrical styles of African-American music have deeply influenced American music at large, distinguishing it from European and African traditions. Elements from folk idioms such as the blues and what is now known as old-time music were adopted and transformed into popular genres with global audiences. Jazz was developed by innovators such as Louis Armstrong and Duke Ellington early in the 20th century. Country music developed in the 1920s, and rhythm and blues in the 1940s.Elvis Presley and Chuck Berry were among the mid-1950s pioneers of rock and roll. Rock bands such as Metallica, the Eagles, and Aerosmith are among the highest grossing in worldwide sales. In the 1960s, Bob Dylan emerged from the folk revival to become one of America's most celebrated songwriters and James Brown led the development of funk. More recent American creations include hip hop and house music. American pop stars such as Presley, Michael Jackson and Madonna have become global celebrities, as have contemporary musical artists such as Katy Perry, Taylor Swift, Lady Gaga, Britney Spears, Mariah Carey, Beyoncé, Jay-Z, Eminem, and Kanye West.
Hollywood, a northern district of Los Angeles, California, is one of the leaders in motion picture production. The world's first commercial motion picture exhibition was given in New York City in 1894, using Thomas Edison's Kinetoscope. Since the early 20th century, the U.S. film industry has largely been based in and around Hollywood, although in the 21st century an increasing number of films are not made there, and film companies have been subject to the forces of globalization.Director D. W. Griffith, the top American filmmaker during the silent film period, was central to the development of film grammar, and producer/entrepreneur Walt Disney was a leader in both animated film and movie merchandising. Directors such as John Ford redefined the image of the American Old West, and, like others such as John Huston, broadened the possibilities of cinema with location shooting. The industry enjoyed its golden years, in what is commonly referred to as the "Golden Age of Hollywood", from the early sound period until the early 1960s, with screen actors such as John Wayne and Marilyn Monroe becoming iconic figures. In the 1970s, "New Hollywood" or the "Hollywood Renaissance" was defined by grittier films influenced by French and Italian realist pictures of the post-war period. In more recent times, directors such as Steven Spielberg, George Lucas and James Cameron have gained renown for their blockbuster films, often characterized by high production costs and earnings. Notable films topping the American Film Institute's AFI 100 list include Orson Welles's Citizen Kane (1941), which is frequently cited as the greatest film of all time, Casablanca (1942), The Godfather (1972), Gone with the Wind (1939), Lawrence of Arabia (1962), The Wizard of Oz (1939), The Graduate (1967), On the Waterfront (1954), Schindler's List (1993), Singin' in the Rain (1952), It's a Wonderful Life (1946) and Sunset Boulevard (1950). The Academy Awards, popularly known as the Oscars, have been held annually by the Academy of Motion Picture Arts and Sciences since 1929, and the Golden Globe Awards have been held annually since January 1944.
American football is by several measures the most popular spectator sport; the National Football League (NFL) has the highest average attendance of any sports league in the world, and the Super Bowl is watched by tens of millions globally. Baseball has been regarded as the U.S. national sport since the late 19th century, with Major League Baseball (MLB) being the top league. Basketball and ice hockey are the country's next two leading professional team sports, with the top leagues being the National Basketball Association (NBA) and the National Hockey League (NHL). College football and basketball attract large audiences. In soccer (a sport that has gained a footing in the United States since the mid-1990's), the country hosted the 1994 FIFA World Cup, the men's national soccer team qualified for ten World Cups and the women's team has won the FIFA Women's World Cup four times; Major League Soccer is the sport's highest league in the United States (featuring 23 American and three Canadian teams). The market for professional sports in the United States is roughly $69 billion, roughly 50% larger than that of all of Europe, the Middle East, and Africa combined.Eight Olympic Games have taken place in the United States. The 1904 Summer Olympics in St. Louis, Missouri, were the first ever Olympic Games held outside of Europe. As of 2017, the United States has won 2,522 medals at the Summer Olympic Games, more than any other country, and 305 in the Winter Olympic Games, the second most behind Norway. While most major U.S. sports such as baseball and American football have evolved out of European practices, basketball, volleyball, skateboarding, and snowboarding are American inventions, some of which have become popular worldwide. Lacrosse and surfing arose from Native American and Native Hawaiian activities that predate Western contact. The most watched individual sports are golf and auto racing, particularly NASCAR.
The four major broadcasters in the U.S. are the National Broadcasting Company (NBC), Columbia Broadcasting System (CBS), American Broadcasting Company (ABC), and Fox Broadcasting Company (FOX). The four major broadcast television networks are all commercial entities. Cable television offers hundreds of channels catering to a variety of niches. Americans listen to radio programming, also largely commercial, on average just over two-and-a-half hours a day.In 1998, the number of U.S. commercial radio stations had grown to 4,793 AM stations and 5,662 FM stations. In addition, there are 1,460 public radio stations. Most of these stations are run by universities and public authorities for educational purposes and are financed by public or private funds, subscriptions, and corporate underwriting. Much public-radio broadcasting is supplied by NPR. NPR was incorporated in February 1970 under the Public Broadcasting Act of 1967; its television counterpart, PBS, was created by the same legislation. As of September 30, 2014, there are 15,433 licensed full-power radio stations in the U.S. according to the U.S. Federal Communications Commission (FCC).Well-known newspapers include The Wall Street Journal, The New York Times, and USA Today. Although the cost of publishing has increased over the years, the price of newspapers has generally remained low, forcing newspapers to rely more on advertising revenue and on articles provided by a major wire service, such as the Associated Press or Reuters, for their national and world coverage. With very few exceptions, all the newspapers in the U.S. are privately owned, either by large chains such as Gannett or McClatchy, which own dozens or even hundreds of newspapers; by small chains that own a handful of papers; or in a situation that is increasingly rare, by individuals or families. Major cities often have "alternative weeklies" to complement the mainstream daily papers, such as New York City's The Village Voice or Los Angeles' LA Weekly. Major cities may also support a local business journal, trade papers relating to local industries, and papers for local ethnic and social groups. Aside from web portals and search engines, the most popular websites are Facebook, YouTube, Wikipedia, Yahoo!, eBay, Amazon, and Twitter.More than 800 publications are produced in Spanish, the second most commonly used language in the United States behind English.
Index of United States-related articles Lists of U.S. state topics Outline of the United States
Internet sources
"United States". The World Factbook. Central Intelligence Agency. United States, from the BBC News Key Development Forecasts for the United States from International FuturesGovernmentOfficial U.S. Government Web Portal Gateway to government sites House Official site of the United States House of Representatives Senate Official site of the United States Senate White House Official site of the president of the United States Supreme Court Official site of the Supreme Court of the United StatesHistoryHistorical Documents Collected by the National Center for Public Policy Research U.S. National Mottos: History and Constitutionality Analysis by the Ontario Consultants on Religious Tolerance USA Collected links to historical dataMapsNational Atlas of the United States Official maps from the U.S. Department of the Interior Wikimedia Atlas of the United States Geographic data related to United States at OpenStreetMap Measure of America A variety of mapped information relating to health, education, income, and demographics for the U.S.PhotosPhotos of the USA
North America is a continent entirely within the Northern Hemisphere and almost all within the Western Hemisphere. It can also be described as a northern subcontinent of the Americas. It is bordered to the north by the Arctic Ocean, to the east by the Atlantic Ocean, to the southeast by South America and the Caribbean Sea, and to the west and south by the Pacific Ocean. North America covers an area of about 24,709,000 square kilometers (9,540,000 square miles), about 16.5% of the Earth's land area and about 4.8% of its total surface. North America is the third-largest continent by area, following Asia and Africa, and the fourth by population after Asia, Africa, and Europe. In 2013, its population was estimated at nearly 579 million people in 23 independent states, or about 7.5% of the world's population, if nearby islands (most notably around the Caribbean) are included. North America was reached by its first human populations during the last glacial period, via crossing the Bering land bridge approximately 40,000 to 17,000 years ago. The so-called Paleo-Indian period is taken to have lasted until about 10,000 years ago (the beginning of the Archaic or Meso-Indian period). The classic stage spans roughly the 6th to 13th centuries. The pre-Columbian era ended in 1492, with the beginning of the transatlantic migrations of European settlers during the Age of Discovery and the early modern period. Present-day cultural and ethnic patterns reflect interactions between European colonists, indigenous peoples, African slaves, immigrants, and the descendants of these groups. Owing to Europe's colonization of the Americas, most North Americans speak European languages such as English, Spanish or French, and their states' cultures commonly reflect Western traditions.
The Americas are usually accepted as having been named after the Italian explorer Amerigo Vespucci by the German cartographers Martin Waldseemüller and Matthias Ringmann. Vespucci, who explored South America between 1497 and 1502, was the first European to suggest that the Americas were not the East Indies, but a different landmass previously unknown by Europeans. In 1507, Waldseemüller produced a world map, in which he placed the word "America" on the continent of South America, in the middle of what is today Brazil. He explained the rationale for the name in the accompanying book Cosmographiae Introductio: ... ab Americo inventore ... quasi Americi terram sive Americam (from Americus the discoverer ... as if it were the land of Americus, thus America). For Waldseemüller, no one should object to the naming of the land after its discoverer. He used the Latinized version of Vespucci's name (Americus Vespucius), but in its feminine form "America", following the examples of "Europa", "Asia" and "Africa". Later, other mapmakers extended the name America to the northern continent. In 1538, Gerard Mercator used America on his map of the world for all the Western Hemisphere.Some argue that because the convention is to use the surname for naming discoveries (except in the case of royalty), the derivation from "Amerigo Vespucci" could be put in question. In 1874, Thomas Belt proposed a derivation from the Amerrique mountains of Central America; the next year, Jules Marcou suggested that the name of the mountain range stemmed from indigenous American languages. Marcou corresponded with Augustus Le Plongeon, who wrote: "The name AMERICA or AMERRIQUE in the Mayan language means, a country of perpetually strong wind, or the Land of the Wind, and ... the [suffixes] can mean ... a spirit that breathes, life itself."Mercator on his map called North America "America or New India" (America sive India Nova).
The United Nations formally recognizes "North America" as comprising three areas: Northern America, Central America, and The Caribbean. This has been formally defined by the UN Statistics Division."Northern America", as a term distinct from "North America", excludes Central America, which itself may or may not include Mexico (see Central America § Different definitions). In the limited context of the North American Free Trade Agreement, the term covers Canada, the United States, and Mexico, which are the three signatories of that treaty. France, Italy, Portugal, Spain, Romania, Greece, and the countries of Latin America use a six-continent model, with the Americas viewed as a single continent and North America designating a subcontinent comprising Canada, the United States, and Mexico, and often Greenland, Saint Pierre et Miquelon, and Bermuda.North America has been historically referred to by other names. Spanish North America (New Spain) was often referred to as Northern America, and this was the first official name given to Mexico.
Geographically the North American continent has many regions and subregions. These include cultural, economic, and geographic regions. Economic regions included those formed by trade blocs, such as the North American Trade Agreement bloc and Central American Trade Agreement. Linguistically and culturally, the continent could be divided into Anglo-America and Latin America. Anglo-America includes most of Northern America, Belize, and Caribbean islands with English-speaking populations (though sub-national entities, such as Louisiana and Quebec, have large Francophone populations; in Quebec, French is the sole official language). The southern North American continent is composed of two regions. These are Central America and the Caribbean. The north of the continent maintains recognized regions as well. In contrast to the common definition of "North America", which encompasses the whole continent, the term "North America" is sometimes used to refer only to Mexico, Canada, the United States, and Greenland.The term Northern America refers to the northernmost countries and territories of North America: the United States, Bermuda, St. Pierre and Miquelon, Canada and Greenland. Although the term does not refer to a unified region, Middle America—not to be confused with the Midwestern United States—groups the regions of Mexico, Central America, and the Caribbean.The largest countries of the continent, Canada and the United States, also contain well-defined and recognized regions. In the case of Canada these are (from east to west) Atlantic Canada, Central Canada, Canadian Prairies, the British Columbia Coast, and Northern Canada. These regions also contain many subregions. In the case of the United States – and in accordance with the US Census Bureau definitions – these regions are: New England, Mid-Atlantic, South Atlantic States, East North Central States, West North Central States, East South Central States, West South Central States, Mountain States, and Pacific States. Regions shared between both nations included the Great Lakes Region. Megalopolises have formed between both nations in the case of the Pacific Northwest and the Great Lakes Megaregion.
North America occupies the northern portion of the landmass generally referred to as the New World, the Western Hemisphere, the Americas, or simply America (which, in many countries is considered as a single continent with North America a subcontinent). North America is the third-largest continent by area, following Asia and Africa. North America's only land connection to South America is at the Isthmus of Darian/Isthmus of Panama. The continent is delimited on the southeast by most geographers at the Darién watershed along the Colombia-Panama border, placing almost all of Panama within North America. Alternatively, some geologists physiographically locate its southern limit at the Isthmus of Tehuantepec, Mexico, with Central America extending southeastward to South America from this point. The Caribbean islands, or West Indies, are considered part of North America. The continental coastline is long and irregular. The Gulf of Mexico is the largest body of water indenting the continent, followed by Hudson Bay. Others include the Gulf of Saint Lawrence and the Gulf of California. Before the Central American isthmus formed, the region had been underwater. The islands of the West Indies delineate a submerged former land bridge, which had connected North and South America via what are now Florida and Venezuela. There are numerous islands off the continent's coasts; principally, the Arctic Archipelago, the Bahamas, Turks & Caicos, the Greater and Lesser Antilles, the Aleutian Islands (some of which are in the Eastern Hemisphere proper), the Alexander Archipelago, the many thousand islands of the British Columbia Coast, and Newfoundland. Greenland, a self-governing Danish island, and the world's largest, is on the same tectonic plate (the North American Plate) and is part of North America geographically. In a geologic sense, Bermuda is not part of the Americas, but an oceanic island which was formed on the fissure of the Mid-Atlantic Ridge over 100 million years ago. The nearest landmass to it is Cape Hatteras, North Carolina. However, Bermuda is often thought of as part of North America, especially given its historical, political and cultural ties to Virginia and other parts of the continent. The vast majority of North America is on the North American Plate. Parts of western Mexico, including Baja California, and of California, including the cities of San Diego, Los Angeles, and Santa Cruz, lie on the eastern edge of the Pacific Plate, with the two plates meeting along the San Andreas fault. The southernmost portion of the continent and much of the West Indies lie on the Caribbean Plate, whereas the Juan de Fuca and Cocos plates border the North American Plate on its western frontier. The continent can be divided into four great regions (each of which contains many subregions): the Great Plains stretching from the Gulf of Mexico to the Canadian Arctic; the geologically young, mountainous west, including the Rocky Mountains, the Great Basin, California and Alaska; the raised but relatively flat plateau of the Canadian Shield in the northeast; and the varied eastern region, which includes the Appalachian Mountains, the coastal plain along the Atlantic seaboard, and the Florida peninsula. Mexico, with its long plateaus and cordilleras, falls largely in the western region, although the eastern coastal plain does extend south along the Gulf. The western mountains are split in the middle into the main range of the Rockies and the coast ranges in California, Oregon, Washington, and British Columbia, with the Great Basin—a lower area containing smaller ranges and low-lying deserts—in between. The highest peak is Denali in Alaska. The United States Geographical Survey (USGS) states that the geographic center of North America is "6 miles [10 km] west of Balta, Pierce County, North Dakota" at about 48°10′N 100°10′W, about 24 kilometres (15 mi) from Rugby, North Dakota. The USGS further states that "No marked or monumented point has been established by any government agency as the geographic center of either the 50 States, the conterminous United States, or the North American continent." Nonetheless, there is a 4.6-metre (15 ft) field stone obelisk in Rugby claiming to mark the center. The North American continental pole of inaccessibility is located 1,650 km (1,030 mi) from the nearest coastline, between Allen and Kyle, South Dakota at 43.36°N 101.97°W﻿ / 43.36; -101.97﻿ (Pole of Inaccessibility North America).
Laurentia is an ancient craton which forms the geologic core of North America; it formed between 1.5 and 1.0 billion years ago during the Proterozoic eon. The Canadian Shield is the largest exposure of this craton. From the Late Paleozoic to Early Mesozoic eras, North America was joined with the other modern-day continents as part of the supercontinent Pangaea, with Eurasia to its east. One of the results of the formation of Pangaea was the Appalachian Mountains, which formed some 480 million years ago, making it among the oldest mountain ranges in the world. When Pangaea began to rift around 200 million years ago, North America became part of Laurasia, before it separated from Eurasia as its own continent during the mid-Cretaceous period. The Rockies and other western mountain ranges began forming around this time from a period of mountain building called the Laramide orogeny, between 80 and 55 million years ago. The formation of the Isthmus of Panama that connected the continent to South America arguably occurred approximately 12 to 15 million years ago, and the Great Lakes (as well as many other northern freshwater lakes and rivers) were carved by receding glaciers about 10,000 years ago. North America is the source of much of what humanity knows about geologic time periods. The geographic area that would later become the United States has been the source of more varieties of dinosaurs than any other modern country. According to paleontologist Peter Dodson, this is primarily due to stratigraphy, climate and geography, human resources, and history. Much of the Mesozoic Era is represented by exposed outcrops in the many arid regions of the continent. The most significant Late Jurassic dinosaur-bearing fossil deposit in North America is the Morrison Formation of the western United States.
Geologically, Canada is one of the oldest regions in the world, with more than half of the region consisting of precambrian rocks that have been above sea level since the beginning of the Palaeozoic era. Canada's mineral resources are diverse and extensive. Across the Canadian Shield and in the north there are large iron, nickel, zinc, copper, gold, lead, molybdenum, and uranium reserves. Large diamond concentrations have been recently developed in the Arctic, making Canada one of the world's largest producers. Throughout the Shield there are many mining towns extracting these minerals. The largest, and best known, is Sudbury, Ontario. Sudbury is an exception to the normal process of forming minerals in the Shield since there is significant evidence that the Sudbury Basin is an ancient meteorite impact crater. The nearby, but less known Temagami Magnetic Anomaly has striking similarities to the Sudbury Basin. Its magnetic anomalies are very similar to the Sudbury Basin, and so it could be a second metal-rich impact crater. The Shield is also covered by vast boreal forests that support an important logging industry.
The lower 48 US states can be divided into roughly five physiographic provinces: The American cordillera The Canadian Shield Northern portion of the upper midwestern United States. The stable platform The coastal plain The Appalachian orogenic beltThe geology of Alaska is typical of that of the cordillera, while the major islands of Hawaii consist of Neogene volcanics erupted over a hot spot.
Central America is geologically active with volcanic eruptions and earthquakes occurring from time to time. In 1976 Guatemala was hit by a major earthquake, killing 23,000 people; Managua, the capital of Nicaragua, was devastated by earthquakes in 1931 and 1972, the last one killing about 5,000 people; three earthquakes devastated El Salvador, one in 1986 and two in 2001; one earthquake devastated northern and central Costa Rica in 2009, killing at least 34 people; in Honduras a powerful earthquake killed seven people in 2009. Volcanic eruptions are common in the region. In 1968 the Arenal Volcano, in Costa Rica, erupted and killed 87 people. Fertile soils from weathered volcanic lavas have made it possible to sustain dense populations in the agriculturally productive highland areas. Central America has many mountain ranges; the longest are the Sierra Madre de Chiapas, the Cordillera Isabelia, and the Cordillera de Talamanca. Between the mountain ranges lie fertile valleys that are suitable for the people; in fact, most of the population of Honduras, Costa Rica, and Guatemala live in valleys. Valleys are also suitable for the production of coffee, beans, and other crops.
North America is a very large continent which surpasses the Arctic Circle, and the Tropic of Cancer. Greenland, along with the Canadian Shield, is tundra with average temperatures ranging from 10 to 20 °C (50 to 68 °F), but central Greenland is composed of a very large ice sheet. This tundra radiates throughout Canada, but its border ends near the Rocky Mountains (but still contains Alaska) and at the end of the Canadian Shield, near the Great Lakes. Climate west of the Cascades is described as being a temperate weather with average precipitation 20 inches (510 mm). Climate in coastal California is described to be Mediterranean, with average temperatures in cities like San Francisco ranging from 57 to 70 °F (14 to 21 °C) over the course of the year.Stretching from the East Coast to eastern North Dakota, and stretching down to Kansas, is the continental-humid climate featuring intense seasons, with a large amount of annual precipitation, with places like New York City averaging 50 inches (1,300 mm). Starting at the southern border of the continental-humid climate and stretching to the Gulf of Mexico (whilst encompassing the eastern half of Texas) is the subtropical climate. This area has the wettest cities in the contiguous U.S. with annual precipitation reaching 67 inches (1,700 mm) in Mobile, Alabama. Stretching from the borders of the continental humid and subtropical climates, and going west to the Cascades Sierra Nevada, south to the southern tip of durango, north to the border with tundra climate, the steppe/desert climate is the driest climate in the U.S. Highland climates cut from north to south of the continent, where subtropical or temperate climates occur just below the tropics, as in central Mexico and Guatemala. Tropical climates appear in the island regions and in the subcontinent's bottleneck. Usually of the savannah type, with rains and high temperatures constants the whole year. Found in countries and states bathed by the Caribbean Sea or to south of the Gulf of Mexico and Pacific Ocean.
Notable North American fauna include the bison, black bear, prairie dog, turkey, pronghorn, raccoon, coyote and monarch butterfly. Notable plants that were domesticated in North America include tobacco, maize, squash, tomato, sunflower, blueberry, avocado, cotton, chile pepper and vanilla.
The indigenous peoples of the Americas have many creation myths by which they assert that they have been present on the land since its creation, but there is no evidence that humans evolved there. The specifics of the initial settlement of the Americas by ancient Asians are subject to ongoing research and discussion. The traditional theory has been that hunters entered the Beringia land bridge between eastern Siberia and present-day Alaska from 27,000 to 14,000 years ago. A growing viewpoint is that the first American inhabitants sailed from Beringia some 13,000 years ago, with widespread habitation of the Americas during the end of the Last Glacial Period, in what is known as the Late Glacial Maximum, around 12,500 years ago. The oldest petroglyphs in North America date from 15,000 to 10,000 years before present. Genetic research and anthropology indicate additional waves of migration from Asia via the Bering Strait during the Early-Middle Holocene.Before contact with Europeans, the natives of North America were divided into many different polities, from small bands of a few families to large empires. They lived in several "culture areas", which roughly correspond to geographic and biological zones and give a good indication of the main way of life of the people who lived there (e.g., the bison hunters of the Great Plains, or the farmers of Mesoamerica). Native groups can also be classified by their language family (e.g., Athapascan or Uto-Aztecan). Peoples with similar languages did not always share the same material culture, nor were they always allies. Anthropologists think that the Inuit people of the high Arctic came to North America much later than other native groups, as evidenced by the disappearance of Dorset culture artifacts from the archaeological record, and their replacement by the Thule people. During the thousands of years of native habitation on the continent, cultures changed and shifted. One of the oldest yet discovered is the Clovis culture (c. 9550–9050 BCE) in modern New Mexico. Later groups include the Mississippian culture and related Mound building cultures, found in the Mississippi river valley and the Pueblo culture of what is now the Four Corners. The more southern cultural groups of North America were responsible for the domestication of many common crops now used around the world, such as tomatoes, squash, and maize. As a result of the development of agriculture in the south, many other cultural advances were made there. The Mayans developed a writing system, built huge pyramids and temples, had a complex calendar, and developed the concept of zero around 400 CE.The first recorded European references to North America are in Norse sagas where it is referred to as Vinland. The earliest verifiable instance of pre-Columbian trans-oceanic contact by any European culture with the North America mainland has been dated to around 1000 CE. The site, situated at the northernmost extent of the island named Newfoundland, has provided unmistakable evidence of Norse settlement. Norse explorer Leif Erikson (c. 970–1020 CE) is thought to have visited the area. Erikson was the first European to make landfall on the continent (excluding Greenland). The Mayan culture was still present in southern Mexico and Guatemala when the Spanish conquistadors arrived, but political dominance in the area had shifted to the Aztec Empire, whose capital city Tenochtitlan was located further north in the Valley of Mexico. The Aztecs were conquered in 1521 by Hernán Cortés.
During the Age of Discovery, Europeans explored and staked claims to various parts of North America. Upon their arrival in the "New World", the Native American population declined substantially, because of violent conflicts with the invaders and the introduction of European diseases to which the Native Americans lacked immunity. Native culture changed drastically and their affiliation with political and cultural groups also changed. Several linguistic groups died out, and others changed quite quickly. In 1513, Juan Ponce de León, who had accompanied Columbus's second voyage, visited and named La Florida. As the colonial period unfolded, Britain, Spain, and France took over extensive territories in North America. In the late 18th and early 19th century, independence movements sprung up across the continent, leading to the founding of the modern countries in the area. The 13 British Colonies on the North Atlantic coast declared independence in 1776, becoming the United States of America. Canada was formed from the unification of northern territories controlled by Britain and France. New Spain, a territory that stretched from the modern-day southern US to Central America, declared independence in 1810, becoming the First Mexican Empire. In 1823 the former Captaincy General of Guatemala, then part of the Mexican Empire, became the first independent state in Central America, officially changing its name to the United Provinces of Central America. Over three decades of work on the Panama Canal led to the connection of Atlantic and Pacific waters in 1913, physically making North America a separate continent.
Economically, Canada and the United States are the wealthiest and most developed nations in the continent, followed by Mexico, a newly industrialized country. The countries of Central America and the Caribbean are at various levels of economic and human development. For example, small Caribbean island-nations, such as Barbados, Trinidad and Tobago, and Antigua and Barbuda, have a higher GDP (PPP) per capita than Mexico due to their smaller populations. Panama and Costa Rica have a significantly higher Human Development Index and GDP than the rest of the Central American nations. Additionally, despite Greenland's vast resources in oil and minerals, much of them remain untapped, and the island is economically dependent on fishing, tourism, and subsidies from Denmark. Nevertheless, the island is highly developed.Demographically, North America is ethnically diverse. Its three main groups are Caucasians, Mestizos and Blacks. There is a significant minority of Indigenous Americans and Asians among other less numerous groups.
The dominant languages in North America are English, Spanish, and French. Danish is prevalent in Greenland alongside Greenlandic, and Dutch is spoken side by side local languages in the Dutch Caribbean. The term Anglo-America is used to refer to the anglophone countries of the Americas: namely Canada (where English and French are co-official) and the United States, but also sometimes Belize and parts of the tropics, especially the Commonwealth Caribbean. Latin America refers to the other areas of the Americas (generally south of the United States) where the Romance languages, derived from Latin, of Spanish and Portuguese (but French speaking countries are not usually included) predominate: the other republics of Central America (but not always Belize), part of the Caribbean (not the Dutch-, English-, or French-speaking areas), Mexico, and most of South America (except Guyana, Suriname, French Guiana (France), and the Falkland Islands (UK)). The French language has historically played a significant role in North America and now retains a distinctive presence in some regions. Canada is officially bilingual. French is the official language of the Province of Quebec, where 95% of the people speak it as either their first or second language, and it is co-official with English in the Province of New Brunswick. Other French-speaking locales include the Province of Ontario (the official language is English, but there are an estimated 600,000 Franco-Ontarians), the Province of Manitoba (co-official as de jure with English), the French West Indies and Saint-Pierre et Miquelon, as well as the US state of Louisiana, where French is also an official language. Haiti is included with this group based on historical association but Haitians speak both Creole and French. Similarly, French and French Antillean Creole is spoken in Saint Lucia and the Commonwealth of Dominica alongside English. A significant number of Indigenous languages are spoken in North America, with 372,000 people in the United States speaking an indigenous language at home, about 225,000 in Canada and roughly 6 million in Mexico. In the United States and Canada, there are approximately 150 surviving indigenous languages of the 300 spoken prior to European contact.
Christianity is the largest religion in the United States, Canada and Mexico. According to a 2012 Pew Research Center survey, 77% of the population considered themselves Christians. Christianity also is the predominant religion in the 23 dependent territories in North America. The United States has the largest Christian population in the world, with nearly 247 million Christians (70%), although other countries have higher percentages of Christians among their populations. Mexico has the world's second largest number of Catholics, surpassed only by Brazil. A 2015 study estimates about 493,000 Christian believers from a Muslim background in North America, most of them belonging to some form of Protestantism.According to the same study religiously unaffiliated (include agnostic and atheist) make up about 17% of the population of Canada and the United States. No religion make up about 24% of the United States population, and 24% of Canada total population.Canada, the United States and Mexico host communities of both Jews (6 million or about 1.8%), Buddhists (3.8 million or 1.1%) and Muslims (3.4 million or 1.0%). The biggest number of Jewish individuals can be found in the United States (5.4 million), Canada (375,000) and Mexico (67,476). The United States host the largest Muslim population in North America with 2.7 million or 0.9%, While Canada host about one million Muslim or 3.2% of the population. While in Mexico there were 3,700 Muslims in the country. In 2012, U-T San Diego estimated U.S. practitioners of Buddhism at 1.2 million people, of whom 40% are living in Southern California.The predominant religion in Central America is Christianity (96%). Beginning with the Spanish colonization of Central America in the 16th century, Roman Catholicism became the most popular religion in the region until the first half of the 20th century. Since the 1960s, there has been an increase in other Christian groups, particularly Protestantism, as well as other religious organizations, and individuals identifying themselves as having no religion. Also Christianity is the predominant religion in the Caribbean (85%). Other religious groups in the region are Hinduism, Islam, Rastafari (in Jamaica), and Afro-American religions such as Santería and Vodou.
North America is the fourth most populous continent after Asia, Africa, and Europe. Its most populous country is the United States with 329.7 million persons. The second largest country is Mexico with a population of 112.3 million. Canada is the third most populous country with 37.0 million. The majority of Caribbean island-nations have national populations under a million, though Cuba, Dominican Republic, Haiti, Puerto Rico (a territory of the United States), Jamaica, and Trinidad and Tobago each have populations higher than a million. Greenland has a small population of 55,984 for its massive size (2,166,000 km2 or 836,300 mi2), and therefore, it has the world's lowest population density at 0.026 pop./km2 (0.067 pop./mi2).While the United States, Canada, and Mexico maintain the largest populations, large city populations are not restricted to those nations. There are also large cities in the Caribbean. The largest cities in North America, by far, are Mexico City and New York. These cities are the only cities on the continent to exceed eight million, and two of three in the Americas. Next in size are Los Angeles, Toronto, Chicago, Havana, Santo Domingo, and Montreal. Cities in the sun belt regions of the United States, such as those in Southern California and Houston, Phoenix, Miami, Atlanta, and Las Vegas, are experiencing rapid growth. These causes included warm temperatures, retirement of Baby Boomers, large industry, and the influx of immigrants. Cities near the United States border, particularly in Mexico, are also experiencing large amounts of growth. Most notable is Tijuana, a city bordering San Diego that receives immigrants from all over Latin America and parts of Europe and Asia. Yet as cities grow in these warmer regions of North America, they are increasingly forced to deal with the major issue of water shortages.Eight of the top ten metropolitan areas are located in the United States. These metropolitan areas all have a population of above 5.5 million and include the New York City metropolitan area, Los Angeles metropolitan area, Chicago metropolitan area, and the Dallas–Fort Worth metroplex. Whilst the majority of the largest metropolitan areas are within the United States, Mexico is host to the largest metropolitan area by population in North America: Greater Mexico City. Canada also breaks into the top ten largest metropolitan areas with the Toronto metropolitan area having six million people. The proximity of cities to each other on the Canada–United States border and Mexico–United States border has led to the rise of international metropolitan areas. These urban agglomerations are observed at their largest and most productive in Detroit–Windsor and San Diego–Tijuana and experience large commercial, economic, and cultural activity. The metropolitan areas are responsible for millions of dollars of trade dependent on international freight. In Detroit-Windsor the Border Transportation Partnership study in 2004 concluded US$13 billion was dependent on the Detroit–Windsor international border crossing while in San Diego-Tijuana freight at the Otay Mesa Port of Entry was valued at US$20 billion.North America has also been witness to the growth of megapolitan areas. In the United States exists eleven megaregions that transcend international borders and comprise Canadian and Mexican metropolitan regions. These are the Arizona Sun Corridor, Cascadia, Florida, Front Range, Great Lakes Megaregion, Gulf Coast Megaregion, Northeast, Northern California, Piedmont Atlantic, Southern California, and the Texas Triangle. Canada and Mexico are also the home of megaregions. These include the Quebec City – Windsor Corridor, Golden Horseshoe – both of which are considered part of the Great Lakes Megaregion – and megalopolis of Central Mexico. Traditionally the largest megaregion has been considered the Boston-Washington, DC Corridor, or the Northeast, as the region is one massive contiguous area. Yet megaregion criterion have allowed the Great Lakes Megalopolis to maintain status as the most populated region, being home to 53,768,125 people in 2000.The top ten largest North American metropolitan areas by population as of 2013, based on national census numbers from the United States and census estimates from Canada and Mexico. †2011 Census figures.
North America's GDP per capita was evaluated in October 2016 by the International Monetary Fund (IMF) to be $41,830, making it the richest continent in the world, followed by Oceania.Canada, Mexico, and the United States have significant and multifaceted economic systems. The United States has the largest economy of all three countries and in the world. In 2016, the U.S. had an estimated per capita gross domestic product (PPP) of $57,466 according to the World Bank, and is the most technologically developed economy of the three. The United States' services sector comprises 77% of the country's GDP (estimated in 2010), industry comprises 22% and agriculture comprises 1.2%. The U.S. economy is also the fastest growing economy in North America and the Americas as a whole, with the highest GDP per capita in the Americas as well. Canada shows significant growth in the sectors of services, mining and manufacturing. Canada's per capita GDP (PPP) was estimated at $44,656 and it had the 11th largest GDP (nominal) in 2014. Canada's services sector comprises 78% of the country's GDP (estimated in 2010), industry comprises 20% and agriculture comprises 2%. Mexico has a per capita GDP (PPP) of $16,111 and as of 2014 is the 15th largest GDP (nominal) in the world. Being a newly industrialized country, Mexico maintains both modern and outdated industrial and agricultural facilities and operations. Its main sources of income are oil, industrial exports, manufactured goods, electronics, heavy industry, automobiles, construction, food, banking and financial services.The North American economy is well defined and structured in three main economic areas. These areas are the North American Free Trade Agreement (NAFTA), Caribbean Community and Common Market (CARICOM), and the Central American Common Market (CACM). Of these trade blocs, the United States takes part in two. In addition to the larger trade blocs there is the Canada-Costa Rica Free Trade Agreement among numerous other free trade relations, often between the larger, more developed countries and Central American and Caribbean countries. The North America Free Trade Agreement (NAFTA) forms one of the four largest trade blocs in the world. Its implementation in 1994 was designed for economic homogenization with hopes of eliminating barriers of trade and foreign investment between Canada, the United States and Mexico. While Canada and the United States already conducted the largest bilateral trade relationship – and to present day still do – in the world and Canada–United States trade relations already allowed trade without national taxes and tariffs, NAFTA allowed Mexico to experience a similar duty-free trade. The free trade agreement allowed for the elimination of tariffs that had previously been in place on United States-Mexico trade. Trade volume has steadily increased annually and in 2010, surface trade between the three NAFTA nations reached an all-time historical increase of 24.3% or US$791 billion. The NAFTA trade bloc GDP (PPP) is the world's largest with US$17.617 trillion. This is in part attributed to the fact that the economy of the United States is the world's largest national economy; the country had a nominal GDP of approximately $14.7 trillion in 2010. The countries of NAFTA are also some of each other's largest trade partners. The United States is the largest trade partner of Canada and Mexico; while Canada and Mexico are each other's third largest trade partners. The Caribbean trade bloc – CARICOM – came into agreement in 1973 when it was signed by 15 Caribbean nations. As of 2000, CARICOM trade volume was US$96 billion. CARICOM also allowed for the creation of a common passport for associated nations. In the past decade the trade bloc focused largely on Free Trade Agreements and under the CARICOM Office of Trade Negotiations (OTN) free trade agreements have been signed into effect. Integration of Central American economies occurred under the signing of the Central American Common Market agreement in 1961; this was the first attempt to engage the nations of this area into stronger financial cooperation. Recent implementation of the Central American Free Trade Agreement (CAFTA) has left the future of the CACM unclear. The Central American Free Trade Agreement was signed by five Central American countries, the Dominican Republic, and the United States. The focal point of CAFTA is to create a free trade area similar to that of NAFTA. In addition to the United States, Canada also has relations in Central American trade blocs. Currently under proposal, the Canada – Central American Free Trade Agreement (CA4) would operate much the same as CAFTA with the United States does. These nations also take part in inter-continental trade blocs. Mexico takes a part in the G3 Free Trade Agreement with Colombia and Venezuela and has a trade agreement with the EU. The United States has proposed and maintained trade agreements under the Transatlantic Free Trade Area between itself and the European Union; the US-Middle East Free Trade Area between numerous Middle Eastern nations and itself; and the Trans-Pacific Strategic Economic Partnership between Southeast Asian nations, Australia, and New Zealand.
The Pan-American Highway route in the Americas is the portion of a network of roads nearly 48,000 km (30,000 mi) in length which travels through the mainland nations. No definitive length of the Pan-American Highway exists because the US and Canadian governments have never officially defined any specific routes as being part of the Pan-American Highway, and Mexico officially has many branches connecting to the US border. However, the total length of the portion from Mexico to the northern extremity of the highway is roughly 26,000 km (16,000 mi). The First Transcontinental Railroad in the United States was built in the 1860s, linking the railroad network of the eastern US with California on the Pacific coast. Finished on 10 May 1869 at the famous golden spike event at Promontory Summit, Utah, it created a nationwide mechanized transportation network that revolutionized the population and economy of the American West, catalyzing the transition from the wagon trains of previous decades to a modern transportation system. Although an accomplishment, it achieved the status of first transcontinental railroad by connecting myriad eastern US railroads to the Pacific and was not the largest single railroad system in the world. The Canadian Grand Trunk Railway (GTR) had, by 1867, already accumulated more than 2,055 km (1,277 mi) of track by connecting Ontario with the Canadian Atlantic provinces west as far as Port Huron, Michigan, through Sarnia, Ontario.
A shared telephone system known as the North American Numbering Plan (NANP) is an integrated telephone numbering plan of 24 countries and territories: the United States and its territories, Canada, Bermuda, and 17 Caribbean nations.
Canada and the United States were both former British colonies. There is frequent cultural interplay between the United States and English-speaking Canada. Greenland has experienced many immigration waves from Northern Canada, e.g. the Thule People. Therefore, Greenland shares some cultural ties with the indigenous people of Canada. Greenland is also considered Nordic and has strong Danish ties due to centuries of colonization by Denmark.Spanish-speaking North America shares a common past as former Spanish colonies. In Mexico and the Central American countries where civilizations like the Maya developed, indigenous people preserve traditions across modern boundaries. Central American and Spanish-speaking Caribbean nations have historically had more in common due to geographical proximity. Northern Mexico, particularly in the cities of Monterrey, Tijuana, Ciudad Juárez, and Mexicali, is strongly influenced by the culture and way of life of the United States. Of the aforementioned cities, Monterrey has been regarded as the most Americanized city in Mexico. Immigration to the United States and Canada remains a significant attribute of many nations close to the southern border of the US. The Anglophone Caribbean states have witnessed the decline of the British Empire and its influence on the region, and its replacement by the economic influence of Northern America in the Anglophone Caribbean. This is partly due to the relatively small populations of the English-speaking Caribbean countries, and also because many of them now have more people living abroad than those remaining at home. Northern Mexico, the Western United States and Alberta, Canada share a cowboy culture.
Canada, Mexico and the US submitted a joint bid to host the 2026 FIFA World Cup. The following table shows the most prominent sports leagues in North America, in order of average revenue.
Outline of North America Flags of North America List of cities in North America Table manners in North America Turtle Island (Native American folklore) – Name for North America among Native Americans
Footnotes Citations
Houghton Mifflin Company, "North America" Interactive SVG version of Non-Native American Nations Control over N America 1750–2008 animation
Latin America is a group of countries and dependencies in the Western Hemisphere where Romance languages such as Spanish, French or Portuguese are predominantly spoken. Some territories such as Quebec, where French is spoken, or areas of the United States where Spanish is predominantly spoken are not included due to the nation being a part of Anglo America. The term is broader than categories such as Hispanic America which specifically refers to Spanish-speaking countries or Ibero-America which specifically refers to both Spanish and Portuguese-speaking countries. The term is also more recent in origin. The term "Latin America" was first used in an 1856 conference with the title "Initiative of America. Idea for a Federal Congress of the Republics" (Iniciativa de la América. Idea de un Congreso Federal de las Repúblicas), by the Chilean politician Francisco Bilbao. The term was further popularised by French emperor Napoleon III's government in the 1860s as Amérique latine to justify France's military involvement in Mexico and try to include French-speaking territories in the Americas such as French Canada, French Louisiana, or French Guiana, in the larger group of countries where Spanish and Portuguese languages prevailed.. Including French-speaking territories, Latin America would consist of 20 countries and 14 dependent territories that cover an area that stretches from Mexico to Tierra del Fuego and includes much of the Caribbean. It has an area of approximately 19,197,000 km2 (7,412,000 sq mi), almost 13% of the Earth's land surface area. As of March 2, 2020, population of Latin America and the Caribbean was estimated at more than 652 million, and in 2019, Latin America had a combined nominal GDP of US$5,188,250 million and a GDP PPP of 10,284,588 million USD.
There is no universal agreement on the origin of the term Latin America. Some historians believe that the term was created by geographers in the 16th century to refer to the parts of the New World colonized by Spain and Portugal, whose Romance languages derive from Latin. Others argue that the term arose in 1860s France during the reign of Napoleon III, as part of the attempt to create a French empire in the Americas. The idea that a part of the Americas has a linguistic affinity with the Romance cultures as a whole can be traced back to the 1830s, in the writing of the French Saint-Simonian Michel Chevalier, who postulated that this part of the Americas was inhabited by people of a "Latin race", and that it could, therefore, ally itself with "Latin Europe", ultimately overlapping the Latin Church, in a struggle with "Teutonic Europe", "Anglo-Saxon America" and "Slavic Europe".Historian John Leddy Phelan located the origins of the term Latin America in the French occupation of Mexico. His argument is that French imperialists used the concept of "Latin" America as a way to counter British imperialism, as well as to challenge the German threat to France. The idea of a "Latin race" was then taken up by Latin American intellectuals and political leaders of the mid- and late-nineteenth century, who no longer looked to Spain or Portugal as cultural models, but rather to France. French ruler Napoleon III had a strong interest in extending French commercial and political power in the region he and his business promoter Felix Belly called "Latin America" to emphasize the shared Latin background of France with the former Viceroyalties of Spain and colonies of Portugal. This led to Napoleon's failed attempt to take military control of Mexico in the 1860s.However, though Phelan thesis is still frequently mentioned in the U.S. academy, two Latin American historians, the Uruguayan Arturo Ardao and the Chilean Miguel Rojas Mix proved decades ago that the term "Latin America" was used earlier than Phelan claimed, and the first use of the term was completely opposite to support imperialist projects in the Americas. Ardao wrote about this subject in his book Génesis de la idea y el nombre de América latina (Genesis of the Idea and the Name of Latin America, 1980), and Miguel Rojas Mix in his article "Bilbao y el hallazgo de América latina: Unión continental, socialista y libertaria" (Bilbao and the Finding of Latin America: a Continental, Socialist and Libertarian Union, 1986). As Michel Gobat reminds in his article "The Invention of Latin America: A Transnational History of Anti-Imperialism, Democracy, and Race", "Arturo Ardao, Miguel Rojas Mix, and Aims McGuinness have revealed [that] the term 'Latin America' had already been used in 1856 by Central Americans and South Americans protesting U.S. expansion into the Southern Hemisphere". Edward Shawcross summarizes Ardao's and Rojas Mix's findings in the following way: "Ardao identified the term in a poem by a Colombian diplomat and intellectual resident in France, José María Torres Caicedo, published on 15 February 1857 in a French based Spanish-language newspaper, while Rojas Mix located it in a speech delivered in France by the radical liberal Chilean politician Francisco Bilbao in June 1856".Now under the administration of the United States, by the late 1850s, the term was being used in local California newspapers such as El Clamor Público by Californios writing about América latina and latinoamérica, and identifying as latinos as the abbreviated term for their "hemispheric membership in la raza latina".So, regarding when the words "Latin" and "America" were combined for the first time in a printed work, the term "Latin America" was first used in 1856 in a conference by the Chilean politician Francisco Bilbao in Paris. The conference had the title "Initiative of the America. Idea for a Federal Congress of Republics." The following year the Colombian writer José María Torres Caicedo also used the term in his poem "The Two Americas". Two events related with the U.S. played a central role in both works. The first event happened less than a decade before the publication of Bilbao's and Torres Caicedo works: the Invasion of Mexico or, in USA the Mexican–American War, after which Mexico lost a third of its territory. The second event, the Walker affair, happened the same year both works were written: the decision by U.S. president Franklin Pierce to recognize the regime recently established in Nicaragua by American William Walker and his band of filibusters who ruled Nicaragua for nearly a year (1856–57) and attempted to reinstate slavery there, where it had been already abolished for three decades In both Bilbao's and Torres Caicedo's works, the Mexican-American War and Walker's expedition to Nicaragua are explicitly mentioned as examples of dangers for the region. For Bilbao, "Latin America" was not a geographical concept, since he excluded Brazil, Paraguay and Mexico. Both authors also ask for the union of all Latin American countries as the only way to defend their territories against further foreign U.S. interventions. Both rejected also European imperialism, claiming that the return of European countries to non-democratic forms of government was another danger for Latin American countries, and used the same word to describe the state of European politics at the time: "despotism." Several years later, during the French invasion of Mexico, Bilbao wrote another work, "Emancipation of the Spirit in America," where he asked all Latin American countries to support the Mexican cause against France, and rejected French imperialism in Asia, Africa, Europe and the Americas. He asked Latin American intellectuals to search for their "intellectual emancipation" by abandoning all French ideas, claiming that France was: "Hypocrite, because she [France] calls herself protector of the Latin race just to subject it to her exploitation regime; treacherous, because she speaks of freedom and nationality, when, unable to conquer freedom for herself, she enslaves others instead!" Therefore, as Michel Gobat puts it, the term Latin America itself had an "anti-imperial genesis," and their creators were far from supporting any form of imperialism in the region, or in any other place of the globe. However, in France the term Latin America was used with the opposite intention. It was employed by the French Empire of Napoleon III during the French invasion of Mexico as a way to include France among countries with influence in the Americas and to exclude Anglophone countries. It played a role in his campaign to imply cultural kinship of the region with France, transform France into a cultural and political leader of the area, and install Maximilian of Habsburg as emperor of the Second Mexican Empire. This term was also used in 1861 by French scholars in La revue des races Latines, a magazine dedicated to the Pan-Latinism movement.
Latin America is often used synonymously with Ibero-America ("Iberian America"), excluding the predominantly French- and English-speaking territories. Thus the countries of Haiti, Belize, Guyana and Suriname, and several French overseas departments, are excluded. Latin America generally refers to territories in the Americas where the Spanish, Portuguese or French languages prevail, including: Mexico, most of Central and South America, and in the Caribbean, Cuba, the Dominican Republic, Haiti, and Puerto Rico. Latin America is, therefore, defined as all those parts of the Americas that were once part of the Spanish, Portuguese and French Empires. The term is sometimes used more broadly to refer to all of the Americas south of the United States, thus including the Guianas (French Guiana, Guyana, and Suriname), the Anglophone Caribbean (and Belize); the Francophone Caribbean; and the Dutch Caribbean. This definition emphasizes a similar socioeconomic history of the region, which was characterized by formal or informal colonialism, rather than cultural aspects (see, for example, dependency theory). As such, some sources avoid this oversimplification by using the phrase "Latin America and the Caribbean" instead, as in the United Nations geoscheme for the Americas. In a more literal definition, which is close to the semantic origin, Latin America designates countries in the Americas where a Romance language (a language derived from Latin) predominates: Spanish, Portuguese, French, and the creole languages based upon these.The distinction between Latin America and Anglo-America is a convention based on the predominant languages in the Americas by which Romance-language and English-speaking cultures are distinguished. Neither area is culturally or linguistically homogeneous; in substantial portions of Latin America (e.g., highland Peru, Bolivia, Mexico, Guatemala), Native American cultures and, to a lesser extent, Amerindian languages, are predominant, and in other areas, the influence of African cultures is strong (e.g., the Caribbean basin – including parts of Colombia and Venezuela). The term is not without controversy. Historian Mauricio Tenorio-Trillo explores at length the "allure and power" of the idea of Latin America. He remarks at the outset, "The idea of 'Latin America' ought to have vanished with the obsolescence of racial theory... But it is not easy to declare something dead when it can hardly be said to have existed," going on to say, "The term is here to stay, and it is important." Following in the tradition of Chilean writer Francisco Bilbao, who excluded Brazil, Argentina and Paraguay from his early conceptualization of Latin America, Chilean historian Jaime Eyzaguirre has criticized the term Latin America for "disguising" and "diluting" the Spanish character of a region (i.e. Hispanic America) with the inclusion of nations that according to him do not share the same pattern of conquest and colonization.
Latin America can be subdivided into several subregions based on geography, politics, demographics and culture. If defined as all of the Americas south of the United States, the basic geographical subregions are North America, Central America, the Caribbean and South America; the latter contains further politico-geographical subdivisions such as the Southern Cone, the Guianas and the Andean states. It may be subdivided on linguistic grounds into Hispanic America, Portuguese America and French America. *: Not a sovereign state
The earliest known settlement was identified at Monte Verde, near Puerto Montt in Southern Chile. Its occupation dates to some 14,000 years ago and there is some disputed evidence of even earlier occupation. Over the course of millennia, people spread to all parts of the continents. By the first millennium CE, South America's vast rainforests, mountains, plains and coasts were the home of tens of millions of people. The earliest settlements in the Americas are of the Las Vegas Culture from about 8000 BCE and 4600 BCE, a sedentary group from the coast of Ecuador, the forefathers of the more known Valdivia culture, of the same era. Some groups formed more permanent settlements such as the Chibcha (or "Muisca" or "Muysca") and the Tairona groups. These groups are in the circum Caribbean region. The Chibchas of Colombia, the Quechuas and Aymaras of Bolivia were the three indigenous groups that settled most permanently. The region was home to many indigenous peoples and advanced civilizations, including the Aztecs, Toltecs, Maya, and Inca. The golden age of the Maya began about 250, with the last two great civilizations, the Aztecs and Incas, emerging into prominence later on in the early fourteenth century and mid-fifteenth centuries, respectively. The Aztec empire was ultimately the most powerful civilization known throughout the Americas, until its downfall in part by the Spanish invasion.
With the arrival of the Spaniards and Portuguese, the indigenous elites, such as the Incas and Aztecs, were deposed and/or co-opted.. Hernándo Cortés seized the Aztec elite's power in alliance with peoples who had been subjugated by this polity. Francisco Pizarro eliminated the Incan rule in Peru. Both Spain and Portugal colonized and settled the Americas, which along with the rest of the uncolonized world, was divided among them by the line of demarcation in 1494. This treaty wh gave Spain all areas to the west, and Portugal all areas to the east (the Portuguese lands in South America subsequently becoming Brazil). By the end of the sixteenth century Spain and Portugal controlled territory extending from Alaska to the southern tips of the Patagonia. Iberian culture, customs and government were introduced with the settlers who widely intermarried with local populations. The Catholic Religion was the only official religion in all territories under Spanish and Portuguese rule. Epidemics of diseases which came with the Spaniards, such as smallpox and measles, wiped out a large portion of the indigenous population. Historians cannot determine the number of natives who died due to European diseases, but some put the figures as high as 85% and as low as 25%. Due to the lack of written records, specific numbers are hard to verify. Many of the survivors were forced to work in European plantations and mines until indigenous slavery was outlawed with the New Laws of 1542. Unlike in English colonies, Intermixing between the indigenous peoples and Iberian colonists was very common and, by the end of the colonial period, people of mixed ancestry (mestizos) formed majorities in several colonies.
Indigenous peoples of the Americas in various colonies were forced to work in plantations and mines; along with African slaves who were also introduced in the proceeding centuries. The Mita of Colonial Latin America was a system of forced labor imposed on the natives. First established by Viceroy Francisco de Toledo (1569–1581), the Mita was upheld by laws that designated how large draft levies were and how much money the workers would receive that was based on how many shifts each individual worker performed. Toledo established Mitas at Potosi and Huancavelica, where the Mitayos—the workers—would be reduced in number to a fraction of how many were originally assigned before the 1700s. While several villages managed to resist the Mita, others offered payment to colonial administrators as a way out. In exchange, free labor became available through volunteers, though the Mita was kept in place as workers like miners, for example, were paid low wages. The Spanish Crown had not made any ruling on the Mita or approved of it when Toledo first established it in spite of the uncertainty of the practice since the Crown could have gained benefits from it. However, the cortes of Spain later abolished it in 1812 once complaints of the Mita violating humanitarian rights were made. Yet complaints also came from: governors; landowners; native leaders known as Kurakas; and even priests, each of whom preferred other methods of economic exploitation. Despite its fall, the Mita made it to the 1800s.Another important group of slaves to mention were the slaves brought over from Africa. The first slaves came over with Christopher Columbus from the very beginning on his earliest voyages. However in the few hundred years, the Atlantic Slave trade would begin delivering slaves, imported by Spain and other colonizers, by the millions. Many of the large scale productions were run by forced slave labor. They were a part of sugar and coffee production, farming (beans, rice, corn, fruit, etc.), Mining, whale oil and multiple other jobs. Slaves were also house workers, servants, military soldiers, and much more. To say the least these people were property and treated as such. Though indigenous slaves existed, they were no match in quantity and lack of quality jobs when compared to the African slave. The slave population was massive compared to the better known slave ownership in the United States. After 1860 Brazil alone had imported over 4 million slaves, which only represented about 35% of the Atlantic slave trade. Despite the large number of slaves in Latin America, there was not as much reproduction of slaves amongst the population. Because most of the slaves then were African-born, they were more subject to rebellion. The United States involvement in the slave trade is well known amongst North America, however it hides a larger and in some ways crueler operation in the south which had a much longer history.
In 1804, Haiti became the first Latin American nation to gain independence, following a violent slave revolt led by Toussaint L'ouverture on the French colony of Saint-Domingue. The victors abolished slavery. Haitian independence inspired independence movements in Spanish America. By the end of the eighteenth century, Spanish and Portuguese power waned on the global scene as other European powers took their place, notably Britain and France. Resentment grew among the majority of the population in Latin America over the restrictions imposed by the Spanish government, as well as the dominance of native Spaniards (Iberian-born Peninsulares) in the major social and political institutions. Napoleon's invasion of Spain in 1808 marked a turning point, compelling Criollo elites to form juntas that advocated independence. Also, the newly independent Haiti, the second oldest nation in the New World after the United States, further fueled the independence movement by inspiring the leaders of the movement, such as Miguel Hidalgo y Costilla of Mexico, Simón Bolívar of Venezuela and José de San Martín of Argentina, and by providing them with considerable munitions and troops. Fighting soon broke out between juntas and the Spanish colonial authorities, with initial victories for the advocates of independence. Eventually, these early movements were crushed by the royalist troops by 1810, including those of Miguel Hidalgo y Costilla in Mexico in the year 1810. Later on Francisco de Miranda in Venezuela by 1812. Under the leadership of a new generation of leaders, such as Simón Bolívar "The Liberator", José de San Martín of Argentina, and other Libertadores in South America, the independence movement regained strength, and by 1825, all Spanish America, except for Puerto Rico and Cuba, had gained independence from Spain. In the same year in Mexico, a military officer, Agustín de Iturbide, led a coalition of conservatives and liberals who created a constitutional monarchy, with Iturbide as emperor. This First Mexican Empire was short-lived, and was followed by the creation of a republic in 1823.
The Brazilian War of Independence, which had already begun along other independent movements around the region, spread through northern, northeastern regions and in Cisplatina province. With the last Portuguese soldiers surrendering on March 8, 1824, Portugal officially recognized Brazil on August 29, 1825. On April 7, 1831, worn down by years of administrative turmoil and political dissensions with both liberal and conservative sides of politics, including an attempt of republican secession, as well as unreconciled with the way that absolutists in Portugal had given to the succession of King John VI, Pedro I went to Portugal to reclaim his daughter's crown, abdicating the Brazilian throne in favor of his five-year-old son and heir (who thus became the Empire's second monarch, with the regnal title of Dom Pedro II).As the new Emperor could not exert his constitutional powers until he became of age, a regency was set up by the National Assembly. In the absence of a charismatic figure who could represent a moderate face of power, during this period a series of localized rebellions took place, as the Cabanagem, the Malê Revolt, the Balaiada, the Sabinada, and the Ragamuffin War, which emerged from the dissatisfaction of the provinces with the central power, coupled with old and latent social tensions peculiar of a vast, slaveholding and newly independent nation state. This period of internal political and social upheaval, which included the Praieira revolt, was overcome only at the end of the 1840s, years after the end of the regency, which occurred with the premature coronation of Pedro II in 1841.During the last phase of the monarchy, an internal political debate was centered on the issue of slavery. The Atlantic slave trade was abandoned in 1850, as a result of the British' Aberdeen Act, but only in May 1888 after a long process of internal mobilization and debate for an ethical and legal dismantling of slavery in the country, was the institution formally abolished.On November 15, 1889, worn out by years of economic stagnation, in attrition with the majority of Army officers, as well as with rural and financial elites (for different reasons), the monarchy was overthrown by a military coup.
After the independence of many Latin American countries, there was a conflict between the people and the government, much of which can be reduced to the contrasting ideologies between liberalism and conservatism. Conservatism was the dominant system of government prior to the revolutions and it was founded on having social classes, including governing by kings. Liberalists wanted to see a change in the ruling systems, and to move away from monarchs and social classes to promote equality. When liberal Guadalupe Victoria became the first president of Mexico in 1824, conservatists relied on their belief that the state had been better off before the new government came into power, so, by comparison, the old government was better in the eyes of the Conservatives. Following this sentiment, the conservatives pushed to take control of the government, and they succeeded. General Santa Anna was elected president in 1833. The following decade, the Mexican–American War (1846–48) caused Mexico to lose a significant amount of territory to the United States. This loss led to a rebellion by the enraged liberal forces against the conservative government. In 1837, conservative Rafael Carrera conquered Guatemala and separated from the Central American Union. The instability that followed the disintegration of the union led to the independence of the other Central American countries. In Brazil, rural aristocrats were in conflict with the urban conservatives. Portuguese control over Brazilian ports continued after Brazil's independence. Following the conservative idea that the old government was better, urbanites tended to support conservatism because more opportunities were available to them as a result of the Portuguese presence. Simón Bolívar became president of Gran Colombia in 1819 after the region gained independence from Spain. He led a military-controlled state. Citizens did not like the government's position under Bolívar: The people in the military were unhappy with their roles, and the civilians were of the opinion that the military had too much power. After the dissolution of Gran Colombia, New Grenada continued to have conflicts between conservatives and liberals. These conflicts were each concentrated in particular regions, with conservatives particularly in the southern mountains and the Valley of Cauca. In the mid-1840s some leaders in Caracas organized a liberal opposition. Antonio Leocadio Guzman was an active participant and journalist in this movement and gained much popularity among the people of Caracas.In Argentina, the conflict manifested itself as a prolonged civil war between unitarianas (i.e. centralists) and federalists, which were in some aspects respectively analogous to liberals and conservatives in other countries. Between 1832 and 1852, the country existed as a confederation, without a head of state, although the federalist governor of Buenos Aires province, Juan Manuel de Rosas, was given the powers of debt payment and international relations and exerted a growing hegemony over the country. A national constitution was only enacted in 1853, reformed in 1860, and the country reorganized as a federal republic led by a liberal-conservative elite. After Uruguay achieved its independence, in 1828, a similar polarization crystallized between blancos and colorados, where the agrarian conservative interests were pitted against the liberal commercial interests based in Montevideo, and which eventually resulted in the Guerra Grande civil war (1839–1851).
Losing most of its North American colonies at the end of the 18th century left Great Britain in need of new markets to supply resources in the early 19th century. In order to solve this problem, Great Britain turned to the Spanish colonies in South America for resources and markets. In 1806 a small British force surprise attacked the capitol of the viceroyalty in Río de la Plata. As a result, the local garrison protecting the capitol was destroyed in an attempt to defend against the British conquest. The British were able to capture large amounts of precious metals, before a French naval force intervened on behalf of the Spanish King and took down the invading force. However, this caused much turmoil in the area as militia took control of the area from the viceroy. The next year the British attacked once again with a much larger force attempting to reach and conquer Montevideo. They failed to reach Montevideo but succeeded in establishing an alliance with the locals. As a result, the British were able to take control of the Indian markets. This newly gained British dominance hindered the development of Latin American industries and strengthened the dependence on the world trade network. Britain now replaced Spain as the region's largest trading partner. Great Britain invested significant capital in Latin America to develop the area as a market for processed goods. From the early 1820s to 1850, the post-independence economies of Latin American countries were lagging and stagnant. Eventually, enhanced trade among Britain and Latin America led to state development such as infrastructure improvements. These improvements included roads and railroads which grew the trades between countries and outside nations such as Great Britain. By 1870, exports dramatically increased, attracting capital from abroad (including Europe and USA).
Between 1821 and 1910, Mexico battled through various civil wars between the established Conservative government and the Liberal reformists ("Mexico Timeline- Page 2)". On May 8, 1827 Baron Damas, the French Minister of Foreign Affairs, and Sebastián Camacho, a Mexican diplomat, signed an agreement called "The Declarations" which contained provisions regarding commerce and navigation between France and Mexico. At this time the French government did not recognize Mexico as an independent entity. It was not until 1861 that the liberalist rebels, led by Benito Juárez, took control of Mexico City, consolidating liberal rule. However, the constant state of warfare left Mexico with a tremendous amount of debt owed to Spain, England, and France, all of whom funded the Mexican war effort (Neeno). As newly appointed president, Benito Juárez suspended payment of debts for next two years, to focus on a rebuilding and stabilization initiative in Mexico under the new government. On December 8, 1861, Spain, England and France landed in Veracruz to seize unpaid debts from Mexico. However, Napoleon III, with intentions of establishing a French client state to further push his economic interests, pressured the other two powers to withdraw in 1862 (Greenspan; "French Intervention in Mexico…"). France under Napoleon III remained and established Maximilian of Habsburg, Archduke of Austria, as Emperor of Mexico. The march by the French to Mexico City enticed heavy resistance by the Mexican government, it resulted in open warfare. The Battle of Puebla in 1862 in particular presented an important turning point in which Ignacio Zaragoza led the Mexican army to victory as they pushed back the French offensive ("Timeline of the Mexican Revolution"). The victory came to symbolize Mexico's power and national resolve against foreign occupancy and as a result delayed France's later attack on Mexico City for an entire year (Cinco de Mayo (Mexican History)). With heavy resistance by Mexican rebels and the fear of United States intervention against France, forced Napoleon III to withdraw from Mexico, leaving Maximilian to surrender, where he would be later executed by Mexican troops under the rule of Porfirio Díaz. Napoleon III's desire to expand France's economic empire influenced the decision to seize territorial domain over the Central American region. The port city of Veracruz, Mexico and France's desire to construct a new canal were of particular interest. Bridging both New World and East Asian trade routes to the Atlantic were key to Napoleon III's economic goals to the mining of precious rocks and the expansion of France's textile industry. Napoleon's fear of the United States' economic influence over the Pacific trade region, and in turn all New World economic activity, pushed France to intervene in Mexico under the pretense of collecting on Mexico's debt. Eventually France began plans to build the Panama Canal in 1881 until 1904 when the United States took over and proceeded with its construction and implementation ("Read Our Story").
The Monroe Doctrine was included in President James Monroe's 1823 annual message to Congress. The doctrine warns European nations that the United States will no longer tolerate any new colonization of Latin American countries. It was originally drafted to meet the present major concerns, but eventually became the precept of U.S. foreign policy in the Western Hemisphere. The doctrine was put into effect in 1865 when the U.S. government supported Mexican president, Benito Juárez, diplomatically and militarily. Some Latin American countries viewed the U.S. interventions, allowed by the Monroe Doctrine when the U.S. deems necessary, with suspicion.Another important aspect of United States involvement in Latin America is the case of the filibuster William Walker. In 1855, he traveled to Nicaragua hoping to overthrow the government and take the land for the United States. With only the aid of 56 followers, he was able to take over the city of Granada, declaring himself commander of the army and installing Patricio Rivas as a puppet president. However, Rivas's presidency ended when he fled Nicaragua; Walker rigged the following election to ensure that he became the next president. His presidency did not last long, however, as he was met with much opposition from political groups in Nicaragua and neighbouring countries. On May 1, 1857, Walker was forced by a coalition of Central American armies to surrender himself to a United States Navy officer who repatriated him and his followers. When Walker subsequently returned to Central America in 1860, he was apprehended by the Honduran authorities and executed.
The Mexican–American War, another instance of U.S. involvement in Latin America, was a war between the United States and Mexico that started in April 1846 and lasted until February 1848. The main cause of the war was the United States' annexation of Texas in 1845 and a dispute afterwards about whether the border between Mexico and the United States ended where Mexico claimed, at the Nueces River, or ended where the United States claimed, at the Rio Grande. Peace was negotiated between the United States and Mexico with the Treaty of Guadalupe Hidalgo, which stated that Mexico was to cede land which would later become part of California and New Mexico as well as give up all claims to Texas, for which the United States would pay $15,000,000. However, tensions between the two countries were still high and over the next six years things only got worse with raids along the border and attacks by Native Americans against Mexican citizens. To defuse the situation, the United States agreed to purchase 29,670 squares miles of land from Mexico for $10,000,000 so a southern railroad could be built to connect the Pacific and Atlantic coasts. This would become known as the Gadsden Purchase. A critical component of U.S. intervention in Latin American affairs took form in the Spanish–American War, which drastically affected the futures of Cuba and Puerto Rico in the Americas, as well as Guam and the Philippines, by acquiring the majority of the last remaining Spanish colonial possessions.
In the late 19th century and early 20th century, the U.S. banana importing companies United Fruit Company, Cuyamel Fruit Company (both ancestors of Chiquita), and Standard Fruit Company (now Dole), acquired large amounts of land in Central American countries like Guatemala, Honduras, and Costa Rica. The companies gained leverage over the governments and a ruling elite in these countries by dominating their economies and paying kickbacks, and exploited local workers. These countries came to be called banana republics. Cubans, with the aid of Dominicans, launched a war for independence in 1868 and, over the next 30 years, suffered 279,000 losses in a brutal war against Spain that culminated in U.S. intervention. The 1898 Spanish–American War resulted in the end of Spanish colonial presence in the Americas. A period of frequent U.S. intervention in Latin America followed, with the acquisition of the Panama Canal Zone in 1903, the so-called Banana Wars in Cuba, Haiti, Dominican Republic, Nicaragua, and Honduras; the Caco Wars in Haiti; and the so-called Border War with Mexico. Some 3,000 Latin Americans were killed between 1914 and 1933. The U.S. press described the occupation of the Dominican Republic as an 'Anglo-Saxon crusade', carried out to keep the Latin Americans 'harmless against the ultimate consequences of their own misbehavior'.After World War I, U.S. interventionism diminished, culminating in President Franklin D. Roosevelt's Good Neighbor policy in 1933.
The Zimmermann Telegram was a 1917 diplomatic proposal from the German Empire for Mexico to join an alliance with Germany in the event of the United States entering World War I against Germany. The proposal was intercepted and decoded by British intelligence. The revelation of the contents outraged the American public and swayed public opinion. President Woodrow Wilson moved to arm American merchant ships to defend themselves against German submarines, which had started to attack them. The news helped generate support for the United States declaration of war on Germany in April of that year.The message came as a coded telegram dispatched by the Foreign Secretary of the German Empire, Arthur Zimmermann, on January 16, 1917. The message was sent to the German ambassador of Mexico, Heinrich von Eckardt. Zimmermann sent the telegram in anticipation of the resumption of unrestricted submarine warfare by Germany on February 1, an act which Germany presumed would lead to war. The telegram instructed Ambassador Eckardt that if the U.S. appeared certain to enter the war, he was to approach the Mexican Government with a proposal for a military alliance, with funding from Germany. As part of the alliance, Germany would assist Mexico in reconquering Texas and the Southwest. Eckardt was instructed to urge Mexico to help broker an alliance between Germany and Japan. Mexico, in the middle of the Mexican Revolution, far weaker militarily, economically and politically than the U.S., ignored the proposal; after the U.S. entered the war, it officially rejected it.
After World War I, in which Brazil was an ally of the United States, Great Britain, and France, the country realized it needed a more capable army but did not have the technology to create it. In 1919, the French Military Mission was established by the French Commission in Brazil. Their main goal was to contain the inner rebellions in Brazil. They tried to assist the army by bringing them up to the European military standard but constant civil missions did not prepare them for World War II. Brazil's President, Getúlio Vargas, wanted to industrialize Brazil, allowing it to be more competitive with other countries. He reached out to Germany, Italy, France, and the United States to act as trade allies. Many Italian and German people immigrated to Brazil many years before World War II began thus creating a Nazi influence. The immigrants held high positions in government and the armed forces. Brazil continued to try to remain neutral to the United States and Germany because it was trying to make sure it could continue to be a place of interest for both opposing countries. Brazil attended continental meetings in Buenos Aires, Argentina (1936); Lima, Peru (1938); and Havana, Cuba (1940) that obligated them to agree to defend any part of the Americas if they were to be attacked. Eventually, Brazil decided to stop trading with Germany once Germany started attacking offshore trading ships resulting in Germany declaring a blockade against the Americas in the Atlantic Ocean. Furthermore, Germany also ensured that they would be attacking the Americas soon. Once the German submarines attacked unarmed Brazilian trading ships, President Vargas met with the United States President Franklin D. Roosevelt to discuss how they could retaliate. On January 22, 1942, Brazil officially ended all relations with Germany, Japan, and Italy, becoming a part of the Allies. The Brazilian Expeditionary Force was sent to Naples, Italy to fight for democracy. Brazil was the only Latin American country to send troops to Europe. Initially, Brazil wanted to only provide resources and shelter for the war to have a chance of gaining a high postwar status but ended up sending 25,000 men to fight.However, it was not a secret that Vargas had an admiration for Hitler's Nazi Germany and its Führer. He even let German Luftwaffe build secret air forces around Brazil. This alliance with Germany became Brazil's second best trade alliance behind the United States. It was recently found that 9,000 war criminals escaped to South America, including Croats, Ukrainians, Russians and other western Europeans who aided the Nazi war machine. Most, perhaps as many as 5,000, went to Argentina; between 1,500 and 2,000 are thought to have made it to Brazil; around 500 to 1,000 to Chile; and the rest to Paraguay and Uruguay.After World War II, the United States and Latin America continued to have a close relationship. For example, USAID created family planning programs in Latin America combining the NGOs already in place, providing the women in largely Catholic areas access to contraception.
Mexico entered World War II in response to German attacks on Mexican ships. The Potrero del Llano, originally an Italian tanker, had been seized in port by the Mexican government in April 1941 and renamed in honor of a region in Veracruz. It was attacked and crippled by the German submarine U-564 on May 13, 1942. The attack killed 13 of 35 crewmen. On May 20, 1942, a second tanker, Faja de Oro, also a seized Italian ship, was attacked and sunk by the German submarine U-160, killing 10 of 37 crewmen. In response, President Manuel Ávila Camacho and the Mexican government declared war on the Axis powers on May 22, 1942. A large part of Mexico's contribution to the war came through an agreement January 1942 that allowed Mexican nationals living in the United States to join the American armed forces. As many as 250,000 Mexicans served in this way. In the final year of the war, Mexico sent one air squadron to serve under the Mexican flag: the Mexican Air Force's Escuadrón Aéreo de Pelea 201 (201st Fighter Squadron), which saw combat in the Philippines in the war against Imperial Japan. Mexico was the only Latin-American country to send troops to the Asia-Pacific theatre of the war. In addition to those in the armed forces, tens of thousands of Mexican men were hired as farm workers in the United States during the war years through the Bracero program, which continued and expanded in the decades after the war.World War II helped spark an era of rapid industrialization known as the Mexican Miracle. Mexico supplied the United States with more strategic raw materials than any other country, and American aid spurred the growth of industry. President Ávila was able to use the increased revenue to improve the country's credit, invest in infrastructure, subsidize food, and raise wages.
President Federico Laredo Brú led Cuba when war broke out in Europe, though real power belonged to Fulgencio Batista as Chief of Staff of the army. In 1940, Laredo Brú infamously denied entry to 900 Jewish refugees who arrived in Havana aboard the MS St. Louis. After both the United States and Canada likewise refused to accept the refugees, they returned to Europe, where many were eventually murdered in the Holocaust. Batista became president in his own right following the election of 1940. He cooperated with the United States as it moved closer to war against the Axis. Cuba declared war on Japan on December 8, 1941, and on Germany and Italy on December 11.Cuba was an important participant in the Battle of the Caribbean and its navy gained a reputation for skill and efficiency. The navy escorted hundreds of Allied ships through hostile waters, flew thousands of hours on convoy and patrol duty, and rescued over 200 victims of German U-Boat attacks from the sea. Six Cuban merchant ships were sunk by U-boats, taking the lives of around eighty sailors. On May 15, 1943, a squadron of Cuban submarine chasers sank the German submarine U-176 near Cayo Blanquizal. Cuba received millions of dollars in American military aid through the Lend-Lease program, which included air bases, aircraft, weapons, and training. The United States naval station at Guantanamo Bay also served as a base for convoys passing between the mainland United States and the Panama Canal or other points in the Caribbean.The Dominican Republic declared war on Germany and Japan following the attack on Pearl Harbor and the Nazi declaration of war on the US. It did not directly contribute with troops, aircraft, or ships, however 112 Dominicans were integrated into the US military and fought in the war. On May 3, 1942, German submarine U-125 sank Dominican ship San Rafael with 1 torpedo and 32 rounds from the deck gun 50 miles west off Jamaica; 1 was killed, 37 survived. On May 21, 1942, German submarine U-156 sank Dominican ship Presidente Trujillo off Fort-de-France, Martinique; 24 were killed, 15 survived. Rumors of pro-Nazi Dominicans supplying German U-boats with food, water and fuel abounded during the war.
There was a Nazi influence in certain parts of the region, but Jewish migration from Europe during the war continued. Only a few people recognized or knew about the Holocaust. Furthermore, numerous military bases were built during the war by the United States, but some also by the Germans. Even now, unexploded bombs from the second world war that need to be made safe still remain.The only international conflicts since World War II have been the Football War between El Salvador and Honduras (1969), the Cenepa War between Ecuador and Peru (1995), along with Argentina's war with the United Kingdom for control of the Falkland Islands (1982). The Falklands War left 649 Argentines (including 143 conscripted privates) dead and 1,188 wounded, while the UK lost 255 (88 Royal Navy, 27 Royal Marines, 16 Royal Fleet Auxiliary, 123 British Army, and 1 Royal Air Force) dead.
The Great Depression caused Latin America to grow at a slow rate, separating it from leading industrial democracies. The two world wars and U.S. Depression also made Latin American countries favor internal economic development, leading Latin America to adopt the policy of import substitution industrialization. Countries also renewed emphasis on exports. Brazil began selling automobiles to other countries, and some Latin American countries set up plants to assemble imported parts, letting other countries take advantage of Latin America's low labor costs. Colombia began to export flowers, emeralds and coffee grains and gold, becoming the world's second-leading flower exporter. Economic integration was called for, to attain economies that could compete with the economies of the United States or Europe. Starting in the 1960s with the Latin American Free Trade Association and Central American Common Market, Latin American countries worked toward economic integration. In efforts to help regain global economic strength, the U.S. began to heavily assist countries involved in World War II at the expense of Latin America. Markets that were previously unopposed as a result of the war in Latin America grew stagnant as the rest of the world no longer needed their goods.
Large countries like Argentina called for reforms to lessen the disparity of wealth between the rich and the poor, which has been a long problem in Latin America that stunted economic growth.Advances in public health caused an explosion of population growth, making it difficult to provide social services. Education expanded, and social security systems introduced, but benefits usually went to the middle class, not the poor. As a result, the disparity of wealth increased. Increasing inflation and other factors caused countries to be unwilling to fund social development programs to help the poor.
Bureaucratic authoritarianism was practised in Brazil after 1964, in Argentina, and in Chile under Augusto Pinochet, in a response to harsh economic conditions. It rested on the conviction that no democracy could take the harsh measures to curb inflation, reassure investors, and quicken economic growth quickly and effectively. Though inflation fell sharply, industrial production dropped with the decline of official protection.
After World War II and the beginning of a Cold War between the United States and the Soviet Union, US diplomats became interested in Asia, Africa, and Latin America, and frequently waged proxy wars against the Soviet Union in these countries. The US sought to stop the spread of communism. Latin American countries generally sided with the US in the Cold War period, even though they were neglected since the US's concern with communism were focused in Europe and Asia, not Latin America. Between 1946 and 1959 Latin America received only 2% of the United States foreign aid despite having poor conditions similar to the main recipients of The Marshall Plan. Some Latin American governments also complained of the US support in the overthrow of some nationalist governments, and intervention through the CIA. In 1947, the US Congress passed the National Security Act, which created the National Security Council in response to the United States's growing obsession with anti-communism. In 1954, when Jacobo Arbenz of Guatemala accepted the support of communists and attacked holdings of the United Fruit Company, the US decided to assist Guatemalan counter-revolutionaries in overthrowing Arbenz. These interventionist tactics featured the use of the CIA rather than the military, which was used in Latin America for the majority of the Cold War in events including the overthrow of Salvador Allende. Latin America was more concerned with issues of economic development, while the United States focused on fighting communism, even though the presence of communism was small in Latin America.Dominican dictator Rafael Leónidas Trujillo (r. 1930–61) achieved support from the US by becoming Latin America's leading anti-communist. Trujillo extended his tyranny to the USA, and his regime committed multiple murders in New York City. American officials had long recognized that the Dominican Republic's conduct under Trujillo was "below the level of recognized civilian nations, certainly not much above that of the communists." But after Castro's seizure of power in 1959, President Dwight D. Eisenhower concluded that Trujillo had become a Cold War liability. In 1960, Trujillo threatened to align with the Communist world in response to US and Latin American rejection of his regime. La Voz Dominicana and Radio Caribe began attacking the US in Marxian terms, and the Dominican Communist party was legalized. Trujillo also unsuccessfully attempted to establish contacts and relations with the Soviet Bloc. In 1961, Trujillo was murdered with weapons supplied by the CIA. Ramfis Trujillo, the dictator's son, remained in de facto control of the government for the next six months through his position as commander of the armed forces. Trujillo's brothers, Hector Bienvenido and Jose Arismendi Trujillo, returned to the country and began immediately to plot against President Balaguer. On 18 November 1961, as a planned coup became more evident, US Secretary of State Dean Rusk issued a warning that the United States would not "remain idle" if the Trujillos attempted to "reassert dictatorial domination" over the Dominican Republic. Following this warning, and the arrival of a fourteen-vessel US naval task force within sight of Santo Domingo, Ramfis and his uncles fled the country on 19 November with $200 million from the Dominican treasury.
By 1959, Cuba was afflicted with a corrupt dictatorship under Batista, and Fidel Castro ousted Batista that year and set up the first communist state in the hemisphere. The United States imposed a trade embargo on Cuba, and combined with Castro's expropriation of private enterprises, this was detrimental to the Cuban economy. Around Latin America, rural guerrilla conflict and urban terrorism increased, inspired by the Cuban example. The United States put down these rebellions by supporting Latin American countries in their counter-guerrilla operations through the Alliance for Progress launched by President John F. Kennedy. This thrust appeared to be successful. A Marxist, Salvador Allende, became president of Chile in 1970, but was overthrown three years later in a military coup backed by the United States. Despite civil war, high crime and political instability, most Latin American countries eventually adopted bourgeois liberal democracies while Cuba maintained its socialist system.
Encouraged by the success of Guatemala in the 1954 Guatemalan coup d'état, in 1960, the U.S. decided to support an attack on Cuba by anti-Castro rebels. The Bay of Pigs invasion was an unsuccessful invasion of Cuba in 1961, financed by the U.S. through the CIA, to overthrow Fidel Castro. The incident proved to be very embarrassing for the new Kennedy administration.The failure of the invasion led to a Soviet-Cuban alliance.
President John F. Kennedy initiated the Alliance for Progress in 1961, to establish economic cooperation between the U.S. and Latin America. The Alliance would provide $20 billion for reform in Latin America, and counterinsurgency measures. Instead, the reform failed because of the simplistic theory that guided it and the lack of experienced American experts who could understand Latin American customs.
Armed Cuban intervention overseas began on June 14, 1959 with an invasion of the Dominican Republic by a group of fifty-six men, who landed a C-56 transport aircraft at the military airport of the town of Constanza. Upon their landing, the fifteen-man Dominican garrison began an ongoing gun battle with the invaders, until the survivors disappeared into the surrounding mountains. Immediately after, the Dominican Air Force bombed the area around Constanza with British made Vampire jets in an unsuccessful attempt to kill the invaders, which instead killed civilians. The invaders either died at the hands of machete-swinging peasants, or the military captured, tortured, and imprisoned them. A week later, two yachts offloaded 186 invaders onto Chris-Craft launches for a landing on the north coast. Dominican Air Force pilots fired rockets from their Vampire jets into the approaching launches, killing most of the invaders. The survivors were brutally tortured and murdered. From 1966 until the late 1980s, the Soviet government upgraded Cuba's military capabilities, and Castro saw to it that Cuba assisted with the independence struggles of several countries across the world, most notably Angola and Mozambique in southern Africa, and the anti-imperialist struggles of countries such as Syria, Algeria, Venezuela, Bolivia, and Vietnam.South Africa developed nuclear weapons due to the threat to its security posed by the presence of large numbers of Cuban troops in Angola and Mozambique. In November 1975, Cuba poured more than 65,000 troops into Angola in one of the fastest military mobilizations in history. On November 10, 1975, Cuban forces defeated the National Liberation Front of Angola (FNLA) in the Battle of Quifangondo. On November 25, 1975, as the South African Defence Force (SADF) tried to cross a bridge, Cubans hidden along the banks of the river attacked, destroying seven armored cars and killing upwards of 90 enemy soldiers. On March 27, 1976, the last South African troops withdrew from Angola. In September 1977, 12 MiG-21s conducted strafing flights over Puerto Plata in Dominican Republic to warn then president Joaquín Balaguer against intercepting Cuban warships headed to or returning from Angola. In 1988, Cuba returned to Angola with a vengeance. The crisis began in 1987 with an assault by Soviet-equipped national army troops against the pro-Western rebel movement UNITA in southeastern Angola. Soon, the SADF invaded to support the beleaguered US-backed faction and the Angolan offensive stalled. Cuba reinforced its African ally with 55,000 troops, tanks, artillery and MiG-23s, prompting Pretoria to call up 140,000 reservists. In June 1988, SADF armor and artillery engaged FAPLA-Cuban forces at Techipa, killing 290 Angolans and 10 Cubans. In retaliation, Cuban warplanes hammered South African troops. However, both sides quickly pulled back to avoid an escalation of hostilities. The Battle of Cuito Cuanavale stalemated, and a peace treaty was signed in September 1988. Within two years, the Cold War was over and Cuba's foreign policy shifted away from military intervention.
Following the American occupation of Nicaragua in 1912, as part of the Banana Wars, the Somoza family political dynasty came to power, and would rule Nicaragua until their ouster in 1979 during the Nicaraguan Revolution. The era of Somoza family rule was characterized by strong U.S. support for the government and its military as well as a heavy reliance on U.S.-based multi-national corporations. The Nicaraguan Revolution (Spanish: Revolución Nicaragüense or Revolución Popular Sandinista) encompassed the rising opposition to the Somoza dictatorship in the 1960s and 1970s, the campaign led by the Sandinista National Liberation Front (FSLN) to violently oust the dictatorship in 1978–79, the subsequent efforts of the FSLN to govern Nicaragua from 1979 until 1990 and the Contra War which was waged between the FSLN and the Contras from 1981 to 1990. The Revolution marked a significant period in Nicaraguan history and revealed the country as one of the major proxy war battlegrounds of the Cold War with the events in the country rising to international attention. Although the initial overthrow of the Somoza regime in 1978–79 was a bloody affair, the Contra War of the 1980s took the lives of tens of thousands of Nicaraguans and was the subject of fierce international debate. During the 1980s both the FSLN (a leftist collection of political parties) and the Contras (a rightist collection of counter-revolutionary groups) received large amounts of aid from the Cold War super-powers (respectively, the Soviet Union and the United States).
The set of specific economic policy prescriptions that were considered the "standard" reform package were promoted for crisis-wracked developing countries by Washington, D.C.-based institutions such as the International Monetary Fund (IMF), World Bank, and the US Department of the Treasury during the 1980s and 1990s. In recent years, several Latin American countries led by socialist or other left wing governments – including Argentina and Venezuela – have campaigned for (and to some degree adopted) policies contrary to the Washington Consensus set of policies. (Other Latin countries with governments of the left, including Brazil, Mexico, Chile and Peru, have in practice adopted the bulk of the policies.) Also critical of the policies as actually promoted by the International Monetary Fund have been some US economists, such as Joseph Stiglitz and Dani Rodrik, who have challenged what are sometimes described as the "fundamentalist" policies of the International Monetary Fund and the US Treasury for what Stiglitz calls a "one size fits all" treatment of individual economies. The term has become associated with neoliberal policies in general and drawn into the broader debate over the expanding role of the free market, constraints upon the state, and US influence on other countries' national sovereignty. This politico-economical initiative was institutionalized in North America by 1994 NAFTA, and elsewhere in the Americas through a series of like agreements. The comprehensive Free Trade Area of the Americas project, however, was rejected by most South American countries at the 2005 4th Summit of the Americas.
In most countries, since the 2000s left-wing political parties have risen to power. The presidencies of Hugo Chávez in Venezuela, Ricardo Lagos and Michelle Bachelet in Chile, Lula da Silva and Dilma Rousseff in Brazil, Néstor Kirchner and his wife Cristina Fernández in Argentina, Tabaré Vázquez and José Mujica in Uruguay, Evo Morales in Bolivia, Daniel Ortega in Nicaragua, Rafael Correa in Ecuador, Fernando Lugo in Paraguay, Manuel Zelaya in Honduras (removed from power by a coup d'état), Mauricio Funes and Salvador Sánchez Cerén in El Salvador are all part of this wave of left-wing politicians who often declare themselves socialists, Latin Americanists, or anti-imperialists (often implying opposition to US policies towards the region). A development of this has been the creation of the eight-member ALBA alliance, or "The Bolivarian Alliance for the Peoples of Our America" (Spanish: Alianza Bolivariana para los Pueblos de Nuestra América) by some of the countries already mentioned. By June 2014, Honduras (Juan Orlando Hernández), Guatemala (Otto Pérez Molina), and Panama (Ricardo Martinelli) had right-wing governments.
In 1982, Mexico announced that it could not meet its foreign debt payment obligations, inaugurating a debt crisis that would "discredit" Latin American economies throughout the decade. This debt crisis would lead to neoliberal reforms that would instigate many social movements in the region. A "reversal of development" reigned over Latin America, seen through negative economic growth, declines in industrial production, and thus, falling living standards for the middle and lower classes. Governments made financial security their primary policy goal over social security, enacting new neoliberal economic policies that implemented privatization of previously national industries and informalization of labor. In an effort to bring more investors to these industries, these governments also embraced globalization through more open interactions with the international economy. Significantly, as democracy spread across much of Latin America, the realm of government became more inclusive (a trend that proved conducive to social movements), the economic ventures remained exclusive to a few elite groups within society. Neoliberal restructuring consistently redistributed income upward while denying political responsibility to provide social welfare rights, and though development projects took place throughout the region, both inequality and poverty increased. Feeling excluded from these new projects, the lower classes took ownership of their own democracy through a revitalization of social movements in Latin America. Both urban and rural populations had serious grievances as a result of the above economic and global trends and have voiced them in mass demonstrations. Some of the largest and most violent of these have been protests against cuts in urban services, such as the Caracazo in Venezuela and the Argentinazo in Argentina. Rural movements have made diverse demands related to unequal land distribution, displacement at the hands of development projects and dams, environmental and indigenous concerns, neoliberal agricultural restructuring, and insufficient means of livelihood. These movements have benefited considerably from transnational support from conservationists and INGOs. The Movement of Rural Landless Workers (MST) is perhaps the largest contemporary Latin American social movement. As indigenous populations are primarily rural, indigenous movements account for a large portion of rural social movements, including the Zapatista rebellion in Mexico, the Confederation of Indigenous Nationalities of Ecuador (CONAIE), indigenous organizations in the Amazon region of Ecuador and Bolivia, pan-Mayan communities in Guatemala, and mobilization by the indigenous groups of Yanomami peoples in the Amazon, Kuna peoples in Panama, and Altiplano Aymara and Quechua peoples in Bolivia. Other significant types of social movements include labor struggles and strikes, such as recovered factories in Argentina, as well as gender-based movements such as the Mothers of the Plaza de Mayo in Argentina and protests against maquila production, which is largely a women's issue because of how it draws on women for cheap labor.
The 2000s commodities boom caused positive effects for many Latin American economies. Another trend is the rapidly increasing importance of the relations with China.With the end of the commodity boom in the 2010s, economic stagnation or recession resulted in some countries. As a result, the left-wing governments of the Pink Tide lost support. The worst-hit was Venezuela, which is facing severe social and economic upheaval. The corruption scandal of Odebrecht, a Brazilian conglomerate, has raised allegations of corruption across the region's governments (see Operation Car Wash). The bribery ring has become the largest corruption scandal in Latin American history. As of July 2017, the highest ranking politicians charged were former Brazilian President Luiz Inácio Lula da Silva (arrested) and former Peruvian presidents Ollanta Humala (arrested) and Alejandro Toledo (fugitive, fled to the US).
The following is a list of the ten largest metropolitan areas in Latin America.
The inhabitants of Latin America are of a variety of ancestries, ethnic groups, and races, making the region one of the most diverse in the world. The specific composition varies from country to country: some have a predominance of European-Amerindian or more commonly referred to as Mestizo or Castizo depending on the admixture, population; in others, Amerindians are a majority; some are dominated by inhabitants of European ancestry; and some countries' populations are primarily Mulatto. Various black, Asian and Zambo (mixed black and Amerindian) minorities are also identified regularly. People with European ancestry are the largest single group, and along with people of part-European ancestry, they combine to make up approximately 80% of the population, or even more.According to Jon Aske: Before Hispanics became such a 'noticeable' group in the U.S., the distinction between black and white was the major racial division and according to the one-drop rule adhered to by the culture at large, one drop of African ancestry usually meant that the person was Black. ... The notion of racial continuum and a separation of race (or skin color) and ethnicity, on the other hand, is the norm in most of Latin America. In the Spanish and Portuguese empires, racial mixing or miscegenation was the norm and something that the Spanish and Portuguese had grown rather accustomed to during the hundreds of years of contact with Arabs and North Africans in the Iberian peninsula. But, demographics may have made this inevitable as well. Thus, for example, of the approximately 13.5 million people who lived in the Spanish colonies in 1800 before independence only about one fifth were white. This contrasts with the U.S., where more than four-fifths were whites (out of a population of 5.3 million in 1801, 900,000 were slaves, plus approximately 60,000 free blacks). ... The fact of the recognition of a racial continuum in Hispanic American (sic) does not mean that there wasn't discrimination, which there was, or that there wasn't an obsession with race, or 'castes', as they were sometimes called. ... In areas with large indigenous Amerindian populations, a racial mixture resulted, which is known in Spanish as mestizos ... who are a majority in Mexico, Central America and most of South America. Similarly, when African slaves were brought to the Caribbean region and Brazil, where there was very little indigenous presence left, unions between them and Spanish produced a population of mixed mulatos ... who are a majority of the population in many of those Spanish-speaking Caribbean basin countries (Cuba, Dominican Republic, Puerto Rico, Colombia, and Venezuela).Aske has also written that: Spanish colonization was rather different from later English, or British, colonization of North America. They had different systems of colonization and different methods of subjugation. While the English were primarily interested in grabbing land, the Spanish in addition had a mandate to incorporate the land's inhabitants into their society, something which was achieved by religious conversion and sexual unions which produced a new 'race' of mestizos, a mixture of Europeans and indigenous peoples. mestizos (sic) form the majority of the population in Mexico, Central America, and much of South America. Racial mixing or miscegenation, after all, was something that the Spanish and Portuguese had been accustomed to during the hundreds of years of contact with Arabs and North Africans. Similarly, later on, when African slaves were introduced into the Caribbean basin region, unions between them and Spaniards produced a population of mulatos, who are a majority of the population in the Caribbean islands (the Antilles) (Cuba, Dominican Republic, Puerto Rico), as well as other areas of the Caribbean region (Colombia, Venezuela and parts of the Central American Caribbean coast). mestizos (sic) and mulatos may not have always have been first class citizens in their countries, but they were never disowned in the way the outcomes of unions of Europeans and Native Americans were in the British colonies, where interracial marriages were taboo and one drop of Black or Amerindian blood was enough to make the person 'impure'.In his famous 1963 book The Rise of the West, William Hardy McNeill wrote that: Racially mixed societies arose in most of Spanish and Portuguese America, compounded in varying proportions from European, Indian, and Negro strands. Fairly frequent resort to manumission mitigated the hardships of slavery in those areas; and the Catholic church positively encouraged marriages between white immigrants and Indian women as a remedy for sexual immorality. However, in the southern English colonies and in most of the Caribbean islands, the importation of Negro slaves created a much more sharply polarized biracial society. Strong race feeling and the servile status of nearly all Negroes interdicted intermarriage, practically if not legally. Such discrimination did not prevent interbreeding; but children of mixed parentage were assigned to the status of their mothers. Mulattoes and Indian half-breeds were thereby excluded from the white community. In Spanish (and, with some differences, Portuguese) territories a more elaborate and less oppressive principle of racial discrimination established itself. The handful of persons who had been born in the homelands claimed topmost social prestige; next came those of purely European descent; while beneath ranged the various racial blends to form a social pyramid whose numerous racial distinctions meant that no one barrier could become as ugly and inpenetrable as that dividing whites from Negroes in the English, Dutch, and French colonies.Thomas C. Wright, meanwhile, has written that: The demographic makeup of colonial Latin America became more complex when, as the native population declined, the Portuguese, Spanish, and the French in Haiti turned to Africa for labor, as did the British in North America. The tricontinental heritage that characterizes Latin America, then, is shared by the United States, but even a casual examination reveals that the outcome of the complex interaction of different peoples has varied. While miscegenation among the three races certainly occurred in North America, it appears to have been much less common than in Latin America. Furthermore, offspring of such liaisons were not recognized as belonging to new, distinct racial categories in North America as they were in Latin America. The terms mestizo or mameluco, mulatto, the general term castas, and dozens of subcategories of racial identity frankly recognized the outcomes of interracial sexual activity in Latin America and established a continuum of race rather than the unrealistic absolute categories of white, black, or Indian as used in the United States. (The U.S. Census Bureau's forms did not allow individuals to list more than one race until 2000.)
Spanish is the predominant language of Latin America. It is spoken as first language by about 60% of the population. Portuguese is spoken by about 30%, and about 10% speak other languages such as Quechua, Mayan languages, Guaraní, Aymara, Nahuatl, English, French, Dutch and Italian. Portuguese is spoken only in Brazil (Brazilian Portuguese), the biggest and most populous country in the region. Spanish is the official language of most of the rest of the countries and territories on the Latin American mainland (Spanish language in the Americas), as well as in Cuba, Puerto Rico (where it is co-official with English), and the Dominican Republic. French is spoken in Haiti and in the French overseas departments of Guadeloupe, Martinique and Guiana. It is also spoken by some Panamanians of Afro-Antillean descent. Dutch is the official language in Suriname, Aruba, Curaçao, and the Netherlands Antilles. (As Dutch is a Germanic language, these territories are not necessarily considered part of Latin America.) However, the native language of Aruba, Bonaire, and Curaçao, is Papiamento, a creole language largely based on Portuguese and Spanish and has a considerable influence coming from the Dutch language and Portuguese-based creole languages. Amerindian languages are widely spoken in Peru, Guatemala, Bolivia, Paraguay and Mexico, and to a lesser degree, in Panama, Ecuador, Brazil, Colombia, Venezuela, Argentina, and Chile amongst other countries. In Latin American countries not named above, the population of speakers of indigenous languages tend to be very small or even non-existent (e.g. Uruguay). Mexico is possibly the only country that contains a wider variety of indigenous languages than any Latin American country, but the most spoken language is Nahuatl. In Peru, Quechua is an official language, alongside Spanish and any other indigenous language in the areas where they predominate. In Ecuador, while holding no official status, the closely related Quichua is a recognized language of the indigenous people under the country's constitution; however, it is only spoken by a few groups in the country's highlands. In Bolivia, Aymara, Quechua and Guaraní hold official status alongside Spanish. Guaraní, along with Spanish, is an official language of Paraguay, and is spoken by a majority of the population (who are, for the most part, bilingual), and it is co-official with Spanish in the Argentine province of Corrientes. In Nicaragua, Spanish is the official language, but on the country's Caribbean coast English and indigenous languages such as Miskito, Sumo, and Rama also hold official status. Colombia recognizes all indigenous languages spoken within its territory as official, though fewer than 1% of its population are native speakers of these languages. Nahuatl is one of the 62 native languages spoken by indigenous people in Mexico, which are officially recognized by the government as "national languages" along with Spanish. Other European languages spoken in Latin America include: English, by some groups in Puerto Rico, as well as in nearby countries that may or may not be considered Latin American, like Belize and Guyana, and spoken by descendants of British settlers in Argentina & Chile; German, in southern Brazil, southern Chile, portions of Argentina, Venezuela and Paraguay; Italian, in Brazil, Argentina, Venezuela, and Uruguay; Ukrainian, Polish and Russian in southern Brazil and Argentina; and Welsh, in southern Argentina.Yiddish and Hebrew are possible to be heard around Buenos Aires and São Paulo especially. Non-European or Asian languages include Japanese in Brazil, Peru, Bolivia, and Paraguay, Korean in Brazil, Argentina, Paraguay, and Chile, Arabic in Argentina, Brazil, Colombia, Venezuela, and Chile, and Chinese throughout South America. In several nations, especially in the Caribbean region, creole languages are spoken. The most widely spoken creole language in Latin America and the Caribbean is Haitian Creole, the predominant language of Haiti; it is derived primarily from French and certain West African tongues with Amerindian, English, Portuguese and Spanish influences as well. Creole languages of mainland Latin America, similarly, are derived from European languages and various African tongues. The Garifuna language is spoken along the Caribbean coast in Honduras, Guatemala, Nicaragua and Belize mostly by the Garifuna people a mixed race Zambo people who were the result of mixing between Indigenous Caribbeans and escaped Black slaves. Primarily an Arawakan language, it has influences from Caribbean and European languages. Archaeologists have deciphered over 15 pre-Columbian distinct writing systems from mesoamerican societies. the ancient Maya had the most sophisticated textually written language, but since texts were largely confined to the religious and administrative elite, traditions were passed down orally. oral traditions also prevailed in other major indigenous groups including, but not limited to the Aztecs and other Nahuatl speakers, Quechua and Aymara of the Andean regions, the Quiché of Central America, the Tupi-Guaraní in today's Brazil, the Guaraní in Paraguay and the Mapuche in Chile.
The vast majority of Latin Americans are Christians (90%), mostly Roman Catholics belonging to the Latin Church. About 70% of the Latin American population consider themselves Catholic. Latin America constitute in absolute terms the second world's largest Christian population, after Europe.According to the detailed Pew multi-country survey in 2014, 69% of the Latin American population is Catholic and 19% is Protestant. Protestants are 26% in Brazil and over 40% in much of Central America. More than half of these are converts from Roman Catholicism.
Due to economic, social and security developments that are affecting the region in recent decades, the focus is now the change from net immigration to net emigration. About 10 million Mexicans live in the United States. 31.7 million Americans listed their ancestry as Mexican as of 2010, or roughly 10% of the population.During the initial stage of the Spanish colonization of the Philippines which were around the 1600s, about 16,500 soldiers levied from Peru and Mexico were sent together with 600 Spanish officers to fight wars, settle, colonize and build cities and presidios in the Philippines. These 16,500 Peruvians and Mexicans supplemented the Native Malay Population which then reached 667,612 people. This initial group of Latin American soldier-settler founders had spread their genes among the sparsely populated Philippines. This resulted into a spread of Latin American admixture among Filipinos as evidenced by a large number of Filipinos possessing Native American ancestry. A Y-DNA compilation organized by the Genetic Company "Applied Biosystems" found that 13.33% of the Filipino Male Population sampled from across the country had Y-DNA of Latin American and Spanish origins.Furthermore, according to a survey dated from 1870 conducted by German ethnologist Fedor Jagor of the population of Luzon island (Which holds half the citizens of the Philippines) 1/3rd of the people possess varying degrees of Spanish and Latin American ancestry. According to the 2005 Colombian census or DANE, about 3,331,107 Colombians currently live abroad.The number of Brazilians living overseas is estimated at about 2 million people. An estimated 1.5 to two million Salvadorans reside in the United States. At least 1.5 million Ecuadorians have gone abroad, mainly to the United States and Spain. Approximately 1.5 million Dominicans live abroad, mostly in the United States. More than 1.3 million Cubans live abroad, most of them in the United States. It is estimated that over 800,000 Chileans live abroad, mainly in Argentina, the United States, Canada, Australia and Sweden. An estimated 700,000 Bolivians were living in Argentina as of 2006 and another 33,000 in the United States.Japanese Brazilian immigrants to Japan numbered 250,000 in 2004, constituting Japan's second-largest immigrant population. Their experiences bear similarities to those of Japanese Peruvian immigrants, who are often relegated to low income jobs typically occupied by foreigners. Central Americans living abroad in 2005 were 3,314,300, of which 1,128,701 were Salvadorans, 685,713 were Guatemalans, 683,520 were Nicaraguans, 414,955 were Hondurans, 215,240 were Panamanians and 127,061 were Costa Ricans.For the period 2000–2005, Chile, Costa Rica, Panama, and Venezuela were the only countries with global positive migration rates, in terms of their yearly averages.As a result of the 2010 Haiti Earthquake and its social and economic impact, there was a significant migration of Haitians to other Latin American countries. During the presidency of Hugo Chávez and his successor Nicolás Maduro, over 3.2 million people fled Venezuela during the Venezuelan refugee crisis as socioeconomic conditions and the quality of life worsened.The countries of Latin America seek to strengthen links between migrants and their states of origin, while promoting their integration in the receiving state. These Emigrant Policies focus on the rights, obligations and opportunities for participation of emigrated citizens who already live outside the borders of the country of origin. Research on Latin America shows that the extension of policies towards migrants is linked to a focus on civil rights and state benefits that can positively influence integration in recipient countries. In addition, the tolerance of dual citizenship has spread more in Latin America than in any other region of the world.
Despite significant progress, education access and school completion remains unequal in Latin America. The region has made great progress in educational coverage; almost all children attend primary school and access to secondary education has increased considerably. Quality issues such as poor teaching methods, lack of appropriate equipment and overcrowding exist throughout the region. These issues lead to adolescents dropping out of the educational system early. Most educational systems in the region have implemented various types of administrative and institutional reforms that have enabled reach for places and communities that had no access to education services in the early 1990s. Compared to prior generations, Latin American youth have seen an increase in their levels of education. On average, they have completed two years schooling more than their parents.However, there are still 23 million children in the region between the ages of 4 and 17 outside of the formal education system. Estimates indicate that 30% of preschool age children (ages 4–5) do not attend school, and for the most vulnerable populations, the poor and rural, this calculation exceeds 40 percent. Among primary school age children (ages 6 to 12), coverage is almost universal; however there is still a need to incorporate 5 million children in the primary education system. These children live mostly in remote areas, are indigenous or Afro-descendants and live in extreme poverty.Among people between the ages of 13 and 17 years, only 80% are full-time students in the education system; among them only 66% advance to secondary school. These percentages are lower among vulnerable population groups: only 75% of the poorest youth between the ages of 13 and 17 years attend school. Tertiary education has the lowest coverage, with only 70% of people between the ages of 18 and 25 years outside of the education system. Currently, more than half of low income children or living in rural areas fail to complete nine years of education.
Latin America and the Caribbean have been cited by numerous sources to be the most dangerous regions in the world. Studies have shown that Latin America contains the majority of the world's most dangerous cities. Many analysts attribute the reason to why the region has such an alarming crime rate and criminal culture is largely due to social and income inequality within the region, they say that growing social inequality is fueling crime in the region. Many agree that the prison crisis will not be resolved until the gap between the rich and the poor is addressed. Crime and violence prevention and public security are now important issues for governments and citizens in Latin America and the Caribbean region. Homicide rates in Latin America are the highest in the world. From the early 1980s through the mid-1990s, homicide rates increased by 50 percent. Latin America and the Caribbean experienced more than 2.5 million murders between 2000 and 2017. There were a total of 63,880 murders in Brazil in 2018.The major victims of such homicides are young men, 69 percent of whom are between the ages of 15 and 19 years old. Countries with the highest homicide rate per year per 100,000 inhabitants as of 2015 were: El Salvador 109, Honduras 64, Venezuela 57, Jamaica 43, Belize 34.4, St. Kitts and Nevis 34, Guatemala 34, Trinidad and Tobago 31, the Bahamas 30, Brazil 26.7, Colombia 26.5, the Dominican Republic 22, St. Lucia 22, Guyana 19, Mexico 16, Puerto Rico 16, Ecuador 13, Grenada 13, Costa Rica 12, Bolivia 12, Nicaragua 12, Panama 11, Antigua and Barbuda 11, and Haiti 10. Most of the top countries with the highest homicide rates are in Africa and Latin America. Countries in Central America, like El Salvador and Honduras, top the list of homicides in the world.Brazil has more overall homicides than any country in the world, at 50,108, accounting for one in 10 globally. Crime-related violence in Latin America represents the most threat to public health, striking more victims than HIV/AIDS or other infectious diseases. Countries with lowest homicide rate per year per 100,000 inhabitants as of 2015 were: Chile 3, Peru 7, Argentina 7, Uruguay 8 and Paraguay 9.
According to Goldman Sachs' BRICS review of emerging economies, by 2050 the largest economies in the world will be as follows: China, United States, India, Japan, Germany, United Kingdom, Brazil and Mexico.
Over the past two centuries, Latin America's GDP per capita has fluctuated around world average. However, there is a substantial gap between Latin America and the developed economies. In the Andean region this gap can be a consequence of low human capital among Inca Indios in Pre-Columbian times. It is evident that the numeracy value of Peruvian Indios in the early 16th century was just half of the numeracy of the Spanish and Portuguese. Between 1820 and 2008, this gap widened from 0.8 to 2.7 times. Since 1980, Latin America also lost growth versus the world average. Many nations such as those in Asia have joined others on a rapid economic growth path, but Latin America has grown at slower pace and its share of world output declined from 9.5% in 1980 to 7.8% in 2008.
Latin America is the region with the highest levels of income inequality in the world. The following table lists all the countries in Latin America indicating a valuation of the country's Human Development Index, GDP at purchasing power parity per capita, measurement of inequality through the Gini index, measurement of poverty through the Human Poverty Index, measurement of extreme poverty based on people living under 1.25 dollars a day, life expectancy, murder rates and a measurement of safety through the Global Peace Index. Green cells indicate the best performance in each category while red indicates the lowest.
Wealth inequality in Latin America and the Caribbean remains a serious issue despite strong economic growth and improved social indicators over the past decade. A report released in 2013 by the UN Department of Economic and Social Affairs entitled Inequality Matters. Report of the World Social Situation, observed that: ‘Declines in the wage share have been attributed to the impact of labour-saving technological change and to a general weakening of labour market regulations and institutions. Such declines are likely to affect individuals in the middle and bottom of the income distribution disproportionately, since they rely mostly on labour income.’ In addition, the report noted that ‘highly-unequal land distribution has created social and political tensions and is a source of economic inefficiency, as small landholders frequently lack access to credit and other resources to increase productivity, while big owners may not have had enough incentive to do so.
The major trade blocs (or agreements) in the region are the Pacific Alliance and Mercosur. Minor blocs or trade agreements are the G3 Free Trade Agreement, the Dominican Republic – Central America Free Trade Agreement (DR-CAFTA), the Caribbean Community (CARICOM) and the Andean Community of Nations (CAN). However, major reconfigurations are taking place along opposing approaches to integration and trade; Venezuela has officially withdrawn from both the CAN and G3 and it has been formally admitted into the Mercosur (pending ratification from the Paraguayan legislature). The president-elect of Ecuador has manifested his intentions of following the same path. This bloc nominally opposes any Free Trade Agreement (FTA) with the United States, although Uruguay has manifested its intention otherwise. Chile, Peru, Colombia and Mexico are the only four Latin American nations that have an FTA with the United States and Canada, both members of the North American Free Trade Agreement (NAFTA).
Latin American culture is a mixture of many cultural expressions worldwide. It is the product of many diverse influences: Indigenous cultures of the people who inhabited the continent prior to European Colonization. Ancient and very advanced civilizations developed their own political, social and religious systems. The Mayas, the Aztecs and the Incas are examples of these. Indigenous legacies in music, dance, foods, arts and crafts, clothing, folk culture and traditions are very strong in Latin America. Linguistic effects on Spanish and Portuguese are also marked, such as in terms like pampa, taco, tamale, cacique. Western civilization, in particular the culture of Europe, was brought mainly by the colonial powers – the Spanish, Portuguese and French – between the 16th and 19th centuries. The most enduring European colonial influence is language and Roman Catholicism. More recently, additional cultural influences came from the United States and Europe during the nineteenth and twentieth centuries, due to the growing influence of the former on the world stage and immigration from the latter. The influence of the United States is particularly strong in northern Latin America, especially Puerto Rico, which is an American territory. Prior to 1959, Cuba, who fought for its independence along American soldiers in the Spanish–American War, was also known to have a close socioeconomic relation with the United States. In addition, the United States also helped Panama become an independent state from Colombia and built the twenty-mile-long Panama Canal Zone in Panama which held from 1903 (the Panama Canal opened to transoceanic freight traffic in 1914) to 1999, when the Torrijos-Carter Treaties restored Panamanian control of the Canal Zone. South America experienced waves of immigration of Europeans, especially Italians, Spaniards, Portuguese, Germans, Austrians, Poles, Ukrainians, French, Dutch, Russians, Croatians, Lithuanians and Ashkenazi Jews. With the end of colonialism, French culture was also able to exert a direct influence in Latin America, especially in the realms of high culture, science and medicine. This can be seen in any expression of the region's artistic traditions, including painting, literature and music, and in the realms of science and politics.Due to the impact of Enlightenment ideals after the French revolution, a certain number of Iberian-American countries decriminalized homosexuality after France and French territories in the Americas in 1791. Some of the countries that abolished sodomy laws or banned any reference to state interference in consensual adult sexuality in the 19th century were Dominican Republic (1822), Brazil (1824), Peru (1836), Mexico (1871), Paraguay (1880), Argentina (1887), Honduras (1899), Guatemala and El Salvador. Today same-sex marriage is legal in Argentina, Brazil, Colombia, Costa Rica, Ecuador, Uruguay, and French overseas departments, as well as in several states of Mexico. Civil unions can be held in Chile. African cultures, whose presence derives from a long history of New World slavery. Peoples of African descent have influenced the ethno-scapes of Latin America and the Caribbean. This is manifested for instance in music, dance and religion, especially in countries like Brazil, Puerto Rico, Venezuela, Colombia, Panama, Haiti, Costa Rica, Dominican Republic, and Cuba. Asian cultures, whose part of the presence derives from the long history of the Coolie trade mostly arriving during the 19th and 20th centuries, and most commonly Chinese workers in Peru and Venezuela. But also from Japanese and Korean immigration especially headed to Brazil. This has largely affected the cuisine, traditions including literature, art and lifestyles and politics. The effects of Asian influences have especially and mostly effected the nations of Brazil, Cuba, Panama and Peru.
Beyond the rich tradition of indigenous art, the development of Latin American visual art owed much to the influence of Spanish, Portuguese and French Baroque painting, which in turn often followed the trends of the Italian Masters. In general, this artistic Eurocentrism began to fade in the early twentieth century, as Latin Americans began to acknowledge the uniqueness of their condition and started to follow their own path. From the early twentieth century, the art of Latin America was greatly inspired by the Constructivist Movement. The Movement quickly spread from Russia to Europe and then into Latin America. Joaquín Torres García and Manuel Rendón have been credited with bringing the Constructivist Movement into Latin America from Europe.An important artistic movement generated in Latin America is muralism represented by Diego Rivera, David Alfaro Siqueiros, José Clemente Orozco and Rufino Tamayo in Mexico, Santiago Martinez Delgado and Pedro Nel Gómez in Colombia and Antonio Berni in Argentina. Some of the most impressive Muralista works can be found in Mexico, Colombia, New York City, San Francisco, Los Angeles and Philadelphia. Painter Frida Kahlo, one of the most famous Mexican artists, painted about her own life and the Mexican culture in a style combining Realism, Symbolism and Surrealism. Kahlo's work commands the highest selling price of all Latin American paintings.The Venezuelan Armando Reverón, whose work begins to be recognized internationally, is one of the most important artists of the 20th century in South America; he is a precursor of Arte Povera and Happening. From the 60s the kinetic art emerges in Venezuela, its main representatives are Jesús Soto, Carlos Cruz-Diez, Alejandro Otero and Gego. Colombian sculptor and painter Fernando Botero is also widely known by his works which, on first examination, are noted for their exaggerated proportions and the corpulence of the human and animal figures.
Latin American film is both rich and diverse. Historically, the main centers of production have been Mexico, Argentina, Brazil, and Cuba. Latin American film flourished after sound was introduced in cinema, which added a linguistic barrier to the export of Hollywood film south of the border. Mexican cinema started out in the silent era from 1896 to 1929 and flourished in the Golden Era of the 1940s. It boasted a huge industry comparable to Hollywood at the time with stars such as María Félix, Dolores del Río, and Pedro Infante. In the 1970s, Mexico was the location for many cult horror and action movies. More recently, films such as Amores Perros (2000) and Y tu mamá también (2001) enjoyed box office and critical acclaim and propelled Alfonso Cuarón and Alejandro González Iñárritu to the front rank of Hollywood directors. Alejandro González Iñárritu directed in 2010 Biutiful and Birdman (2014), Alfonso Cuarón directed Harry Potter and the Prisoner of Azkaban in 2004 and Gravity (2013). Close friend of both, Guillermo del Toro, a top rank Hollywood director in Hollywood and Spain, directed Pan's Labyrinth (2006) and produced El Orfanato (2007). Carlos Carrera (The Crime of Father Amaro), and screenwriter Guillermo Arriaga are also some of the most known present-day Mexican film makers. Rudo y Cursi released in December (2008) in Mexico was directed by Carlos Cuarón. Argentine cinema has also been prominenent since the first half of the 20th century and today averages over 60 full-length titles yearly. The industry suffered during the 1976–1983 military dictatorship; but re-emerged to produce the Academy Award winner The Official Story in 1985. A wave of imported U.S. films again damaged the industry in the early 1990s, though it soon recovered, thriving even during the Argentine economic crisis around 2001. Many Argentine movies produced during recent years have been internationally acclaimed, including Nueve reinas (2000), Son of the Bride (2001), El abrazo partido (2004), El otro (2007), the 2010 Foreign Language Academy Award winner El secreto de sus ojos and Wild Tales (2014). In Brazil, the Cinema Novo movement created a particular way of making movies with critical and intellectual screenplays, a clearer photography related to the light of the outdoors in a tropical landscape, and a political message. The modern Brazilian film industry has become more profitable inside the country, and some of its productions have received prizes and recognition in Europe and the United States, with movies such as Central do Brasil (1999), Cidade de Deus (2002) and Tropa de Elite (2007). Puerto Rican cinema has produced some notable films, such as Una Aventura Llamada Menudo, Los Diaz de Doris and Casi Casi. An influx of Hollywood films affected the local film industry in Puerto Rico during the 1980s and 1990s, but several Puerto Rican films have been produced since and it has been recovering. Cuban cinema has enjoyed much official support since the Cuban revolution and important film-makers include Tomás Gutiérrez Alea.
Pre-Columbian cultures were primarily oral, though the Aztecs and Mayans, for instance, produced elaborate codices. Oral accounts of mythological and religious beliefs were also sometimes recorded after the arrival of European colonizers, as was the case with the Popol Vuh. Moreover, a tradition of oral narrative survives to this day, for instance among the Quechua-speaking population of Peru and the Quiché (K'iche') of Guatemala. From the very moment of Europe's discovery of the continents, early explorers and conquistadores produced written accounts and crónicas of their experience – such as Columbus's letters or Bernal Díaz del Castillo's description of the conquest of Mexico. During the colonial period, written culture was often in the hands of the church, within which context Sor Juana Inés de la Cruz wrote memorable poetry and philosophical essays. Towards the end of the 18th Century and the beginning of the 19th, a distinctive criollo literary tradition emerged, including the first novels such as Lizardi's El Periquillo Sarniento (1816). The 19th century was a period of "foundational fictions" (in critic Doris Sommer's words), novels in the Romantic or Naturalist traditions that attempted to establish a sense of national identity, and which often focussed on the indigenous question or the dichotomy of "civilization or barbarism" (for which see, say, Domingo Sarmiento's Facundo (1845), Juan León Mera's Cumandá (1879), or Euclides da Cunha's Os Sertões (1902)). The 19th century also witnessed the realist work of Machado de Assis, who made use of surreal devices of metaphor and playful narrative construction, much admired by critic Harold Bloom. At the turn of the 20th century, modernismo emerged, a poetic movement whose founding text was Nicaraguan poet Rubén Darío's Azul (1888). This was the first Latin American literary movement to influence literary culture outside of the region, and was also the first truly Latin American literature, in that national differences were no longer so much at issue. José Martí, for instance, though a Cuban patriot, also lived in Mexico and the United States and wrote for journals in Argentina and elsewhere. However, what really put Latin American literature on the global map was no doubt the literary boom of the 1960s and 1970s, distinguished by daring and experimental novels (such as Julio Cortázar's Rayuela (1963)) that were frequently published in Spain and quickly translated into English. The Boom's defining novel was Gabriel García Márquez's Cien años de soledad (1967), which led to the association of Latin American literature with magic realism, though other important writers of the period such as the Peruvian Mario Vargas Llosa and Carlos Fuentes do not fit so easily within this framework. Arguably, the Boom's culmination was Augusto Roa Bastos's monumental Yo, el supremo (1974). In the wake of the Boom, influential precursors such as Juan Rulfo, Alejo Carpentier, and above all Jorge Luis Borges were also rediscovered. Contemporary literature in the region is vibrant and varied, ranging from the best-selling Paulo Coelho and Isabel Allende to the more avant-garde and critically acclaimed work of writers such as Diamela Eltit, Giannina Braschi, Ricardo Piglia, or Roberto Bolaño. There has also been considerable attention paid to the genre of testimonio, texts produced in collaboration with subaltern subjects such as Rigoberta Menchú. Finally, a new breed of chroniclers is represented by the more journalistic Carlos Monsiváis and Pedro Lemebel. The region boasts six Nobel Prize winners: in addition to the two Chilean poets Gabriela Mistral (1945) and Pablo Neruda (1971), there is also the Guatemalan novelist Miguel Angel Asturias (1967), the Colombian writer Gabriel García Márquez (1982), the Mexican poet and essayist Octavio Paz (1990), and the Peruvian novelist Mario Vargas Llosa (2010).
Latin America has produced many successful worldwide artists in terms of recorded global music sales. Among the most successful have been Juan Gabriel (Mexico) only Latin American musician to have sold over 200 million records worldwide, Gloria Estefan (Cuba), Carlos Santana, Luis Miguel (Mexico) of whom have sold over 90 million records, Shakira (Colombia) and Vicente Fernández (Mexico) with over 50 million records sold worldwide. Enrique Iglesias, although not a Latin American, has also contributed for the success of Latin music. Other notable successful mainstream acts through the years, include RBD, Celia Cruz, Soda Stereo, Thalía, Ricky Martin, Maná, Marc Anthony, Ricardo Arjona, Selena, and Menudo. Caribbean Hispanic music, such as merengue, bachata, salsa, and more recently reggaeton, from such countries as the Dominican Republic, Puerto Rico, Trinidad and Tobago, Cuba, and Panama, has been strongly influenced by African rhythms and melodies. Haiti's compas is a genre of music that is influenced by its Caribbean Hispanic counterparts, along with elements of jazz and modern sounds. Another well-known Latin American musical genre includes the Argentine and Uruguayan tango (with Carlos Gardel as the greatest exponent), as well as the distinct nuevo tango, a fusion of tango, acoustic and electronic music popularized by bandoneón virtuoso Ástor Piazzolla. Samba, North American jazz, European classical music and choro combined to form bossa nova in Brazil, popularized by guitarist João Gilberto with singer Astrud Gilberto and pianist Antonio Carlos Jobim. Other influential Latin American sounds include the Antillean soca and calypso, the Honduran (Garifuna) punta, the Colombian cumbia and vallenato, the Chilean cueca, the Ecuadorian boleros, and rockoleras, the Mexican ranchera and the mariachi which is the epitome of Mexican soul, the Nicaraguan palo de Mayo, the Peruvian marinera and tondero, the Uruguayan candombe, the French Antillean zouk (derived from Haitian compas) and the various styles of music from pre-Columbian traditions that are widespread in the Andean region. The classical composer Heitor Villa-Lobos (1887–1959) worked on the recording of native musical traditions within his homeland of Brazil. The traditions of his homeland heavily influenced his classical works. Also notable is the recent work of the Cuban Leo Brouwer and guitar work of the Venezuelan Antonio Lauro and the Paraguayan Agustín Barrios. Latin America has also produced world-class classical performers such as the Chilean pianist Claudio Arrau, Brazilian pianist Nelson Freire and the Argentine pianist and conductor Daniel Barenboim. Brazilian opera soprano Bidu Sayão, one of Brazil's most famous musicians, was a leading artist of the Metropolitan Opera in New York City from 1937 to 1952. Arguably, the main contribution to music entered through folklore, where the true soul of the Latin American and Caribbean countries is expressed. Musicians such as Yma Súmac, Chabuca Granda, Atahualpa Yupanqui, Violeta Parra, Víctor Jara, Jorge Cafrune, Facundo Cabral, Mercedes Sosa, Jorge Negrete, Luiz Gonzaga, Caetano Veloso, Susana Baca, Chavela Vargas, Simon Diaz, Julio Jaramillo, Toto la Momposina, Gilberto Gil, Maria Bethânia, Nana Caymmi, Nara Leão, Gal Costa, Ney Matogrosso as well as musical ensembles such as Inti Illimani and Los Kjarkas are magnificent examples of the heights that this soul can reach. Latin pop, including many forms of rock, is popular in Latin America today (see Spanish language rock and roll). A few examples are Café Tacuba, Soda Stereo, Maná, Rita Lee, Mutantes, Secos e Molhados Legião Urbana, Titãs, Paralamas do Sucesso, Cazuza, Barão Vermelho, Skank, Miranda!, Cansei de Ser Sexy or CSS, and Bajo Fondo. More recently, reggaeton, which blends Jamaican reggae and dancehall with Latin America genres such as bomba and plena, as well as hip hop, is becoming more popular, in spite of the controversy surrounding its lyrics, dance steps (Perreo) and music videos. It has become very popular among populations with a "migrant culture" influence – both Latino populations in the United States, such as southern Florida and New York City, and parts of Latin America where migration to the United States is common, such as Trinidad and Tobago, Dominican Republic, Colombia, Ecuador, El Salvador, and Mexico.
The following is a list of the ten countries with the most World Heritage Sites in Latin America.
IDB Education Initiative Latin American Network Information Center Latin America Data Base Washington Office on Latin America Council on Hemispheric Affairs Codigos De Barra Infolatam. Information and analysis of Latin America at the Library of Congress Web Archives (archived September 8, 2008) Map of Land Cover: Latin America and Caribbean (FAO) Lessons From Latin America by Benjamin Dangl, The Nation, March 4, 2009 Keeping Latin America on the World News Agenda – Interview with Michael Reid of The Economist at the Wayback Machine (archived June 24, 2010) Cold War in Latin America, CSU Pomona University at Archive.today (archived December 14, 2012) Latin America Cold War Resources, Yale University Latin America Cold War, Harvard University http://larc.ucalgary.ca/ Latin American Research Centre, University of Calgary The war on Democracy, by John Pilger
North America is a continent entirely within the Northern Hemisphere and almost all within the Western Hemisphere. It can also be described as a northern subcontinent of the Americas. It is bordered to the north by the Arctic Ocean, to the east by the Atlantic Ocean, to the southeast by South America and the Caribbean Sea, and to the west and south by the Pacific Ocean. North America covers an area of about 24,709,000 square kilometers (9,540,000 square miles), about 16.5% of the Earth's land area and about 4.8% of its total surface. North America is the third-largest continent by area, following Asia and Africa, and the fourth by population after Asia, Africa, and Europe. In 2013, its population was estimated at nearly 579 million people in 23 independent states, or about 7.5% of the world's population, if nearby islands (most notably around the Caribbean) are included. North America was reached by its first human populations during the last glacial period, via crossing the Bering land bridge approximately 40,000 to 17,000 years ago. The so-called Paleo-Indian period is taken to have lasted until about 10,000 years ago (the beginning of the Archaic or Meso-Indian period). The classic stage spans roughly the 6th to 13th centuries. The pre-Columbian era ended in 1492, with the beginning of the transatlantic migrations of European settlers during the Age of Discovery and the early modern period. Present-day cultural and ethnic patterns reflect interactions between European colonists, indigenous peoples, African slaves, immigrants, and the descendants of these groups. Owing to Europe's colonization of the Americas, most North Americans speak European languages such as English, Spanish or French, and their states' cultures commonly reflect Western traditions.
The Americas are usually accepted as having been named after the Italian explorer Amerigo Vespucci by the German cartographers Martin Waldseemüller and Matthias Ringmann. Vespucci, who explored South America between 1497 and 1502, was the first European to suggest that the Americas were not the East Indies, but a different landmass previously unknown by Europeans. In 1507, Waldseemüller produced a world map, in which he placed the word "America" on the continent of South America, in the middle of what is today Brazil. He explained the rationale for the name in the accompanying book Cosmographiae Introductio: ... ab Americo inventore ... quasi Americi terram sive Americam (from Americus the discoverer ... as if it were the land of Americus, thus America). For Waldseemüller, no one should object to the naming of the land after its discoverer. He used the Latinized version of Vespucci's name (Americus Vespucius), but in its feminine form "America", following the examples of "Europa", "Asia" and "Africa". Later, other mapmakers extended the name America to the northern continent. In 1538, Gerard Mercator used America on his map of the world for all the Western Hemisphere.Some argue that because the convention is to use the surname for naming discoveries (except in the case of royalty), the derivation from "Amerigo Vespucci" could be put in question. In 1874, Thomas Belt proposed a derivation from the Amerrique mountains of Central America; the next year, Jules Marcou suggested that the name of the mountain range stemmed from indigenous American languages. Marcou corresponded with Augustus Le Plongeon, who wrote: "The name AMERICA or AMERRIQUE in the Mayan language means, a country of perpetually strong wind, or the Land of the Wind, and ... the [suffixes] can mean ... a spirit that breathes, life itself."Mercator on his map called North America "America or New India" (America sive India Nova).
The United Nations formally recognizes "North America" as comprising three areas: Northern America, Central America, and The Caribbean. This has been formally defined by the UN Statistics Division."Northern America", as a term distinct from "North America", excludes Central America, which itself may or may not include Mexico (see Central America § Different definitions). In the limited context of the North American Free Trade Agreement, the term covers Canada, the United States, and Mexico, which are the three signatories of that treaty. France, Italy, Portugal, Spain, Romania, Greece, and the countries of Latin America use a six-continent model, with the Americas viewed as a single continent and North America designating a subcontinent comprising Canada, the United States, and Mexico, and often Greenland, Saint Pierre et Miquelon, and Bermuda.North America has been historically referred to by other names. Spanish North America (New Spain) was often referred to as Northern America, and this was the first official name given to Mexico.
Geographically the North American continent has many regions and subregions. These include cultural, economic, and geographic regions. Economic regions included those formed by trade blocs, such as the North American Trade Agreement bloc and Central American Trade Agreement. Linguistically and culturally, the continent could be divided into Anglo-America and Latin America. Anglo-America includes most of Northern America, Belize, and Caribbean islands with English-speaking populations (though sub-national entities, such as Louisiana and Quebec, have large Francophone populations; in Quebec, French is the sole official language). The southern North American continent is composed of two regions. These are Central America and the Caribbean. The north of the continent maintains recognized regions as well. In contrast to the common definition of "North America", which encompasses the whole continent, the term "North America" is sometimes used to refer only to Mexico, Canada, the United States, and Greenland.The term Northern America refers to the northernmost countries and territories of North America: the United States, Bermuda, St. Pierre and Miquelon, Canada and Greenland. Although the term does not refer to a unified region, Middle America—not to be confused with the Midwestern United States—groups the regions of Mexico, Central America, and the Caribbean.The largest countries of the continent, Canada and the United States, also contain well-defined and recognized regions. In the case of Canada these are (from east to west) Atlantic Canada, Central Canada, Canadian Prairies, the British Columbia Coast, and Northern Canada. These regions also contain many subregions. In the case of the United States – and in accordance with the US Census Bureau definitions – these regions are: New England, Mid-Atlantic, South Atlantic States, East North Central States, West North Central States, East South Central States, West South Central States, Mountain States, and Pacific States. Regions shared between both nations included the Great Lakes Region. Megalopolises have formed between both nations in the case of the Pacific Northwest and the Great Lakes Megaregion.
North America occupies the northern portion of the landmass generally referred to as the New World, the Western Hemisphere, the Americas, or simply America (which, in many countries is considered as a single continent with North America a subcontinent). North America is the third-largest continent by area, following Asia and Africa. North America's only land connection to South America is at the Isthmus of Darian/Isthmus of Panama. The continent is delimited on the southeast by most geographers at the Darién watershed along the Colombia-Panama border, placing almost all of Panama within North America. Alternatively, some geologists physiographically locate its southern limit at the Isthmus of Tehuantepec, Mexico, with Central America extending southeastward to South America from this point. The Caribbean islands, or West Indies, are considered part of North America. The continental coastline is long and irregular. The Gulf of Mexico is the largest body of water indenting the continent, followed by Hudson Bay. Others include the Gulf of Saint Lawrence and the Gulf of California. Before the Central American isthmus formed, the region had been underwater. The islands of the West Indies delineate a submerged former land bridge, which had connected North and South America via what are now Florida and Venezuela. There are numerous islands off the continent's coasts; principally, the Arctic Archipelago, the Bahamas, Turks & Caicos, the Greater and Lesser Antilles, the Aleutian Islands (some of which are in the Eastern Hemisphere proper), the Alexander Archipelago, the many thousand islands of the British Columbia Coast, and Newfoundland. Greenland, a self-governing Danish island, and the world's largest, is on the same tectonic plate (the North American Plate) and is part of North America geographically. In a geologic sense, Bermuda is not part of the Americas, but an oceanic island which was formed on the fissure of the Mid-Atlantic Ridge over 100 million years ago. The nearest landmass to it is Cape Hatteras, North Carolina. However, Bermuda is often thought of as part of North America, especially given its historical, political and cultural ties to Virginia and other parts of the continent. The vast majority of North America is on the North American Plate. Parts of western Mexico, including Baja California, and of California, including the cities of San Diego, Los Angeles, and Santa Cruz, lie on the eastern edge of the Pacific Plate, with the two plates meeting along the San Andreas fault. The southernmost portion of the continent and much of the West Indies lie on the Caribbean Plate, whereas the Juan de Fuca and Cocos plates border the North American Plate on its western frontier. The continent can be divided into four great regions (each of which contains many subregions): the Great Plains stretching from the Gulf of Mexico to the Canadian Arctic; the geologically young, mountainous west, including the Rocky Mountains, the Great Basin, California and Alaska; the raised but relatively flat plateau of the Canadian Shield in the northeast; and the varied eastern region, which includes the Appalachian Mountains, the coastal plain along the Atlantic seaboard, and the Florida peninsula. Mexico, with its long plateaus and cordilleras, falls largely in the western region, although the eastern coastal plain does extend south along the Gulf. The western mountains are split in the middle into the main range of the Rockies and the coast ranges in California, Oregon, Washington, and British Columbia, with the Great Basin—a lower area containing smaller ranges and low-lying deserts—in between. The highest peak is Denali in Alaska. The United States Geographical Survey (USGS) states that the geographic center of North America is "6 miles [10 km] west of Balta, Pierce County, North Dakota" at about 48°10′N 100°10′W, about 24 kilometres (15 mi) from Rugby, North Dakota. The USGS further states that "No marked or monumented point has been established by any government agency as the geographic center of either the 50 States, the conterminous United States, or the North American continent." Nonetheless, there is a 4.6-metre (15 ft) field stone obelisk in Rugby claiming to mark the center. The North American continental pole of inaccessibility is located 1,650 km (1,030 mi) from the nearest coastline, between Allen and Kyle, South Dakota at 43.36°N 101.97°W﻿ / 43.36; -101.97﻿ (Pole of Inaccessibility North America).
Laurentia is an ancient craton which forms the geologic core of North America; it formed between 1.5 and 1.0 billion years ago during the Proterozoic eon. The Canadian Shield is the largest exposure of this craton. From the Late Paleozoic to Early Mesozoic eras, North America was joined with the other modern-day continents as part of the supercontinent Pangaea, with Eurasia to its east. One of the results of the formation of Pangaea was the Appalachian Mountains, which formed some 480 million years ago, making it among the oldest mountain ranges in the world. When Pangaea began to rift around 200 million years ago, North America became part of Laurasia, before it separated from Eurasia as its own continent during the mid-Cretaceous period. The Rockies and other western mountain ranges began forming around this time from a period of mountain building called the Laramide orogeny, between 80 and 55 million years ago. The formation of the Isthmus of Panama that connected the continent to South America arguably occurred approximately 12 to 15 million years ago, and the Great Lakes (as well as many other northern freshwater lakes and rivers) were carved by receding glaciers about 10,000 years ago. North America is the source of much of what humanity knows about geologic time periods. The geographic area that would later become the United States has been the source of more varieties of dinosaurs than any other modern country. According to paleontologist Peter Dodson, this is primarily due to stratigraphy, climate and geography, human resources, and history. Much of the Mesozoic Era is represented by exposed outcrops in the many arid regions of the continent. The most significant Late Jurassic dinosaur-bearing fossil deposit in North America is the Morrison Formation of the western United States.
Geologically, Canada is one of the oldest regions in the world, with more than half of the region consisting of precambrian rocks that have been above sea level since the beginning of the Palaeozoic era. Canada's mineral resources are diverse and extensive. Across the Canadian Shield and in the north there are large iron, nickel, zinc, copper, gold, lead, molybdenum, and uranium reserves. Large diamond concentrations have been recently developed in the Arctic, making Canada one of the world's largest producers. Throughout the Shield there are many mining towns extracting these minerals. The largest, and best known, is Sudbury, Ontario. Sudbury is an exception to the normal process of forming minerals in the Shield since there is significant evidence that the Sudbury Basin is an ancient meteorite impact crater. The nearby, but less known Temagami Magnetic Anomaly has striking similarities to the Sudbury Basin. Its magnetic anomalies are very similar to the Sudbury Basin, and so it could be a second metal-rich impact crater. The Shield is also covered by vast boreal forests that support an important logging industry.
The lower 48 US states can be divided into roughly five physiographic provinces: The American cordillera The Canadian Shield Northern portion of the upper midwestern United States. The stable platform The coastal plain The Appalachian orogenic beltThe geology of Alaska is typical of that of the cordillera, while the major islands of Hawaii consist of Neogene volcanics erupted over a hot spot.
Central America is geologically active with volcanic eruptions and earthquakes occurring from time to time. In 1976 Guatemala was hit by a major earthquake, killing 23,000 people; Managua, the capital of Nicaragua, was devastated by earthquakes in 1931 and 1972, the last one killing about 5,000 people; three earthquakes devastated El Salvador, one in 1986 and two in 2001; one earthquake devastated northern and central Costa Rica in 2009, killing at least 34 people; in Honduras a powerful earthquake killed seven people in 2009. Volcanic eruptions are common in the region. In 1968 the Arenal Volcano, in Costa Rica, erupted and killed 87 people. Fertile soils from weathered volcanic lavas have made it possible to sustain dense populations in the agriculturally productive highland areas. Central America has many mountain ranges; the longest are the Sierra Madre de Chiapas, the Cordillera Isabelia, and the Cordillera de Talamanca. Between the mountain ranges lie fertile valleys that are suitable for the people; in fact, most of the population of Honduras, Costa Rica, and Guatemala live in valleys. Valleys are also suitable for the production of coffee, beans, and other crops.
North America is a very large continent which surpasses the Arctic Circle, and the Tropic of Cancer. Greenland, along with the Canadian Shield, is tundra with average temperatures ranging from 10 to 20 °C (50 to 68 °F), but central Greenland is composed of a very large ice sheet. This tundra radiates throughout Canada, but its border ends near the Rocky Mountains (but still contains Alaska) and at the end of the Canadian Shield, near the Great Lakes. Climate west of the Cascades is described as being a temperate weather with average precipitation 20 inches (510 mm). Climate in coastal California is described to be Mediterranean, with average temperatures in cities like San Francisco ranging from 57 to 70 °F (14 to 21 °C) over the course of the year.Stretching from the East Coast to eastern North Dakota, and stretching down to Kansas, is the continental-humid climate featuring intense seasons, with a large amount of annual precipitation, with places like New York City averaging 50 inches (1,300 mm). Starting at the southern border of the continental-humid climate and stretching to the Gulf of Mexico (whilst encompassing the eastern half of Texas) is the subtropical climate. This area has the wettest cities in the contiguous U.S. with annual precipitation reaching 67 inches (1,700 mm) in Mobile, Alabama. Stretching from the borders of the continental humid and subtropical climates, and going west to the Cascades Sierra Nevada, south to the southern tip of durango, north to the border with tundra climate, the steppe/desert climate is the driest climate in the U.S. Highland climates cut from north to south of the continent, where subtropical or temperate climates occur just below the tropics, as in central Mexico and Guatemala. Tropical climates appear in the island regions and in the subcontinent's bottleneck. Usually of the savannah type, with rains and high temperatures constants the whole year. Found in countries and states bathed by the Caribbean Sea or to south of the Gulf of Mexico and Pacific Ocean.
Notable North American fauna include the bison, black bear, prairie dog, turkey, pronghorn, raccoon, coyote and monarch butterfly. Notable plants that were domesticated in North America include tobacco, maize, squash, tomato, sunflower, blueberry, avocado, cotton, chile pepper and vanilla.
The indigenous peoples of the Americas have many creation myths by which they assert that they have been present on the land since its creation, but there is no evidence that humans evolved there. The specifics of the initial settlement of the Americas by ancient Asians are subject to ongoing research and discussion. The traditional theory has been that hunters entered the Beringia land bridge between eastern Siberia and present-day Alaska from 27,000 to 14,000 years ago. A growing viewpoint is that the first American inhabitants sailed from Beringia some 13,000 years ago, with widespread habitation of the Americas during the end of the Last Glacial Period, in what is known as the Late Glacial Maximum, around 12,500 years ago. The oldest petroglyphs in North America date from 15,000 to 10,000 years before present. Genetic research and anthropology indicate additional waves of migration from Asia via the Bering Strait during the Early-Middle Holocene.Before contact with Europeans, the natives of North America were divided into many different polities, from small bands of a few families to large empires. They lived in several "culture areas", which roughly correspond to geographic and biological zones and give a good indication of the main way of life of the people who lived there (e.g., the bison hunters of the Great Plains, or the farmers of Mesoamerica). Native groups can also be classified by their language family (e.g., Athapascan or Uto-Aztecan). Peoples with similar languages did not always share the same material culture, nor were they always allies. Anthropologists think that the Inuit people of the high Arctic came to North America much later than other native groups, as evidenced by the disappearance of Dorset culture artifacts from the archaeological record, and their replacement by the Thule people. During the thousands of years of native habitation on the continent, cultures changed and shifted. One of the oldest yet discovered is the Clovis culture (c. 9550–9050 BCE) in modern New Mexico. Later groups include the Mississippian culture and related Mound building cultures, found in the Mississippi river valley and the Pueblo culture of what is now the Four Corners. The more southern cultural groups of North America were responsible for the domestication of many common crops now used around the world, such as tomatoes, squash, and maize. As a result of the development of agriculture in the south, many other cultural advances were made there. The Mayans developed a writing system, built huge pyramids and temples, had a complex calendar, and developed the concept of zero around 400 CE.The first recorded European references to North America are in Norse sagas where it is referred to as Vinland. The earliest verifiable instance of pre-Columbian trans-oceanic contact by any European culture with the North America mainland has been dated to around 1000 CE. The site, situated at the northernmost extent of the island named Newfoundland, has provided unmistakable evidence of Norse settlement. Norse explorer Leif Erikson (c. 970–1020 CE) is thought to have visited the area. Erikson was the first European to make landfall on the continent (excluding Greenland). The Mayan culture was still present in southern Mexico and Guatemala when the Spanish conquistadors arrived, but political dominance in the area had shifted to the Aztec Empire, whose capital city Tenochtitlan was located further north in the Valley of Mexico. The Aztecs were conquered in 1521 by Hernán Cortés.
During the Age of Discovery, Europeans explored and staked claims to various parts of North America. Upon their arrival in the "New World", the Native American population declined substantially, because of violent conflicts with the invaders and the introduction of European diseases to which the Native Americans lacked immunity. Native culture changed drastically and their affiliation with political and cultural groups also changed. Several linguistic groups died out, and others changed quite quickly. In 1513, Juan Ponce de León, who had accompanied Columbus's second voyage, visited and named La Florida. As the colonial period unfolded, Britain, Spain, and France took over extensive territories in North America. In the late 18th and early 19th century, independence movements sprung up across the continent, leading to the founding of the modern countries in the area. The 13 British Colonies on the North Atlantic coast declared independence in 1776, becoming the United States of America. Canada was formed from the unification of northern territories controlled by Britain and France. New Spain, a territory that stretched from the modern-day southern US to Central America, declared independence in 1810, becoming the First Mexican Empire. In 1823 the former Captaincy General of Guatemala, then part of the Mexican Empire, became the first independent state in Central America, officially changing its name to the United Provinces of Central America. Over three decades of work on the Panama Canal led to the connection of Atlantic and Pacific waters in 1913, physically making North America a separate continent.
Economically, Canada and the United States are the wealthiest and most developed nations in the continent, followed by Mexico, a newly industrialized country. The countries of Central America and the Caribbean are at various levels of economic and human development. For example, small Caribbean island-nations, such as Barbados, Trinidad and Tobago, and Antigua and Barbuda, have a higher GDP (PPP) per capita than Mexico due to their smaller populations. Panama and Costa Rica have a significantly higher Human Development Index and GDP than the rest of the Central American nations. Additionally, despite Greenland's vast resources in oil and minerals, much of them remain untapped, and the island is economically dependent on fishing, tourism, and subsidies from Denmark. Nevertheless, the island is highly developed.Demographically, North America is ethnically diverse. Its three main groups are Caucasians, Mestizos and Blacks. There is a significant minority of Indigenous Americans and Asians among other less numerous groups.
The dominant languages in North America are English, Spanish, and French. Danish is prevalent in Greenland alongside Greenlandic, and Dutch is spoken side by side local languages in the Dutch Caribbean. The term Anglo-America is used to refer to the anglophone countries of the Americas: namely Canada (where English and French are co-official) and the United States, but also sometimes Belize and parts of the tropics, especially the Commonwealth Caribbean. Latin America refers to the other areas of the Americas (generally south of the United States) where the Romance languages, derived from Latin, of Spanish and Portuguese (but French speaking countries are not usually included) predominate: the other republics of Central America (but not always Belize), part of the Caribbean (not the Dutch-, English-, or French-speaking areas), Mexico, and most of South America (except Guyana, Suriname, French Guiana (France), and the Falkland Islands (UK)). The French language has historically played a significant role in North America and now retains a distinctive presence in some regions. Canada is officially bilingual. French is the official language of the Province of Quebec, where 95% of the people speak it as either their first or second language, and it is co-official with English in the Province of New Brunswick. Other French-speaking locales include the Province of Ontario (the official language is English, but there are an estimated 600,000 Franco-Ontarians), the Province of Manitoba (co-official as de jure with English), the French West Indies and Saint-Pierre et Miquelon, as well as the US state of Louisiana, where French is also an official language. Haiti is included with this group based on historical association but Haitians speak both Creole and French. Similarly, French and French Antillean Creole is spoken in Saint Lucia and the Commonwealth of Dominica alongside English. A significant number of Indigenous languages are spoken in North America, with 372,000 people in the United States speaking an indigenous language at home, about 225,000 in Canada and roughly 6 million in Mexico. In the United States and Canada, there are approximately 150 surviving indigenous languages of the 300 spoken prior to European contact.
Christianity is the largest religion in the United States, Canada and Mexico. According to a 2012 Pew Research Center survey, 77% of the population considered themselves Christians. Christianity also is the predominant religion in the 23 dependent territories in North America. The United States has the largest Christian population in the world, with nearly 247 million Christians (70%), although other countries have higher percentages of Christians among their populations. Mexico has the world's second largest number of Catholics, surpassed only by Brazil. A 2015 study estimates about 493,000 Christian believers from a Muslim background in North America, most of them belonging to some form of Protestantism.According to the same study religiously unaffiliated (include agnostic and atheist) make up about 17% of the population of Canada and the United States. No religion make up about 24% of the United States population, and 24% of Canada total population.Canada, the United States and Mexico host communities of both Jews (6 million or about 1.8%), Buddhists (3.8 million or 1.1%) and Muslims (3.4 million or 1.0%). The biggest number of Jewish individuals can be found in the United States (5.4 million), Canada (375,000) and Mexico (67,476). The United States host the largest Muslim population in North America with 2.7 million or 0.9%, While Canada host about one million Muslim or 3.2% of the population. While in Mexico there were 3,700 Muslims in the country. In 2012, U-T San Diego estimated U.S. practitioners of Buddhism at 1.2 million people, of whom 40% are living in Southern California.The predominant religion in Central America is Christianity (96%). Beginning with the Spanish colonization of Central America in the 16th century, Roman Catholicism became the most popular religion in the region until the first half of the 20th century. Since the 1960s, there has been an increase in other Christian groups, particularly Protestantism, as well as other religious organizations, and individuals identifying themselves as having no religion. Also Christianity is the predominant religion in the Caribbean (85%). Other religious groups in the region are Hinduism, Islam, Rastafari (in Jamaica), and Afro-American religions such as Santería and Vodou.
North America is the fourth most populous continent after Asia, Africa, and Europe. Its most populous country is the United States with 329.7 million persons. The second largest country is Mexico with a population of 112.3 million. Canada is the third most populous country with 37.0 million. The majority of Caribbean island-nations have national populations under a million, though Cuba, Dominican Republic, Haiti, Puerto Rico (a territory of the United States), Jamaica, and Trinidad and Tobago each have populations higher than a million. Greenland has a small population of 55,984 for its massive size (2,166,000 km2 or 836,300 mi2), and therefore, it has the world's lowest population density at 0.026 pop./km2 (0.067 pop./mi2).While the United States, Canada, and Mexico maintain the largest populations, large city populations are not restricted to those nations. There are also large cities in the Caribbean. The largest cities in North America, by far, are Mexico City and New York. These cities are the only cities on the continent to exceed eight million, and two of three in the Americas. Next in size are Los Angeles, Toronto, Chicago, Havana, Santo Domingo, and Montreal. Cities in the sun belt regions of the United States, such as those in Southern California and Houston, Phoenix, Miami, Atlanta, and Las Vegas, are experiencing rapid growth. These causes included warm temperatures, retirement of Baby Boomers, large industry, and the influx of immigrants. Cities near the United States border, particularly in Mexico, are also experiencing large amounts of growth. Most notable is Tijuana, a city bordering San Diego that receives immigrants from all over Latin America and parts of Europe and Asia. Yet as cities grow in these warmer regions of North America, they are increasingly forced to deal with the major issue of water shortages.Eight of the top ten metropolitan areas are located in the United States. These metropolitan areas all have a population of above 5.5 million and include the New York City metropolitan area, Los Angeles metropolitan area, Chicago metropolitan area, and the Dallas–Fort Worth metroplex. Whilst the majority of the largest metropolitan areas are within the United States, Mexico is host to the largest metropolitan area by population in North America: Greater Mexico City. Canada also breaks into the top ten largest metropolitan areas with the Toronto metropolitan area having six million people. The proximity of cities to each other on the Canada–United States border and Mexico–United States border has led to the rise of international metropolitan areas. These urban agglomerations are observed at their largest and most productive in Detroit–Windsor and San Diego–Tijuana and experience large commercial, economic, and cultural activity. The metropolitan areas are responsible for millions of dollars of trade dependent on international freight. In Detroit-Windsor the Border Transportation Partnership study in 2004 concluded US$13 billion was dependent on the Detroit–Windsor international border crossing while in San Diego-Tijuana freight at the Otay Mesa Port of Entry was valued at US$20 billion.North America has also been witness to the growth of megapolitan areas. In the United States exists eleven megaregions that transcend international borders and comprise Canadian and Mexican metropolitan regions. These are the Arizona Sun Corridor, Cascadia, Florida, Front Range, Great Lakes Megaregion, Gulf Coast Megaregion, Northeast, Northern California, Piedmont Atlantic, Southern California, and the Texas Triangle. Canada and Mexico are also the home of megaregions. These include the Quebec City – Windsor Corridor, Golden Horseshoe – both of which are considered part of the Great Lakes Megaregion – and megalopolis of Central Mexico. Traditionally the largest megaregion has been considered the Boston-Washington, DC Corridor, or the Northeast, as the region is one massive contiguous area. Yet megaregion criterion have allowed the Great Lakes Megalopolis to maintain status as the most populated region, being home to 53,768,125 people in 2000.The top ten largest North American metropolitan areas by population as of 2013, based on national census numbers from the United States and census estimates from Canada and Mexico. †2011 Census figures.
North America's GDP per capita was evaluated in October 2016 by the International Monetary Fund (IMF) to be $41,830, making it the richest continent in the world, followed by Oceania.Canada, Mexico, and the United States have significant and multifaceted economic systems. The United States has the largest economy of all three countries and in the world. In 2016, the U.S. had an estimated per capita gross domestic product (PPP) of $57,466 according to the World Bank, and is the most technologically developed economy of the three. The United States' services sector comprises 77% of the country's GDP (estimated in 2010), industry comprises 22% and agriculture comprises 1.2%. The U.S. economy is also the fastest growing economy in North America and the Americas as a whole, with the highest GDP per capita in the Americas as well. Canada shows significant growth in the sectors of services, mining and manufacturing. Canada's per capita GDP (PPP) was estimated at $44,656 and it had the 11th largest GDP (nominal) in 2014. Canada's services sector comprises 78% of the country's GDP (estimated in 2010), industry comprises 20% and agriculture comprises 2%. Mexico has a per capita GDP (PPP) of $16,111 and as of 2014 is the 15th largest GDP (nominal) in the world. Being a newly industrialized country, Mexico maintains both modern and outdated industrial and agricultural facilities and operations. Its main sources of income are oil, industrial exports, manufactured goods, electronics, heavy industry, automobiles, construction, food, banking and financial services.The North American economy is well defined and structured in three main economic areas. These areas are the North American Free Trade Agreement (NAFTA), Caribbean Community and Common Market (CARICOM), and the Central American Common Market (CACM). Of these trade blocs, the United States takes part in two. In addition to the larger trade blocs there is the Canada-Costa Rica Free Trade Agreement among numerous other free trade relations, often between the larger, more developed countries and Central American and Caribbean countries. The North America Free Trade Agreement (NAFTA) forms one of the four largest trade blocs in the world. Its implementation in 1994 was designed for economic homogenization with hopes of eliminating barriers of trade and foreign investment between Canada, the United States and Mexico. While Canada and the United States already conducted the largest bilateral trade relationship – and to present day still do – in the world and Canada–United States trade relations already allowed trade without national taxes and tariffs, NAFTA allowed Mexico to experience a similar duty-free trade. The free trade agreement allowed for the elimination of tariffs that had previously been in place on United States-Mexico trade. Trade volume has steadily increased annually and in 2010, surface trade between the three NAFTA nations reached an all-time historical increase of 24.3% or US$791 billion. The NAFTA trade bloc GDP (PPP) is the world's largest with US$17.617 trillion. This is in part attributed to the fact that the economy of the United States is the world's largest national economy; the country had a nominal GDP of approximately $14.7 trillion in 2010. The countries of NAFTA are also some of each other's largest trade partners. The United States is the largest trade partner of Canada and Mexico; while Canada and Mexico are each other's third largest trade partners. The Caribbean trade bloc – CARICOM – came into agreement in 1973 when it was signed by 15 Caribbean nations. As of 2000, CARICOM trade volume was US$96 billion. CARICOM also allowed for the creation of a common passport for associated nations. In the past decade the trade bloc focused largely on Free Trade Agreements and under the CARICOM Office of Trade Negotiations (OTN) free trade agreements have been signed into effect. Integration of Central American economies occurred under the signing of the Central American Common Market agreement in 1961; this was the first attempt to engage the nations of this area into stronger financial cooperation. Recent implementation of the Central American Free Trade Agreement (CAFTA) has left the future of the CACM unclear. The Central American Free Trade Agreement was signed by five Central American countries, the Dominican Republic, and the United States. The focal point of CAFTA is to create a free trade area similar to that of NAFTA. In addition to the United States, Canada also has relations in Central American trade blocs. Currently under proposal, the Canada – Central American Free Trade Agreement (CA4) would operate much the same as CAFTA with the United States does. These nations also take part in inter-continental trade blocs. Mexico takes a part in the G3 Free Trade Agreement with Colombia and Venezuela and has a trade agreement with the EU. The United States has proposed and maintained trade agreements under the Transatlantic Free Trade Area between itself and the European Union; the US-Middle East Free Trade Area between numerous Middle Eastern nations and itself; and the Trans-Pacific Strategic Economic Partnership between Southeast Asian nations, Australia, and New Zealand.
The Pan-American Highway route in the Americas is the portion of a network of roads nearly 48,000 km (30,000 mi) in length which travels through the mainland nations. No definitive length of the Pan-American Highway exists because the US and Canadian governments have never officially defined any specific routes as being part of the Pan-American Highway, and Mexico officially has many branches connecting to the US border. However, the total length of the portion from Mexico to the northern extremity of the highway is roughly 26,000 km (16,000 mi). The First Transcontinental Railroad in the United States was built in the 1860s, linking the railroad network of the eastern US with California on the Pacific coast. Finished on 10 May 1869 at the famous golden spike event at Promontory Summit, Utah, it created a nationwide mechanized transportation network that revolutionized the population and economy of the American West, catalyzing the transition from the wagon trains of previous decades to a modern transportation system. Although an accomplishment, it achieved the status of first transcontinental railroad by connecting myriad eastern US railroads to the Pacific and was not the largest single railroad system in the world. The Canadian Grand Trunk Railway (GTR) had, by 1867, already accumulated more than 2,055 km (1,277 mi) of track by connecting Ontario with the Canadian Atlantic provinces west as far as Port Huron, Michigan, through Sarnia, Ontario.
A shared telephone system known as the North American Numbering Plan (NANP) is an integrated telephone numbering plan of 24 countries and territories: the United States and its territories, Canada, Bermuda, and 17 Caribbean nations.
Canada and the United States were both former British colonies. There is frequent cultural interplay between the United States and English-speaking Canada. Greenland has experienced many immigration waves from Northern Canada, e.g. the Thule People. Therefore, Greenland shares some cultural ties with the indigenous people of Canada. Greenland is also considered Nordic and has strong Danish ties due to centuries of colonization by Denmark.Spanish-speaking North America shares a common past as former Spanish colonies. In Mexico and the Central American countries where civilizations like the Maya developed, indigenous people preserve traditions across modern boundaries. Central American and Spanish-speaking Caribbean nations have historically had more in common due to geographical proximity. Northern Mexico, particularly in the cities of Monterrey, Tijuana, Ciudad Juárez, and Mexicali, is strongly influenced by the culture and way of life of the United States. Of the aforementioned cities, Monterrey has been regarded as the most Americanized city in Mexico. Immigration to the United States and Canada remains a significant attribute of many nations close to the southern border of the US. The Anglophone Caribbean states have witnessed the decline of the British Empire and its influence on the region, and its replacement by the economic influence of Northern America in the Anglophone Caribbean. This is partly due to the relatively small populations of the English-speaking Caribbean countries, and also because many of them now have more people living abroad than those remaining at home. Northern Mexico, the Western United States and Alberta, Canada share a cowboy culture.
Canada, Mexico and the US submitted a joint bid to host the 2026 FIFA World Cup. The following table shows the most prominent sports leagues in North America, in order of average revenue.
Outline of North America Flags of North America List of cities in North America Table manners in North America Turtle Island (Native American folklore) – Name for North America among Native Americans
Footnotes Citations
Houghton Mifflin Company, "North America" Interactive SVG version of Non-Native American Nations Control over N America 1750–2008 animation
Central America (Spanish: América Central, pronounced [aˈmeɾika senˈtɾal] (listen), Centroamérica pronounced [sentɾoaˈmeɾika] (listen)) is sometimes defined as a subregion of the Americas. This region is bordered by Mexico to the north, Colombia to the southeast, the Caribbean Sea to the east and the Pacific Ocean to the west and south. Central America consists of seven countries: El Salvador, Costa Rica, Belize, Guatemala, Honduras, Nicaragua and Panama. The combined population of Central America is estimated at 44.53 million (2016).Central America is a part of the Mesoamerican biodiversity hotspot, which extends from northern Guatemala to central Panama. Due to the presence of several active geologic faults and the Central America Volcanic Arc, there is a great deal of seismic activity in the region, such as volcanic eruptions and earthquakes, which has resulted in death, injury and property damage. In the Pre-Columbian era, Central America was inhabited by the indigenous peoples of Mesoamerica to the north and west and the Isthmo-Colombian peoples to the south and east. Following the Spanish expedition of Christopher Columbus' voyages to the Americas, Spain began to colonize the Americas. From 1609 to 1821, the majority of Central American territories (except for what would become Belize and Panama, and including the modern Mexican state of Chiapas) were governed by the viceroyalty of New Spain from Mexico City as the Captaincy General of Guatemala. On 24 August 1821, Spanish Viceroy Juan de O'Donojú signed the Treaty of Córdoba, which established New Spain's independence from Spain. On 15 September 1821, the Act of Independence of Central America was enacted to announce Central America's separation from the Spanish Empire and provide for the establishment of a new Central American state. Some of New Spain's provinces in the Central American region (i.e. what would become Guatemala, Honduras, El Salvador, Nicaragua and Costa Rica) were annexed to the First Mexican Empire; however, in 1823 they seceded from Mexico to form the Federal Republic of Central America until 1838. In 1838, Nicaragua, Honduras, Costa Rica and Guatemala became the first of Central America's seven states to become independent autonomous countries, followed by El Salvador in 1841, Panama in 1903 and Belize in 1981. Despite the dissolution of the Federal Republic of Central America, there is anecdotal evidence that demonstrates that Salvadorans, Panamanians, Costa Ricans, Guatemalans, Hondurans and Nicaraguans continue to maintain a Central American identity. For instance, Central Americans sometimes refer to their nations as if they were provinces of a Central American state. It is not unusual to write "C.A." after the country's name in formal and informal contexts. Governments in the region sometimes reinforce this sense of belonging to Central America in its citizens. Belizeans are usually identified as culturally West Indian rather than Central American.
"Central America" may mean different things to various people, based upon different contexts: The United Nations geoscheme for the Americas defines the region as all states of mainland North America south of the United States and specifically includes all of Mexico. Middle America is usually thought to comprise Mexico to the north of the 7 states of Central America as well as Colombia and Venezuela to the south. Usually, the whole of the Caribbean to the northeast, and sometimes the Guyanas, are also included. According to one source, the term "Central America" was used as a synonym for "Middle America" at least as recently as 1962. In Ibero-America (Spanish and Portuguese speaking American countries), the Americas is considered a single continent, and Central America is considered a subregion of the Americas comprising the seven countries south of Mexico and north of Colombia. For the people living in the five countries formerly part of the Federal Republic of Central America there is a distinction between the Spanish language terms "América Central" and "Centroamérica". While both can be translated into English as "Central America", "América Central" is generally used to refer to the geographical area of the seven countries between Mexico and Colombia, while "Centroamérica" is used when referring to the former members of the Federation emphasizing the shared culture and history of the region. In Portuguese as a rule and occasionally in Spanish and other languages, the entirety of the Antilles is often included in the definition of Central America. Indeed, the Dominican Republic is a full member of the Central American Integration System.
In the Pre-Columbian era, the northern areas of Central America were inhabited by the indigenous peoples of Mesoamerica. Most notable among these were the Mayans, who had built numerous cities throughout the region, and the Aztecs, who had created a vast empire. The pre-Columbian cultures of eastern El Salvador, eastern Honduras, Caribbean Nicaragua, most of Costa Rica and Panama were predominantly speakers of the Chibchan languages at the time of European contact and are considered by some culturally different and grouped in the Isthmo-Colombian Area. Following the Spanish expedition of Christopher Columbus's voyages to the Americas, the Spanish sent many expeditions to the region, and they began their conquest of Maya territory in 1523. Soon after the conquest of the Aztec Empire, Spanish conquistador Pedro de Alvarado commenced the conquest of northern Central America for the Spanish Empire. Beginning with his arrival in Soconusco in 1523, Alvarado's forces systematically conquered and subjugated most of the major Maya kingdoms, including the K'iche', Tz'utujil, Pipil, and the Kaqchikel. By 1528, the conquest of Guatemala was nearly complete, with only the Petén Basin remaining outside the Spanish sphere of influence. The last independent Maya kingdoms – the Kowoj and the Itza people – were finally defeated in 1697, as part of the Spanish conquest of Petén.In 1538, Spain established the Real Audiencia of Panama, which had jurisdiction over all land from the Strait of Magellan to the Gulf of Fonseca. This entity was dissolved in 1543, and most of the territory within Central America then fell under the jurisdiction of the Audiencia Real de Guatemala. This area included the current territories of Costa Rica, El Salvador, Guatemala, Honduras, Nicaragua, and the Mexican state of Chiapas, but excluded the lands that would become Belize and Panama. The president of the Audiencia, which had its seat in Antigua Guatemala, was the governor of the entire area. In 1609 the area became a captaincy general and the governor was also granted the title of captain general. The Captaincy General of Guatemala encompassed most of Central America, with the exception of present-day Belize and Panama. The Captaincy General of Guatemala lasted for more than two centuries, but began to fray after a rebellion in 1811 which began in the intendancy of San Salvador. The Captaincy General formally ended on 15 September 1821, with the signing of the Act of Independence of Central America. Mexican independence was achieved at virtually the same time with the signing of the Treaty of Córdoba and the Declaration of Independence of the Mexican Empire, and the entire region was finally independent from Spanish authority by 28 September 1821. From its independence from Spain in 1821 until 1823, the former Captaincy General remained intact as part of the short-lived First Mexican Empire. When the Emperor of Mexico abdicated on 19 March 1823, Central America again became independent. On 1 July 1823, the Congress of Central America peacefully seceded from Mexico and declared absolute independence from all foreign nations, and the region formed the Federal Republic of Central America.The Federal Republic of Central America was a representative democracy with its capital at Guatemala City. This union consisted of the provinces of Costa Rica, El Salvador, Guatemala, Honduras, Los Altos, Mosquito Coast, and Nicaragua. The lowlands of southwest Chiapas, including Soconusco, initially belonged to the Republic until 1824, when Mexico annexed most of Chiapas and began its claims to Soconusco. The Republic lasted from 1823 to 1838, when it disintegrated as a result of civil wars. The territory that now makes up Belize was heavily contested in a dispute that continued for decades after Guatemala achieved independence (see History of Belize (1506–1862). Spain, and later Guatemala, considered this land a Guatemalan department. In 1862, Britain formally declared it a British colony and named it British Honduras. It became independent as Belize in 1981.Panama, situated in the southernmost part of Central America on the Isthmus of Panama, has for most of its history been culturally and politically linked to South America. Panama was part of the Province of Tierra Firme from 1510 until 1538 when it came under the jurisdiction of the newly formed Audiencia Real de Panama. Beginning in 1543, Panama was administered as part of the Viceroyalty of Peru, along with all other Spanish possessions in South America. Panama remained as part of the Viceroyalty of Peru until 1739, when it was transferred to the Viceroyalty of New Granada, the capital of which was located at Santa Fé de Bogotá. Panama remained as part of the Viceroyalty of New Granada until the disestablishment of that viceroyalty in 1819. A series of military and political struggles took place from that time until 1822, the result of which produced the republic of Gran Colombia. After the dissolution of Gran Colombia in 1830, Panama became part of a successor state, the Republic of New Granada. From 1855 until 1886, Panama existed as Panama State, first within the Republic of New Granada, then within the Granadine Confederation, and finally within the United States of Colombia. The United States of Colombia was replaced by the Republic of Colombia in 1886. As part of the Republic of Colombia, Panama State was abolished and it became the Isthmus Department. Despite the many political reorganizations, Colombia was still deeply plagued by conflict, which eventually led to the secession of Panama on 3 November 1903. Only after that time did some begin to regard Panama as a North or Central American entity.By the 1930s the United Fruit Company owned 14,000 square kilometres (3.5 million acres) of land in Central America and the Caribbean and was the single largest land owner in Guatemala. Such holdings gave it great power over the governments of small countries. That was one of the factors that led to the coining of the phrase banana republic.After more than two hundred years of social unrest, violent conflict, and revolution, Central America today remains in a period of political transformation. Poverty, social injustice, and violence are still widespread. Nicaragua is the second poorest country in the western hemisphere (only Haiti is poorer).
Central America is a tapering isthmus running from the southern extent of Mexico to the northwestern portion of South America. The Pacific Ocean lies to the southwest, the Caribbean Sea lies to the northeast, and the Gulf of Mexico lies to the north. Some physiographists define the Isthmus of Tehuantepec as the northern geographic border of Central America, while others use the northwestern borders of Belize and Guatemala. From there, the Central American land mass extends southeastward to the Atrato River, where it connects to the Pacific Lowlands in northwestern South America. Of the many mountain ranges within Central America, the longest are the Sierra Madre de Chiapas, the Cordillera Isabelia and the Cordillera de Talamanca. At 4,220 meters (13,850 ft), Volcán Tajumulco is the highest peak in Central America. Other high points of Central America are as listed in the table below: Between the mountain ranges lie fertile valleys that are suitable for the raising of livestock and for the production of coffee, tobacco, beans and other crops. Most of the population of Honduras, Costa Rica and Guatemala lives in valleys.Trade winds have a significant effect upon the climate of Central America. Temperatures in Central America are highest just prior to the summer wet season, and are lowest during the winter dry season, when trade winds contribute to a cooler climate. The highest temperatures occur in April, due to higher levels of sunlight, lower cloud cover and a decrease in trade winds.
Central America is part of the Mesoamerican biodiversity hotspot, boasting 7% of the world's biodiversity. The Pacific Flyway is a major north–south flyway for migratory birds in the Americas, extending from Alaska to Tierra del Fuego. Due to the funnel-like shape of its land mass, migratory birds can be seen in very high concentrations in Central America, especially in the spring and autumn. As a bridge between North America and South America, Central America has many species from the Nearctic and the Neotropical realms. However the southern countries (Costa Rica and Panama) of the region have more biodiversity than the northern countries (Guatemala and Belize), meanwhile the central countries (Honduras, Nicaragua and El Salvador) have the least biodiversity. The table below shows recent statistics: Over 300 species of the region's flora and fauna are threatened, 107 of which are classified as critically endangered. The underlying problems are deforestation, which is estimated by FAO at 1.2% per year in Central America and Mexico combined, fragmentation of rainforests and the fact that 80% of the vegetation in Central America has already been converted to agriculture.Efforts to protect fauna and flora in the region are made by creating ecoregions and nature reserves. 36% of Belize's land territory falls under some form of official protected status, giving Belize one of the most extensive systems of terrestrial protected areas in the Americas. In addition, 13% of Belize's marine territory are also protected. A large coral reef extends from Mexico to Honduras: the Mesoamerican Barrier Reef System. The Belize Barrier Reef is part of this. The Belize Barrier Reef is home to a large diversity of plants and animals, and is one of the most diverse ecosystems of the world. It is home to 70 hard coral species, 36 soft coral species, 500 species of fish and hundreds of invertebrate species. So far only about 10% of the species in the Belize barrier reef have been discovered.
From 2001 to 2010, 5,376 square kilometers (2,076 sq mi) of forest were lost in the region. In 2010 Belize had 63% of remaining forest cover, Costa Rica 46%, Panama 45%, Honduras 41%, Guatemala 37%, Nicaragua 29%, and El Salvador 21%. Most of the loss occurred in the moist forest biome, with 12,201 square kilometers (4,711 sq mi). Woody vegetation loss was partially set off by a gain in the coniferous forest biome with 4,730 square kilometers (1,830 sq mi), and a gain in the dry forest biome at 2,054 square kilometers (793 sq mi). Mangroves and deserts contributed only 1% to the loss in forest vegetation. The bulk of the deforestation was located at the Caribbean slopes of Nicaragua with a loss of 8,574 square kilometers (3,310 sq mi) of forest in the period from 2001 to 2010. The most significant regrowth of 3,050 square kilometers (1,180 sq mi) of forest was seen in the coniferous woody vegetation of Honduras.The Central American pine-oak forests ecoregion, in the tropical and subtropical coniferous forests biome, is found in Central America and southern Mexico. The Central American pine-oak forests occupy an area of 111,400 square kilometers (43,000 sq mi), extending along the mountainous spine of Central America, extending from the Sierra Madre de Chiapas in Mexico's Chiapas state through the highlands of Guatemala, El Salvador, and Honduras to central Nicaragua. The pine-oak forests lie between 600–1,800 metres (2,000–5,900 ft) elevation, and are surrounded at lower elevations by tropical moist forests and tropical dry forests. Higher elevations above 1,800 metres (5,900 ft) are usually covered with Central American montane forests. The Central American pine-oak forests are composed of many species characteristic of temperate North America including oak, pine, fir, and cypress. Laurel forest is the most common type of Central American temperate evergreen cloud forest, found in almost all Central American countries, normally more than 1,000 meters (3,300 ft) above sea level. Tree species include evergreen oaks, members of the laurel family, and species of Weinmannia, Drimys, and Magnolia. The cloud forest of Sierra de las Minas, Guatemala, is the largest in Central America. In some areas of southeastern Honduras there are cloud forests, the largest located near the border with Nicaragua. In Nicaragua, cloud forests are situated near the border with Honduras, but many were cleared to grow coffee. There are still some temperate evergreen hills in the north. The only cloud forest in the Pacific coastal zone of Central America is on the Mombacho volcano in Nicaragua. In Costa Rica, there are laurel forests in the Cordillera de Tilarán and Volcán Arenal, called Monteverde, also in the Cordillera de Talamanca. The Central American montane forests are an ecoregion of the tropical and subtropical moist broadleaf forests biome, as defined by the World Wildlife Fund. These forests are of the moist deciduous and the semi-evergreen seasonal subtype of tropical and subtropical moist broadleaf forests and receive high overall rainfall with a warm summer wet season and a cooler winter dry season. Central American montane forests consist of forest patches located at altitudes ranging from 1,800–4,000 metres (5,900–13,100 ft), on the summits and slopes of the highest mountains in Central America ranging from Southern Mexico, through Guatemala, El Salvador, and Honduras, to northern Nicaragua. The entire ecoregion covers an area of 13,200 square kilometers (5,100 sq mi) and has a temperate climate with relatively high precipitation levels.
Ecoregions are not only established to protect the forests themselves but also because they are habitats for an incomparably rich and often endemic fauna. Almost half of the bird population of the Talamancan montane forests in Costa Rica and Panama are endemic to this region. Several birds are listed as threatened, most notably the resplendent quetzal (Pharomacrus mocinno), three-wattled bellbird (Procnias tricarunculata), bare-necked umbrellabird (Cephalopterus glabricollis), and black guan (Chamaepetes unicolor). Many of the amphibians are endemic and depend on the existence of forest. The golden toad that once inhabited a small region in the Monteverde Reserve, which is part of the Talamancan montane forests, has not been seen alive since 1989 and is listed as extinct by IUCN. The exact causes for its extinction are unknown. Global warming may have played a role, because the development of that frog is typical for this area may have been compromised. Seven small mammals are endemic to the Costa Rica-Chiriqui highlands within the Talamancan montane forest region. Jaguars, cougars, spider monkeys, as well as tapirs, and anteaters live in the woods of Central America. The Central American red brocket is a brocket deer found in Central America's tropical forest.
Central America is geologically very active, with volcanic eruptions and earthquakes occurring frequently, and tsunamis occurring occasionally. Many thousands of people have died as a result of these natural disasters. Most of Central America rests atop the Caribbean Plate. This tectonic plate converges with the Cocos, Nazca, and North American plates to form the Middle America Trench, a major subduction zone. The Middle America Trench is situated some 60–160 kilometers (37–99 mi) off the Pacific coast of Central America and runs roughly parallel to it. Many large earthquakes have occurred as a result of seismic activity at the Middle America Trench. For example, subduction of the Cocos Plate beneath the North American Plate at the Middle America Trench is believed to have caused the 1985 Mexico City earthquake that killed as many as 40,000 people. Seismic activity at the Middle America Trench is also responsible for earthquakes in 1902, 1942, 1956, 1982, 1992, 2001, 2007, 2012, 2014, and many other earthquakes throughout Central America. The Middle America Trench is not the only source of seismic activity in Central America. The Motagua Fault is an onshore continuation of the Cayman Trough which forms part of the tectonic boundary between the North American Plate and the Caribbean Plate. This transform fault cuts right across Guatemala and then continues offshore until it merges with the Middle America Trench along the Pacific coast of Mexico, near Acapulco. Seismic activity at the Motagua Fault has been responsible for earthquakes in 1717, 1773, 1902, 1976, 1980, and 2009. Another onshore continuation of the Cayman Trough is the Chixoy-Polochic Fault, which runs parallel to, and roughly 80 kilometers (50 mi) to the north, of the Motagua Fault. Though less active than the Motagua Fault, seismic activity at the Chixoy-Polochic Fault is still thought to be capable of producing very large earthquakes, such as the 1816 earthquake of Guatemala.Managua, the capital of Nicaragua, was devastated by earthquakes in 1931 and 1972. Volcanic eruptions are also common in Central America. In 1968 the Arenal Volcano, in Costa Rica, erupted killing 87 people as the 3 villages of Tabacon, Pueblo Nuevo and San Luis were buried under pyroclastic flows and debris. Fertile soils from weathered volcanic lava have made it possible to sustain dense populations in the agriculturally productive highland areas.
The official language majority in all Central American countries is Spanish, except in Belize, where the official language is English. Mayan languages constitute a language family consisting of about 26 related languages. Guatemala formally recognized 21 of these in 1996. Xinca and Garifuna are also present in Central America.
This region of the continent is very rich in terms of ethnic groups. The majority of the population is mestizo, with sizable Mayan and African descendent populations present, including Xinca and Garifuna minorities. The immigration of Arabs, Jews, Chinese, Europeans and others brought additional groups to the area.
The predominant religion in Central America is Christianity (95.6%). Beginning with the Spanish colonization of Central America in the 16th century, Roman Catholicism became the most popular religion in the region until the first half of the 20th century. Since the 1960s, there has been an increase in other Christian groups, particularly Protestantism, as well as other religious organizations, and individuals identifying themselves as having no religion.
Central American music Central American cuisine List of cuisines of the Americas – Central American cuisine
Central American Games Central American and Caribbean Games 1926 Central American and Caribbean Games – the first time this event occurred Central American Football Union Surfing
Central America is currently undergoing a process of political, economic and cultural transformation that started in 1907 with the creation of the Central American Court of Justice. In 1951 the integration process continued with the signature of the San Salvador Treaty, which created the ODECA, the Organization of Central American States. However, the unity of the ODECA was limited by conflicts between several member states. In 1991, the integration agenda was further advanced by the creation of the Central American Integration System (Sistema para la Integración Centroamericana, or SICA). SICA provides a clear legal basis to avoid disputes between the member states. SICA membership includes the 7 nations of Central America plus the Dominican Republic, a state that is traditionally considered part of the Caribbean. On 6 December 2008, SICA announced an agreement to pursue a common currency and common passport for the member nations. No timeline for implementation was discussed. Central America already has several supranational institutions such as the Central American Parliament, the Central American Bank for Economic Integration and the Central American Common Market. On 22 July 2011, President Mauricio Funes of El Salvador became the first president pro tempore to SICA. El Salvador also became the headquarters of SICA with the inauguration of a new building.
Until recently, all Central American countries have maintained diplomatic relations with Taiwan instead of China. President Óscar Arias of Costa Rica, however, established diplomatic relations with China in 2007, severing formal diplomatic ties with Taiwan. After breaking off relations with the Republic of China in 2017, Panama established diplomatic relations with the People's Republic of China. In August 2018, El Salvador also severed ties with Taiwan to formally start recognizing the People's Republic of China as sole China, a move many considered lacked transparency due to its abruptness and reports of the Chinese government's desires to invest in the department of La Union while also promising to fund the ruling party's reelection campaign. President of El Salvador, Nayib Bukele, broke diplomatic relations with Taiwan and establish better ones with China.
The Central American Parliament (also known as PARLACEN) is a political and parliamentary body of SICA. The parliament started around 1980, and its primary goal was to resolve conflicts in Nicaragua, Guatemala, and El Salvador. Although the group was disbanded in 1986, ideas of unity of Central Americans still remained, so a treaty was signed in 1987 to create the Central American Parliament and other political bodies. Its original members were Guatemala, El Salvador, Nicaragua and Honduras. The parliament is the political organ of Central America, and is part of SICA. New members have since then joined including Panama and the Dominican Republic. Costa Rica is not a member State of the Central American Parliament and its adhesion remains as a very unpopular topic at all levels of the Costa Rican society due to existing strong political criticism towards the regional parliament, since it is regarded by Costa Ricans as a menace to democratic accountability and effectiveness of integration efforts. Excessively high salaries for its members, legal immunity of jurisdiction from any member State, corruption, lack of a binding nature and effectiveness of the regional parliament's decisions, high operative costs and immediate membership of Central American Presidents once they leave their office and presidential terms, are the most common reasons invoked by Costa Ricans against the Central American Parliament.
Signed in 2004, the Central American Free Trade Agreement (CAFTA) is an agreement between the United States, Costa Rica, El Salvador, Guatemala, Honduras, Nicaragua, and the Dominican Republic. The treaty is aimed at promoting free trade among its members. Guatemala has the largest economy in the region. Its main exports are coffee, sugar, bananas, petroleum, clothing, and cardamom. Of its 10.29 billion dollar annual exports, 40.2% go to the United States, 11.1% to neighboring El Salvador, 8% to Honduras, 5.5% to Mexico, 4.7% to Nicaragua, and 4.3% to Costa Rica.The region is particularly attractive for companies (especially clothing companies) because of its geographical proximity to the[United States], very low wages and considerable tax advantages. In addition, the decline in the prices of coffee and other export products and the structural adjustment measures promoted by the international financial institutions have partly ruined agriculture, favouring the emergence of maquiladoras. This sector accounts for 42 per cent of total exports from El Salvador, 55 per cent from Guatemala, and 65 per cent from Honduras. However, its contribution to the economies of these countries is disputed; raw materials are imported, jobs are precarious and low-paid, and tax exemptions weaken public finances.They are also criticised for the working conditions of employees: insults and physical violence, abusive dismissals (especially of pregnant workers), working hours, non-payment of overtime. According to Lucrecia Bautista, coordinator of the maquilas sector of the audit firm Coverco, labour law regulations are regularly violated in maquilas and there is no political will to enforce their application. In the case of infringements, the labour inspectorate shows remarkable leniency. It is a question of not discouraging investors. Trade unionists are subject to pressure, and sometimes to kidnapping or murder. In some cases, business leaders have used the services of the maras. Finally, black lists containing the names of trade unionists or political activists are circulating in employers' circles.Economic growth in Central America is projected to slow slightly in 2014–15, as country-specific domestic factors offset the positive effects from stronger economic activity in the United States.
Tourism in Belize has grown considerably in more recent times, and it is now the second largest industry in the nation. Belizean Prime Minister Dean Barrow has stated his intention to use tourism to combat poverty throughout the country. The growth in tourism has positively affected the agricultural, commercial, and finance industries, as well as the construction industry. The results for Belize's tourism-driven economy have been significant, with the nation welcoming almost one million tourists in a calendar year for the first time in its history in 2012. Belize is also the only country in Central America with English as its official language, making this country a comfortable destination for English-speaking tourists.Costa Rica is the most visited nation in Central America. Tourism in Costa Rica is one of the fastest growing economic sectors of the country, having become the largest source of foreign revenue by 1995. Since 1999, tourism has earned more foreign exchange than bananas, pineapples and coffee exports combined. The tourism boom began in 1987, with the number of visitors up from 329,000 in 1988, through 1.03 million in 1999, to a historical record of 2.43 million foreign visitors and $1.92-billion in revenue in 2013. In 2012 tourism contributed with 12.5% of the country's GDP and it was responsible for 11.7% of direct and indirect employment.Tourism in Nicaragua has grown considerably recently, and it is now the second largest industry in the nation. Nicaraguan President Daniel Ortega has stated his intention to use tourism to combat poverty throughout the country. The growth in tourism has positively affected the agricultural, commercial, and finance industries, as well as the construction industry. The results for Nicaragua's tourism-driven economy have been significant, with the nation welcoming one million tourists in a calendar year for the first time in its history in 2010.
Central America. Columbia Encyclopedia, 6th ed. 2001–6. New York: Columbia University Press. American Heritage Dictionaries, Central America. WordNet Princeton University: Central America. Central America. Columbia Gazetteer of the World Online. 2006. New York: Columbia University Press. Hernández, Consuelo (2009). Reconstruyendo a Centroamérica a través de la poesía. Voces y perspectivas en la poesia latinoamericana del siglo XX. Madrid: Visor.
Central America Video Links from the Dean Peter Krogh Foreign Affairs Digital Archives Central America country pages Teaching Central America
The Americas (also collectively called America) is a landmass comprising the totality of North and South America. The Americas make up most of the land in Earth's Western Hemisphere and comprise the New World.Along with their associated islands, the Americas cover 8% of Earth's total surface area and 28.4% of its land area. The topography is dominated by the American Cordillera, a long chain of mountains that runs the length of the west coast. The flatter eastern side of the Americas is dominated by large river basins, such as the Amazon, St. Lawrence River–Great Lakes basin, Mississippi, and La Plata. Since the Americas extend 14,000 km (8,700 mi) from north to south, the climate and ecology vary widely, from the arctic tundra of Northern Canada, Greenland, and Alaska, to the tropical rain forests in Central America and South America. Humans first settled the Americas from Asia between 42,000 and 17,000 years ago. A second migration of Na-Dene speakers followed later from Asia. The subsequent migration of the Inuit into the neoarctic around 3500 BCE completed what is generally regarded as the settlement by the indigenous peoples of the Americas. The first known European settlement in the Americas was by the Norse explorer Leif Erikson. However, the colonization never became permanent and was later abandoned. The Spanish voyages of Christopher Columbus from 1492 to 1504 resulted in permanent contact with European (and subsequently, other Old World) powers, which eventually led to the Columbian exchange and inaugurated a period of exploration, conquest, and colonization whose effects and consequences persist to the present. The Spanish presence involved the enslavement of large numbers of the indigenous population of America.Diseases introduced from Europe and West Africa devastated the indigenous peoples, and the European powers colonized the Americas. Mass emigration from Europe, including large numbers of indentured servants, and importation of African slaves largely replaced the indigenous peoples. Decolonization of the Americas began with the American Revolution in the 1770s and largely ended with the Spanish–American War in the late 1890s. Currently, almost all of the population of the Americas resides in independent countries; however, the legacy of the colonization and settlement by Europeans is that the Americas share many common cultural traits, most notably Christianity and the use of Indo-European languages: primarily Spanish, English, Portuguese, French, and, to a lesser extent, Dutch. The Americas are home to over a billion inhabitants, two-thirds of whom reside in the United States, Brazil, and Mexico. It is home to eight megacities (metropolitan areas with ten million inhabitants or more): New York City (23.9 million), Mexico City (21.2 million), São Paulo (21.2 million), Los Angeles (18.8 million), Buenos Aires (15.6 million), Rio de Janeiro (13.0 million), Bogotá (10.4 million), and Lima (10.1 million).
The name America was first recorded in 1507. A two-dimensional globe created by Martin Waldseemüller was the earliest recorded use of the term. The name was also used (together with the related term Amerigen) in the Cosmographiae Introductio, apparently written by Matthias Ringmann, in reference to South America. It was applied to both North and South America by Gerardus Mercator in 1538. America derives from Americus, the Latin version of Italian explorer Amerigo Vespucci's first name. The feminine form America accorded with the feminine names of Asia, Africa, and Europa.In modern English, North and South America are generally considered separate continents, and taken together are called the Americas, or more rarely America. When conceived as a unitary continent, the form is generally the continent of America in the singular. However, without a clarifying context, singular America in English commonly refers to the United States of America.Historically, in the English-speaking world, the term America usually referred to a single continent until the 1950s (as in Van Loon's Geography of 1937): According to historians Kären Wigen and Martin W. Lewis, While it might seem surprising to find North and South America still joined into a single continent in a book published in the United States in 1937, such a notion remained fairly common until World War II. It cannot be coincidental that this idea served American geopolitical designs at the time, which sought both Western Hemispheric domination and disengagement from the "Old World" continents of Europe, Asia, and Africa. By the 1950s, however, virtually all American geographers had come to insist that the visually distinct landmasses of North and South America deserved separate designations.
The first inhabitants migrated into the Americas from Asia. Habitation sites are known in Alaska and the Yukon from at least 20,000 years ago, with suggested ages of up to 40,000 years. Beyond that, the specifics of the Paleo-Indian migration to and throughout the Americas, including the dates and routes traveled, are subject to ongoing research and discussion. Widespread habitation of the Americas occurred during the late glacial maximum, from 16,000 to 13,000 years ago. The traditional theory has been that these early migrants moved into the Beringia land bridge between eastern Siberia and present-day Alaska around 40,000–17,000 years ago, when sea levels were significantly lowered during the Quaternary glaciation. These people are believed to have followed herds of now-extinct pleistocene megafauna along ice-free corridors that stretched between the Laurentide and Cordilleran ice sheets. Another route proposed is that, either on foot or using primitive boats, they migrated down the Pacific coast to South America. Evidence of the latter would since have been covered by a sea level rise of hundreds of meters following the last ice age. Both routes may have been taken, although the genetic evidences suggests a single founding population. The micro-satellite diversity and distributions specific to South American Indigenous people indicates that certain populations have been isolated since the initial colonization of the region.A second migration occurred after the initial peopling of the Americas; Na Dene speakers found predominantly in North American groups at varying genetic rates with the highest frequency found among the Athabaskans at 42% derive from this second wave. Linguists and biologists have reached a similar conclusion based on analysis of Amerindian language groups and ABO blood group system distributions. Then the people of the Arctic small tool tradition, a broad cultural entity that developed along the Alaska Peninsula, around Bristol Bay, and on the eastern shores of the Bering Strait c. 2,500 BCE moved into North America. The Arctic small tool tradition, a Paleo-Eskimo culture branched off into two cultural variants, including the Pre-Dorset, and the Independence traditions of Greenland. The descendants of the Pre-Dorset cultural group, the Dorset culture was displaced by the final migrants from the Bering sea coast line the ancestors of modern Inuit, the Thule people by 1000 Common Era (CE). Around the same time as the Inuit migrated into Greenland, Viking settlers began arriving in Greenland in 982 and Vinland shortly thereafter, establishing a settlement at L'Anse aux Meadows, near the northernmost tip of Newfoundland. The Viking settlers quickly abandoned Vinland, and disappeared from Greenland by 1500.
The pre-Columbian era incorporates all period subdivisions in the history and prehistory of the Americas before the appearance of significant European influences on the American continents, spanning the time of the original settlement in the Upper Paleolithic to European colonization during the Early Modern period. The term Pre-Columbian is used especially often in the context of the great indigenous civilizations of the Americas, such as those of Mesoamerica (the Olmec, the Toltec, the Teotihuacano, the Zapotec, the Mixtec, the Aztec, and the Maya) and the Andes (Inca, Moche, Muisca, Cañaris). Many pre-Columbian civilizations established characteristics and hallmarks which included permanent or urban settlements, agriculture, civic and monumental architecture, and complex societal hierarchies. Some of these civilizations had long faded by the time of the first permanent European arrivals (c. late 15th–early 16th centuries), and are known only through archeological investigations. Others were contemporary with this period, and are also known from historical accounts of the time. A few, such as the Maya, had their own written records. However, most Europeans of the time viewed such texts as pagan, and much was destroyed in Christian pyres. Only a few hidden documents remain today, leaving modern historians with glimpses of ancient culture and knowledge.
Although there had been previous trans-oceanic contact, large-scale European colonization of the Americas began with the first voyage of Christopher Columbus in 1492. The first Spanish settlement in the Americas was La Isabela in northern Hispaniola. This town was abandoned shortly after in favor of Santo Domingo de Guzmán, founded in 1496, the oldest American city of European foundation. This was the base from which the Spanish monarchy administered its new colonies and their expansion. Santo Domingo was subject to frequent raids by English and French pirates. On the continent, Panama City on the Pacific coast of Central America, founded on August 15, 1519, played an important role, being the base for the Spanish conquest of South America. Conquistador Lucas Vázquez de Ayllón established San Miguel de Guadalupe, the first European settlement in what is now the United States, on the Pee Dee River in South Carolina. During the first half of the 16th century, Spanish colonists conducted raids throughout the Caribbean Basin, bringing captives from Central America, northern South America, and Florida back to Hispaniola and other Spanish settlements.France, led by Jacques Cartier and Giovanni da Verrazano, focused primarily on North America. English explorations of the Americas were led by Giovanni Caboto and Sir Walter Raleigh. The Dutch in New Netherland confined their operations to Manhattan Island, Long Island, the Hudson River Valley, and what later became New Jersey. The spread of new diseases brought by Europeans and African slaves killed many of the inhabitants of North America and South America, with a general population crash of Native Americans occurring in the mid-16th century, often well ahead of European contact. One of the most devastating diseases was smallpox.European immigrants were often part of state-sponsored attempts to found colonies in the Americas. Migration continued as people moved to the Americas fleeing religious persecution or seeking economic opportunities. Millions of individuals were forcibly transported to the Americas as slaves, prisoners or indentured servants. Decolonization of the Americas began with the American Revolution and the Haitian Revolution in the late 1700s. This was followed by numerous Latin American wars of independence in the early 1800s. Between 1811 and 1825, Paraguay, Argentina, Chile, Gran Colombia, the United Provinces of Central America, Mexico, Brazil, Peru, and Bolivia gained independence from Spain and Portugal in armed revolutions. After the Dominican Republic won independence from Haiti, it was re-annexed by Spain in 1861, but reclaimed its independence in 1865 at the conclusion of the Dominican Restoration War. The last violent episode of decolonization was the Cuban War of Independence which became the Spanish–American War, which resulted in the independence of Cuba in 1898, and the transfer of sovereignty over Puerto Rico from Spain to the United States. Peaceful decolonization began with the purchase by the United States of Louisiana from France in 1803, Florida from Spain in 1819, of Alaska from Russia in 1867, and the Danish West Indies from Denmark in 1916. Canada became independent of the United Kingdom, starting with the Balfour Declaration of 1926, Statute of Westminster 1931, and ending with the patriation of the Canadian Constitution in 1982. The Dominion of Newfoundland similarly achieved partial independence under the Balfour Declaration and Statute of Westminster, but was re-absorbed into the United Kingdom in 1934. It was subsequently confederated with Canada in 1949. The remaining European colonies in the Caribbean began to achieve peaceful independence well after World War II. Jamaica and Trinidad and Tobago became independent in 1962, and Guyana and Barbados both achieved independence in 1966. In the 1970s, the Bahamas, Grenada, Dominica, St. Lucia, and St. Vincent and the Grenadines all became independent of the United Kingdom, and Suriname became independent of the Netherlands. Belize, Antigua and Barbuda, and Saint Kitts and Nevis achieved independence from the United Kingdom in the 1980s.
The Americas make up most of the land in Earth's western hemisphere. The northernmost point of the Americas is Kaffeklubben Island, which is the most northerly point of land on Earth. The southernmost point is the islands of Southern Thule, although they are sometimes considered part of Antarctica. The mainland of the Americas is the world's longest north-to-south landmass. The distance between its two polar extremities, the Boothia Peninsula in northern Canada and Cape Froward in Chilean Patagonia, is roughly 14,000 km (8,700 mi). The mainland's most westerly point is the end of the Seward Peninsula in Alaska; Attu Island, further off the Alaskan coast to the west, is considered the westernmost point of the Americas. Ponta do Seixas in northeastern Brazil forms the easternmost extremity of the mainland, while Nordostrundingen, in Greenland, is the most easterly point of the continental shelf.
South America broke off from the west of the supercontinent Gondwana around 135 million years ago, forming its own continent. Around 15 million years ago, the collision of the Caribbean Plate and the Pacific Plate resulted in the emergence of a series of volcanoes along the border that created a number of islands. The gaps in the archipelago of Central America filled in with material eroded off North America and South America, plus new land created by continued volcanism. By three million years ago, the continents of North America and South America were linked by the Isthmus of Panama, thereby forming the single landmass of the Americas. The Great American Interchange resulted in many species being spread across the Americas, such as the cougar, porcupine, opossums, armadillos and hummingbirds.
The geography of the western Americas is dominated by the American cordillera, with the Andes running along the west coast of South America and the Rocky Mountains and other North American Cordillera ranges running along the western side of North America. The 2,300-kilometer-long (1,400 mi) Appalachian Mountains run along the east coast of North America from Alabama to Newfoundland. North of the Appalachians, the Arctic Cordillera runs along the eastern coast of Canada.The largest mountain ranges are the Andes and Rocky Mountains. The Sierra Nevada and the Cascade Range reach similar altitudes as the Rocky Mountains, but are significantly smaller. In North America, the greatest number of fourteeners are in the United States, and more specifically in the U.S. state of Colorado. The highest peaks of the Americas are located in the Andes, with Aconcagua of Argentina being the highest; in North America Denali (Mount McKinley) in the U.S. state of Alaska is the tallest. Between its coastal mountain ranges, North America has vast flat areas. The Interior Plains spread over much of the continent, with low relief. The Canadian Shield covers almost 5 million km2 of North America and is generally quite flat. Similarly, the north-east of South America is covered by the flat Amazon Basin. The Brazilian Highlands on the east coast are fairly smooth but show some variations in landform, while farther south the Gran Chaco and Pampas are broad lowlands.
The climate of the Americas varies significantly from region to region. Tropical rainforest climate occurs in the latitudes of the Amazon, American cloud forests, southeastern Florida and Darien Gap. In the Rocky Mountains and Andes, dry and continental climates are observed. Often the higher altitudes of these mountains are snow-capped. Southeastern North America is well known for its occurrence of tornadoes and hurricanes, of which the vast majority of tornadoes occur in the United States' Tornado Alley, as well as in the southerly Dixie Alley in the North American late-winter and early spring seasons. Often parts of the Caribbean are exposed to the violent effects of hurricanes. These weather systems are formed by the collision of dry, cool air from Canada and wet, warm air from the Atlantic.
With coastal mountains and interior plains, the Americas have several large river basins that drain the continents. The largest river basin in North America is that of the Mississippi, covering the second largest watershed on the planet. The Mississippi-Missouri river system drains most of 31 states of the U.S., most of the Great Plains, and large areas between the Rocky and Appalachian mountains. This river is the fourth longest in the world and tenth most powerful in the world. In North America, to the east of the Appalachian Mountains, there are no major rivers but rather a series of rivers and streams that flow east with their terminus in the Atlantic Ocean, such as the Hudson River, Saint John River, and Savannah River. A similar instance arises with central Canadian rivers that drain into Hudson Bay; the largest being the Churchill River. On the west coast of North America, the main rivers are the Colorado River, Columbia River, Yukon River, Fraser River, and Sacramento River. The Colorado River drains much of the Southern Rockies and parts of the Great Basin and Range Province. The river flows approximately 1,450 miles (2,330 km) into the Gulf of California, during which over time it has carved out natural phenomena such as the Grand Canyon and created phenomena such as the Salton Sea. The Columbia is a large river, 1,243 miles (2,000 km) long, in central western North America and is the most powerful river on the West Coast of the Americas. In the far northwest of North America, the Yukon drains much of the Alaskan peninsula and flows 1,980 miles (3,190 km) from parts of Yukon and the Northwest Territory to the Pacific. Draining to the Arctic Ocean of Canada, the Mackenzie River drains waters from the Arctic Great Lakes of Arctic Canada, as opposed to the Saint-Lawrence River that drains the Great Lakes of Southern Canada into the Atlantic Ocean. The Mackenzie River is the largest in Canada and drains 1,805,200 square kilometers (697,000 sq mi).The largest river basin in South America is that of the Amazon, which has the highest volume flow of any river on Earth. The second largest watershed of South America is that of the Paraná River, which covers about 2.5 million km2.
North America and South America began to develop a shared population of flora and fauna around 2.5 million years ago, when continental drift brought the two continents into contact via the Isthmus of Panama. Initially, the exchange of biota was roughly equal, with North American genera migrating into South America in about the same proportions as South American genera migrated into North America. This exchange is known as the Great American Interchange. The exchange became lopsided after roughly a million years, with the total spread of South American genera into North America far more limited in scope than the spread on North American genera into South America.
There are 35 sovereign states in the Americas, as well as an autonomous country of Denmark, three overseas departments of France, three overseas collectivities of France, and one uninhabited territory of France, eight overseas territories of the United Kingdom, three constituent countries of the Netherlands, three public bodies of the Netherlands, two unincorporated territories of the United States, and one uninhabited territory of the United States.
In 2015 the total population of the Americas was about 985 million people, divided as follows: North America: 569 million (includes Central America and the Caribbean) South America: 416 million
There are three urban centers that each hold titles for being the largest population area based on the three main demographic concepts: City properA city proper is the locality with legally fixed boundaries and an administratively recognized urban status that is usually characterized by some form of local government.Urban areaAn urban area is characterized by higher population density and vast human features in comparison to areas surrounding it. Urban areas may be cities, towns or conurbations, but the term is not commonly extended to rural settlements such as villages and hamlets. Urban areas are created and further developed by the process of urbanization and do not include large swaths of rural land, as do metropolitan areas.Metropolitan areaUnlike an urban area, a metropolitan area includes not only the urban area, but also satellite cities plus intervening rural land that is socio-economically connected to the urban core city, typically by employment ties through commuting, with the urban core city being the primary labor market.In accordance with these definitions, the three largest population centers in the Americas are: Mexico City, anchor to the largest metropolitan area in the Americas; New York City, anchor to the largest urban area in the Americas; and São Paulo, the largest city proper in the Americas. All three cities maintain Alpha classification and large scale influence. Urban centers within the Americas
The population of the Americas is made up of the descendants of four large ethnic groups and their combinations. The Indigenous peoples of the Americas, being Amerindians, Inuit, and Aleuts. Those of European ancestry, mainly Spanish, British and Irish, Portuguese, German, Italian, French and Dutch. Those of African ancestry, mainly of West African descent. Asians, that is, those of Eastern, South, and Southeast Asian ancestry. Mestizos (Metis people in Canada), those of mixed European and Amerindian ancestry. Mulattoes, people of mixed African and European ancestry. Zambos (Spanish) or Cafuzos (Portuguese), those of mixed African and Indigenous ancestry.The majority of the population live in Latin America, named for its predominant cultures, rooted in Latin Europe (including the two dominant languages, Spanish and Portuguese, both Romance languages), more specifically in the Iberian nations of Portugal and Spain (hence the use of the term Ibero-America as a synonym). Latin America is typically contrasted with Anglo-America, where English, a Germanic language, is prevalent, and which comprises Canada (with the exception of francophone Canada rooted in Latin Europe [France]—see Québec and Acadia) and the United States. Both countries are located in North America, with cultures deriving predominantly from Anglo-Saxon and other Germanic roots.
The most prevalent faiths in the Americas are as follows: Christianity (86 percent)Roman Catholicism: Practiced by 69 percent of the Latin American population, 81 percent in Mexico and 61 percent in Brazil whose Roman Catholic population of 134 million is the greatest of any nation's; approximately 24 percent of the United States' population and about 39 percent of Canada's. Protestantism: Practiced mostly in the United States, where half of the population are Protestant, Canada, with slightly more than a quarter of the population, and Greenland; there is a growing contingent of Evangelical and Pentecostal movements in predominantly Catholic Latin America. Eastern Orthodoxy: Found mostly in the United States (1 percent) and Canada; this Christian group is growing faster than many other Christian groups in Canada and now represents roughly 3 percent of the Canadian population. Non-denominational Christians and other Christians (some 1,000 different Christian denominations and sects practiced in the Americas). Irreligion: About 12 percent, including atheists and agnostics, as well as those who profess some form of spirituality but do not identify themselves as members of any organized religion. Islam: Together, Muslims constitute about 1 percent of the North American population and 0.3 percent of all Latin Americans. It is practiced by 3 percent of Canadians and 0.6 percent of the U.S. population. Argentina has the largest Muslim population in Latin America with up to 600,000 persons, or 1.9 percent of the population. Judaism (practiced by 2 percent of North Americans—approximately 2.5 percent of the U.S. population and 1.2 percent of Canadians—and 0.23 percent of Latin Americans—Argentina has the largest Jewish population in Latin America with 200,000 members)Other faiths include Buddhism; Hinduism; Sikhism; Baháʼí Faith; a wide variety of indigenous religions, many of which can be categorized as animistic; new age religions and many African and African-derived religions. Syncretic faiths can also be found throughout the Americas.
Various languages are spoken in the Americas. Some are of European origin, others are spoken by indigenous peoples or are the mixture of various languages like the different creoles.The most widely spoken language in the Americas is Spanish. The dominant language of Latin America is Spanish, though the most populous nation in Latin America, Brazil, speaks Portuguese. Small enclaves of French-, Dutch- and English-speaking regions also exist in Latin America, notably in French Guiana, Suriname, and Belize and Guyana respectively. Haitian Creole is dominant in the nation of Haiti, where French is also spoken. Native languages are more prominent in Latin America than in Anglo-America, with Nahuatl, Quechua, Aymara and Guaraní as the most common. Various other native languages are spoken with less frequency across both Anglo-America and Latin America. Creole languages other than Haitian Creole are also spoken in parts of Latin America. The dominant language of Anglo-America is English. French is also official in Canada, where it is the predominant language in Quebec and an official language in New Brunswick along with English. It is also an important language in Louisiana, and in parts of New Hampshire, Maine, and Vermont. Spanish has kept an ongoing presence in the Southwestern United States, which formed part of the Viceroyalty of New Spain, especially in California and New Mexico, where a distinct variety of Spanish spoken since the 17th century has survived. It has more recently become widely spoken in other parts of the United States because of heavy immigration from Latin America. High levels of immigration in general have brought great linguistic diversity to Anglo-America, with over 300 languages known to be spoken in the United States alone, but most languages are spoken only in small enclaves and by relatively small immigrant groups. The nations of Guyana, Suriname, and Belize are generally considered not to fall into either Anglo-America or Latin America because of their language differences from Latin America, geographic differences from Anglo-America, and cultural and historical differences from both regions; English is the primary language of Guyana and Belize, and Dutch is the primary language of Suriname. Most of the non-native languages have, to different degrees, evolved differently from the mother country, but are usually still mutually intelligible. Some have combined, however, which has even resulted in completely new languages, such as Papiamento, which is a combination of Portuguese, Spanish, Dutch (representing the respective colonizers), native Arawak, various African languages, and, more recently English. The lingua franca Portuñol, a mixture of Portuguese and Spanish, is spoken in the border regions of Brazil and neighboring Spanish-speaking countries. More specifically, Riverense Portuñol is spoken by around 100,000 people in the border regions of Brazil and Uruguay. Because of immigration, there are many communities where other languages are spoken from all parts of the world, especially in the United States, Brazil, Argentina, Canada, Chile, Costa Rica and Uruguay—very important destinations for immigrants.
Speakers of English generally refer to the landmasses of North America and South America as the Americas, the Western Hemisphere, or the New World. The adjective American may be used to indicate something pertains to the Americas, but this term is primarily used in English to indicate something pertaining to the United States. Some non-ambiguous alternatives exist, such as the adjective Pan-American, or New Worlder as a demonym for a resident of the closely related New World. Use of America in the hemispherical sense is sometimes retained, or can occur when translated from other languages. For example, the Association of National Olympic Committees (ANOC) in Paris maintains a single continental association for "America", represented by one of the five Olympic rings.American essayist H.L. Mencken said, "The Latin-Americans use Norteamericano in formal writing, but, save in Panama, prefer nicknames in colloquial speech." To avoid "American" one can use constructed terms in their languages derived from "United States" or even "North America". In Canada, its southern neighbor is often referred to as "the United States", "the U.S.A.", or (informally) "the States", while U.S. citizens are generally referred to as "Americans". Most Canadians resent being referred to as "Americans".
In Spanish, América is a single continent composed of the subcontinents of América del Sur and América del Norte, the land bridge of América Central, and the islands of the Antillas. Americano or americana in Spanish refers to a person from América in a similar way that europeo or europea refers to a person from Europa. The terms sudamericano/a, centroamericano/a, antillano/a and norteamericano/a can be used to more specifically refer to the location where a person may live. Citizens of the United States of America are normally referred to by the term estadounidense (rough literal translation: "United Statesian") instead of americano or americana which is discouraged, and the country's name itself is officially translated as Estados Unidos de América (United States of America), commonly abbreviated as Estados Unidos (EEUU). Also, the term norteamericano (North American) may refer to a citizen of the United States. This term is primarily used to refer to citizens of the United States, and less commonly to those of other North American countries.
In French the word américain may be used for things relating to the Americas; however, similar to English, it is most often used for things relating to the United States, with the term états-unien sometimes used for clarity. Panaméricain may be used as an adjective to refer to the Americas without ambiguity. French speakers may use the noun Amérique to refer to the whole landmass as one continent, or two continents, Amérique du Nord and Amérique du Sud. In French, Amérique is seldom used to refer to the United States, leading to some ambiguity when it is. Similar to English usage, les Amériques or des Amériques is used to refer unambiguously to the Americas.
In Dutch, the word Amerika mostly refers to the United States. Although the United States is equally often referred to as de Verenigde Staten ("the United States") or de VS ("the US"), Amerika relatively rarely refers to the Americas, but it is the only commonly used Dutch word for the Americas. This often leads to ambiguity; and to stress that something concerns the Americas as a whole, Dutch uses a combination, namely Noord- en Zuid-Amerika (North and South America). Latin America is generally referred to as Latijns Amerika or Midden-Amerika for Central America. The adjective Amerikaans is most often used for things or people relating to the United States. There are no alternative words to distinguish between things relating to the United States or to the Americas. Dutch uses the local alternative for things relating to elsewhere in the Americas, such as Argentijns for Argentine, etc.
The following is a list of multinational organizations in the Americas.
Dominica, Panama and the Dominican Republic have the fastest-growing economy in the Americas according to the International Monetary Fund (IMF),In 2016, five to seven countries in the southern part of the Americas had weakening economies in decline, compared to only three countries in the northern part of the Americas. Haiti has the lowest GDP per capita in the Americas, although its economy was growing slightly as of 2016.
United Nations population data by latest available Census: 2008–2009 Organization of American States Council on Hemispheric Affairs Gannett, Henry; Ingersoll, Ernest; Winship, George Parker (1905). "America and others" . New International Encyclopedia.
Captain America is a superhero appearing in American comic books published by Marvel Comics. Created by cartoonists Joe Simon and Jack Kirby, the character first appeared in Captain America Comics #1 (cover dated March 1941) from Timely Comics, a predecessor of Marvel Comics. Captain America was designed as a patriotic supersoldier who often fought the Axis powers of World War II and was Timely Comics' most popular character during the wartime period. The popularity of superheroes waned following the war, and the Captain America comic book was discontinued in 1950, with a short-lived revival in 1953. Since Marvel Comics revived the character in 1964, Captain America has remained in publication. The character wears a costume bearing an American flag motif, and he utilizes a nearly indestructible shield that he throws as a projectile. Captain America is the alter ego of Steve Rogers, a frail young man enhanced to the peak of human perfection by an experimental serum to aid the United States government's efforts in World War II. Near the end of the war, he was trapped in ice and survived in suspended animation until he was revived in modern times. Although Captain America often struggles to maintain his ideals as a man out of his time, he remains a highly respected figure in his community, which includes becoming the long-time leader of the Avengers. Captain America was the first Marvel Comics character to appear in media outside comics with the release of the 1944 movie serial, Captain America. Since then, the character has been featured in other films and television series. In the Marvel Cinematic Universe (MCU), the character is portrayed by Chris Evans. Captain America was ranked sixth on IGN's "Top 100 Comic Book Heroes of All Time" in 2011, second in their list of "The Top 50 Avengers" in 2012, and second in their "Top 25 best Marvel superheroes" list in 2014.
Captain America Comics #1 — cover-dated March 1941 and on sale December 20, 1940, a year before the attack on Pearl Harbor, but a full year into World War II — showed the protagonist punching Nazi leader Adolf Hitler; it sold nearly one million copies. While most readers responded favorably to the comic, some took objection. Simon noted, "When the first issue came out we got a lot of ... threatening letters and hate mail. Some people really opposed what Cap stood for." The threats, which included menacing groups of people loitering out on the street outside of the offices, proved so serious that police protection was posted with New York Mayor Fiorello La Guardia personally contacting Simon and Kirby to give his support.Though preceded as a "patriotically themed superhero" by MLJ's The Shield, Captain America immediately became the most prominent and enduring of that wave of superheroes introduced in American comic books prior to and during World War II, as evidenced by the unusual move at the time of premiering the character in his own title instead of an anthology title first. This popularity drew the attention and a complaint from MLJ that the character's triangular shield too closely resembled the chest symbol of their Shield character. In response, Goodman had Simon and Kirby create a distinctive round shield for issue 2, which went on to become an iconic element of the character. With his sidekick Bucky, Captain America faced Nazis, Japanese, and other threats to wartime America and the Allies. Stanley Lieber, now better known as Stan Lee, contributed to the character in issue #3 in the filler text story "Captain America Foils the Traitor's Revenge", which introduced the character's use of his shield as a returning throwing weapon. Captain America soon became Timely's most popular character and even had a fan-club called the "Sentinels of Liberty".Circulation figures remained close to a million copies per month after the debut issue, which outstripped even the circulation of news magazines such as Time during the period. The character was widely imitated by other comics publishers, with around 40 red-white-and-blue patriotic heroes debuting in 1941 alone. After the Simon and Kirby team moved to DC Comics in late 1941, having produced Captain America Comics through issue #10 (January 1942), Al Avison and Syd Shores became regular pencillers of the celebrated title, with one generally inking over the other. The character was featured in All Winners Comics #1–19 (Summer 1941 – Fall 1946), Marvel Mystery Comics #80–84 and #86–92, USA Comics #6–17 (Dec. 1942 – Fall 1945), and All Select Comics #1–10 (Fall 1943 – Summer 1946). In the post-war era, with the popularity of superheroes fading, Captain America led Timely's first superhero team, the All-Winners Squad, in its two published adventures, in All Winners Comics #19 and #21 (Fall–Winter 1946; there was no issue #20). After Bucky was shot and wounded in a 1948 Captain America story, he was succeeded by Captain America's girlfriend, Betsy Ross, who became the superheroine Golden Girl. Captain America Comics ran until issue #73 (July 1949), at which time the series was retitled Captain America's Weird Tales for two issues, with the finale being a horror/suspense anthology issue with no superheroes. Atlas Comics attempted to revive its superhero titles when it reintroduced Captain America, along with the original Human Torch and the Sub-Mariner, in Young Men #24 (Dec. 1953). Billed as "Captain America, Commie Smasher!" Captain America appeared during the next year in Young Men #24–28 and Men's Adventures #27–28, as well as in issues #76–78 of an eponymous title. Atlas' attempted superhero revival was a commercial failure, and the character's title was canceled with Captain America #78 (Sept. 1954).
In 1966, Joe Simon sued the owners of Marvel Comics, asserting that he—not Marvel—was legally entitled to renew the copyright upon the expiration of the original 28-year term. The two parties settled out of court, with Simon agreeing to a statement that the character had been created under terms of employment by the publisher, and therefore it was work for hire owned by them.In 1999, Simon filed to claim the copyright to Captain America under a provision of the Copyright Act of 1976, which allowed the original creators of works that had been sold to corporations to reclaim them after the original 56-year copyright term (but not the longer term enacted by the new legislation) had expired. Marvel Entertainment challenged the claim, arguing that the settlement of Simon's 1966 suit made the character ineligible for termination of the copyright transfer. Simon and Marvel settled out of court in 2003, in a deal that paid Simon royalties for merchandising and licensing use of the character.
Steven Grant Rogers was born in the Lower East Side of Manhattan, New York City, in 1920 to poor Irish immigrants, Sarah and Joseph Rogers. Joseph died when Steve was a child, and Sarah died of pneumonia while Steve was a teen. By early 1940, before America's entry into World War II, Rogers is a tall, scrawny fine arts student specializing in illustration and a comic book writer and artist. Disturbed by the devastation of Europe by the Nazis, Rogers attempts to enlist but is rejected due to his frail body. His resolution attracts the notice of U.S. Army General Chester Phillips and "Project: Rebirth". Rogers is used as a test subject for the Super-Soldier project, receiving a special serum made by "Dr. Josef Reinstein", later retroactively changed to a code name for the scientist Abraham Erskine.The serum is a success and transforms Steve Rogers into a nearly perfect human being with peak strength, agility, stamina, and intelligence. The success of the program leaves Erskine wondering about replicating the experiment on other human beings. The process itself has been inconsistently detailed: While in the original material Rogers is shown receiving injections of the Super-Serum, when the origin was retold in the 1960s, the Comic Code Authority had already put a veto over graphic description of drug intake and abuse, and thus the Super-Serum was retconned into an oral formula.Erskine refused to write down every crucial element of the treatment, leaving behind a flawed, imperfect knowledge of the steps. Thus, when the Nazi spy Heinz Kruger killed him, Erskine's method of creating new Super-Soldiers died. Captain America, in his first act after his transformation, avenges Erskine. In the 1941 origin story and in Tales of Suspense #63, Kruger dies when running into machinery but is not killed by Rogers; in the Captain America #109 and #255 revisions, Rogers causes the spy's death by punching him into machinery.Unable to create new Super-Soldiers and willing to hide the Project Rebirth fiasco, the American government casts Rogers as a patriotic superhero, able to counter the menace of the Red Skull as a counter-intelligence agent. He is supplied with a patriotic uniform of his own design, a bulletproof shield, a personal side arm, and the codename Captain America, while posing as a clumsy infantry private at Camp Lehigh in Virginia. He forms a friendship with the camp's teenage mascot, James Buchanan "Bucky" Barnes.Barnes learns of Rogers' dual identity and offers to keep the secret if he can become Captain America's sidekick. During their adventures, Franklin D. Roosevelt presents Captain America with a new shield, forged from an alloy of steel and vibranium, fused by an unknown catalyst, so effective that it replaces his own firearm. Throughout World War II, Captain America and Bucky fight the Nazi menace both on their own and as members of the superhero team the Invaders as seen in the 1970s comic of the same name. Captain America fights in numerous battles in World War II, primarily as a member of 1st Battalion, 26th Infantry Regiment "Blue Spaders". Captain America battles a number of criminal menaces on American soil, including a wide variety of costumed villains: the Wax Man, the Hangman, the Fang, the Black Talon, and the White Death, among others. In addition to Bucky, Captain America was occasionally assisted by the Sentinels of Liberty. Sentinels of Liberty was the title given to members of the Captain America Comics fan club who Captain America sometimes addressed as an aside, or as characters in the Captain America Comics stories. In late April 1945, during the closing days of World War II, Captain America and Bucky try to stop the villainous Baron Zemo from destroying an experimental drone plane. Zemo launches the plane with an armed explosive on it with Rogers and Barnes in hot pursuit. The pair reaches the plane just before takeoff. When Bucky tries to defuse the bomb, it explodes in mid-air. Rogers is hurled into the freezing waters of the North Atlantic. Both are presumed dead, though it is later revealed that neither had died.
Captain America appeared in comics for the next few years, changing from World War II-era hero fighting the Nazis to confronting the United States' newest enemy, Communism. The revival of the character in the mid-1950s was short-lived, and events during that time period are later retconned to show that multiple people operated using the code name to explain the changes in the character. These post World War II successors are listed as William Naslund and Jeffrey Mace. They are assisted by Fred Davis continuing the role of Bucky. The last of these other official Captains, William Burnside, was a history graduate enamored with the Captain America mythos, having his appearance surgically altered to resemble Rogers and legally changing his name to "Steve Rogers", becoming the new "1950s Captain America". He administered to himself and his pupil James "Jack" Monroe a flawed, incomplete copy of the Super-Serum, which made no mention about the necessary Vita-Ray portion of the treatment. As a result, while Burnside and Monroe became the new Captain America and Bucky, they became violently paranoid, often raving about innocent people being communist sympathizers during the height of the Red Scare of the 1950s. Their insanity forced the U.S. government to place them in indefinite cryogenic storage until they could be cured of their mental illness. Monroe would later be cured and assume the Nomad identity.
Years later, the superhero team the Avengers discovers Steve Rogers' body in the North Atlantic. After he revives, they piece together that Rogers has been preserved in a block of ice since 1945, surviving because of his enhancements from Project: Rebirth. The block began to melt after the Sub-Mariner, enraged that an Inuit tribe is worshipping the frozen figure, throws it into the ocean. Rogers accepts membership in the Avengers, and his experience in individual combat service and his time with the Invaders makes him a valuable asset. He quickly assumes leadership and has typically returned to that position throughout the team's history. Captain America is plagued by guilt for having been unable to prevent Bucky's death. Although he takes the young Rick Jones (who closely resembles Bucky) under his tutelage, he refuses for some time to allow Jones to take up the Bucky identity, not wishing to be responsible for another youth's death. Insisting that his hero move on from that loss, Jones convinces Rogers to let him don the Bucky costume, but this partnership lasts only a short time; a disguised Red Skull, impersonating Rogers with the help of the Cosmic Cube, drives Jones away. Rogers reunites with his old war comrade Nick Fury, who is similarly well-preserved due to the "Infinity Formula". As a result, Rogers regularly undertakes missions for the security agency S.H.I.E.L.D., for which Fury is public director. Through Fury, Rogers befriends Sharon Carter, a S.H.I.E.L.D. agent, with whom he eventually begins a romantic relationship. Rogers later meets and trains Sam Wilson, who becomes the superhero the Falcon, the first African-American superhero in mainstream comic books. The characters established an enduring friendship and adventuring partnership, sharing the series title for some time as Captain America and the Falcon. The two later encounter the revived but still insane 1950s Captain America. Although Rogers and the Falcon defeat the faux Rogers and Jack Monroe, Rogers becomes deeply disturbed that he could have suffered his counterpart's fate. During this period, Rogers temporarily gains super strength.The series dealt with the Marvel Universe's version of the Watergate scandal, making Rogers so uncertain about his role that he abandons his Captain America identity in favor of one called Nomad, emphasizing the word's meaning as "man without a country". During this time, several men unsuccessfully assume the Captain America identity. Rogers eventually re-assumes it after coming to consider that the identity could be a symbol of American ideals and not its government; it's a personal conviction epitomized when he later confronted a corrupt Army officer attempting to manipulate him by appealing to his loyalty, "I'm loyal to nothing, General ... except the [American] Dream." Jack Monroe, cured of his mental instability, later takes up the Nomad alias. Sharon Carter is believed to have been killed while under the mind control of Dr. Faustus.
The 1980s included a run by writer Roger Stern and artist John Byrne. Stern had Rogers consider a run for President of the United States in Captain America #250 (June 1980), an idea originally developed by Roger McKenzie and Don Perlin. Stern, in his capacity as editor of the title, originally rejected the idea but later changed his mind about the concept. McKenzie and Perlin received credit for the idea on the letters page at Stern's insistence. Stern additionally introduced a new love interest, law student Bernie Rosenthal, in Captain America #248 (Aug. 1980).Writer J. M. DeMatteis revealed the true face and full origin of the Red Skull in Captain America #298–300, and had Captain America take on Jack Monroe, Nomad, as a partner for a time. The heroes gathered by the Beyonder elect Rogers as leader during their stay on Battleworld. Homophobia is dealt with as Rogers runs into a childhood friend named Arnold Roth who is gay.Mark Gruenwald became the writer of the series with issue #307 (July 1985) and wrote 137 issues for 10 consecutive years from until #443 (Sept. 1995), the most issues by any single author in the character's history. Gruenwald created several new foes, including Crossbones and the Serpent Society. Other Gruenwald characters included Diamondback, Super Patriot, and Demolition Man. Gruenwald explored numerous political and social themes as well, such as extreme idealism when Captain America fights the anti-nationalist terrorist Flag-Smasher; and vigilantism when he hunts the murderous Scourge of the Underworld.Rogers receives a large back-pay reimbursement dating back to his disappearance at the end of World War II, and a government commission orders him to work directly for the U.S. government. Already troubled by the corruption he had encountered with the Nuke incident in New York City, where the gangster supervillain, The Kingpin, used his corrupted contacts in the US military to have the psychopathic test subject of a secret failed attempt to recreate Project Rebirth's body enhancements, Nuke, to attack Hell's Kitchen in a murderous rampage to draw Daredevil out of hiding Rogers chooses instead to resign his identity, and then takes the alias of "the Captain". A replacement Captain America, John Walker, struggles to emulate Rogers' ideals until pressure from hidden enemies helps to drive Walker insane. Rogers returns to the Captain America identity while a recovered Walker becomes the U.S. Agent.Sometime afterward, Rogers avoids the explosion of a methamphetamine lab, but the drug triggers a chemical reaction in the Super Soldier Serum in his system. To combat the reaction, Rogers has the serum removed from his body and trains constantly to maintain his physical condition. A retcon later establishes that the serum was not a drug per se, which would have metabolized out of his system, but in fact a virus-like organism that effected a biochemical and genetic change. This additionally explained how nemesis the Red Skull, who at the time inhabited a body cloned from Rogers' cells, has the formula in his body. Because of his altered biochemistry, Rogers' body begins to deteriorate, and for a time he must wear a powered exoskeleton and is eventually placed again in suspended animation. During this time, he is given a transfusion of blood from the Red Skull, which cures his condition and stabilizes the Super-Soldier virus in his system. Captain America returns to crime fighting and the Avengers.Following Gruenwald's departure from the series, Mark Waid took over and resurrected Sharon Carter as Cap's love interest. The title was then relaunched under Rob Liefeld as Cap became part of the Heroes Reborn universe for 13 issues before another relaunch restored Waid to the title in an arc that saw Cap lose his shield for a time using an energy based shield as a temporary replacement. Following Waid's run, Dan Jurgens took over and introduced new foe Protocide, a failed recipient of the Super Soldier Serum prior to the experiment that successfully created Rogers. Some time after this, Rogers' original shield was retrieved, but subtle damage sustained during the battle with the Beyonder resulted in it being shattered and a 'vibranium cancer' being triggered that would destroy all vibranium in the world, with Rogers nearly being forced to destroy the shield before a confrontation with the villain Klaw saw Klaw's attacks unwittingly repair the shield's fractured molecular bonds and negate cancer.
In the aftermath of the September 11 terrorist attacks, Rogers reveals his identity to the world and establishes a residence in the Red Hook neighborhood of Brooklyn, New York, as seen in Captain America vol. 4, #1–7 (June 2002 – Feb. 2003). Following the disbandment of the Avengers in the "Avengers Disassembled" story arc, Rogers, now employed by S.H.I.E.L.D., discovers Bucky is alive, having been saved and deployed by the Soviets as the Winter Soldier. Rogers resumes his on-again, off-again relationship with S.H.I.E.L.D. agent Sharon Carter. After a mass supervillain break-out of the Raft, Rogers and Tony Stark assemble a new team of Avengers to hunt the escapees. In the 2006–2007 company-wide story arc "Civil War", Rogers opposes the new mandatory federal registration of super-powered beings, and leads the underground anti-registration movement. After significant rancor and danger to the public as the two sides clash, Captain America voluntarily surrenders and orders the Anti-Registration forces to stand down, feeling that the fight has reached a point where the principle originally cited by the anti-registration forces has been lost.In the story arc "The Death of Captain America", Rogers is fatally shot by Sharon Carter, whose actions are manipulated by the villain Dr. Faustus. The miniseries Fallen Son: The Death of Captain America #1–5 (June–Aug. 2007) examines the reaction of the stunned superhero community to Rogers' assassination, with each of the five issues focusing a different character's reaction. Bucky takes on the mantle of Captain America, per Rogers' antemortem request.Captain America: Reborn #1 (Aug. 2009) reveals that Rogers did not die, as the gun Sharon Carter had been hypnotized into firing at Rogers caused his consciousness to phase in and out of space and time, appearing at various points in his lifetime. Although Rogers manages to relay a message to the future by giving a time-delayed command to the Vision during the Kree-Skrull War, the Skull returns Rogers to the present, where he takes control of Rogers' mind and body. Rogers eventually regains control, and, with help from his allies, defeats the Skull. In the subsequent one-shot comic Captain America: Who Will Wield the Shield?, Rogers formally grants Bucky his Captain America shield and asks him to continue as Captain America. The President of the United States grants Rogers a full pardon for his anti-registration actions.
Following the company-wide "Dark Reign" and "Siege" story arcs, the Steve Rogers character became part of the "Heroic Age" arc.The President of the United States appoints Rogers, in his civilian identity, as "America's top cop" and head of the nation's security, replacing Norman Osborn as the tenth Executive Director of S.H.I.E.L.D.. The Superhuman Registration Act is repealed and Rogers re-establishes the superhero team the Avengers, spearheaded by Iron Man, Thor, and Bucky as Captain America. In the miniseries Steve Rogers: Super Soldier, he encounters Jacob Erskine, the grandson of Professor Abraham Erskine and the son of Tyler Paxton, one of Rogers' fellow volunteers in the Super-Soldier program. Shortly afterward, Rogers becomes leader of the Secret Avengers, a black-ops superhero team.During the Fear Itself storyline, Steve Rogers is present when the threat of the Serpent is known. Following the apparent death of Bucky at the hands of Sin (in the form of Skadi), Steve Rogers ends up changing into his Captain America uniform. When the Avengers and the New Avengers are fighting Skadi, the Serpent ends up joining the battle and breaks Captain America's shield with his bare hands. Captain America and the Avengers teams end up forming a militia for a last stand against the forces of the Serpent. When it comes to the final battle, Captain America uses Thor's hammer to fight Skadi until Thor manages to kill the Serpent. In the aftermath of the battle, Iron Man presents him with his reforged shield, now stronger for its uru-infused enhancements despite the scar it bears. It is then revealed that Captain America, Nick Fury, and Black Widow are the only ones who know that Bucky actually survived the fight with Skadi as Bucky resumes his identity as Winter Soldier.During the "Spider-Island" storyline, Captain America had been captured turned into the Spider King by Spider Queen and Jackal. He was restored to normal following his fight with Venom.In the Avengers vs. X-Men story arc, Captain America attempts to apprehend Hope Summers of the X-Men. She is the targeted vessel for the Phoenix Force, a destructive cosmic entity. Captain America believes that this Phoenix Force is too dangerous to entrust in one person and seeks to prevent Hope from having it. Cyclops and the X-Men believe that the Phoenix Force will save their race, and oppose Captain America's wishes. The result is a series of battles that eventually take both teams to the blue area of the moon. The Phoenix Force eventually possesses the five X-Men present, leaving the Avengers at an extreme disadvantage. The Phoenix Five, who become corrupted by the power of the Phoenix, are eventually defeated and scattered, with Cyclops imprisoned for turning the world into a police state and murdering Charles Xavier after being pushed too far, only for him to note that, in the end, he was proven right about the Phoenix's intentions. From there, Captain America proceeds to assemble the Avengers Unity Squad, a new team of Avengers composed of both classic Avengers and X-Men.After Cyclops was incarcerated, and Steve accepted the Avengers should have done more to help mutants, and allowed the world to hate them, he started planning a new sub-team of Avengers in the hopes of unifying mutant and humankind alike. He chose Havok to lead his team and become the new face to represent mutants as Professor X and Cyclops once were.Their first threat was the return of the Red Skull- more specifically, a clone of the Skull created in 1942 and kept in stasis in the event of the original's death- who usurped Professor X's body to provide himself with telepathic powers, which he would use to provoke citizens of New York into a mass assault against mutants, or anyone who could be one, and force the Scarlet Witch and Rogue to allow themselves to be attacked. With the help of the S-Man Honest John, he managed to even manipulate Thor.The Red Skull's skills were still erratic, and could not completely control Captain America, an attack against him was enough of a distraction to lose control of Rogue and the Scarlet Witch. After being overpowered by the rest of the Uncanny Avengers, the Red Skull escapes, but promises to return. In the aftermath, both Rogue and the Scarlet Witch joined the team.During a battle with an enemy called the Iron Nail, the Super Soldier Serum within Rogers's body was neutralized, causing him to age rapidly to match his chronological age of over 90 years. No longer able to take part in field missions but retaining his sharp mind, Rogers decided to take on a role as mission coordinator, organizing the Avengers' plans of attack from the mansion, while appointing Sam Wilson as his official "replacement" as Captain America.When various Avengers and X-Men were inverted into villains and several villains inverted into heroism due to a miscast spell by the Scarlet Witch and Doctor Doom, Rogers not only coordinated the efforts of Spider-Man and the inverted villains, now called the "Astonishing Avengers", but also donned his old armor to battle the inverted Falcon, until the heroes and villains could be returned to normal with the aid of the White Skull (the inverted Red Skull).During the "Time Runs Out" storyline, Steve Rogers wears armor when he confronts Iron Man. The ensuing fight between the two old friends led Steve Rogers to force Iron Man to admit that he had lied to him and all of their allies, when he had known about the incursions between alternate Earths all along, but Iron Man also confessed that he wouldn't change a thing. The final incursion started and Earth-1610 started approaching Earth-616 while Iron Man and Steve Rogers kept fighting. Earth-1610's S.H.I.E.L.D. launched a full invasion to destroy Earth-616, where Tony Stark and Steve Rogers were crushed by a Helicarrier.As part of the All-New, All-Different Marvel, Steve Rogers became the new Chief of Civilian Oversight for S.H.I.E.L.D. He returned to the Uncanny Avengers where the team is now using the Schaefer Theater as their headquarters.Steve Rogers later has an encounter with an alternate Logan from Earth-807128. After defeating Logan and bringing him to Alberta, Canada, Rogers tried to "reassure" Logan that this was not "his" past by showing him the adamantium-frozen body of Earth-616's Logan. This sight reminds Logan of the need to enjoy being alive rather than brooding over the ghosts of his past. Although he told Steve Rogers what he had experienced in his timeline, Logan declined Steve's offer of help.
During the 2016 "Avengers: Standoff!" storyline, Steve Rogers learns from Rick Jones that S.H.I.E.L.D. has established Pleasant Hill, a gated community where they use Kobik to transform villains into ordinary citizens. When Rogers is brought to Pleasant Hill, he confronts Maria Hill about the Kobik project. Their argument is interrupted when Baron Helmut Zemo and Fixer restore the inmates to normal. After Hill is injured, Rogers convinces Zemo to let Hill get medical attention. Rogers is then escorted to Dr. Erik Selvig's clinic by Father Patrick. Selvig tells Rogers that Kobik is at the Pleasant Hill Bowling Alley. During an attempt to reason with Kobik, Rogers is attacked by Crossbones. Before Rogers can be killed, Kobik uses her abilities to restore him back to his prime. Declaring that "It's good to be back," Steve defeats Crossbones as Captain America and the Winter Soldier catch up with him. They resume their search for Kobik, and discover that Baron Zemo had Fixer invent a device that would make Kobik subservient to them. Rogers rallies the heroes so that they can take the fight to Zemo. In the aftermath of the incident, Steve and Sam plan to keep what happened at Pleasant Hill under wraps for the time being.In Captain America: Steve Rogers #1 (July 2016), the final panel apparently revealed that Rogers has been a Hydra double-agent since his early youth. This is subsequently revealed to be the result of Kobik's restoration of Rogers' youth, as she had been taught by the Red Skull that Hydra was good for the world, and having the mind of a four-year-old child, Kobik changed reality so that Rogers would be the greatest man he could be: believing Hydra to be good, Kobik permanently altered his memories so that Rogers believed that he had always been a member of Hydra. Some of Rogers' original heroic attributes remain intact, such as covering the death of another Hydra member within S.H.I.E.L.D., Erik Selvig, as well as knowing of Jack Flag's tragic life and his immortality, which is why Steve pushes him from Zemo's airplane (resulting in coma, not death). Additionally, it is revealed that Rogers' abusive father, Joseph, was actually killed by Hydra, and that Hydra deceived him into thinking Joseph died of a heart attack. It is also revealed that Rogers witnessed his mother, Sarah, being killed by Sinclair's Hydra goons and kidnapped him, which is the reason why Steve held a grudge towards Hydra's evilness and plans to kill the Red Skull's clone and restore Hydra's lost honor. As part of his long-term plans, Steve further compromised Sam Wilson's current image as 'the' Captain America by using his greater familiarity with the shield to deliberately put Wilson in a position where he would be unable to use the shield to save a senator from Flag-Smasher, with the final goal of demoralizing Sam to the point where he will return the shield to Rogers of his own free will, not wanting to kill Wilson and risk creating a martyr.During the 2016 "Civil War II" storyline, with the discovery of new Inhuman Ulysses – who has the ability to "predict" the future by calculating complex patterns – Rogers has set out to prevent Ulysses from learning of his true plans and allegiance. Rogers does this by "forcing" certain predictions on him, such as anonymously providing Bruce Banner with new gamma research to provoke a vision that would drive the Avengers to kill Banner, although this plan has apparently backfired with a recent vision showing the new Spider-Man standing over the dead Steve Rogers. Despite this revelation, Rogers presents himself as the voice of reason by allowing Spider-Man to flee with Thor. This inspires doubt in Tony Stark for his current stance by suggesting that he is just acting against Danvers because he does not like being top dog. He then goes to Washington, D.C., the location seen in Ulysses' vision, to talk to Spider-Man, who was trying to understand the vision like he was. When Captain Marvel attempts to arrest Spider-Man, Tony, wearing the War Machine armor, confronts her and the two begin to fight.Later, Rogers goes to Sokovia and joins forces with Black Widow to liberate freedom fighters from a prison so they can reclaim their country. After that, he goes to his base where Doctor Selvig expresses concern of his plan to kill the Red Skull. He then reveals that he has Baron Zemo in a cell, planning to recruit him. He eventually kills the Skull after the villain is captured by the Unity Squad and the Xavier brain fragment extracted by the Beast, Rogers throwing the Skull out of a window over a cliff after Sin and Crossbones affirm their new allegiance to Rogers.In the 2017 "Secret Empire" storyline, Rogers, as the head of S.H.I.E.L.D, uses a subsequent alien invasion and a mass supervillain assault in order to seize control of the United States. He neutralizes the superheroes that might oppose him, and seeks the Cosmic Cube to bring about a reality in which Hydra won World War II. When Rick smuggles information about the Cube's rewriting of Rogers' reality to the remaining free Avengers, a disheveled, bearded man in a torn World War II army uniform appears who introduces himself as Steve Rogers. As the Avengers and Hydra search for fragments of the shattered Cube, it is revealed that this amnesic Steve Rogers is actually a manifestation of Rogers existing within the Cube itself, created by Kobik's memories of Rogers before he was converted to Hydra, as she comes to recognize that her decision to 'rewrite' Rogers as an agent of Hydra was wrong. Although Hydra Rogers is able to mostly reassemble the Cosmic Cube, Sam Wilson and Bucky are able to use a fragment of the cube to restore the 'memory' of pre-Hydra Rogers in the Cube to corporeal existence, allowing him to defeat his Hydra self, subsequently using the Cube to undo most of the damage caused by Hydra manipulating reality even if the physical damage remains. 'Hydra Cap' continues to exist as a separate entity and is kept trapped in a prison where he is the only inmate, mocking the restored Rogers about the challenge he will face rebuilding his reputation. For himself, Rogers muses that this troubling affair has a silver lining, that this experience will teach everyone not to place such blind trust in another.
Rogers' battle experience and training make him an expert tactician and an excellent field commander, with his teammates frequently deferring to his orders in battle. The Avengers, X-Men, Fantastic Four, and other heroes choose Rogers as their leader during the Secret Wars; Thor says that Rogers is one of the very few mortals he will take orders from, and follow "through the gates of Hades".Rogers' reflexes and senses are extraordinarily keen. He has blended Aikido, Boxing, Judo, Karate, Jujutsu, Kickboxing, and gymnastics into his own unique fighting style and is a master of multiple martial arts. Years of practice with his near-indestructible shield make him able to aim and throw it with almost unerring accuracy. His skill with his shield is such that he can attack multiple targets in succession with a single throw or even cause a boomerang-like return from a throw to attack an enemy from behind. In canon, he is regarded by other skilled fighters as one of the best hand-to-hand combatants in the Marvel Universe, limited only by his human physique. Although the Super Soldier Serum is an important part of his strength, Rogers has shown himself still sufficiently capable against stronger opponents, even when the serum has been deactivated reverting him to his pre-Captain America physique.Stan Lee claimed that he'd "always been fascinated by the fact that, although Captain America has the least spectacular super-power of all, the mantle of leadership falls naturally upon him, as though he was born to command... Cap is one of the hardest hero characters to write, because the writer cannot use some exotic super-power to make his episodes seem colorful... All he has to serve him are his extraordinary combat skills, his shield, and his unquenchable love for freedom and justice."Rogers has vast U.S. military knowledge and is often shown to be familiar with ongoing, classified Defense Department operations. He is an expert in combat strategy, survival, acrobatics, parkour, military strategy, piloting, and demolitions. Despite his high profile as one of the world's most popular and recognizable superheroes, Rogers has a broad understanding of the espionage community, largely through his ongoing relationship with S.H.I.E.L.D.
Steve Rodgers is often considered to be the pinnacle of human potential and operates at peak physical performance due to his enhancement via the Super Soldier Serum. The Super Soldier Serum enhances all of his metabolic functions and prevents the build-up of fatigue poisons in his muscles, giving him endurance far in excess of an ordinary human being. This accounts for many of his extraordinary feats, including bench pressing 1,100 pounds (500 kg) as a warm-up and running a mile (1.6 km) in less than a minute (60 mph/97 km/h, nearly twice the maximum speed achieved by the best human sprinters). Furthermore, his enhancements are the reason why he was able to survive being frozen in suspended animation for decades. He is highly resistant to hypnosis or gases that could limit his focus. The secrets of creating a super-soldier were lost with the death of its creator, Dr. Abraham Erskine. In the ensuing decades there have been numerous attempts to recreate Erskine's treatment, only to have them end in failure. Even worse, the attempts have instead often created psychopathic supervillains of which Captain America's 1950s imitator and Nuke are the most notorious examples.
Captain America has used multiple shields throughout his history, the most prevalent of which is a nigh-indestructible disc-shaped shield made from a unique combination of Vibranium, Steel alloy, and an unknown third component that has never been duplicated called Proto-Adamantium. The shield was cast by American metallurgist Dr. Myron MacLain, who was contracted by the U.S. government, from orders of President Franklin D. Roosevelt, to create an impenetrable substance to use for tanks during World War II. This alloy was created by accident and never duplicated, although efforts to reverse-engineer it resulted in the discovery of adamantium.Captain America often uses his shield as an offensive throwing weapon. The first instance of Captain America's trademark ricocheting shield-toss occurs in Stan Lee's first comics writing, the two-page text story "Captain America Foils the Traitor's Revenge" in Captain America Comics #3 (May 1941). The legacy of the shield among other comics characters includes the time-traveling mutant superhero Cable telling Captain America that his shield still exists in one of the possible futures; Cable carries it into battle and brandishes it as a symbol.When without his trademark shield, Captain America sometimes uses other shields made from less durable metals such as steel, or even a photonic energy shield designed to mimic a vibranium matrix. Rogers, having relinquished his regular shield to Barnes, carried a variant of the energy shield which can be used with either arm, and used to either block attacks or as an improvised offensive weapon able to cut through metal with relative ease. Much like his Vibranium shield, the energy shield can be thrown, including ricocheting off multiple surfaces and returning to his hand.Captain America's uniform is made of a fire-retardant material, and he wears a lightweight, bulletproof duralumin scale armor beneath his uniform for added protection. Originally, Rogers' mask was a separate piece of material, but an early engagement had it dislodged, thus almost exposing his identity. To prevent a recurrence of the situation, Rogers modified the mask with connecting material to his uniform, an added benefit of which was extending his armor to cover his previously exposed neck. As a member of the Avengers, Rogers has an Avengers priority card, which serves as a communications device. Captain America has used a custom specialized motorcycle, modified by the S.H.I.E.L.D. weapons laboratory, as well as a custom-built battle van, constructed by the Wakanda Design Group with the ability to change its color for disguise purposes (red, white and blue), and fitted to store and conceal the custom motorcycle in its rear section with a frame that allows Rogers to launch from the vehicle riding it.
Captain America has faced numerous foes in over 70 years of published adventures. Many of his recurring foes embody ideologies contrary to the American values that Captain America is shown to strive for and believes in. Some examples of these opposing values are Nazism (Red Skull, Baron Zemo), neo-Nazism (Crossbones, Doctor Faustus), technocratic fascism (AIM, Arnim Zola), Communism (Aleksander Lukin), amoral capitalism (Roxxon Energy Corporation), anti-patriotism (Flag Smasher) and international and domestic terrorism (Hydra).
"Captain America" is the name of several fictional characters appearing in American comic books published by Marvel Comics. The first and primary character is Steve Rogers, who was created by Joe Simon and Jack Kirby. Other characters have adopted the alias over the years, most notably Bucky Barnes and Sam Wilson.
Captain Steven Rogers, the 18th century Earth-616 ancestor of the World War 2 Super-Soldier serum recipient, wore a colorful costume and carried a round cast iron shield.
The Marvel 1602 limited series presents an alternative history, Earth-311, in which a Captain America from the late 21st century is transported to the year 1602 after the Purple Man takes over the world – his enemy wanting to dispose of Rogers in such a way that there is nothing left of him in the present to inspire others – where he assumes the identity of Rojhaz a white Native American who is presumed by the Europeans to be of Welsh ancestry. His arrival causes numerous alterations in reality, causing analogues of various Marvel Universe characters to appear in the 17th century instead, speculated by Uatu to be the result of the universe attempting to generate a means of repairing the damage caused to reality. Rogers refuses to return to the future because he wants to nurture a new United States free of prejudice from its very beginnings, but the 1602 version of Nick Fury forces him to return, accompanying him on the journey. Rogers noted that in his version of the late 21st century, he was the last true superhero and was left alone fighting his own country – the United States – which had fallen under the rule of a tyrannical life-term President.
In the Age of X reality, Rogers was the leader of the Avengers, here a strike team intended to hunt down mutants. Although he initially believed in his mission to contain the danger that mutants could pose to the world, an encounter with a mutant 'nursery' protecting young children forced Rogers to recognize that he was on the wrong side, he and his team subsequently sacrificing themselves to stop the psychotic Hulk from launching a bioweapon at the mutant stronghold. Rogers' memories were 'stored' by Legacy, a mutant who was able to convey his plan of using various mutants to generate force fields around the facility to cut it off from the outside world.
In the Amalgam Comics universe, Captain America is combined with DC's Superman to create Super-Soldier. In this reality, Clark Kent is given a Super-Soldier serum created from DNA harvested from the body of a dead baby Kal-El. The serum gives him the powers of the main universe Superman. Frozen in ice after a battle with Ultra-Metallo at the end of World War II, Super-Soldier is revived decades later and continues his fight for justice.
In Bishop's future the Witness, a future version of Gambit, possesses Captain America's shattered shield.
The five-issue limited series Bullet Points, written by J. Michael Straczynski and illustrated by Tommy Lee Edwards, tells of an alternative reality in which Doctor Erskine is killed the day before implementing the Captain America program. Steve Rogers, still frail, volunteers for the 'Iron Man' program, which bonds him to a robotic weapons-suit. He uses this to achieve victories against the Axis. Years after the end of the war, Rogers is killed in a battle with Peter Parker, who is the Hulk of that reality.
A member of the Captain Britain Corps, Captain Colonies (Stephen Rogers) appears in Excalibur #44. His name, combined with his membership in the Captain Britain Corps imply that in his universe, the Thirteen Colonies did not declare independence to form the United States as they did in our own universe (and most of the other Marvel universes) but instead remain part of Britain.
The 2014 mobile game Marvel: Contest of Champions includes an exclusive version of Captain America named Civil Warrior. This version of Steve Rogers, set in Earth-TRN634, killed Tony Stark during the Civil War. Rogers then incorporated Stark's armor into his uniform, and uses a modified shield containing a version of the ARC reactor.
The daughter of Luke Cage and Jessica Jones, Dani Cage operates as Captain America in an alternate future where New York City has been flooded. She uses the magnetic components Steve once used on the shield in order to better control it, and has the abilities of both her parents. She first appears in Ultron Forever, and returns to the present as a member of the U.S.Avengers.
Captain America appears in the Marvel/DC crossover DC vs. Marvel. He first appears fighting with HYDRA before being summoned to the DC Earth. He is later shown in a brawl with Bane, winning when he throws his shield so that it strikes Bane in the back of the head before Bane can break his back. He is then seen fighting with Batman in the sewers of Manhattan. After a pitched hand-to-hand standoff, they realize that neither one of them can gain an advantage over the other. Afterward, they team up with each other to stop the entities, the fundamental similarities between the two unique men who trained themselves to the peak of human development—and their lack of interest in 'proving' their superiority over their counterpart forcing the Brothers to halt their conflict.
In the 7th issue in the series, Deadpool visits a world where Captain America is known as General America, and is after a female version of Deadpool called Lady Deadpool. Deadpool intervenes and sends Headpool (the zombie version) after him, and Headpool bites him on the arm. To prevent the zombie plague from affecting that Earth, Deadpool cuts off Cap's arm and leaves with it. In promos for Deadpool Corps, General America is shown to have a robotic arm.
In the Exiles arc "A World Apart", the Earth was conquered by the Skrulls in the nineteenth century. Captain America has become a gladiator known as the Captain, fighting for the Skrulls against other superhumans in contents. He is defeated by Mimic, who, disgusted at Captain America having become nothing but a puppet to the Skrulls rather than the symbol he should be to others, uses Cyclops's optic blasts.In "Forever Avengers", the Exiles visit a timeline where Captain America was turned into a vampire by Baron Blood. He later turns the Avengers into vampires and becomes the new Vampire King. The now Cursed Avengers (composed of Hawkeye, Wasp, Giant-Man, Falcon and Polaris) plan to turn New York's population into zombies, but their plans are thwarted by the Exiles with the help of that Earth's Union Jack Kenneth Crichton. One of the Exiles, Sunfire, is bitten by a vampire. Before she can completely turn, Baron Crichton destroys Captain America and reveals himself to be the grandnephew of the original Baron Blood and a vampire as well, and becomes the newest King of the Vampire by blood right.
Captain America is the leader of the Avengers in the JLA/Avengers limited series, in which the two super teams travel to each other's universe. His mind affected by subtle incompatibilities between the two universes, he sees the Justice League as overlords who demand praise and worship in return for heroic actions. He especially gets angry at Superman, who (likewise affected) sees the Avengers as heroes who do not do enough and have let their world down. After Cap and Batman battle to a standstill, the two team up to solve the mystery of the game. Using an inter-dimensional vehicle that allows them to reach the Grandmaster's headquarters, they discover that the Avengers are fighting for Krona. Their intervention in the last battle, where Cap makes sure that Batman can get the cube so the JLA wins the game, causes the villain Krona to go mad and attack the Grandmaster. The Grandmaster causes the two universes to merge, imprisoning Krona between them. Cap, still subconsciously aware of the reality changes, attacks Superman, who is also subconsciously aware of the changes. This shatters the fixed reality, freeing Krona. Cap and Superman again argue, but are stopped by Wonder Woman. The two teams find the Grandmaster, who reveals their true realities. Despite seeing shocking revelations, the two teams decide to face Krona. Cap leads the teams as a battle tactician at Superman's suggestion, communicating orders through the Martian Manhunter's telepathy, and gives Superman his shield. After the two teams defeat Krona and restore their universes, Cap and Superman salute each other as they are transported back to their own dimensions, saying that they fight on.
A future incarnation of Captain America, known as Commander A, is a major character in the Captain America Corps limited series, and is stated to be of mixed Japanese, African-American, Latino, and Native American descent. He is also implied to be a descendant of Luke Cage. He wields two energy force-field shields, similar to the one that Steve Rogers used once when he temporarily lost his vibranium shield.
The two-issue limited series The Last Avengers Story (November–December 1995) tells of a possible alternative future for Captain America and the Avengers. Appalled with the American government after the "Villain Massacre", Captain America leaves his life as a superhero and runs for president. His presidency is a large success, but he is shot and seemingly killed in his third term, causing the other heroes to lose faith. However, Cap is not dead, but placed in suspended animation in a secret location until the technology to heal him can be developed. Using a sophisticated series of computer monitors, Captain America watches his friends win their final battle and records it for historical purposes.
In the Spider-Ham comic books, the funny animal version of Captain America is Captain Americat (Steve Mouser) an anthropomorphic cat who works for the Daily Beagle.
Two younger versions of Captain America were created by writer/artist Skottie Young. The first appears in the 2015 Secret Wars tie-in, Giant Size Little Marvel, written and illustrated by Young. In the Battleworld town of Marville, the mainstream superheroes are all elementary school age children, using their superpowers to engage in very destructive roughhousing. This Captain America is still the leader of the Avengers, though their headquarters are in a tree house instead of Avengers Mansion. As in the mainstream "Avengers vs. X-Men" storyline, Captain America faces off against Cyclops and the X-Men, only this time in an attempt to get two new kids on the block to join their respective group.An even younger version of Captain America appears in A-Babies vs X-Babies, a 2012 Skottie Young scripted story, illustrated by Gurihiru. In this story, Captain America and his fellow superheroes are all babies, but still superpowered. When baby Captain America's favorite stuffed bear Bucky goes missing, he assembles his baby Avengers and battles the baby X-Men for its return. This issue and the four Giant Size Little Marvel issues were collected into the Giant Size Little Marvel 2016 trade edition (ISBN 978-0785198703).
In Marvel 2099 a man masquerading as the original Captain America became ruler of the U.S. after a successful coup deposed Doom 2099. The man was killed when Doom 2099 dropped nano-machines on the Red House. The real Captain America appears in 2099: Manifest Destiny and takes up the role of Thor before giving Mjolnir to Spider-Man 2099.In Secret Wars, a new version of Captain America was created by Alchemax and resides in the Battleworld domain of 2099. Roberta Mendez was forcefully subjected to take the Super-Soldier Serum by her husband, Harry and became the leader of Alchemax's Avengers. Roberta and Captain America are two different personas of the same woman, with Roberta unknowing of her counterpart. She physically and mentally becomes Captain America if her trigger words, "Avengers Assemble", are said, and she reverts to Roberta if someone says "Dismissed". In the Secret Wars title, Captain America goes against Miguel Stone's orders to treat the Defenders as criminals and worked with the Defenders and Avengers to stop Baron Mordo and the Dweller-In-Darkness.Following Secret Wars, Roberta is transported to the prime Marvel Universe with hallucinations of her past life. She was a supporting character in the All-New, All-Different Marvel Spider-Man 2099 comic, where she was an employee at Parker Industries with Miguel O'Hara as her boss. After Roberta's powers resurface again, she becomes a recurring ally for Spider-Man 2099. During the Civil War II storyline, Roberta goes back to 2099 to find her family, despite Miguel's warnings. The Public Eye attempt to arrest her, until she is rescued by Ravage 2099. In the present, Miguel receives a call from Peter Parker, who tells him of a vision the Inhuman Ulysses had of the future: the death of Roberta Mendez. He goes back to 2099. Roberta learns from Ravage about the Anti-Powers Act, a law outlawing superpowers. Roberta and Ravage are taken to the downtown area by Hawkeye 2099, where they meet the remaining heroes. Spider-Man convinces Doctor Strange 2099 to help him out in exchange for his help in eliminating the A.P.A. Meanwhile, the CEO of Alchemax calls on Power Pack to defeat the heroes. Upon finding Roberta, Strange takes Spider-Man downtown, while Roberta leaves to find her husband upon learning his location. Roberta finds her husband Harry, who claims that she died and that they do not have kids, and gets captured by Power Pack. After Strange reveals that the CEO of Alchemax is J. Jonah Jameson, Spider-Man rallies the heroes to launch an assault on S.H.I.E.L.D. HQ and rescue Roberta. In the process, they discover that "Jameson" and "Power Pack" are actually Skrull impostors. Spider-Man and Roberta then go back to 2016 to restore the timeline. In the book's ending, Roberta and Miguel's son save Miguel from death and return to 2099 on New Year's Eve. Thanks to Miguel's sacrifice, Roberta's family history is restored. In other mediaCaptain America 2099 (Roberta Mendez) appears in Marvel: Future Fight, as alternative costume to Captain America. Captain America 2099 (Roberta Mendez) appears as a playable character in Lego Marvel Super Heroes 2.
In the Marvel Apes Universe, Captain America leads the Ape-vengers (which contain a lot of reformed supervillains). Secretly, he is a vampire along with his version of the Invaders, and plots to enter the 616 universe for sustenance. To accomplish this, he has already killed his world's version of Mr. Fantastic. However, it is revealed that the vampire Captain America was really Baron Blood, who took on Cap's form and increased his strength through the Super-Soldier Serum inside him. The real America was still frozen in ice up to the modern era, and helped the Gibbon, Wolverine, and Speedball fight off the vampire Namor. Afterwards, they stop Baron Blood. This version of Captain America turns out to be nearly as brutal as his impersonator; for example he is willing to kill Spider-Monkey for the 'crime' of helping innocent dimensional travelers.
In the Marvel Mangaverse reality, the original Captain America is decapitated and killed by Doctor Doom, but Carol Danvers assumes the identity. This is done mostly out of a desire of self-defense, but she is encouraged to keep it for the foreseeable future by Sharon Carter. The original Mangaverse Captain America is both the leader of the Avengers and the President of the United States. His costume gives him the power to generate and manipulate energy shields.
In the 2005–2006 miniseries Marvel Zombies, and the follow-up 2007 Marvel Zombies vs. The Army of Darkness, Captain America is known as Colonel America and once served as the President of the United States. He is among the superheroes infected, along with his other fellow Avengers, by the zombified Sentry. Colonel America is responsible for infecting Spider-Man in Marvel Zombies vs. The Army Of Darkness by biting him on the shoulder. He is apparently killed by a zombie Red Skull, who rips off his left arm and scoops his exposed brains out before he himself is decapitated by a zombified Spider-Man. Zombie Ant-Man then steps on the Red Skull. As his intellect was partly retained in the remaining portion of his brain, he was transplanted into Black Panther's son T'Channa's dead body, and given a mechanical left arm. The transplant is successful, but the resulting brain damage turns Colonel America into a battle-crazed zombie leader, manageable but unable to focus on anything that is not related to war, confrontation, and battle. Colonel America (Steve Rogers/T'Channa) also has a role in Marvel Zombies Return, where he was transported to Earth-Z.Marvel Zombies 3 features a zombie version called "Captain Mexica", who comes from an alternate universe in which the Aztec Empire in Mexico never fell. He is killed after Machine Man cuts him in half.
In the alternative reality MC2 universe, Captain America leads the original Avengers on a mission to an alternative reality, which claims the majority of the team. He stays behind to aid the rebels in that reality, thus adding to the list of the dead / missing in action. The next iteration of MC2 Avengers aids him in A-Next #10-11, at the end of which he gives American Dream the shield that had belonged to that universe's Captain America. Captain America and Thunderstrike return to their home universe to aid in the fight against Seth in Spider-Girl #59.In the 2005 limited series Last Hero Standing, the MC2 Captain America is fatally injured leading a group of young heroes in battle against the Norse god Loki. Thor uses his power to transform Captain America into a new star. In the sequel, Last Planet Standing, Galactus states that this new star is the key to his escaping his world-devouring hunger.
In this potential future, all the Marvel Universe superheroes were killed when the supervillains combined forces. The villains then conquer and divide up control of the United States. Captain America is shown in a flashback as having been killed by the Red Skull in the ruins of the U.S. Capitol. The Red Skull subsequently takes Cap's costume and wears it as President of America.
In an alternate universe where World War II is still raging, Steve Rogers and Professor Erskine are both assassinated before the Super-Soldier Serum is administered, so Peggy Carter steps up to participate in Project: Rebirth. Although British, she takes up the shield and American flag to fight as Captain America. In this universe, Becky Barnes serves alongside Captain Peggy.The concept of Peggy Carter serving as Captain America was created for the game Marvel Puzzle Quest for Captain America's 75th anniversary. She was adapted into the third series of the comic Exiles.Peggy Carter also appears as a version of Captain America (named Captain Britain) in the first episode of the Marvel Studios animated series What If...? In this version, Peggy takes the Super-Soldier Serum, while Steve Rogers later joins the fight with an armored suit built by Howard Stark and becomes Iron Man.
Captain America is a S.H.I.E.L.D. agent on Earth-65, who apprehends Spider-Gwen during her battle with the Lizard (this reality's Peter Parker). This Captain America is an African American woman named Samantha Wilson a genderbent version of Sam Wilson/Falcon. During the 1940s, Samantha volunteered for Project: Rebirth after other test subjects were shot and killed or badly injured by Nazis. She became trapped in an alternate dimension after seemingly sacrificing herself to stop Arnim Zola, but later managed to return home to find that 75 years had passed. Steve Rogers would go on to become a famous comic creator, who writes stories of Samantha's dimensional journeys that he saw in his dreams, which Sam confirmed as being accurate.
In this retelling of Spider-Island as part of the "Secret Wars" storyline, Captain America and the other heroes are mutated into monster spiders and he is still the Spider Queen's "Spider King" in the Battleworld domain of Spider-Island. However, Agent Venom gives Captain America the Godstone and turns him into a Man-Wolf (as an homage to the time when Captain America was a werewolf called Capwolf), releasing Steve from the Spider Queen's control. He uses his new form to fight for the resistance.
Spider-Man: Life Story takes place in an alternate continuity where characters naturally age after Peter Parker debuts as Spider-Man in 1962. In 1966, Captain America is pressured by the public to join the efforts in Vietnam and decides to go to see the conflict for himself. A year later, American soldiers label Steve as a traitor when he decides to protect a Vietnamese village. Captain America also gets himself involved in the Superhuman Civil War in the 2000s. In the 2010s, it is unknown if he is dead or in hiding after Doctor Doom took over the planet.
In the 2003 limited series Truth: Red, White & Black, black soldiers act as test subjects for the WWII Super-Soldier program of 1942. Most of the subjects die, or become deformed with the exception of one, Isaiah Bradley. Isaiah substitutes for Captain America on an assignment, discovering Jewish concentration camp detainees subjected to experiments.In Captain America (vol. 4) #28 (August 2004), an Isaiah Bradley from an alternative Earth became Captain America and never married. Later, he is elected president and serves two terms. He travels back in time, accidentally crossing to Earth-616, and brings the mainstream Captain America and Rebecca Quan forward into his own time to prevent his daughter, Rebecca "Becky" Barnes, from traveling to Earth-616.
In addition to the WWII era hero, a 1960s version of Captain America (a.k.a. "Captain America of the Vietnam War") exists as an Ultimate Marvel Universe parallel to the William Burnside/Captain America of the 1950s, who succeeded Rogers in the role after he is accidentally frozen. The 1960s Captain America is in fact Frank Simpson, better known in the Earth-616 Marvel Universe as Nuke. As scientists were unable to recreate the Super-Soldier Serum, they used cybernetics and steroids to enhance Simpson, which eventually eroded his sanity.
In an alternate future of the Ultimate Universe, Scott Summers assumes the mantle of Captain America after Steve Rogers dies and leads a small team of X-Men to fight for mutant justice.
Steve Rogers is selected for the Weapon X program. He is given a procedure similar to Wolverine's that bonds vibranium to his skeleton. He is given the code name Vibram.
In 1893, at the age of 33, Bates, an English professor at Wellesley College, had taken a train trip to Colorado Springs, Colorado, to teach a short summer school session at Colorado College. Several of the sights on her trip inspired her, and they found their way into her poem, including the World's Columbian Exposition in Chicago, the "White City" with its promise of the future contained within its gleaming white buildings; the wheat fields of America's heartland Kansas, through which her train was riding on July 16; and the majestic view of the Great Plains from high atop Pikes Peak.On the pinnacle of that mountain, the words of the poem started to come to her, and she wrote them down upon returning to her hotel room at the original Antlers Hotel. The poem was initially published two years later in The Congregationalist to commemorate the Fourth of July. It quickly caught the public's fancy. An amended version was published in 1904. The first known melody written for the song was sent in by Silas Pratt when the poem was published in The Congregationalist. By 1900, at least 75 different melodies had been written. A hymn tune composed in 1882 by Samuel A. Ward, the organist and choir director at Grace Church, Newark, was generally considered the best music as early as 1910 and is still the popular tune today. Just as Bates had been inspired to write her poem, Ward, too, was inspired. The tune came to him while he was on a ferryboat trip from Coney Island back to his home in New York City after a leisurely summer day and he immediately wrote it down. He composed the tune for the old hymn "O Mother Dear, Jerusalem", retitling the work "Materna". Ward's music combined with Bates's poem were first published together in 1910 and titled "America the Beautiful".Ward died in 1903, not knowing the national stature his music would attain. Bates was more fortunate, since the song's popularity was well established by the time of her death in 1929. It is included in songbooks in many religious congregations in the United States.At various times in the more than one hundred years that have elapsed since the song was written, particularly during the John F. Kennedy administration, there have been efforts to give "America the Beautiful" legal status either as a national hymn or as a national anthem equal to, or in place of, "The Star-Spangled Banner", but so far this has not succeeded. Proponents prefer "America the Beautiful" for various reasons, saying it is easier to sing, more melodic, and more adaptable to new orchestrations while still remaining as easily recognizable as "The Star-Spangled Banner". Some prefer "America the Beautiful" over "The Star-Spangled Banner" due to the latter's war-oriented imagery; others prefer "The Star-Spangled Banner" for the same reason. While that national dichotomy has stymied any effort at changing the tradition of the national anthem, "America the Beautiful" continues to be held in high esteem by a large number of Americans, and was even being considered before 1931 as a candidate to become the national anthem of the United States.
Bing Crosby included the song in a medley on his album 101 Gang Songs (1961). Frank Sinatra recorded the song with Nelson Riddle during the sessions for The Concert Sinatra in February 1963, for a projected 45 single release. The 45 was not commercially issued however, but the song was later added as a bonus track to the enhanced 2012 CD release of The Concert Sinatra. In 1976, while the United States celebrated its bicentennial, a soulful version popularized by Ray Charles peaked at number 98 on the US R&B chart.Three different renditions of the song have entered the Hot Country Songs charts. The first was by Charlie Rich, which went to number 22 in 1976. A second, by Mickey Newbury, peaked at number 82 in 1980. An all-star version of "America the Beautiful" performed by country singers Trace Adkins, Sherrié Austin, Billy Dean, Vince Gill, Carolyn Dawn Johnson, Toby Keith, Brenda Lee, Lonestar, Lyle Lovett, Lila McCann, Lorrie Morgan, Jamie O'Neal, The Oak Ridge Boys, Collin Raye, Kenny Rogers, Keith Urban and Phil Vassar reached number 58 in July 2001. The song re-entered the chart following the September 11 attacks.Popularity of the song increased greatly following the September 11 attacks; at some sporting events it was sung in addition to the traditional singing of the national anthem. During the first taping of the Late Show with David Letterman following the attacks, CBS newsman Dan Rather cried briefly as he quoted the fourth verse.For Super Bowl XLVIII, The Coca-Cola Company aired a multilingual version of the song, sung in several different languages. The commercial received some criticism on social media sites, such as Twitter and Facebook, and from some conservatives, such as Glenn Beck. Despite the controversies, Coca-Cola later reused the Super Bowl ad during Super Bowl LI, the opening ceremonies of the 2014 Winter Olympics and 2016 Summer Olympics and for patriotic holidays.
Lynn Sherr's 2001 book America the Beautiful discusses the origins of the song and the backgrounds of its authors in depth. The book points out that the poem has the same meter as that of "Auld Lang Syne"; the songs can be sung interchangeably. Additionally, Sherr discusses the evolution of the lyrics, for instance, changes to the original third verse written by Bates.Melinda M. Ponder, in her 2017 biography Katharine Lee Bates: From Sea to Shining Sea, draws heavily on Bates's diaries and letters to trace the history of the poem and its place in American culture.
MP3 and RealAudio recordings available at the United States Library of Congress Free sheet music of America the Beautiful from Cantorion.org Words, sheet music & MIDI file at the Cyber Hymnal America the Beautiful Park in Colorado Springs named for Katharine Lee Bates' words. Archival collection of America the Beautiful lantern slides from the 1930s. Another free sheet music
Voice of America (VOA) is a state-controlled international television and radio network funded by the U.S. federal tax budget. It is the largest U.S. international broadcaster. VOA produces digital, TV, and radio content in 47 languages which it distributes to affiliate stations around the globe. It is primarily viewed by foreign audiences, so VOA programming has an influence on public opinion abroad regarding the United States and its people.VOA was established in 1942, and the VOA charter (Public Laws 94-350 and 103-415) was signed into law in 1976 by President Gerald Ford. VOA is headquartered in Washington, D.C., and overseen by the U.S. Agency for Global Media, an independent agency of the U.S. government. Funds are appropriated annually by Congress under the budget for embassies and consulates. In 2016, VOA broadcast an estimated 1,800 hours of radio and TV programming each week to approximately 236.6 million people worldwide with about 1,050 employees and a taxpayer-funded annual budget of US$218.5 million.Some commentators consider Voice of America to be a form of propaganda.In response to the request of the United States Department of Justice that RT register as a foreign agent under the Foreign Agents Registration Act, Russia's Justice Ministry labeled Voice of America and Radio Free Europe/Radio Liberty as foreign agents in December 2017.
The Voice of America website had five English language broadcasts as of 2014 (worldwide, Special English, Cambodia, Zimbabwe and Tibet). Additionally, the VOA website has versions in 46 foreign languages (radio programs are marked with an asterisk; TV programs with a plus symbol and icon ): The number of languages varies according to the priorities of the United States government and the world situation.
Before World War II, all American shortwave stations were in private hands. Privately controlled shortwave networks included the National Broadcasting Company's International Network (or White Network), which broadcast in six languages, the Columbia Broadcasting System's Latin American international network, which consisted of 64 stations located in 18 different countries, the Crosley Broadcasting Corporation in Cincinnati, Ohio, and General Electric which owned and operated WGEO and WGEA, both based in Schenectady, New York, and KGEI in San Francisco, all of which had shortwave transmitters. Experimental programming began in the 1930s, but there were fewer than 12 transmitters in operation. In 1939, the Federal Communications Commission set the following policy: A licensee of an international broadcast station shall render only an international broadcast service which will reflect the culture of this country and which will promote international goodwill, understanding and cooperation. Any program solely intended for, and directed to an audience in the continental United States does not meet the requirements for this service. This policy was intended to enforce the State Department's Good Neighbor Policy, but some broadcasters felt that it was an attempt to direct censorship.Shortwave signals to Latin America were regarded as vital to counter Nazi propaganda around 1940. Initially, the Office of Coordination of Information sent releases to each station, but this was seen as an inefficient means of transmitting news. The director of Latin American relations at the Columbia Broadcasting System was Edmund A. Chester, and he supervised the development of CBS's extensive "La Cadena de las Americas" radio network to improve broadcasting to South America during the 1940s.Also included among the cultural diplomacy programming on the Columbia Broadcasting System was the musical show Viva America (1942-1949) which featured the Pan American Orchestra and the artistry of several noted musicians from both North and South America, including Alfredo Antonini, Juan Arvizu, Eva Garza, Elsa Miranda, Nestor Mesta Chaires, Miguel Sandoval, John Serry Sr., and Terig Tucci. By 1945, broadcasts of the show were carried by 114 stations on CBS's "La Cadena de las Americas" network in 20 Latin American nations. These broadcasts proved to be highly successful in supporting President Franklin Roosevelt's policy of Pan-Americanism throughout South America during World War II.
Even before the Japanese attack on Pearl Harbor, the U.S. government's Office of the Coordinator of Information (COI, in Washington) had already begun providing war news and commentary to the commercial American shortwave radio stations for use on a voluntary basis through its Foreign Information Service (FIS, in New York) headed by playwright Robert E. Sherwood, the playwright who served as president Roosevelt’s speech writer and information advisor. Direct programming began a week after the United States’ entry into World War II in December 1941, with the first broadcast from the San Francisco office of the FIS via General Electric’s KGEI transmitting to the Philippines in English (other languages followed). The next step was to broadcast to Germany, which was called Stimmen aus Amerika ("Voices from America") and was transmitted on February 1, 1942. It was introduced by "The Battle Hymn of the Republic" and included the pledge: "Today, and every day from now on, we will be with you from America to talk about the war... The news may be good or bad for us – We will always tell you the truth." Roosevelt approved this broadcast, which then-Colonel William J. Donovan (COI) and Sherwood (FIS) had recommended to him. It was Sherwood who actually coined the term "The Voice of America" to describe the shortwave network that began its transmissions on February 1, from 270 Madison Avenue in New York City. The Office of War Information, when organized in the middle of 1942, officially took over VOA's operations. VOA reached an agreement with the British Broadcasting Corporation to share medium-wave transmitters in Britain, and expanded into Tunis in North Africa and Palermo and Bari, Italy as the Allies captured these territories. The OWI also set up the American Broadcasting Station in Europe.Asian transmissions started with one transmitter in California in 1941; services were expanded by adding transmitters in Hawaii and, after recapture, the Philippines.By the end of the war, VOA had 39 transmitters and provided service in 40 languages. Programming was broadcast from production centers in New York and San Francisco, with more than 1,000 programs originating from New York. Programming consisted of music, news, commentary, and relays of U.S. domestic programming, in addition to specialized VOA programming.About half of VOA's services, including the Arabic service, were discontinued in 1945. In late 1945, VOA was transferred to the Department of State.
In 1947, VOA started broadcasting to the Soviet citizens in Russia under the pretext of countering "more harmful instances of Soviet propaganda directed against American leaders and policies" on the part of the internal Soviet Russian-language media, according to John B. Whitton's treatise, Cold War Propaganda. The Soviet Union responded by initiating electronic jamming of VOA broadcasts on April 24, 1949.Charles W. Thayer headed VOA in 1948–49. Over the next few years, the U.S. government debated the best role of Voice of America. The decision was made to use VOA broadcasts as a part of its foreign policy to fight the propaganda of the Soviet Union and other countries. The Arabic service resumed on January 1, 1950, with a half-hour program. This program grew to 14.5 hours daily during the Suez Crisis of 1956, and was six hours a day by 1958.In 1952, Voice of America installed a studio and relay facility aboard a converted U.S. Coast Guard cutter renamed Courier whose target audience was Soviet Union and other members of Warsaw Pact. The Courier was originally intended to become the first in a fleet of mobile, radio broadcasting ships (see offshore radio) that built upon U.S. Navy experience during WWII in using warships as floating broadcasting stations. However, the Courier eventually dropped anchor off the island of Rhodes, Greece with permission of the Greek government to avoid being branded as a pirate radio broadcasting ship. This VOA offshore station stayed on the air until the 1960s when facilities were eventually provided on land. The Courier supplied training to engineers who later worked on several of the European commercial offshore broadcasting stations of the 1950s and 1960s. Control of VOA passed from the State Department to the U.S. Information Agency when the latter was established in 1953 to transmit worldwide, including to the countries behind the Iron Curtain and to the People's Republic of China (PRC). Starting in the 1950s, VOA broadcast American jazz on Voice of America Jazz Hour from 1955 until 2003. Hosted for most of that period by Willis Conover, the program had 30 million listeners at its peak. A program aimed at South Africa in 1956 broadcast two hours nightly, and special programs such as The Newport Jazz Festival were also transmitted. This was done in association with tours by U.S. musicians, such as Dizzy Gillespie, Louis Armstrong, and Duke Ellington, sponsored by the State Department. From August 1952 through May 1953, Billy Brown, a high school senior in Westchester County, New York, had a Monday night program in which he shared everyday happenings in Yorktown Heights, New York. Brown's program ended due to its popularity: his "chatty narratives" attracted so much fan mail, VOA couldn't afford the $500 a month in clerical and postage costs required to respond to listeners' letters.Throughout the Cold War, many of the targeted countries' governments sponsored jamming of VOA broadcasts, which sometimes led critics to question the broadcasts' actual impact. For example, in 1956, Polish People's Republic stopped jamming VOA transmissions, but People's Republic of Bulgaria continued to jam the signal through the 1970s. Chinese language VOA broadcasts were jammed beginning in 1956 and extending through 1976. However, after the collapse of the Warsaw Pact and the Soviet Union, interviews with participants in anti-Soviet movements verified the effectiveness of VOA broadcasts in transmitting information to socialist societies. The People's Republic of China diligently jams VOA broadcasts. Cuba has also been reported to interfere with VOA satellite transmissions to Iran from its Russian-built transmission site at Bejucal. David Jackson, former director of Voice of America, noted: "The North Korean government doesn't jam us, but they try to keep people from listening through intimidation or worse. But people figure out ways to listen despite the odds. They're very resourceful."Throughout the 1960s and 1970s, VOA covered some of the era's most important news, including Martin Luther King Jr.'s 1963 "I Have a Dream" speech and Neil Armstrong's 1969 first walk on the Moon. During the 1962 Cuban Missile Crisis, VOA broadcast around-the-clock in Spanish.In the early 1980s, VOA began a $1.3 billion rebuilding program to improve broadcast with better technical capabilities. Also in the 1980s, VOA also added a television service, as well as special regional programs to Cuba, Radio Martí and TV Martí. Cuba has consistently attempted to jam such broadcasts and has vociferously protested U.S. broadcasts directed at Cuba. In September 1980, VOA started broadcasting to Afghanistan in Dari and in Pashto in 1982. At the same time, VOA started to broadcast U.S. government editorials, clearly separated from the programming by audio cues. In 1985, VOA Europe was created as a special service in English that was relayed via satellite to AM, FM, and cable affiliates throughout Europe. With a contemporary format including live disc jockeys, the network presented top musical hits as well as VOA news and features of local interest (such as "EuroFax") 24 hours a day. VOA Europe was closed down without advance public notice in January 1997 as a cost-cutting measure. It was followed by VOA Express, which from July 4, 1999 revamped into VOA Music Mix. Since November 1, 2014 stations are offered VOA1 (which is a rebranding of VOA Music Mix). In 1989, Voice of America expanded its Mandarin and Cantonese programming to reach the millions of Chinese and inform the country about the pro-democracy movement within the country, including the demonstration in Tiananmen Square. Starting in 1990, the U.S. consolidated its international broadcasting efforts, with the establishment of the Bureau of Broadcasting.
With the breakup of the Soviet bloc in Eastern Europe, VOA added many additional language services to reach those areas. This decade was marked by the additions of Tibetan, Kurdish (to Iran and Iraq), Croatian, Serbian, Bosnian, Macedonian, and Rwanda-Rundi language services. In 1993, the Clinton administration advised cutting funding for Radio Free Europe/Radio Liberty as it was felt post-Cold War information and influence was not needed in Europe. This plan was not well received, and he then proposed the compromise of the International Broadcasting Act. The Broadcasting Board of Governors was established and took control from the Board for International Broadcasters which previously oversaw funding for RFE/RL.In 1994, President Bill Clinton signed the International Broadcasting Act into law. This law established the International Broadcasting Bureau as a part of the U.S. Information Agency and created the Broadcasting Board of Governors with oversight authority. In 1998, the Foreign Affairs Reform and Restructuring Act was signed into law and mandated that BBG become an independent federal agency as of October 1, 1999. This act also abolished the U.S.I.A. and merged most of its functions with those of the State Department. In 1994, Voice of America became the first broadcast-news organization to offer continuously updated programs on the Internet. In April 2020, the Trump administration accused Voice of America of being a mouthpiece for authoritarian regimes that "speaks for America’s adversaries," and of "promoting propaganda" instead of "promoting freedom and democracy."
The Arabic Service was abolished in 2002 and replaced by a new radio service, called the Middle East Radio Network or Radio Sawa, with an initial budget of $22 million. Radio Sawa offered mostly Western and Middle Eastern popular songs with periodic brief news bulletins. Today, the network has expanded to television with Alhurra and to various social media and websites.On May 16, 2004; Worldnet, a satellite television service, was merged into the VOA network. Radio programs in Russian ended in July 2008. In September 2008, VOA eliminated the Hindi language service after 53 years. Broadcasts in Ukrainian, Serbian, Macedonian and Bosnian also ended. These reductions were part of American efforts to concentrate more resources to broadcast to the Muslim world.In September 2010, VOA began its radio broadcasts in Sudan. As U.S. interests in South Sudan have grown, there is a desire to provide people with free information.In 2013, VOA ended foreign language transmissions on shortwave and medium wave to Albania, Georgia, Iran and Latin America; as well as English language broadcasts to the Middle East and Afghanistan. The movement was done due to budget cuts.On July 1, 2014, VOA cut most of its shortwave transmissions in English to Asia. Shortwave broadcasts in Azerbaijani, Bengali, Khmer, Kurdish, Lao, and Uzbek were dropped too. On August 11, 2014, the Greek service ended after 72 years on air.
1941–1942 Robert E. Sherwood (Foreign Information Service)1942–1943 John Houseman 1943–1945 Louis G. Cowan 1945–1946 John Ogilvie 1948–1949 Charles W. Thayer 1949–1952 Foy D. Kohler 1952–1953 Alfred H. Morton 1953–1954 Leonard Erikson 1954–1956 John R. Poppele 1956–1958 Robert E. Burton 1958–1965 Henry Loomis 1965–1967 John Chancellor 1967–1968 John Charles Daly 1969–1977 Kenneth R. Giddens 1977–1979 R. Peter Straus 1980–1981 Mary Bitterman 1981–1982 James B. Conkling 1982 John Hughes 1982–1984 Kenneth Tomlinson 1985 Gene Pell 1986–1991 Dick Carlson 1991–1993 Chase Untermeyer 1994–1996 Geoffrey Cowan 1997–1999 Evelyn S. Lieberman 1999–2001 Sanford J. Ungar 2001–2002 Robert R. Reilly 2002–2006 David S. Jackson 2006–2011 Danforth W. Austin 2011–2015 David Ensor 2016 - 2020 Amanda Bennett 2020–Present Michael Pack
Voice of America has been a part of several agencies. From its founding in 1942 to 1945, it was part of the Office of War Information, and then from 1945 to 1953 as a function of the State Department. VOA was placed under the U.S. Information Agency in 1953. When the USIA was abolished in 1999, VOA was placed under the Broadcasting Board of Governors, or BBG, which is an autonomous U.S. government agency, with bipartisan membership. The Secretary of State has a seat on the BBG. The BBG was established as a buffer to protect VOA and other U.S.-sponsored, non-military, international broadcasters from political interference. It replaced the Board for International Broadcasting (BIB) that oversaw the funding and operation of Radio Free Europe/Radio Liberty, a branch of VOA.
From 1948 until its amendment in 2013, Voice of America was forbidden to broadcast directly to American citizens under § 501 of the Smith–Mundt Act. The act was amended as a result of the passing of the Smith-Mundt Modernization Act provision of the National Defense Authorization Act for 2013. The intent of the legislation in 1948 was to protect the American public from propaganda actions by their own government and to have no competition with private American companies. The amendment had the intent of adapting to the Internet and allow American citizens to request access to VOA content.
Under the Eisenhower administration in 1959, VOA Director Henry Loomis commissioned a formal statement of principles to protect the integrity of VOA programming and define the organization's mission, and was issued by Director George V. Allen as a directive in 1960 and was endorsed in 1962 by USIA director Edward R. Murrow. The principles were signed into law on July 12, 1976, by President Gerald Ford. It reads: The long-range interests of the United States are served by communicating directly with the peoples of the world by radio. To be effective, the Voice of America must win the attention and respect of listeners. These principles will therefore govern Voice of America (VOA) broadcasts. 1. VOA will serve as a consistently reliable and authoritative source of news. VOA news will be accurate, objective, and comprehensive. 2. VOA will represent America, not any single segment of American society, and will therefore present a balanced and comprehensive projection of significant American thought and institutions. 3. VOA will present the policies of the United States clearly and effectively, and will also present responsible discussions and opinion on these policies.
The Voice of America Firewall was put in place with the 1976 VOA Charter and laws passed in 1994 and 2016 as a way of ensuring the integrity of VOA's journalism. This policy fights against propaganda and promotes unbiased and objective journalistic standards in the agency. The charter is one part of this firewall and the other laws assist in ensuring high standards of journalism.
According to former VOA correspondent Alan Heil, the internal policy of VOA News is that any story broadcast must have two independently corroborating sources or have a staff correspondent witness an event.
Voice of America's central newsroom has hundreds of journalists and dozens of full-time domestic and overseas correspondents, who are employees of the U.S. government or paid contractors. They are augmented by hundreds of contract correspondents and stringers throughout the world, who file in English or in one of VOA's other radio and television broadcast languages. In late 2005, VOA shifted some of its central-news operation to Hong Kong where contracted writers worked from a "virtual" office with counterparts on the overnight shift in Washington, D.C., but this operation was shut down in early 2008.
By December 2014, the number of transmitters and frequencies used by VOA had been greatly reduced. VOA still uses shortwave transmissions to cover some areas of Africa and Asia. Shortwave broadcasts still take place in these languages: Afaan Oromoo, Amharic, Bambara, Cantonese, Chinese, English, Indonesian, Korean and Swahili.
VOA Radiogram was an experimental Voice of America program starting in March 2013 which transmitted digital text and images via shortwave radiograms. There were 220 editions of the program, transmitted each weekend from the Edward R. Murrow transmitting station. The audio tones that comprised the bulk of each 30 minute program were transmitted via an analog transmitter, and could be decoded using a basic AM shortwave receiver with freely downloadable software of the Fldigi family. This software is available for Windows, Apple (OSX), Linux, and FreeBSD systems. Broadcasts can also be decoded using the free TIVAR app from the Google Play store using any Android device. The mode used most often on VOA Radiogram, for both text and images, was MFSK32, but other modes were also occasionally transmitted. The final edition of VOA Radiogram was transmitted during the weekend of June 17–18, 2017, a week before the retirement of the program producer from VOA. An offer to continue the broadcasts on a contract basis was declined, so a follow-on show called Shortwave Radiogram began transmission on June 25, 2017 from the WRMI transmitting site in Okeechobee, Florida. Shortwave Radiogram program schedule
One of VOA's radio transmitter facilities was originally based on a 625-acre (2.53 km2) site in Union Township (now West Chester Township) in Butler County, Ohio, near Cincinnati. The site is now a recreational park with a lake, lodge, dog park, and Voice of America museum. The Bethany Relay Station operated from 1944 to 1994. Other former sites include California (Dixon, Delano), Hawaii, Okinawa, (Monrovia) Liberia, Costa Rica, Belize, and at least two in Greece.Between 1983 and 1990, VOA made significant upgrades to transmission facilities in Botswana, Morocco, Thailand, Kuwait, and Sao Tome.Currently, VOA and USAGM continue to operate shortwave radio transmitters and antenna farms at International Broadcasting Bureau Greenville Transmitting Station in the United States, close to Greenville, North Carolina, "Site B." They do not use FCC-issued callsigns, since the FCC does not regulate communications by other federal government agencies. (The FCC regulates broadcasting by private companies and other businesses, state governments, nonprofit organizations [NPOs] and non-government organizations [NGOs], and private individuals.) The IBB also operates a transmission facility on São Tomé and (Tinang) Concepcion, Tarlac, Philippines for VOA.
In late September 2001, VOA aired a report that contained brief excerpts of an interview with then Taliban leader Mullah Omar Mohammad, along with segments from President Bush's post-9/11 speech to Congress, an expert in Islam from Georgetown University, and comments by the foreign minister of Afghanistan's anti-Taliban Northern Alliance. State Department officials including Richard Armitage and others argued that the report amounted to giving terrorists a platform to express their views. In response, reporters and editors argued for the VOA's editorial independence from its governors. VOA received praise from press organizations for its protests, and the following year in 2002, it won the University of Oregon's Payne Award for Ethics in Journalism.
On April 2, 2007, Abdul Malik Rigi, the leader of Jundullah, a militant group with possible links to al-Qaeda, appeared on Voice of America's Persian language service. VOA introduced Rigi as "the leader of popular Iranian resistance movement." The interview resulted in public condemnation by the Iranian-American community, as well as the Iranian government. Jundullah is a militant organization that has been linked to numerous attacks on civilians, such as the 2009 Zahedan explosion.
In February 2013, a documentary released by China Central Television interviewed a Tibetan self-immolator who failed to kill himself. The interviewee said he was motivated by Voice of America's broadcasts of commemorations of people who committed suicide in political self-immolation. VOA denied any allegations of instigating self-immolations and demanded that the Chinese station retract its report.
After the inauguration of US President Donald Trump, several tweets by Voice of America (one of which was later removed) seemed to support the widely criticized statements by White House press secretary Sean Spicer about the crowd size and biased media coverage. This first raised concerns over possible attempts by Trump to politicize the state-funded agency. This amplified already growing propaganda concerns over the provisions in the National Defense Authorization Act for Fiscal Year 2017, signed into law by Barack Obama, which replaced the board of the Broadcasting Board of Governors with a CEO appointed by the president. Trump sent two of his political aides, Matthew Ciepielowski and Matthew Schuck, to the agency to aid its current CEO during the transition to the Trump administration. Criticism was raised over Trump's choice of aides; Schuck was a staff writer for right-wing website The Daily Surge until April 2015, while Ciepielowski was a field director at the conservative advocacy group Americans for Prosperity. VOA officials responded with assurances that they would not become "Trump TV". BBG head John F. Lansing told NPR that it would be illegal for the administration to tell VOA what to broadcast, while VOA director Amanda Bennett stressed that while "government-funded", the agency is not "government-run".On April 10, 2020, the White House published an article in its daily newsletter critical of VOA coverage of the coronavirus pandemic. Emails revealed in a Freedom of Information Act request showed Centers for Disease Control and Prevention (CDC) press official Michawn Rich had sent a memo to agency employees stating in part, "as a rule, do not send up [interview] requests for Greta Van Susteren or anyone affiliated with Voice of America," referencing the White House story. On April 30, the Washington Post reported Vice President Mike Pence's office "threatened to retaliate against a reporter who revealed that Pence’s office had told journalists they would need masks for Pence’s visit to the Mayo Clinic — a requirement Pence himself did not follow."On June 3, 2020, the Senate confirmed Michael Pack, a maker of conservative documentaries and close ally of Steve Bannon, to serve as head of the United States Agency for Global Media, which oversees VOA. Subsequently, Director Bennet and deputy director Sandy Sugawara resigned from VOA. CNN reported on June 16 that plans for a leadership shakeup at VOA were being discussed, including the possibility that controversial former White House aide Sebastian Gorka would be given a leadership role at VOA. On June 17, the heads of VOA's Middle East Broadcasting, Radio Free Asia, Radio Free Europe/Radio Liberty and the Open Technology Fund were all fired, their boards were dissolved and external communications from VOA employees made to require approval from senior agency personnel in what one source described as an "unprecedented" move, while Jeffrey Shapiro, like Pack a Bannon ally, was rumored to be in line to head the Office of Cuba Broadcasting. Four former members of the advisory boards subsequently filed suit challenging Pack's standing to fire them. On July 9, NPR reported VOA would not renew the work visas of dozens of non-resident reporters, many of whom could face repercussions in their home countries. In late July, four contracters and the head of VOA's Urdu language service were suspended after a video featuring extensive clips from a Muslim-American voter conference, including a campaign message from Democratic Presidential candidate Joe Biden, was determined not to meet editorial standards and taken down.On August 12, 2020, USAGM chief financial officer Grant Turner and general counsel David Kligerman were removed from their positions and stripped of their security clearances, reportedly for their opposition to what Turner called "gross mismanagement," along with four other senior agency officials. Politico reported on August 13 that Trump administration official and former shock jock Frank Wuco had been hired as a USAGM senior advisor, responsible for auditing the agency's office of policy and research. As a radio host, Wuco issued insults and groundless claims against former US President Barack Obama, CIA Director John O. Brennan and Speaker of the House Nancy Pelosi. VOA's Twitter account during this period featured stories favorable to Vice President Mike Pence and White House advisor Ivanka Trump.In response to Pack's August 27 interview with The Federalist website in which he "joked...about deporting his own employees and forcing them to adopt unsafe workplace practices that could expose them to COVID-19" and "said the agency was ripe for espionage and possibly rife with spies," a group of VOA journalists sent a letter to VOA Acting Director Elez Biberaj complaining that his "comments and decisions 'endanger the personal security of VOA reporters at home and abroad, as well as threatening to harm U.S. national security objectives.'" VOA's response was that that "it would not respond directly to the letter because it was 'improper' and 'failed to follow procedure.' Instead, the leadership of USAGM and VOA 'are handling the choice of complaint transmission as an administrative issue,' which suggested that the journalists could face sanctions for their letter," according to the Washington Post. In the same story, the Post reported that VOA Spanish-language service White House correspondent's Brigo Segovia's interview with an official about the administration's response to Pack's personnel and other moves had been censored and his own access to VOA's computer system restricted.On September 29, six senior USAGM officials filed a whistleblower complaint in which they alleged that Pack or one of his aides had ordered research conducted into the voting history of at least one agency employee, which would be a violation of laws protecting civil servants from undue political influence. NPR reported that two Pack aides had compiled a report on VoA White House bureau chief Steven L. Herman's social media postings and other writings in an attempt to charge him with a conflict of interest, and that the agency released a conflict of interest policy stating in part that a "journalist who on Facebook 'likes' a comment or political cartoon that aggressively attacks or disparages the President must recuse themselves from covering the President."Suspended officials from Voice of America sued the agency news outlet on October 8. They accused its chief operating officer, Michael Pack, of using Voice of America as a vehicle to promote the personal agenda of President Trump and of violating a statutory firewall intended to prevent political interference with the agency, and they are seeking their reinstatement.
On April 19, 2017, VOA interviewed the Chinese real estate tycoon Guo Wengui in a live broadcast. The whole interview was scheduled for 3 hours. After Guo Wengui alleged to own evidence of corruption among the members of the Politburo Standing Committee of China, the highest political authority of China, the interview was abruptly cut off, after only one hour and seventeen minutes of broadcasting. Guo's allegations involved Fu Zhenhua and Wang Qishan, the latter being a member of the Politburo Standing Committee and the leader of the massive anti-graft movement. It was reported that Beijing warned VOA's representatives not to interview Guo for his "unsubstantiated allegations". Four members of the U.S. Congress requested the Office of Inspector General to conduct an investigation into this interruption on August 27, 2017. The OIG investigation concluded that the decision to curtail the Guo interview was based solely on journalistic best practices rather than any pressure from the Chinese government.Another investigation, by Mark Feldstein, Chair of Broadcast Journalism at the University of Maryland, College Park and a journalist with decades of experiences as an award-winning television investigative reporter, concluded that "The failure to comply with leadership’s instructions during the Guo interview "was a colossal and unprecedented violation of journalistic professionalism and broadcast industry standards." The report also said that "There had been a grossly negligent approach" to pre-interview vetting and failure to "corroborate the authenticity of Guo’s evidence or interview other sources" in violation of industry standards. The interview team apparently "demonstrated greater loyalty to its source than to its employer — at the expense of basic journalistic standards of accuracy, verification, and fairness," the Feldstein report concluded.
The VOA started its operations during the Cold War and that is when its influence first started as well. Foy Kohler, the director of VOA during the Cold War, strongly believed that the VOA was serving its purpose, which he identified as aiding in the fight against communism. He argued that the numbers of listeners they were getting such as 194,000 regular listeners in Sweden, and 2.1 million regular listeners in France, was an indication of a positive impact. As further evidence, he noted that the VOA received 30,000 letters a month from listeners all over the world, and hundreds of thousands of requests for broadcasting schedules. There was an analysis done of some of those letters sent in 1952 and 1953 while Kohler was still director. The study found that letter writing could be an indicator of successful, actionable persuasion. It was also found that broadcasts in different countries were having different effects. In one country, regular listeners adopted and practiced American values presented by the broadcast. Age was also a factor: younger and older audiences tended to like different types of programs no matter the country. Kohler used all of this as evidence to claim that the VOA helped to grow and strengthen the free world. It also influenced the UN in their decision to condemn communist actions in Korea, and was a major factor in the decline of communism in the "free world, including key countries such as Italy and France. In Italy, the VOA did not just bring an end to communism, but it caused the country to Americanize. The VOA also had an impact behind the Iron Curtain. Practically all defectors during Kohler's time claimed the VOA helped in their decision to defect. Another indication of impact, according to Kohler, was the Soviet response. Kohler argued that the soviets responded because the VOA was having an impact. Based on Soviet responses, it can be presumed that the most effective programs were ones that compared the lives of those behind and outside the iron curtain, questions on the practice of slave labor, as well as lies and errors in Stalin’s version of Marxism.
DEEWA Radio, of the VOA, airs in Pakistan. Although some listeners are suspicious that the program is promoting an American agenda, others claim to be experiencing a positive effect. Some listeners feel that the programs are giving a voice to the voiceless, leading them to a sense of empowerment.
VOA's service in Iran has had a negative impact on Kurds and Kurdistan according to the publication, Kurdish Life. They claim that the VOA has exacerbated the conflict between the Talabani and the Barzani. They further claim that the VOA is covering up wrongful imprisonments, wrongful arrests, and the building of extremist mosques. According to the same publication, Kurds are being turned into fanatics, and a new generation of terrorists is forming because of the VOA. They claim the VOA is doing this to help PUK.
There is evidence to suggest that the people who listen to the Latin American service are being influenced, but not in the way the VOA wants. Instead of understanding and adopting the American way of life, listeners are parroting values and beliefs that do not mesh with their lives. However, others have adopted a negative view of America, because they think that the VOA is propaganda.
A study was done on Chinese students in America. It found that through the VOA, they disapproved of the actions of the Chinese government. Another study was done on Chinese scholars in America, and found that the VOA had an effect on their political beliefs. Their political beliefs did not change in relation to China, though, as they did not tend to believe the VOA's reports on China.
International broadcasting Alhurra BBC World Service Deutsche Welle France 24 Propaganda in the United States State media Radio Free Europe/Radio Liberty Radio Free Asia Radio Rebelde Russia Today TV Telesur Voice of America Indonesia VOA Hausa VOA people Frank Shozo Baba Willis Conover George Kao
Official website
The Bank of America Corporation (simply referred to as Bank of America, often abbreviated as BofA) is an American multinational investment bank and financial services holding company headquartered in Charlotte, North Carolina, with central hubs in New York City, London, Hong Kong, Dallas, and Toronto. Founded in San Francisco, Bank of America was formed through NationsBank's acquisition of BankAmerica in 1998. It is the second largest banking institution in the United States, after JPMorgan Chase, and the eighth largest bank in the world. Bank of America is one of the Big Four banking institutions of the United States. It services approximately 10.73% of all American bank deposits, in direct competition with JPMorgan Chase, Citigroup, and Wells Fargo. Its primary financial services revolve around commercial banking, wealth management, and investment banking. One branch of its history stretches back to Bank of Italy, founded by Amadeo Pietro Giannini in 1904, which provided Italian immigrants who faced service discrimination various banking options. Originally headquartered in San Francisco, California, Giannini acquired Banca d'America e d'Italia (Bank of America and Italy) in 1922. The passage of landmark federal banking legislation facilitated a rapid growth in the 1950s, quickly establishing a prominent market share. After suffering a significant loss after the 1998 Russian bond default, BankAmerica, as it was then known, was acquired by the Charlotte-based NationsBank for US$62 billion. Following what was then the largest bank acquisition in history, the Bank of America Corporation was founded. Through a series of mergers and acquisitions, it built upon its commercial banking business by establishing Merrill Lynch for wealth management and Bank of America Merrill Lynch for investment banking in 2008 and 2009, respectively (since renamed BofA Securities).Both Bank of America and Merrill Lynch Wealth Management retain large market shares in their respective offerings. The investment bank is considered within the "Bulge Bracket" as the third largest investment bank in the world, as of 2018. Its wealth management side manages US$1.081 trillion in assets under management (AUM) as the second largest wealth manager in the world, after UBS. In commercial banking, Bank of America operates—but does not necessarily maintain retail branches—in all 50 states of the United States, the District of Columbia and more than 40 other countries. Its commercial banking footprint encapsulates 46 million consumer and small business relationships at 4,600 banking centers and 15,900 automated teller machines (ATMs). The bank's large market share, business activities, and economic impact has led to numerous lawsuits and investigations regarding both mortgages and financial disclosures dating back to the 2008 financial crisis. Its corporate practices of servicing the middle class and wider banking community has yielded a substantial market share since the early 20th century. As of August 2018, Bank of America has a $313.5 billion market capitalization, making it the 13th largest company in the world. As the sixth largest American public company, it garnered $102.98 billion in sales as of June 2018. Bank of America was ranked #24 on the 2018 Fortune 500 rankings of the largest US corporations by total revenue. Bank of America was named the "World's Best Bank" by the Euromoney Institutional Investor in their 2018 Awards for Excellence.
The Bank of America name first appeared in 1923, with the formation of Bank of America, Los Angeles. In 1928, it was acquired by Bank of Italy of San Francisco, which took the Bank of America name two years later.The eastern portion of the Bank of America franchise can be traced to 1784, when Massachusetts Bank was chartered—the first iteration of FleetBoston, which Bank of America acquired in 2004. In 1874, Commercial National Bank was founded in Charlotte. That bank merged with American Trust Company in 1958 to form American Commercial Bank. Two years later it became North Carolina National Bank when it merged with Security National Bank of Greensboro. In 1991, it merged with C&S/Sovran Corporation of Atlanta and Norfolk to form NationsBank. The central portion of the franchise dates to 1910, when Commercial National Bank and Continental National Bank of Chicago merged in 1910 to form Continental & Commercial National Bank, which evolved into Continental Illinois National Bank & Trust.
From a naming perspective, the history of Bank of America dates back to October 17, 1904, when Amadeo Pietro Giannini founded the Bank of Italy in San Francisco. In 1922, Bank of America, Los Angeles was established with Giannini as a minority investor. The two banks merged in 1928 and consolidated it with other bank holdings to create what would become the largest banking institution in the country. In 1986, Deutsche Bank AG acquired 100% of Banca d'America e d'Italia, a bank established in Naples in 1917 following the name-change of Banca dell'Italia Meridionale with the latter established in 1918. In 1918, another corporation, Bancitaly Corporation, was organized by A. P. Giannini, the largest stockholder of which was Stockholders Auxiliary Corporation. This company acquired the stocks of various banks located in New York City and certain foreign countries. In 1918, the Bank opened a Delegation in New York in order to follow American political, economic and financial affairs more closely. In 1928, Giannini merged his bank with Bank of America, Los Angeles, headed by Orra E. Monnette. Bank of Italy was renamed on November 3, 1930, to Bank of America National Trust and Savings Association, which was the only such designated bank in the United States at that time. Giannini and Monnette headed the resulting company, serving as co-chairs.
Giannini introduced branch banking shortly after 1909 legislation in California allowed for branch banking in the state, establishing the bank's first branch outside San Francisco in 1909 in San Jose. By 1929 the bank had 453 banking offices in California with aggregate resources of over US$1.4 billion. There is a replica of the 1909 Bank of Italy branch bank in History Park in San Jose, and the 1925 Bank of Italy Building is an important downtown landmark. Giannini sought to build a national bank, expanding into most of the western states as well as into the insurance industry, under the aegis of his holding company, Transamerica Corporation. In 1953 regulators succeeded in forcing the separation of Transamerica Corporation and Bank of America under the Clayton Antitrust Act. The passage of the Bank Holding Company Act of 1956 prohibited banks from owning non-banking subsidiaries such as insurance companies. Bank of America and Transamerica were separated, with the latter company continuing in the insurance sector. However, federal banking regulators prohibited Bank of America's interstate banking activity, and Bank of America's domestic banks outside California were forced into a separate company that eventually became First Interstate Bancorp, later acquired by Wells Fargo and Company in 1996. Only in the 1980s, with a change in federal banking legislation and regulation, could Bank of America again expand its domestic consumer banking activity outside California. New technologies also allowed the direct linking of credit cards with individual bank accounts. In 1958, the bank introduced the BankAmericard, which changed its name to Visa in 1977. A coalition of regional bankcard associations introduced Interbank in 1966 to compete with BankAmericard. Interbank became Master Charge in 1966 and then MasterCard in 1979.
Following the passage of the Bank Holding Company Act of 1956, BankAmerica Corporation was established for the purpose of owning and operating Bank of America and its subsidiaries. Bank of America expanded outside California in 1983, with its acquisition, orchestrated in part by Stephen McLin, of Seafirst Corporation of Seattle, Washington, and its wholly owned banking subsidiary, Seattle-First National Bank. Seafirst was at risk of seizure by the federal government after becoming insolvent due to a series of bad loans to the oil industry. BankAmerica continued to operate its new subsidiary as Seafirst rather than Bank of America until the 1998 merger with NationsBank.BankAmerica experienced huge losses in 1986 and 1987 due to the placement of a series of bad loans in the Third World, particularly in Latin America. The company fired its CEO, Sam Armacost in 1986. Though Armacost blamed the problems on his predecessor, A.W. (Tom) Clausen, Clausen was appointed to replace Armacost. The losses resulted in a huge decline of BankAmerica stock, making it vulnerable to a hostile takeover. First Interstate Bancorp of Los Angeles (which had originated from banks once owned by BankAmerica), launched such a bid in the fall of 1986, although BankAmerica rebuffed it, mostly by selling operations. It sold its FinanceAmerica subsidiary to Chrysler and the brokerage firm Charles Schwab and Co. back to Mr. Schwab. It also sold Bank of America and Italy to Deutsche Bank. By the time of the 1987 stock-market crash, BankAmerica's share price had fallen to $8, but by 1992 it had rebounded mightily to become one of the biggest gainers of that half-decade.BankAmerica's next big acquisition came in 1992. The company acquired Security Pacific Corporation and its subsidiary Security Pacific National Bank in California and other banks in Arizona, Idaho, Oregon, and Washington, which Security Pacific had acquired in a series of acquisitions in the late 1980s. This represented, at the time, the largest bank acquisition in history. Federal regulators, however, forced the sale of roughly half of Security Pacific's Washington subsidiary, the former Rainier Bank, as the combination of Seafirst and Security Pacific Washington would have given BankAmerica too large a share of the market in that state. The Washington branches were divided and sold to West One Bancorp (now U.S. Bancorp) and KeyBank. Later that year, BankAmerica expanded into Nevada by acquiring Valley Bank of Nevada.In 1994 BankAmerica acquired the Continental Illinois National Bank and Trust Co. of Chicago. At the time, no bank possessed the resources to bail out Continental, so the federal government operated the bank for nearly a decade. Illinois then regulated branch banking extremely heavily, so Bank of America Illinois was a single-unit bank until the 21st century. BankAmerica moved its national lending department to Chicago in an effort to establish a financial beachhead in the region. These mergers helped BankAmerica Corporation to once again become the largest U.S. bank holding company in terms of deposits, but the company fell to second place in 1997 behind North Carolina's fast-growing NationsBank Corporation, and to third in 1998 behind First Union Corp. On the capital markets side, the acquisition of Continental Illinois helped BankAmerica to build a leveraged finance origination- and distribution business, which allowed the firm's existing broker-dealer, BancAmerica Securities (originally named BA Securities), to become a full-service franchise. In addition, in 1997, BankAmerica acquired Robertson Stephens, a San Francisco–based investment bank specializing in high technology for $540 million. Robertson Stephens was integrated into BancAmerica Securities and the combined subsidiary was renamed "BancAmerica Robertson Stephens".
In 1997, BankAmerica lent hedge fund D. E. Shaw & Co. $1.4 billion in order to run various businesses for the bank. However, D.E. Shaw suffered significant loss after the 1998 Russia bond default. NationsBank of Charlotte acquired BankAmerica in October 1998 in what was the largest bank acquisition in history at that time.While NationsBank was the nominal survivor, the merged bank took the better-known name of Bank of America. Hence, the holding company was renamed Bank of America Corporation, while NationsBank, N.A. merged with Bank of America NT&SA to form Bank of America, N.A. as the remaining legal bank entity. The combined bank operates under Federal Charter 13044, which was granted to Giannini's Bank of Italy on March 1, 1927. However, the merged company was and still is headquartered in Charlotte, and retains NationsBank's pre-1998 stock price history. All U.S. Securities and Exchange Commission (SEC) filings before 1998 are listed under NationsBank, not Bank of America. NationsBank president, chairman, and CEO Hugh McColl, took on the same roles with the merged company.In 1998, Bank of America possessed combined assets of $570 billion, as well as 4,800 branches in 22 states. Despite the size of the two companies, federal regulators insisted only upon the divestiture of 13 branches in New Mexico, in towns that would be left with only a single bank following the combination. The broker-dealer, NationsBanc Montgomery Securities, was named Banc of America Securities in 1998.
In 2001, McColl stepped down and named Ken Lewis as his successor. In 2004, Bank of America announced it would purchase Boston-based bank FleetBoston Financial for $47 billion in cash and stock. By merging with Bank of America, all of its banks and branches were given the Bank of America logo. At the time of merger, FleetBoston was the seventh largest bank in United States with $197 billion in assets, over 20 million customers and revenue of $12 billion. Hundreds of FleetBoston workers lost their jobs or were demoted, according to The Boston Globe. On June 30, 2005, Bank of America announced it would purchase credit card giant MBNA for $35 billion in cash and stock. The Federal Reserve Board gave final approval to the merger on December 15, 2005, and the merger closed on January 1, 2006. The acquisition of MBNA provided Bank of America a leading domestic and foreign credit card issuer. The combined Bank of America Card Services organization, including the former MBNA, had more than 40 million U.S. accounts and nearly $140 billion in outstanding balances. Under Bank of America, the operation was renamed FIA Card Services. Bank of America operated under the name BankBoston in many other Latin American countries, including Brazil. In 2006, Bank of America sold BankBoston's operations to Brazilian bank Banco Itaú, in exchange for Itaú shares. The BankBoston name and trademarks were not part of the transaction and, as part of the sale agreement, cannot be used by Bank of America (ending the BankBoston brand). In May 2006, Bank of America and Banco Itaú (Investimentos Itaú S.A.) entered into an acquisition agreement through which Itaú agreed to acquire BankBoston's operations in Brazil and was granted an exclusive right to purchase Bank of America's operations in Chile and Uruguay. The deal was signed in August 2006 under which Itaú agreed to purchase Bank of America's operations in Chile and Uruguay. Prior to the transaction, BankBoston's Brazilian operations included asset management, private banking, a credit card portfolio, and small, middle-market, and large corporate segments. It had 66 branches and 203,000 clients in Brazil. BankBoston in Chile had 44 branches and 58,000 clients and in Uruguay, it had 15 branches. In addition, there was a credit card company, OCA, in Uruguay, which had 23 branches. BankBoston N.A. in Uruguay, together with OCA, jointly served 372,000 clients. While the BankBoston name and trademarks were not part of the transaction, as part of the sale agreement, they cannot be used by Bank of America in Brazil, Chile or Uruguay following the transactions. Hence, the BankBoston name has disappeared from Brazil, Chile and Uruguay. The Itaú stock received by Bank of America in the transactions has allowed Bank of America's stake in Itaú to reach 11.51%. Banco de Boston de Brazil had been founded in 1947. On November 20, 2006, Bank of America announced the purchase of The United States Trust Company for $3.3 billion, from the Charles Schwab Corporation. US Trust had about $100 billion of assets under management and over 150 years of experience. The deal closed July 1, 2007.On September 14, 2007, Bank of America won approval from the Federal Reserve to acquire LaSalle Bank Corporation from ABN AMRO for $21 billion. With this purchase, Bank of America possessed $1.7 trillion in assets. A Dutch court blocked the sale until it was later approved in July. The acquisition was completed on October 1, 2007. Many of LaSalle's branches and offices had already taken over smaller regional banks within the previous decade, such as Lansing and Detroit-based Michigan National Bank. The acquisition also included the Chicago Marathon event, which ABN AMRO acquired in 1996. Bank of America took over the event starting with the 2007 race. The deal increased Bank of America's presence in Illinois, Michigan, and Indiana by 411 branches, 17,000 commercial bank clients, 1.4 million retail customers, and 1,500 ATMs. Bank of America became the largest bank in the Chicago market with 197 offices and 14% of the deposit share, surpassing JPMorgan Chase. LaSalle Bank and LaSalle Bank Midwest branches adopted the Bank of America name on May 5, 2008.Ken Lewis, who had lost the title of Chairman of the Board, announced that he would retire as CEO effective December 31, 2009, in part due to controversy and legal investigations concerning the purchase of Merrill Lynch. Brian Moynihan became president and CEO effective January 1, 2010, and afterward credit card charge offs and delinquencies declined in January. Bank of America also repaid the $45 billion it had received from the Troubled Assets Relief Program.
On August 23, 2007, the company announced a $2 billion repurchase agreement for Countrywide Financial. This purchase of preferred stock was arranged to provide a return on investment of 7.25% per annum and provided the option to purchase common stock at a price of $18 per share.On January 11, 2008, Bank of America announced that it would buy Countrywide Financial for $4.1 billion. In March 2008, it was reported that the Federal Bureau of Investigation (FBI) was investigating Countrywide for possible fraud relating to home loans and mortgages. This news did not hinder the acquisition, which was completed in July 2008, giving the bank a substantial market share of the mortgage business, and access to Countrywide's resources for servicing mortgages. The acquisition was seen as preventing a potential bankruptcy for Countrywide. Countrywide, however, denied that it was close to bankruptcy. Countrywide provided mortgage servicing for nine million mortgages valued at $1.4 trillion as of December 31, 2007.This purchase made Bank of America Corporation the leading mortgage originator and servicer in the U.S., controlling 20–25% of the home loan market. The deal was structured to merge Countrywide with the Red Oak Merger Corporation, which Bank of America created as an independent subsidiary. It has been suggested that the deal was structured this way to prevent a potential bankruptcy stemming from large losses in Countrywide hurting the parent organization by keeping Countrywide bankruptcy remote. Countrywide Financial has changed its name to Bank of America Home Loans. In December 2011, the Justice Department announced a $335 million settlement with Bank of America over discriminatory lending practice at Countrywide Financial. Attorney General Eric Holder said a federal probe found discrimination against qualified African-American and Latino borrowers from 2004 to 2008. He said that minority borrowers who qualified for prime loans were steered into higher-interest-rate subprime loans.
On September 14, 2008, Bank of America announced its intention to purchase Merrill Lynch & Co., Inc. in an all-stock deal worth approximately $50 billion. Merrill Lynch was at the time within days of collapse, and the acquisition effectively saved Merrill from bankruptcy. Around the same time Bank of America was reportedly also in talks to purchase Lehman Brothers, however a lack of government guarantees caused the bank to abandon talks with Lehman. Lehman Brothers filed for bankruptcy the same day Bank of America announced its plans to acquire Merrill Lynch. This acquisition made Bank of America the largest financial services company in the world. Temasek Holdings, the largest shareholder of Merrill Lynch & Co., Inc., briefly became one of the largest shareholders of Bank of America, with a 3% stake. However, taking a loss Reuters estimated at $3 billion, the Singapore sovereign wealth fund sold its whole stake in Bank of America in the first quarter of 2009.Shareholders of both companies approved the acquisition on December 5, 2008, and the deal closed January 1, 2009. Bank of America had planned to retain various members of the then Merrill Lynch's CEO, John Thain's management team after the merger. However, after Thain was removed from his position, most of his allies left. The departure of Nelson Chai, who had been named Asia-Pacific president, left just one of Thain's hires in place: Tom Montag, head of sales and trading.The bank, in its January 16, 2009, earnings release, revealed massive losses at Merrill Lynch in the fourth quarter, which necessitated an infusion of money that had previously been negotiated with the government as part of the government-persuaded deal for the bank to acquire Merrill. Merrill recorded an operating loss of $21.5 billion in the quarter, mainly in its sales and trading operations, led by Tom Montag. The bank also disclosed it tried to abandon the deal in December after the extent of Merrill's trading losses surfaced, but was compelled to complete the merger by the U.S. government. The bank's stock price sank to $7.18, its lowest level in 17 years, after announcing earnings and the Merrill mishap. The market capitalization of Bank of America, including Merrill Lynch, was then $45 billion, less than the $50 billion it offered for Merrill just four months earlier, and down $108 billion from the merger announcement. Bank of America CEO Kenneth Lewis testified before Congress that he had some misgivings about the acquisition of Merrill Lynch and that federal officials pressured him to proceed with the deal or face losing his job and endangering the bank's relationship with federal regulators.Lewis' statement is backed up by internal emails subpoenaed by Republican lawmakers on the House Oversight Committee. In one of the emails, Richmond Federal Reserve President Jeffrey Lacker threatened that if the acquisition did not go through, and later Bank of America were forced to request federal assistance, the management of Bank of America would be "gone". Other emails, read by Congressman Dennis Kucinich during the course of Lewis' testimony, state that Mr. Lewis had foreseen the outrage from his shareholders that the purchase of Merrill would cause, and asked government regulators to issue a letter stating that the government had ordered him to complete the deal to acquire Merrill. Lewis, for his part, states he didn't recall requesting such a letter. The acquisition made Bank of America the number one underwriter of global high-yield debt, the third largest underwriter of global equity and the ninth largest adviser on global mergers and acquisitions. As the credit crisis eased, losses at Merrill Lynch subsided, and the subsidiary generated $3.7 billion of Bank of America's $4.2 billion in profit by the end of quarter one in 2009, and over 25% in quarter 3 2009.On September 28, 2012, Bank of America settled the class action lawsuit over the Merrill Lynch acquisition and will pay $2.43 billion. This was one of the first major securities class action lawsuits stemming from the financial crisis of 2007–2008 to settle. Many major financial institutions had a stake in this lawsuit, including Chicago Clearing Corporation, hedge funds, and bank trusts, due to the belief that Bank of America stock was a sure investment.
Bank of America received $20 billion in the federal bailout from the U.S. government through the Troubled Asset Relief Program (TARP) on January 16, 2009, and a guarantee of $118 billion in potential losses at the company. This was in addition to the $25 billion given to them in the fall of 2008 through TARP. The additional payment was part of a deal with the U.S. government to preserve Bank of America's merger with the troubled investment firm Merrill Lynch. Since then, members of the U.S. Congress have expressed considerable concern about how this money has been spent, especially since some of the recipients have been accused of misusing the bailout money. Then CEO Ken Lewis was quoted as claiming "We are still lending, and we are lending far more because of the TARP program." Members of the U.S. House of Representatives, however, were skeptical and quoted many anecdotes about loan applicants (particularly small business owners) being denied loans and credit card holders facing stiffer terms on the debt in their card accounts. According to an article in The New York Times published on March 15, 2009, Bank of America received an additional $5.2 billion in government bailout money, channeled through American International Group.As a result of its federal bailout and management problems, The Wall Street Journal reported that the Bank of America was operating under a secret "memorandum of understanding" (MOU) from the U.S. government that requires it to "overhaul its board and address perceived problems with risk and liquidity management". With the federal action, the institution has taken several steps, including arranging for six of its directors to resign and forming a Regulatory Impact Office. Bank of America faces several deadlines in July and August and if not met, could face harsher penalties by federal regulators. Bank of America did not respond to The Wall Street Journal story.On December 2, 2009, Bank of America announced it would repay the entire $45 billion it received in TARP and exit the program, using $26.2 billion of excess liquidity along with $18.6 billion to be gained in "common equivalent securities" (Tier 1 capital). The bank announced it had completed the repayment on December 9. Bank of America's Ken Lewis said during the announcement, "We appreciate the critical role that the U.S. government played last fall in helping to stabilize financial markets, and we are pleased to be able to fully repay the investment, with interest.... As America's largest bank, we have a responsibility to make good on the taxpayers' investment, and our record shows that we have been able to fulfill that commitment while continuing to lend."
In 2010, the U.S. government accused the bank of defrauding schools, hospitals, and dozens of state and local government organizations via misconduct and illegal activities involving the investment of proceeds from municipal bond sales. As a result, the bank agreed to pay $137.7 million, including $25 million to the Internal Revenue Service and $4.5 million to the state attorney general, to the affected organizations to settle the allegations.Former bank official Douglas Campbell pleaded guilty to antitrust, conspiracy, and wire fraud charges. As of January 2011, other bankers and brokers are under indictment or investigation.On October 24, 2012, the top federal prosecutor in Manhattan filed a lawsuit alleging that Bank of America fraudulently cost American taxpayers more than $1 billion when Countrywide Financial sold toxic mortgages to Fannie Mae and Freddie Mac. The scheme was called 'Hustle', or High Speed Swim Lane. On May 23, 2016, the Second U.S. Circuit Court of Appeals ruled that the finding of fact by the jury that low quality mortgages were supplied by Countrywide to Fannie Mae and Freddie Mac in the "Hustle" case supported only "intentional breach of contract," not fraud. The action, for civil fraud, relied on provisions of the Financial Institutions Reform, Recovery and Enforcement Act. The decision turned on lack of intent to defraud at the time the contract to supply mortgages was made.
During 2011, Bank of America began conducting personnel reductions of an estimated 36,000 people, contributing to intended savings of $5 billion per year by 2014.In December 2011, Forbes ranked Bank of America's financial wealth 91st out of the nation's largest 100 banks and thrift institutions.Bank of America cut around 16,000 jobs in a quicker fashion by the end of 2012 as revenue continued to decline because of new regulations and a slow economy. This put a plan one year ahead of time to eliminate 30,000 jobs under a cost-cutting program, called Project New BAC. In the first quarter of 2014, Berkshire bank purchased 20 Bank of America branches in Central and eastern New York for 14.4 million dollars. The branches were from Utica/Rome region and down the Mohawk Valley east to the capital region. In April and May 2014, Bank of America sold two dozen branches in Michigan to Huntington Bancshares. The locations were converted to Huntington National Bank branches in September.As part of its new strategy Bank of America is focused on growing its mobile banking platform. As of 2014, Bank of America has 31 million active online users and 16 million mobile users. Its retail banking branches have decreased to 4,900 as a result of increased mobile banking use and a decline in customer branch visits. By 2018, the number of mobile users has increased to 25.3 million and the number of locations fell to 4,411 at the end of June.
In August 2014, Bank of America agreed to a near–$17 billion deal to settle claims against it relating to the sale of toxic mortgage-linked securities including subprime home loans, in what was believed to be the largest settlement in U.S. corporate history. The bank agreed with the U.S. Justice Department to pay $9.65 billion in fines, and $7 billion in relief to the victims of the faulty loans which included homeowners, borrowers, pension funds and municipalities. Real estate economist Jed Kolko said the settlement is a "drop in the bucket" compared to the $700 billion in damages done to 11 million homeowners. Since the settlement covered such a substantial portion of the market, he said for most consumers "you're out of luck."Much of the government's prosecution was based on information provided by three whistleblowers – Shareef Abdou (a senior vice president at the bank), Robert Madsen (a professional appraiser employed by a bank subsidiary), and Edward O'Donnell (a Fannie Mae official). The three men received $170 million in whistleblower awards.
Bank of America has formed a partnership with the United States Department of Defense creating a newly chartered bank DOD Community Bank ("Community Bank") providing full banking services to military personnel at 68 branches and ATM locations on U.S. military installations in Guantanamo Bay Naval Base Cuba, Diego Garcia, Germany, Japan, Italy, Kwajalein Atoll, South Korea, the Netherlands, and the United Kingdom. Even though Bank of America operates Community Bank, customer services are not interchangeable between the two financial institutions, meaning that a Community Bank customer cannot go to a Bank of America branch and withdraw from their account and vice versa. Deposits made into checking and savings accounts are insured by the Federal Deposit Insurance Corporation up to $250,000 despite the fact that none of Community's operating branches are located within the jurisdictional borders of the United States.
In April 2018, Bank of America announced that it would stop providing financing to makers of military-style weapons such as the AR-15 rifle. In announcing the decision, Bank of America referenced recent mass shootings and said that it wanted to "contribute in any way we can" to reduce them.
In 2015, Bank of America began expanding organically, opening branches in cities where it previously did not have a retail presence. They started that year in Denver, followed by Minneapolis–Saint Paul and Indianapolis, in all cases having at least one of its Big Four competitors, with Chase Bank being available in Denver and Indianapolis, while Wells Fargo is available in Denver and the Twin Cities. The Twin Cities market is also the home market of U.S. Bancorp, the largest non-Big Four rival. In January 2018, Bank of America announced an organic expansion of its retail footprint into Pittsburgh and surrounding areas, to supplement its existing commercial lending and investment businesses in the area. Before the expansion, Pittsburgh had been one of the largest US cities without a retail presence by any of the Big Four, with locally based PNC Financial Services (no. 6 nationally) having a commanding market share in the area. The following month, Bank of America announced it would expand into Ohio across the state's three biggest cities (Cleveland, Columbus, and Cincinnati), which are also strongholds of Chase.
Bank of America generates 90% of its revenues in its domestic market. The core of Bank of America's strategy is to be the number one bank in its domestic market. It has achieved this through key acquisitions.
Consumer Banking, the largest division in the company, provides financial services to consumers and small businesses including, banking, investments, and lending products including business loans, mortgages, and credit cards. It provides stockbroker services via Merrill Edge, an electronic trading platform. The consumer banking division represented 38% of the company's total revenue in 2016. The company earns revenue from interest income, service charges, and fees. The company is also a mortgage servicer. It competes primarily with the retail banking arms of America's three other megabanks: Citigroup, JPMorgan Chase, and Wells Fargo. The Consumer Banking organization includes over 4,600 retail financial centers and approximately 15,900 automated teller machines. Bank of America is a member of the Global ATM Alliance, a joint venture of several major international banks that provides for reduced fees for consumers using their ATM card or check card at another bank within the Global ATM Alliance when traveling internationally. This feature is restricted to withdrawals using a debit card and users are still subject to foreign currency conversion fees, credit card withdrawals are still subject to cash advance fees and foreign currency conversion fees.
The Global Banking division provides banking services, including investment banking and lending products to businesses. It includes the businesses of Global Corporate Banking, Global Commercial Banking, Business Banking, and Global Investment Banking. The division represented 22% of the company's revenue in 2016.Before Bank of America's acquisition of Merrill Lynch, the Global Corporate and Investment Banking (GCIB) business operated as Banc of America Securities LLC. The bank's investment banking activities operate under the Merrill Lynch subsidiary and provided mergers and acquisitions advisory, underwriting, capital markets, as well as sales & trading in fixed income and equities markets. Its strongest groups include Leveraged Finance, Syndicated Loans, and mortgage-backed securities. It also has one of the largest research teams on Wall Street. Bank of America Merrill Lynch is headquartered in New York City.
The Global Wealth and Investment Management (GWIM) division manages investment assets of institutions and individuals. It includes the businesses of Merrill Lynch Global Wealth Management and U.S. Trust and represented 21% of the company's total revenue in 2016. It is among the 10 largest U.S. wealth managers. It has over $2.5 trillion in client balances. GWIM has five primary lines of business: Premier Banking & Investments (including Bank of America Investment Services, Inc.), The Private Bank, Family Wealth Advisors, and Bank of America Specialist.
The Global Markets division offers services to institutional clients, including trading in financial securities. The division provides research and other services such as market maker, and risk management using derivatives. The division represented 19% of the company's total revenues in 2016.
The Bank of America principal executive offices are located in the Bank of America Corporate Center, Charlotte, North Carolina. The skyscraper is located at 100 North Tryon Street, and stands at 871 ft (265 m), having been completed in 1992. In 2012, Bank of America cut ties to the American Legislative Exchange Council (ALEC).
Bank of America's Global Corporate and Investment Banking has its U.S. headquarters in Charlotte, European headquarters in Dublin, and Asian headquarters in Hong Kong and Singapore.
In 2007, the bank offered employees a $3,000 rebate for the purchase of hybrid vehicles. The company also provided a $1,000 rebate or a lower interest rate for customers whose homes qualified as energy efficient. In 2007, Bank of America partnered with Brighter Planet to offer an eco-friendly credit card, and later a debit card, which help build renewable energy projects with each purchase. In 2010, the bank completed construction of the 1 Bank of America Center in Charlotte center city. The tower, and accompanying hotel, is a LEED-certified building.Bank of America has also donated money to help health centers in Massachusetts and made a $1 million donation in 2007 to help homeless shelters in Miami.In 1998, the bank made a ten-year commitment of $350 billion to provide affordable mortgage, build affordable housing, support small business and create jobs in disadvantaged neighborhoods.In 2004, the bank pledged $750 million over a ten-year period for community development lending and affordable housing programs.
Pursuant to Section 953(b) of the Dodd-Frank Wall Street Reform and Consumer Protection Act, publicly traded companies are required to disclose (1) the median total annual compensation of all employees other than the CEO and (2) the ratio of the CEO's annual total compensation to that of the median employee.Total 2018 compensation for Brian Moynihan, CEO, amounted to $22,765,354, and total compensation of the median employee was determined to be $92,040. The resulting pay ratio is estimated to be 247:1.
In August 2011, Bank of America was sued for $10 billion by American International Group. Another lawsuit filed in September 2011 pertained to $57.5 billion in mortgage-backed securities Bank of America sold to Fannie Mae and Freddie Mac. That December, Bank of America agreed to pay $335 million to settle a federal government claim that Countrywide Financial had discriminated against Hispanic and African-American homebuyers from 2004 to 2008, prior to being acquired by BofA. In September 2012, BofA settled out of court for $2.4 billion in a class action lawsuit filed by BofA shareholders who felt they were misled about the purchase of Merrill Lynch. On February 9, 2012, it was announced that the five largest mortgage servicers (Ally/GMAC, Bank of America, Citi, JPMorgan Chase, and Wells Fargo) agreed to a historic settlement with the federal government and 49 states. The settlement, known as the National Mortgage Settlement (NMS), required the servicers to provide about $26 billion in relief to distressed homeowners and in direct payments to the states and the federal government. This settlement amount makes the NMS the second largest civil settlement in U.S. history, only trailing the Tobacco Master Settlement Agreement. The five banks were also required to comply with 305 new mortgage servicing standards. Oklahoma held out and agreed to settle with the banks separately. On October 24, 2012, American federal prosecutors filed a $1 billion civil lawsuit against Bank of America for mortgage fraud under the False Claims Act, which provides for possible penalties of triple the damages suffered. The government asserted that Countrywide, which was acquired by Bank of America, rubber-stamped mortgage loans to risky borrowers and forced taxpayers to guarantee billions of bad loans through Fannie Mae and Freddie Mac. The suit was filed by Preet Bharara, the United States attorney in Manhattan, the inspector general of FHFA and the special inspector for the Troubled Asset Relief Program. In March 2014, Bank of America settled the suit by agreeing to pay $6.3 billion to Fannie Mae and Freddie Mac and to buy back around $3.2 billion worth of mortgage bonds.In April 2014, the Consumer Financial Protection Bureau (CFPB) ordered Bank of America to provide an estimated $727 million in relief to consumers harmed by practices related to credit card add-on products. According to the Bureau, roughly 1.4 million customers were affected by deceptive marketing of add-on products, and 1.9 million customers were illegally charged for credit monitoring and reporting services they were not receiving. The deceptive marketing misconduct involved telemarketing scripts containing misstatements and off-script sales pitches made by telemarketers that were misleading and omitted pertinent information. The unfair billing practices involved billing customers for privacy-related products without having the authorization necessary to perform the credit monitoring and credit report retrieval services. As a result, the company billed customers for services they did not receive, unfairly charged consumers for interest and fees, illegally charged approximately 1.9 million accounts, and failed to provide the product benefit.A $7.5 million settlement was reached in April 2014 with former chief financial officer for Bank of America, Joe L. Price, over allegations that the bank's management withheld material information related to its 2008 merger with Merrill Lynch. In August 2014, the United States Department of Justice and the bank agreed to a $16.65 billion agreement over the sale of risky, mortgage-backed securities before the Great Recession; the loans behind the securities were transferred to the company when it acquired banks such as Merrill Lynch and Countrywide in 2008. As a whole, the three firms provided $965 billion of mortgage-backed securities from 2004 to 2008. The settlement was structured to give $7 billion in consumer relief and $9.65 billion in penalty payments to the federal government and state governments; California, for instance, received $300 million to recompense public pension funds. The settlement was the largest in United States history between a single company and the federal government.In 2018, former senior executive Omeed Malik filed a $100 million arbitration case through FINRA against Bank of America after the company investigated him for alleged sexual misconduct. His defamation claim was on the basis of retaliation, breach of contract, and discrimination against his Muslim background. Malik received an eight-figure settlement in July.
Parmalat SpA is a multinational Italian dairy and food corporation. Following Parmalat's 2003 bankruptcy, the company sued Bank of America for $10 billion, alleging the bank profited from its knowledge of Parmalat's financial difficulties. The parties announced a settlement in July 2009, resulting in Bank of America paying Parmalat $98.5 million in October 2009. In a related case, on April 18, 2011, an Italian court acquitted Bank of America and three other large banks, along with their employees, of charges they assisted Parmalat in concealing its fraud, and of lacking sufficient internal controls to prevent such frauds. Prosecutors did not immediately say whether they would appeal the rulings. In Parma, the banks were still charged with covering up the fraud.
In January 2008, Bank of America began notifying some customers without payment problems that their interest rates were more than doubled, up to 28%. The bank was criticized for raising rates on customers in good standing, and for declining to explain why it had done so. In September 2009, a Bank of America credit card customer, Ann Minch, posted a video on YouTube criticizing the bank for raising her interest rate. After the video went viral, she was contacted by a Bank of America representative who lowered her rate. The story attracted national attention from television and internet commentators. More recently, the bank has been criticized for allegedly seizing three properties that were not under their ownership, apparently due to incorrect addresses on their legal documents.
In October 2009, Julian Assange of WikiLeaks claimed that his organization possessed a 5 gigabyte hard drive formerly used by a Bank of America executive and that Wikileaks intended to publish its contents.In November 2010, Forbes published an interview with Assange in which he stated his intent to publish information which would turn a major U.S. bank "inside out". In response to this announcement, Bank of America stock dropped 3.2%.In December 2010, Bank of America announced that it would no longer service requests to transfer funds to WikiLeaks, stating that "Bank of America joins in the actions previously announced by MasterCard, PayPal, Visa Europe and others and will not process transactions of any type that we have reason to believe are intended for WikiLeaks... This decision is based upon our reasonable belief that WikiLeaks may be engaged in activities that are, among other things, inconsistent with our internal policies for processing payments."Later in December, it was announced that Bank of America purchased more than 300 Internet domain names in an attempt to preempt bad publicity that might be forthcoming in the anticipated WikiLeaks release. The domain names included as BrianMoynihanBlows.com, BrianMoynihanSucks.com and similar names for other top executives of the bank.Sometime before August 2011, WikiLeaks claimed that 5 GB of Bank of America leaks was part of the deletion of over 3500 communications by Daniel Domscheit-Berg, a now ex-WikiLeaks volunteer.
On March 14, 2011, members of hacker group Anonymous began releasing emails said to be from a former Bank of America employee. According to the group, the emails documented alleged "corruption and fraud". The source, identified publicly as Brian Penny, was a former LPI Specialist from Balboa Insurance, a firm which used to be owned by the bank, but was sold to Australian Reinsurance Company QBE.
In 2010 the state of Arizona launched an investigation into Bank of America for misleading homeowners who sought to modify their mortgage loans. According to the attorney general of Arizona, the bank "repeatedly has deceived" such mortgagors. In response to the investigation, the bank has given some modifications on the condition that the homeowners remove some information criticizing the bank online.
On May 6, 2015, Bank of America announced it would reduce its financial exposure to coal companies. The announcement came following pressure from universities and environmental groups. The new policy was announced as part of the bank's decision to continue to reduce credit exposure over time to the coal mining sector.
Bank of America's major competitors are Wells Fargo, Santander, PNC Financial Services, Ally Financial, Capital One, Chase Bank, US Bank, Citizens Financial Group, Citigroup and M&T Bank.
Notable buildings which Bank of America currently occupies include: Bank of America Tower in Phoenix, Arizona Bank of America Center in Los Angeles, California Transamerica Pyramid, in San Francisco 555 California Street, formerly the Bank of America Center and world headquarters, in San Francisco, California Bank of America Plaza in Fort Lauderdale, Florida Bank of America Tower in Jacksonville, Florida Bank of America Financial Center (Brickell) and Bank of America Museum Tower (Downtown Miami) in Miami, Florida Bank of America Center in Orlando, Florida Bank of America Tower in St. Petersburg, Florida Bank of America Plaza in Tampa, Florida Bank of America Plaza in Atlanta, Georgia Bank of America Building, formerly the LaSalle Bank Building in Chicago, Illinois One City Center, often called the Bank of America building due to signage rights, in Portland, Maine Bank of America Building in Baltimore, Maryland Bank of America Plaza in St Louis, Missouri Bank of America Tower in Albuquerque, New Mexico Bank of America Tower in New York City Bank of America Corporate Center in Charlotte, North Carolina (the corporate headquarters) Bank of America Plaza in Charlotte, North Carolina Bank of America Tower in Charlotte, North Carolina Hearst Tower in Charlotte, North Carolina Bank of America Plaza in Dallas, Texas Bank of America Center in Houston, Texas Bank of America Tower in Midland, Texas Bank of America Plaza in San Antonio, Texas Bank of America Fifth Avenue Plaza in Seattle, Washington Columbia Center in Seattle, Washington Bank of America Tower in Hong Kong City Place I, also known as United Healthcare Center, in Hartford, Connecticut (the tallest building in Connecticut) 9454 Wilshire Boulevard in Beverly Hills, California
The Robert B. Atwood Building in Anchorage, Alaska was at one time named the Bank of America Center, renamed in conjunction with the bank's acquisition of building tenant Security Pacific Bank. This particular branch was later acquired by Alaska-based Northrim Bank and moved across the street to the Linny Pacillo Parking Garage. The Bank of America Building (Providence) opened in 1928 as the Industrial Trust Building and remains the tallest building in Rhode Island. Through a number of mergers it was later known as the Industrial National Bank building and the Fleet Bank building. The building was leased by Bank of America from 2004 to 2012 and has been vacant since March 2013. The building is commonly known as the Superman Building based on a popular belief that it was the model for the Daily Planet building in the Superman comic books. The Miami Tower iconic in its appearance in Miami Vice was known as the Bank of America Tower for many years. It is located in Downtown Miami. On April 18, 2012, the AIA's Florida Chapter placed it on its list of Florida Architecture: 100 Years. 100 Places as the Bank of America Tower.
Official website DOD Community Bank operated by Bank of America official website Business data for Bank of America Corp:
USA Today is an internationally distributed American daily middle-market newspaper that is the flagship publication of its owner, Gannett. Founded by Al Neuharth on September 15, 1982, it operates from Gannett's corporate headquarters in McLean, Virginia. It is printed at 37 sites across the United States and at five additional sites internationally. Its dynamic design influenced the style of local, regional, and national newspapers worldwide through its use of concise reports, colorized images, informational graphics, and inclusion of popular culture stories, among other distinct features.With a weekly print circulation of 726,906, a digital only subscriber base of 504,000, and an approximate daily readership of 2.6 million, USA Today is ranked first by circulation on the list of newspapers in the United States. It has been shown to maintain a generally centrist audience, in regards to political persuasion. USA Today is distributed in all 50 states, Washington, D.C., and Puerto Rico, and an international edition is distributed in Asia, Canada, Europe, and the Pacific Islands.
The genesis of USA Today was on February 29, 1980, when a company task force known as "Project NN" met with Gannett chairman Al Neuharth in Cocoa Beach, Florida to develop a national newspaper. Early regional prototypes included East Bay Today, an Oakland, California-based publication published in the late 1970s to serve as the morning edition of the Oakland Tribune, an afternoon newspaper which Gannett owned at the time. On June 11, 1981, Gannett printed the first prototypes of the proposed publication. The two proposed design layouts were mailed to newsmakers and prominent leaders in journalism, for review and feedback. Gannett's board of directors approved the launch of the national newspaper, titled USA Today, on December 5, 1981. At launch, Neuharth was appointed president and publisher of the newspaper, adding those responsibilities to his existing position as Gannett's chief executive officer.Gannett announced the launch of the paper on April 20, 1982. USA Today began publishing on September 15, 1982, initially in the Baltimore and Washington, D.C. metropolitan areas for a newsstand price of 25¢ (equivalent to 66¢ today). After selling out the first issue, Gannett gradually expanded the national distribution of the paper, reaching an estimated circulation of 362,879 copies by the end of 1982, double the amount of sales that Gannett projected. The design uniquely incorporated color graphics and photographs. Initially, only its front news section pages were rendered in four-color, while the remaining pages were printed in a spot color format. The paper's overall style and elevated use of graphics – developed by Neuharth, in collaboration with staff graphics designers George Rorick, Sam Ward, Suzy Parker, John Sherlock and Web Bryant – was derided by critics, who referred to it as a "McPaper" or "television you can wrap fish in", because it opted to incorporate concise nuggets of information more akin to the style of television news, rather than in-depth stories like traditional newspapers, which many in the newspaper industry considered to be a dumbing down of content. Although USA Today had been profitable for just ten years as of 1997, it changed the appearance and feel of newspapers around the world.On July 2, 1984, the newspaper switched from predominantly black-and-white to full color photography and graphics in all four sections. The next week on July 10, USA Today launched an international edition intended for U.S. readers abroad, followed four months later on October 8 with the rollout of the first transmission via satellite of its international version to Singapore. On April 8, 1985, the paper published its first special bonus section, a 12-page section called "Baseball '85", which previewed the 1985 Major League Baseball season.By the fourth quarter of 1985, USA Today had become the second-largest newspaper in the United States, reaching a daily circulation of 1.4 million copies. Total daily readership of the paper by 1987 (according to Simmons Market Research Bureau statistics) had reached 5.5 million, the largest of any daily newspaper in the U.S. On May 6, 1986, USA Today began production of its international edition in Switzerland. USA Today operated at a loss for most of its first four years of operation, accumulating a total deficit of $233 million after taxes, according to figures released by Gannett in July 1987; the newspaper began turning its first profit in May 1987, six months ahead of Gannett corporate revenue projections.On January 29, 1988, USA Today published the largest edition in its history, a 78-page weekend edition featuring a section previewing Super Bowl XXII; the edition included 44.38 pages of advertising and sold 2,114,055 copies, setting a single-day record for an American newspaper (and surpassed seven months later on September 2, when its Labor Day weekend edition sold 2,257,734 copies). On April 15, USA Today launched a third international printing site, based in Hong Kong. The international edition set circulation and advertising records during August 1988, with coverage of the 1988 Summer Olympics, selling more than 60,000 copies and 100 pages of advertising.By July 1991, Simmons Market Research Bureau estimated that USA Today had a total daily readership of nearly 6.6 million, an all-time high and the largest readership of any daily newspaper in the United States. On September 1, 1991, USA Today launched a fourth printsite for its international edition in London for the United Kingdom and the British Isles. The international edition's schedule was changed as of April 1, 1994 to Monday through Friday, rather than from Tuesday through Saturday, in order to accommodate business travelers; on February 1, 1995, USA Today opened its first editorial bureau outside the United States at its Hong Kong publishing facility; additional editorial bureaus were launched in London and Moscow in 1996.On April 17, 1995, USA Today launched its website, www.usatoday.com to provide real-time news coverage; in June 2002 the site expanded to include USATODAY.com Travel, providing travel information and booking tools. On August 28, 1995, a fifth international publishing site was launched in Frankfurt, Germany, to print and distribute the international edition throughout most of Europe.On October 4, 1999, USA Today began running advertisements on its front page for the first time. In 2017, some pages of USA Today's website features Auto-Play functionality for video or audio-aided stories. On February 8, 2000, Gannett launched USA Today Live, a broadcast and Internet initiative designed to provide coverage from the newspaper to broadcast television stations nationwide for use in their local newscasts and their websites; the venture also provided integration with the USA Today website, which transitioned from a text-based format to feature audio and video clips of news content.The paper launched a sixth printing site for its international edition on May 15, 2000, in Milan, Italy, followed on July 10 by the launch of an international printing facility in Charleroi, Belgium.In 2001, two interactive units were launched: on June 19, USA Today and Gannett Newspapers launched the USA Today Careers Network (now Careers.com), a website featuring localized employment listings, then on July 18, the USA Today News Center was launched as an interactive television news service developed through a joint venture with the On Command Corporation that was distributed to hotels around the United States. On September 12 of that year, the newspaper set an all-time single day circulation record, selling 3,638,600 copies for its edition covering the September 11 attacks. That November, USA Today migrated its operations from Gannett's previous corporate headquarters in Arlington, Virginia to the company's new headquarters in nearby McLean.On December 12, 2005, Gannett announced that it would combine the separate newsroom operations of the online and print entities of USA Today, with USAToday.com's vice president and editor-in-chief Kinsey Wilson promoted to co-executive editor, alongside existing executive editor John Hillkirk.In December 2010, USA Today launched the USA Today API for sharing data with partners of all types.
On August 27, 2010, USA Today announced that it would undergo a reorganization of its newsroom, announcing the layoffs of 130 staffers. It also announced that the paper would shift its focus away from print and place more emphasis on its digital platforms (including USAToday.com and its related mobile applications) and launch of a new publication called USA Today Sports. On January 24, 2011, to reverse a revenue slide, the paper introduced a tweaked format that modified the appearance of its front section pages, which included a larger logo at the top of each page; coloring tweaks to section front pages; a new sans-serif font, called Prelo, for certain headlines of main stories (replacing the Gulliver typeface that had been implemented for story headers in April 2000); an updated "Newsline" feature featuring larger, "newsier" headline entry points; and the increasing and decreasing of mastheads and white space to present a cleaner style.
On September 14, 2012, USA Today underwent the first major redesign in its history, in commemoration for the 30th anniversary of the paper's first edition. Developed in conjunction with brand design firm Wolff Olins, the print edition of USA Today added a page covering technology stories and expanded travel coverage within the Life section and increased the number of color pages included in each edition, while retaining longtime elements. The "globe" logo used since the paper's inception was replaced with a new logo featuring a large circle rendered in colors corresponding to each of the sections, serving as an infographic that changes with news stories, containing images representing that day's top stories.The paper's website was also extensively overhauled using a new, in-house content management system known as Presto and a design created by Fantasy Interactive, that incorporates flipboard-style navigation to switch between individual stories (which obscure most of the main and section pages), clickable video advertising and a responsive design layout. The site was designed to be more interactive, provide optimizations for mobile and touchscreen devices, provide "high impact" advertising units, and provide the ability for Gannett to syndicate USA Today content to the websites of its local properties, and vice versa. To accomplish this goal, Gannett migrated its newspaper and television station websites to the Presto platform and the USA Today site design throughout 2013 and 2014 (although archive content accessible through search engines remains available through the pre-relaunch design).
On October 6, 2013, Gannett test launched a condensed daily edition of USA Today (part of what was internally known within Gannett as the "Butterfly" initiative) for distribution as an insert in four of its newspapers – The Indianapolis Star, the Rochester Democrat & Chronicle, the Fort Myers-based The News-Press and the Appleton, Wisconsin-based The Post-Crescent. The launch of the syndicated insert caused USA Today to restructure its operations to allow seven-day-a-week production to accommodate the packaging of its national and international news content and enterprise stories (comprising about 10 pages for the weekday and Saturday editions, and up to 22 pages for the Sunday edition) into the pilot insert. Gannett later announced on December 11, that it would formally launch the condensed daily edition of USA Today in 31 additional local newspapers nationwide through April 2014 (with the Palm Springs, California-based The Desert Sun and the Lafayette, Louisiana-based Advertiser being the first newspapers outside of the pilot program participants to add the supplement on December 15), citing "positive feedback" to the feature from readers and advertisers of the initial four papers. Gannett was given permission from the Alliance for Audited Media to count the circulation figures from the syndicated local insert with the total circulation count for the flagship national edition of USA Today.On January 4, 2014, USA Today acquired the consumer product review website Reviewed. In the first quarter of 2014, Gannett launched a condensed USA Today insert into 31 other newspapers in its network, thereby increasing the number of inserts to 35, in an effort to shore up circulation after it regained its position as the highest circulated week daily newspaper in the United States in October 2013. On September 3, 2014, USA Today announced that it would lay off roughly 70 employees in a restructuring of its newsroom and business operations. In October 2014, USA Today and OpenWager Inc. entered into a partnership to release a Bingo mobile app called USA TODAY Bingo Cruise.On December 3, 2015, Gannett formally launched the USA Today Network, a national digital newsgathering service providing shared content between USA Today and the company's 92 local newspapers throughout the United States as well as pooling advertising services on both a hyperlocal and national reach. The Louisville Courier-Journal had earlier soft-launched the service as part of a pilot program started on November 17, coinciding with an imaging rebrand for the Louisville, Kentucky-based newspaper; Gannett's other local newspaper properties, as well as those it acquired through its merger with the Journal Media Group, gradually began identifying themselves as part of the USA Today Network (foregoing use of the Gannett name outside of requisite ownership references) through early January 2016.
USA Today is known for synthesizing news down to easy-to-read-and-comprehend stories. In the main edition circulated in the United States and Canada, each edition consists of four sections: News (the oft-labeled "front page" section), Money, Sports, and Life. Since March 1998, the Friday edition of Life has been separated into two distinct sections: the regular Life focusing on entertainment (subtitled Weekend; section E), which features television reviews and listings, a DVD column, film reviews and trends, and a travel supplement called Destinations & Diversions (section D). The international edition of the paper features two sections: News and Money in one; with Sports and Life in the other. Atypical of most daily newspapers, the paper does not print on Saturdays and Sundays; the Friday edition serves as the weekend edition (although USA Today has published special Saturday and Sunday editions in the past, the first being published on January 19, 1991, when it released a Saturday "Extra" edition updating coverage of the Gulf War from the previous day; the paper published special seven-day-a-week editions for the first time on July 19, 1996, when it published special editions for exclusive distribution in the host city of Atlanta and surrounding areas for the two-week duration of the 1996 Summer Olympics). USA Today prints each complete story on the front page of the respective section with the exception of the cover story. The cover story is a longer story that requires a jump (readers must turn to another page in the paper to complete the story, usually the next page of that section). On certain days, the news or sports section will take up two paper sections, and there will be a second cover story within the second section. Each section is denoted by a certain color to differentiate sections beyond lettering and is seen in a box the top-left corner of the first page; the principal section colors are blue for News (section A), green for Money (section B), red for Sports (section C), and purple for Life (section D); in the paper's early years, the Life and Money sections were also assigned blue nameplates and spot color, as the presses used at USA Today' printing facilities did not yet accommodate the use of other colors to denote all four original sections. Orange is used for bonus sections (section E or above), which are published occasionally such as for business travel trends and the Olympics; other bonus sections for sports (such as for the PGA Tour preview, NCAA Basketball Tournaments, Memorial Day auto races (Indianapolis 500 and Coca-Cola 600), NFL opening weekend and the Super Bowl) previously used the orange color, but now use the red designated for sports in their bonus sections. To increase their ties to USA Today, Gannett incorporated the USA Today coloring scheme into an internally created graphics package for news programming that the company began phasing in across its television station group – which were spun-off in July 2015 into the separate broadcast and digital media company Tegna – in late 2012 (the package utilizes the color scheme for a rundown graphic used on most stations – outside those that Gannett acquired in 2014 from London Broadcasting, which began implementing the package in late 2015 – that persists throughout its stations' newscasts, as well as bumpers for individual story topics). Gannett's television stations began to a new on-air appearance that uses a color-coding system identical to that of the paper. In many ways, USA Today is set up to break the typical newspaper layout. Some examples of that divergence from tradition include using the left-hand quarter of each section as reefers (front-page paragraphs referring to stories on inside pages), sometimes using sentence-length blurbs to describe stories inside; the lead reefer is the cover page feature "Newsline", which shows summarized descriptions of headline stories featured in all four main sections and any special sections. As a national newspaper, USA Today cannot focus on the weather for any one city. Therefore, the entire back page of the News section is used for weather maps for the continental United States, Puerto Rico and the United States Virgin Islands, and temperature lists for many cities throughout the U.S. and the world (temperatures for individual cities on the primary forecast map and temperature lists are suffixed with a one- or two-letter code, such as "t" for thunderstorms, referencing the expected weather conditions); the colorized forecast map, originally created by staff designer George Rorick (who left USA Today for a similar position at The Detroit News in 1986), was copied by newspapers around the world, breaking from the traditional style of using monochrome contouring or simplistic text to denote temperature ranges. National precipitation maps for the next three days (previously five days until the 2012 redesign), and four-day forecasts and air quality indexes for 36 major U.S. cities (originally 16 cities prior to 1999) – with individual cities color-coded by the temperature contour corresponding to the given area on the forecast map – are also featured. Weather data is provided by AccuWeather, which has served as the forecast provider for USA Today for most of the paper's existence (with an exception from January 2002 to September 2012, when The Weather Channel provided data through a long-term multimedia content agreement with Gannett). In the bottom left-hand corner of the weather page is "Weather Focus", a graphic which explains various meteorological phenomena. On some days, the Weather Focus could be a photo of a rare meteorological event. On Mondays, the Money section uses its back page for "Market Trends", a feature that launched in June 2002 and presents an unusual graphic depicting the performance of various industry groups as a function of quarterly, monthly, and weekly movements against the S&P 500. On days featuring bonus sections or business holidays, the Money and Life sections are usually combined into one section, while combinations of the Friday Life editions into one section are common during quiet weeks. Advertising coverage is seen in the Monday Money section, which often includes a review of a current television ad, and after Super Bowl Sunday, a review of the ads aired during the broadcast with the results of the Ad Track live survey. Stock tables for individual stock exchanges (comprising one subsection for companies traded on the New York Stock Exchange, and another for companies trading on NASDAQ and the American Stock Exchange) and mutual indexes were discontinued with the 2012 redesign due to the myriad of electronic ways to check individual stock prices, in line with most newspapers. Book coverage, including reviews and a national sales chart (the latter of which debuted on October 28, 1994), is seen on Thursdays in Life, with the official full A.C. Nielsen television ratings chart printed on Wednesdays or Thursdays, depending on release. The paper also publishes the Mediabase survey for several genres of music, based on radio airplay spins on Tuesdays, along with their own chart of the top ten singles in general on Wednesdays. Because of the same limitations cited for its nationalized forecasts, the television page in Life – which provides prime time and late night listings (running from 8:00 p.m. to 12:30 a.m. Eastern Time Zone) – incorporates a boilerplate "Local news" or "Local programming" descriptions to denote time periods in which the five major English language broadcast networks (ABC, NBC, CBS, Fox and The CW) cede airtime to allow their affiliates to carry syndicated programs or local newscasts; the television page has never been accompanied by a weekly listings supplement with broader scheduling information similar to those featured in local newspapers. Like most national papers, USA Today does not carry comic strips. One of the staples of the News section is "Across the USA", a state-by-state roundup of headlines. The summaries consist of paragraph-length Associated Press reports highlighting one story of note in each state, the District of Columbia, and one U.S. territory. Similarly, the "For the Record" page of the Sports section (which features sports scores for both the previous four days of league play and individual non-league events, seasonal league statistics and wagering lines for the current day's games). The page previously featured a rundown of winning numbers from the previous deadline date for all participating state lotteries and individual multi-state lotteries. Some traditions have been retained. The lead story still appears on the upper-right hand of the front page. Commentary and political cartoons occupy the last few pages of the News section. Stock and mutual fund data are presented in the Money section. But USA Today is sufficiently different in aesthetics to be recognized on sight, even in a mix of other newspapers, such as at a newsstand. The overall design and layout of USA Today has been described as neo-Victorian.Also, in most of the sections' front pages, on the lower left-hand corner, are "USA Today Snapshots", which give statistics of various lifestyle interests according to the section it is in (for example, a snapshot in "Life" could show how many people tend to watch a certain genre of television show based upon the type of mood they are in at the time). These "Snapshots" are shown through graphs that are made up of various illustrations of objects that roughly pertain to the graphs subject matter (using the example above, the graph's bars could be made up of several TV sets, or ended by one). These are usually loosely based on research by a national institute (with the credited source mentioned in fine print in the box below the graph). The newspaper also features an occasional magazine supplement called Open Air, which launched on March 7, 2008 and appears several times a year. Various other advertorials appear throughout the year, mainly on Fridays.
The opinion section prints USA Today editorials, columns by guest writers and members of the Editorial Board of Contributors, letters to the editor, and editorial cartoons. One unique feature of the USA Today editorial page is the publication of opposing points of view; alongside the editorial board's piece on the day's topic runs an opposing view by a guest writer, often an expert in the field. The opinion pieces featured in each edition are decided by the Board of Contributors, which are separate from the paper's news staff.From 1999 to 2002 and from 2004 to 2015, the editorial page editor was Brian Gallagher, who has worked for the newspaper since its founding in 1982. Other members of the Editorial Board included deputy editorial page editor Bill Sternberg, executive forum editor John Siniff, op-ed/forum page editor Glen Nishimura, operations editor Thuan Le Elston, letters editor Michelle Poblete, web content editor Eileen Rivers, and editorial writers Dan Carney, George Hager, and Saundra Torry. The newspaper's website calls this group "demographically and ideologically diverse."Beginning with the 1984 United States presidential election, USA Today has traditionally maintained a policy not to endorse candidates for the President of the United States or any other state or federal political office, which has been since re-evaluated by the paper's Board of Contributors through an independent process during each four-year election cycle, with any decision to circumvent the policy based on a consensus vote in which fewer than two of the editorial board's members dissent or hold differing opinions. For most of its history, the paper's political editorials (most of them linked to the then-current Presidential election cycle) had focused instead on providing opinion on major issues based on the differing concerns of voters, the vast amount of information on these themes, and the board's aim to provide a fair viewpoint through the diverse political ideologies of its members and avoid reader perceptions of bias. Such avoidance of doing political editorials played a great part in USA Today's long-standing reputation for "fluff", but after its 30th anniversary revamp, the paper took a more active stance on political issues, calling for stronger gun laws after the Sandy Hook Elementary School shooting in 2012. It heavily criticized the Republican Party for both the 2013 government shutdown and the 2015 revolts in the United States House of Representatives that ended with the resignation of John Boehner as House Speaker. It also called out then-President Barack Obama and other top members of the Democratic Party for what they perceived as "inaction" over several issues during 2013–14, particularly over the NSA scandal and the ISIL beheading incidents. The editorial board broke from the "non-endorsement" policy for the first time on September 29, 2016, when it published an op-ed piece condemning the candidacy of Republican nominee Donald Trump, calling him "unfit for the presidency" due to his inflammatory campaign rhetoric (particularly that aimed at the press, with certain media organizations being openly targeted and even banned from campaign rallies, including The New York Times, The Washington Post, CNN and the BBC, military veterans who had been prisoners of war, including 2008 Republican presidential candidate and Vietnam War veteran John McCain, immigrants, and various ethnic and religious groups); his temperament and lack of financial transparency; his "checkered" business record; his use of false and hyperbolic statements; the inconsistency of his viewpoints and issues with his vision on domestic and foreign policy; and, based on comments he has made during his campaign and criticisms by both Democrats and Republicans on these views, the potential risks to national security and constitutional ethics under a Trump administration, asking voters to "resist the siren song of a dangerous demagogue". The board noted that the piece was not a "qualified endorsement" of Democratic nominee Hillary Clinton, for whom the board was unable to reach a consensus for endorsing (some editorial board members expressed that Clinton's public service record would help her "serve the nation ably as its president", while others had "serious reservations about [her] sense of entitlement, [...] lack of candor and [...] extreme carelessness in handling classified information"), endorsing instead tactical voting against Trump and GOP seats in swing states, advising voters to decide whether to vote for either Clinton, Libertarian nominee Gary Johnson, Green Party nominee Jill Stein or a write-in candidate for President; or focus on Senate, House and other down-ballot political races.In February 2018, USA Today stirred controversy by publishing an op-ed by Jerome Corsi, the DC bureau chief for the fringe conspiracy website InfoWars. Corsi, a prominent conspiracy theorist, was described by USA Today as an "author" and "investigative journalist". Corsi was a prominent proponent of the false conspiracy theory that Barack Obama was not a US citizen, and Infowars has promoted conspiracy theories such as 9/11 being an inside job and the Sandy Hook massacre being a hoax staged by child actors.In October 2018, USA Today was criticized for publishing an editorial by President Trump that was replete with inaccuracies. The Washington Post fact-checker said that "almost every sentence contained a misleading statement or a falsehood." In 2020, USA Today endorsed a specific presidential candidate for the first time, endorsing Democrat nominee Joe Biden. The newspaper also published an opposing editorial by Vice President Mike Pence, which called for the re-election of Trump.
In May 2012, Larry Kramer – a 40-year media industry veteran and former president of CBS Digital Media – was appointed president and publisher of USA Today, replacing David Hunke, who had been publisher of the newspaper since 2009. Kramer was tasked with developing a new strategy for the paper as it sought to increase revenue from its digital operations.In July 2012, Kramer hired David Callaway – whom the former had hired as lead editor of MarketWatch in 1999, two years after Kramer founded the website – as the paper's editor-in-chief. Callaway had previously worked at Bloomberg News covering the banking, investment-banking and asset-management businesses throughout Europe and at the Boston Herald, where he co-wrote a daily financial column on "comings and goings in the Boston business district". Conservative activist Peter Gemma has written more than 100 op-ed pieces for USA Today.The current Editor-in-Chief is Nicole Carroll, who has served since February 2018.
Bill Sternberg David Mastio Jill Lawrence – see Politics Daily Dan Carney Thuan Le Elston Josh Rivera Eileen Rivers Saundra Torry – also active in the Reporters Committee for Freedom of the Press since 2000
USA Weekend was a sister publication that launched in 1953 as Family Weekly, a national Sunday magazine supplement intended for the Sunday editions of various U.S. newspapers; it adopted its final title following Gannett's purchase of the magazine in 1985. The magazine – which was distributed to approximately 800 newspapers nationwide at its peak with most Gannett-owned local newspapers carrying it by default within their Sunday editions – focused primarily on social issues, entertainment, health, food and travel. On December 5, 2014, Gannett announced that it would cease publishing USA Weekend after the December 26–28, 2014 edition, citing increasing operational costs and reduced advertising revenue, with most of its participating newspapers choosing to replace it with competing Sunday magazine Parade.
USA Today Sports Weekly is a weekly magazine that covers news and statistics from Major League Baseball, Minor League Baseball and NCAA baseball, the National Football League (NFL) and NASCAR. It was first published on April 5, 1991 as USA Today Baseball Weekly, a tabloid-sized baseball-focused publication released on Wednesdays, on a weekly basis during the baseball season and bi-weekly during the off-season; the magazine expanded its sports coverage on September 4, 2002, when it adopted its current title after added stories about the NFL. Sports Weekly added coverage of NASCAR on February 15, 2006, lasting only during that year's race season; and added coverage of NCAA college football on August 8, 2007. The editorial operations of Sports Weekly originally operated autonomously from USA Today, before being integrated with the newspaper's sports department in late 2005.
In 1987, Gannett and producer/former NBC CEO Grant Tinker began developing a news magazine series for broadcast syndication that attempted to bring the breezy style of USA Today to television. The result was USA Today: The Television Show (later retitled USA Today on TV, then shortened to simply USA Today), which premiered on September 12, 1988. Correspondents on the program included Edie Magnus, Robin Young, Boyd Matson, Kenneth Walker, Dale Harimoto, Ann Abernathy, Bill Macatee and Beth Ruyak. As with the newspaper itself, the show was divided into four "sections" corresponding to the different parts of the paper: News (focusing on the major headlines of the day), Money (focusing on financial news and consumer reports), Sports (focusing on sports news and scores) and Life (focusing on entertainment and lifestyle-related stories). The series was plagued by low ratings and negative reviews from critics throughout its run. The program also suffered from being scheduled in undesirable timeslots in certain markets; this was a particular case in New York City, the country's largest media market, where CBS owned-and-operated station WCBS-TV (channel 2) aired the program in a pre-dawn early morning slot, before the program was picked up by NBC O&O WNBC five months into its run; after initially airing it in an equally undesirable 5:30 a.m. slot, the series was later moved to a more palatable 9:30 a.m. time period, but still did not fare any better on its new station (in contrast, CITY-DT in Toronto, Ontario, Canada [now the flagship of the Citytv television network], ran it at 5:00 p.m.). Although the series was renewed for a second season, these setbacks led to the mid-season cancellation of the TV version of USA Today in November 1989, after one-and-a-half seasons; the final edition aired on January 7, 1990.Gannett announced plans to develop a USA Today-branded weekly half-hour television program, to have been titled Sports Page, as part of a renewed initiative to extend the brand into television; this program, which was tapped for a fall 2004 debut, ultimately never launched.
VRtually There is a weekly virtual reality news program produced by the USA Today Network, which debuted on October 20, 2016. The program, which is available on the USA Today mobile app and on YouTube (which maintains content exclusivity through the program's dedicated channel for 60 days after each broadcast), showcases three original segments outlining news stories through a first-person perspective, recorded and produced by journalists from USA Today and its co-owned local newspapers. The program also incorporates "cubemercials", long-form advertisements created by Gannett's in-house creative studio GET Creative, which are designed to allow consumer engagenent in fully immersive experiences through virtual reality.
USA Today Minor League Player of the Year Award – First presented in 1988, this annual award has been given to a particular Minor league baseball player judged to have had the most outstanding season by a thirteen-person panel of baseball experts. USA Today All-USA high school baseball team – First presented in 1998, the award honors between nine and eleven outstanding baseball players from high schools around the United States to be part on the team (separate awards honoring the High School Baseball Player of the Year and High School Baseball Coach of the Year have been given since 1989). USA Today All-USA high school basketball team – First presented in 1983, the award honors outstanding male and female basketball players from high schools around the United States with a place on the team, with one member of each team being named as the High School Basketball Player of the Year as well as coaches from a select boys' and girls' team as the High School Basketball Coach of the Year. USA Today All-Joe Team (NFL) – First presented in 1992 in tribute to Kansas City Chiefs veteran defensive lineman Joe Phillips, the award honors 52 rookie players from throughout the NFL for their exemplary performance during the previous league season. USA Today/National Prep Poll High School Football National Championship – Predating the first publication of USA Today under the sole decision of the National Prep Poll, it is a national championship honor awarded to the best high school football team(s) in the United States, based on rankings decided by the newspaper's sports editorial department. USA Today All-USA high school football team – First presented in 1982, the award honors outstanding football players from high schools around the United States (includes ranks for the Super 25 teams in the U.S. and Top 10 teams in the East, South, Midwest and West, and USA Today High School Football Player of the Year).USA Today High School Football Coach of the Year – First presented in 1982, the award awards a coach from one of the teams selected for the All-USA football team for the honor. USA TODAY Road Warrior of the Year first presented to Joyce Gioia in 2013; never presented again.
USA Today Super Bowl Ad Meter Viewtron
Official website
Conference USA (C-USA or CUSA) is an intercollegiate athletic conference whose current member institutions are located within the Southern United States. The conference participates in the NCAA's Division I in all sports. C-USA's offices are located in Dallas, Texas.
C-USA was founded in 1995 by the merger of the Metro Conference and Great Midwest Conference, two Division I conferences that did not sponsor football. However, the merger did not include either Great Midwest member Dayton or Metro members VCU and Virginia Tech. Since this left an uneven number of schools in the conference, Houston of the dissolving Southwest Conference was extended an invitation and agreed to join following the SWC's disbanding at the end of the 1995–96 academic year. The conference immediately started competition in all sports, except football which started in 1996. Being the result of a merger, C-USA was originally a sprawling, large league that stretched from Florida to Missouri, Wisconsin to Texas. Many of its original schools were located in major urban centers and had strong basketball traditions, which helped establish the league on a national basis.
On November 27, 2012, it was announced that Tulane would leave the conference to join the Big East in all sports, and East Carolina would join the Big East for football only (ECU's membership was upgraded to all-sports in March 2013 after the Big East's non-football members, save for ACC-bound Notre Dame, announced they were leaving to form a new conference which took the Big East name, leaving the football-playing members to become the American Athletic Conference). Conference USA responded by adding Middle Tennessee and Florida Atlantic, both from the Sun Belt. On April 1, 2013, Conference USA announced they were adding Western Kentucky, also from the Sun Belt, to offset Tulsa's departure to The American in all sports which was confirmed the next day.Citing financial difficulties, the UAB football program was shut down on December 2, 2014. According to Conference USA bylaws, member schools must sponsor football. In January 2015, UAB announced an independent re-evaluation of the program and the finances involved, leaving open a possible resumption of the program as early as the 2016 season. On January 29, 2015, the conference announced that there was no time pressure in making a decision regarding UAB's future membership. The conference also stated that it would wait for the results of the new study before any further discussions on the subject. On June 1, UAB announced that it would reinstate football effective with the 2016 season, presumably keeping the school in C-USA for the immediate future. The return of football was later pushed back to 2017. The Blazers won the 2018 conference championship their second year back.
Commissioner Britton Banowsky stepped down on September 15, 2015 to become the head of the College Football Playoff Foundation. Executive associate commissioner and chief operating officer Judy MacLeod was subsequently named interim commissioner. On October 26 MacLeod was named the conference's third official commissioner, also becoming the first woman to head an FBS conference.
In 2019, Conference USA inducted its first Hall of Fame class, comprising 20 student-athletes, three coaches, and two administrators. The inductees included former University of Cincinnati basketball player Kenyon Martin, baseball player Kevin Youkilis, and men’s basketball head coach Bob Huggins.
Notes
In this table, all dates reflect the calendar year of entry into Conference USA, which for spring sports is the year before the start of competition.
Notes
In this table, all dates reflect each school's actual entry into and departure from Conference USA. For spring sports, the joining date is the calendar year before the start of competition. For fall sports, the departure date is the calendar year after the last season of competition. Notes
Full members (all-sports) Full members (non-football) Affiliate members (football-only) Affiliate member (other sport)Other Conference Other Conference
Michael Slive 1995–2002 Britton Banowsky 2002–2015 Judy MacLeod 2015–present
Conference USA sponsors championship competition in nine men's and ten women's NCAA sanctioned sports. Two schools are affiliate members for men's soccer.
Men's varsity sports not sponsored by Conference USA which are played by current full C-USA members:
Women's varsity sports not sponsored by Conference USA which are played by current full C-USA members:
This list goes through the 2012–13 season.
No team has won an NCAA team championship as a member of C-USA. However, the following C-USA teams have won national championships when they were not affiliated with C-USA:
Notes
In 2016, C-USA began a long-term television contract with lead partners ESPN and CBS Sports Network, with ESPN carrying 5 football games and the football championship game; and CBSSN carrying 6 football games, 5 basketball games, and both the men's and women's basketball championship games. C-USA also renewed and expanded its partnership with American Sports Network; owned and operated by Sinclair Broadcast Group, ASN will carry between 15 and 30 football games; between 13 and 55 men's basketball games; and between 2 and 5 women's basketball games. ASN will also carry 10 events in other C-USA sports.The conference also entered into a contract with beIN Sports for 10 football games (marking the first domestic American football rights the network has ever acquired, and the first broadcast rights deal it had ever entered into with a college conference), 10 men's and 10 women's basketball games, 12 baseball and 12 softball games, 10 men's and 10 women's soccer games (excluding conference men's soccer games at Kentucky and South Carolina, covered by their primary conference's contract), and 10 women's volleyball games.The total values of the 2016 contracts are notably lower than those of the previous contracts (which included Fox Sports).Men's soccer associate members Kentucky and South Carolina have an agreement with their primary conference for other sports to carry all home matches online through the SEC Network service, including all Conference USA conference matches. ESPN and the SEC Network will have first rights to all C-USA home men's soccer matches featuring both schools. In 2017 American Sports Network and Campus Insiders merged creating Stadium. Stadium's C-USA content will be available to stream on Twitter and Pluto TV. In 2017 Stadium completed a deal with Facebook to exclusively stream some C-USA football games. In 2017 C-USA entered an agreement with the streaming subscription service FloSports to stream three football games.
In 2016 C-USA partnered with SIDEARM Sports to create a subscription based streaming service named CUSA.tv. In a statement C-USA Commissioner Judy MacLeod said. "Thanks to our partnership with SIDEARM Sports, this new site showcases a clean modern look with easy access to information and we are proud to offer live content and original feature stories through our CUSA.tv." Various sports including football, basketball, and baseball will exclusively air on CUSA.tv when they are not picked up by other networks.
One of the current member schools, Rice University is a member of the Association of American Universities (AAU), an organization of 62 leading research universities in the United States and Canada. Six of the Conference's fourteen members are doctorate-granting universities with "very high research activity," the highest classification given by the Carnegie Foundation for the Advancement of Teaching. A majority of the Conference's members are ranked as Tier One National Universities in U.S. News and World Report's 2021 Best Colleges rankings. Notes
Official website
USA Network (on-air simply as USA, stylized as usa since 2005) is an American basic cable channel owned by the NBCUniversal Television and Streaming division of NBCUniversal, a subsidiary of Comcast. It was originally launched in 1977 as Madison Square Garden Sports Network, one of the first national sports cable television channels, before being relaunched as USA Network in 1980. Once a minor player in basic-tier pay television, USA has steadily gained popularity due to its original programming; it is one of four major subscription-television networks (with TBS, TNT and FX) that also broadcasts syndicated reruns of current and former network television series and theatrically-released feature films, as well as limited sports programming and WWE. As of September 2018, USA Network is available to about 90.4 million households (98% of households with pay television) in the US.
USA Network originally launched on September 22, 1977 as the Madison Square Garden Sports Network (not to be confused with the New York City-area regional sports network of the same name now simply known as the MSG Network). The network was founded by cable provider UA-Columbia Cablevision and Kay Koplovitz. The channel was one of the first national cable television channels, utilizing satellite delivery as opposed to the then-industry standard microwave relay to distribute its programming to cable systems. Initially, the network ran a mix of college and less well-known professional sports, similar to those found during the early years of ESPN. The channel began its broadcast day after 5:00 p.m. Eastern Time on weekdays and 12:00 p.m. Eastern Time on weekends. On April 9, 1980, the channel changed its name to USA Network after the ownership structure was reorganized under a joint operating agreement by UA-Columbia and the then-MCA Inc./Universal City Studios. That fall, USA began signing on at noon Eastern Time on weekdays; it also added some talk shows and a children's program called Calliope to its schedule. Sports programming began airing at 5:00 p.m. Eastern Time weekdays, and aired all day on weekends. In the fall of 1981, USA began its daily programming at 6:00 a.m. Eastern Time, with talk shows and children's programs running until noon, sports airing from noon onward during weekends and until 3:00 p.m. weekdays, talk shows from 3:00 to 6:00 p.m. weekdays, and sports airing again after 6:00 p.m. Eastern Time. Later, in 1982, Time Inc. and Gulf+Western's Paramount Pictures unit (now part of ViacomCBS) would buy stakes in the venture. The three partners had a non-compete clause that would prevent them from owning other basic cable networks independently from the USA joint venture, but the said clause would cause Time Inc. to drop out of the venture in 1987, as the company attempted (but failed) to buy CNN from Ted Turner and run it independently from USA. MCA and Paramount subsequently became the sole owners of the channel (with each company owning a 50% interest). In the fall of 1982, USA began operating on a 24-hour-a-day schedule, running a mix of talk shows, children's shows, and a low-budget movie from 6:00 a.m. to 6:00 p.m. Eastern Time. The channel began running a mix of 1960s and 1970s Hanna-Barbera cartoons each weekday evening from 6:00 to 7:00 p.m. as part of the USA Cartoon Express block, with sports programming airing after 7:00 p.m., which were rebroadcast during the overnight hours. Weekends featured a mix of movies, some older drama series and talk shows during the morning hours, and sports during the afternoons and evenings. Overnights consisted of old low-budget films and film shorts, and music as part of a show called Night Flight. Between 1984 and 1986, USA's programming focus began shifting away from sports, and shifted towards general entertainment programs not found on broadcast stations, including some less common network drama series and cartoons. For the 1985-1986 season, the channel had 4 hours of original and exclusive shows. One original series from the 1985-1986 season was the comedy Check It Out!. USA, wanting to become the flagship cable channel and compete directly with the broadcast networks, committed to 26 half-hours of part exclusive off-broadcast network and part original programming for the 1986-1987 season at an increase of $30 million. In one case, the channel picked up Airwolf for 58 off-network episodes, while commissioning 24 new episodes without the original cast.One tradition on USA was an afternoon lineup of game show reruns mixed in with several original low-budget productions that aired over the years. It began in October 1984 with reruns of The Gong Show and Make Me Laugh. In September 1985, the network began airing its first original game show, a revival of the mid-1970s game show Jackpot; two more original game shows, Love Me, Love Me Not, and a revival of the short-lived 1980 series Chain Reaction, were added in September 1986. More shows were progressively added soon afterward such as The Joker's Wild, Tic-Tac-Dough, Press Your Luck, High Rollers, and Hollywood Squares (with John Davidson as its "Square-Master", or host), along with Wipeout, Face the Music, and Name That Tune. In June 1987, the channel debuted another original game show, Bumper Stumpers. (All four USA original game shows in this era were taped in Canada.) When it began, the game-show block ran for an hour, but it expanded significantly the following year. By 1989, the network ran game shows Monday through Fridays from 12:00 to 5:00 p.m. Eastern Time. In January 1989, USA debuted USA Up All Night, a showcase of low-budget feature films that aired as part of its weekend overnight schedule. Up All Night became a cult favorite among viewers for the comedic wraparound segments that were usually shown during breaks leading into (and sometimes, out of) commercials and between films that were hosted by comedian Gilbert Gottfried and model/actress Rhonda Shear, the latter of whom had replaced original co-host Caroline Schlitt in 1991. Though this program was discontinued on March 7, 1998, late-night movie telecasts on USA continued to be branded under the "Up All Night" banner until 2002. Short news updates, branded as USA Updates, were shown from as early as 1989 until 2000. These segments were first produced out of KYW-TV in Philadelphia, owing to the fact that the station had already produced a number of syndicated news services (including the Group W Newsfeed) and Steve Bell, the former newsreader on Good Morning America, was employed as a primary anchor at the station. However, when KYW's news operations were heavily revamped in response to falling ratings in 1991, production of USA Updates was then taken over by the "All News Channel" (operated as a joint venture of Hubbard Broadcasting's and Viacom's CONUS Communications). The ANC-produced updates continued through 2000 (ANC was suffering heavily around this time due to competition with other cable news channels such as CNN and the then-similarly formatted Headline News, and ended up shutting down in 2002); USA Network has not carried any news programming since the news updates were discontinued. USA was the first basic cable channel to pre-empt the syndicated TV market by purchasing a package of 26 films from the Touchstone Pictures library in October 1989. To obtain the package, it spent an estimated $50 million to $60 million, with films including such box office hits as Dead Poets Society, Good Morning, Vietnam, and Three Men and a Baby.The tradition of game-show reruns continued into the 1990s with the $25,000 and $100,000 Pyramids, the early 1990s revivals of The Joker's Wild and Tic-Tac-Dough, and other well-known shows such as Scrabble, Sale of the Century, Talk About, and Caesars Challenge. Additionally, two more original game shows were added in June 1994; these were Free 4 All and Quicksilver. In September 1991, the block was reduced to three hours, from 2:00 to 5:00 p.m. Eastern. However, an additional hour was added in March 1993. In November 1994, the game-show block was cut back to only two hours, from 2:00 to 4:00 p.m. On September 24, 1992, USA launched a sister network, the Sci-Fi Channel (now Syfy), focusing on science fiction series and films. In January 1993, the channel began showing WWF Monday Night Raw, which was the first major professional wrestling program to show storylines playing out in front of an audience. In September 1993, USA adopted a new on-air look centering on the slogan "The Remote Stops Here," with flat graphics suggesting a television camera's in-lens symbols and music consisting of electric guitar and synthesized noises, though the movie presentation openers were retained from the previous design.
In 1994, Paramount Pictures parent Paramount Communications was sold to the original iteration of Viacom; the following year, MCA was acquired by Seagram. In April 1996, Viacom, which also owned MTV Networks, launched a new classic television network called TV Land. MCA subsequently sued Viacom for breach of contract, claiming that it had violated the non-compete clause in its joint venture agreement with MCA. A judge presiding over the case sided with MCA, and Viacom subsequently sold its stake in USA and the Sci-Fi Channel to Seagram for $1.7 billion. In turn, Seagram sold a controlling interest in the networks to Barry Diller in February 1998, which led to the creation of USA Networks, Inc.; the company also merged the cable channels with Diller's existing television properties including the Home Shopping Network and its broadcasting unit Silver King Broadcasting (which was restructured as USA Broadcasting, and eventually sold its stations to Univision Communications in 2001 to form the nucleus of Telefutura/UniMás). In October 1995, the network dropped the entire game show block; it was replaced with a block called USA Live, which carried reruns of Love Connection and The People's Court, with live hosted wraparound segments between shows; that block was dropped by 1997 (some of the game shows that USA had aired can still be seen on GSN and Buzzr). In 1994, USA began simulcasting the upstart business news channel Bloomberg Information TV each weekday morning from 5:00 to 8:00 a.m. Eastern and Pacific Time (and later, from 5:00 to 6:00 a.m. Eastern and Pacific on Saturdays); in 2004, the Bloomberg simulcast moved to E!, where it ran until 2007 (USA was actually the second television network to simulcast Bloomberg's programming, the now-defunct American Independent Network also carried a simulcast of the channel during the mid-1990s). On June 17, 1996, the network unveiled a new on-air appearance, which included the introduction of a new logo (incorporating a star ridged into the "U" of the now-serifed "USA" logotype, replacing the Futura-typeface logo that had been in use since the network's start under the USA Network name in 1980), and a three-note jingle. Network IDs, feature presentation intros for movies and promo graphics were based around a behind-the-scenes look at the fictional "USA Studios"; some of the IDs showed people in the control room, while a studio that was being set-up by a crew was the backdrop for the "Tonight" menu that displayed the evening's schedule. Opening sequences leading into movie telecasts showed people running through the "USA Studios Film Vault". The new look coincided with a shift in focus, more towards off-network reruns and original programming; game shows and court shows were dropped from the schedule, while cartoons were phased out. USA Studios also became the branding for USA-produced programming at this point. This logo was replaced in July 1999 in favor of a 'USA flag'-styled logo (whose design was slightly modified in 2002). In September 1996, USA replaced the USA Cartoon Express with the action-oriented children's block, USA Action Extreme Team; the channel discontinued its animation block outright in September 1998 (other than airing the first-run teen sitcom USA High and reruns of Saved by the Bell: The New Class from 1997 to 2001, USA has not aired children's programming since that time), and replaced it with a block called "USAM", which advertised itself as "Primetime Comedy in the Morning". The block mainly featured sitcoms originally aired on network television that were cancelled before making it to 100 episodes (such as The Jeff Foxworthy Show, Hearts Afire and Something So Right); however, for a time, the block also included the 1989–1994 episodes of the Bob Saget run of America's Funniest Home Videos. "USAM" was discontinued in 2002; by that point, the only sitcoms airing on USA were daytime and late night reruns of Martin and overnight airings of Living Single, Cheers and Wings, with drama series and movies populating much of the channel's daytime and primetime schedule. In 2000, USA Networks bought Canadian media company North American Television, Inc. (a joint partnership between the Canadian Broadcasting Corporation and Power Corporation of Canada), owner of cable television channels Trio and Newsworld International (the CBC continued to handle programming responsibilities for NWI until 2005, when eventual USA owner Vivendi sold the channel to a group led by Al Gore, who relaunched it as Current TV). One major shock happened when USA lost the broadcasting rights of the WWF to Viacom in June 2000; Raw (which had been retitled Raw is War) was moved to TNN in September of that year.
In 2001, USA Networks sold its non-shopping television and film assets (including USA Network, the Sci-Fi Channel, Trio, USA Films (which was rechristened as Focus Features and Studios USA) to Vivendi Universal. USA and the other channels were folded into Vivendi's Universal Television Group. In July 2002, the channel debuted Monk, which became one of USA Network's first breakout hit series. It is the comedy-drama police procedural that starred Tony Shalhoub as Adrian Monk, a former San Francisco police inspector-turned-consultant who suffers from various obsessive-compulsive behaviors that include the ability to pay attention to detail when solving crimes. It ran for eight seasons until it ended on December 4, 2009.
In 2003, General Electric agreed to merge NBC and its sibling companies with Vivendi Universal's North American-based filmed entertainment assets, including Universal Pictures and Universal Television Group in a multibillion-dollar purchase, renaming the merged company NBC Universal. GE retained an 80% ownership stake in the new company, while Vivendi retained a 20% stake. NBC Universal officially took over as owner of USA and its sibling cable channels (except for Newsworld International) in 2004. That year, USA premiered the sci-fi series The 4400.
In 2005, USA Network introduced a new logo and associated marketing campaign, "Characters Welcome". The slogan was designed to help emphasize the wide range of programming the network offered, and to help USA Network establish itself more prominently as a brand. The launch of the campaign featured promos themed around the daily lives of characters from the network's programs. To contrast itself from the "grittier" offerings of other mainstream cable networks, USA Network's original programming during this era was marked by a focus on comedic and "optimistic" action and drama series, referred to as a "blue sky" approach. Notable examples of this programming strategy included Psych (2006) (which ran for eight seasons, becoming the network's longest-running series), Burn Notice (2007), and Royal Pains (2009). In October 2005, Raw returned to USA after Viacom did not renew its broadcasting agreement with the WWE. On May 13, 2007 (in advance of NBC's 2007–08 fall upfronts presentation), NBC Universal announced that new episodes of Law & Order: Criminal Intent would be moved to USA beginning with the drama's seventh season in the fall of 2007; episodes would then be re-aired later in the season on NBC, most likely to shore up any programming holes created by the cancellation of a failed new series. Although this is not the first time a broadcast series has moved to cable (USA had acquired first-run rights to the revival of Alfred Hitchcock Presents from NBC in 1987, while The Paper Chase had moved beforehand from CBS to Showtime in 1983), it marked the first time that a series which moved its first-run episodes from broadcast to cable television would continue to air episodes on a broadcast network while it was still a first-run program. On December 7, 2007, it was announced that USA Network would continue broadcasting first-run episodes of Raw through at least 2010.The June 1, 2008 premiere of In Plain Sight, starring Mary McCormack, was USA's highest-rated series premiere since the 2006 debut of Psych, with 5.3 million viewers. In early 2009, USA Network acquired the network television rights for 24 recent and upcoming Universal Pictures films, including Duplicity, Funny People, Frost/Nixon, Land of the Lost, Milk, and State of Play.In 2011, control and majority ownership of then-parent NBC Universal passed from General Electric to Comcast. Comcast would buy out GE's remaining ownership in NBCU two years later. USA Network was considered the key piece of the NBC-Comcast merger; Wunderlich Securities analyst Matthew Harrigan projected that USA contributed $9.5 billion to NBCUniversal's $44.8 billion value, with NBC contributing only $408 million. In 2014, the channel had dropped 18% in viewership and out of first place among the major cable channels. USA has been a key NBCUniversal asset accounting for one-third of advertising revenue for NBCUniversal Cable Entertainment Group and $1 billion in annual earnings over the past few years.In April 2015, it was announced that WWE SmackDown would move to USA from sister network Syfy.
In April 2016, USA Network unveiled a new branding campaign and slogan, "We the Bold". The campaign was designed to reflect the channel's current focus on "rich, captivating stories about unlikely heroes who defy the status quo, push boundaries and are willing to risk everything for what they believe in". USA had quietly discontinued the "Characters Welcome" tagline in the lead-up to the rebranding, whose associated programming shift was led by the premieres of Mr. Robot and Colony. Variety reported that the new programming strategy was designed to appeal to themes of "authenticity, resiliency, bravery and innovation". The Washington Post felt that the re-branding symbolically marked the end of USA's "blue sky" era, as the channel had been increasingly producing more "intense" series with darker themes. NBCUniversal marketing executive Alexandra Shapiro explained that the "Characters Welcome" campaign and associated programming was reflective of the "weirdly optimistic" mood of the network's key demographic at the time.In August 2016, NBCUniversal acquired the television rights to the Harry Potter film franchise from 2018 through 2025, including the main film series and their spin-offs (with the first, Fantastic Beasts and Where to Find Them, to have its cable premiere in 2019), and other content. On cable, the films are to primarily be aired by USA Network and Syfy, and the deal also includes the ability for Universal Parks & Resorts to offer "exclusive content and events" related to the franchise (Universal Parks had already been involved in The Wizarding World of Harry Potter attractions). The deal succeeded one with Freeform; The Wall Street Journal reported the deal was valued around $250 million over the length of the agreement, making it one of the highest-valued film franchise deals. To launch the new rights, Syfy and USA aired a joint marathon over the July 13–15, 2018 weekend, airing all eight films (including directors' cuts of the first six) with limited commercial interruption.
USA Network has achieved a viewership foothold with its original programming; this began in the 1990s with initial hits such as Silk Stalkings and La Femme Nikita, which were gradually followed in the following two decades by series such as Monk, Psych, Shooter, White Collar, Covert Affairs, Mr. Robot, Suits, Burn Notice and Royal Pains. Most of its original series are scripted dramas, some of which incorporate comedic elements. In addition to its original productions, the network airs syndicated reruns of current and former network series such as Law & Order: Special Victims Unit, Law & Order: Criminal Intent (which spent the final four seasons of its run as a first-run program on USA) and NCIS. The network also broadcasts a variety of films from the Universal Pictures library and select films from other movie studios (such as Sony Pictures Entertainment, Paramount Pictures, Walt Disney Studios Motion Pictures and Warner Bros. Entertainment), airing primarily as part of its overnight and weekend schedule, and occasionally during primetime on nights when original programming or marathons of its acquired programs are not scheduled. From 1984 to 2016, the network was the longtime home of the Westminster Kennel Club Dog Show. USA is also the home of WWE's flagship cable program Raw; the series originally aired on the channel from its debut in January 1993 (when the promotion was known as the World Wrestling Federation; Raw itself replaced longtime Monday night standby WWF Prime Time Wrestling) until the series moved to TNN in September 2000, before returning to the channel in October 2005. On January 7, 2016, WWE's second flagship program SmackDown moved to USA Network from Syfy. In 2018, USA renewed its rights to Raw for five additional years, but lost the rights for SmackDown to Fox beginning October 2019. In August 2019, WWE announced that its tertiary weekly program WWE NXT would return to USA Network on September 18, 2019, airing on Wednesday nights in a two-hour live format.
USA Network has a longstanding history with sports, dating back to its existence as the Madison Square Garden Network. The network carried Major League Baseball games on Thursday nights from 1979 to 1983, and the NHL on USA ran from 1979 to 1985. College Football on USA ran from 1980 to 1986, and its telecast of the 1981 Liberty Bowl was the first college bowl game to be exclusively broadcast on cable television. The NBA on USA also aired from 1979 to 1984, the first time that the NBA had a cable television partner. Professional wrestling company WWE has had a longstanding relationship with the network; WWF Prime Time Wrestling broadcast on USA from 1985-1993 until it was superseded by WWE Raw from 1993-2000, and again since 2005. WWE SmackDown aired on the network from January 2016 until October 2019, when it moved to FOX. For 17 years from 1981 to 1998, USA aired a weekly boxing show, USA Tuesday Night Fights, which showcased bouts featuring up-and-coming boxers. Tennis on USA aired professional tournaments in the United States from 1984 to 2008, and was the longtime cable home of the US Open before its cable television rights moved to ESPN2 and the Tennis Channel in 2009. The PGA Tour on USA covered the opening two rounds of the Masters Tournament from 1982 to 2007, Ryder Cup matches from 1989 to 2010, and various other events. The USA Network aired most games of the World League of American Football (later NFL Europe) in its first two seasons of operation in 1991 and 1992; one innovation introduced for the network's WLAF telecasts was the in-helmet camera. Upon the 2004 purchase of Vivendi Universal by NBC, USA's sports division was immediately merged into NBC Sports. Since 2004, the network has broadcast select events from the Olympic Games, as part of an expansion of NBCUniversal's broadcast rights to the Summer and Winter Olympics that allowed several of the company's cable channels rights to telecast Olympic events live (some of which are later re-aired on tape delay on NBC as part of the network's primetime and late night Olympic coverage). USA Network also carried games from the International Ice Hockey Federation in 2006 and 2010. During the 2014 Winter Olympics, USA aired English Premier League soccer matches in lieu of sister channel NBCSN, due to that channel's full devotion to carrying coverage of Olympic events. After ratings success with those matches, USA began to air mid-afternoon Saturday games weekly during the 2015–16 season. USA also participates in NBC Sports' broader effort of carrying all ten Survival Sunday matches across its numerous channels the second week of May each year. Starting in 2015, USA Network became used as an overflow feed for coverage of NHL playoff games that cannot be aired by either NBCSN or CNBC. In 2016, USA aired three NASCAR races (the Sprint Cup Series race at Watkins Glen International and two Xfinity Series races at Mid-Ohio Sports Car Course and Bristol Motor Speedway) due to NBC broadcasting the 2016 Summer Olympics.
USA Network operates a high definition simulcast feed of the channel, that broadcasts in the 1080i resolution format, and is available on nearly all pay-TV providers.
In February 2007, Shaw Communications submitted an application to the Canadian Radio-television and Telecommunications Commission (CRTC), to carry the USA Network in Canada as a foreign service that would be eligible for carriage by domestic cable and satellite providers (and to automatically allow all English-language general interest cable networks from the United States into Canada). However, because of programming rights issues with other Canadian specialty channels, certain programs would be subjected to blackout restrictions, including WWE Raw.In September 2007, the CRTC refused Shaw's request to carry USA Network in Canada on the basis that the channel carried too much programming that overlapped with the English language digital cable specialty channel Mystery TV (which is then owned by Canwest – later Shaw Media – and formerly, Groupe TVA). However, on September 20, the CRTC stated that it would reconsider their denial of the eligible foreign carriage proposal for USA Network at a later date, when Shaw instead offered to carry the channel on the digital cable tiers of its Shaw Cable systems. In spite of this, the CRTC has since rejected the restructured proposal on the basis that USA's programming would be competitive with Mystery TV. Many of USA's original programs currently air on either Showcase or CTV Drama Channel. WWE programming that airs on USA also airs on Rogers Media-owned Sportsnet 360.
Regional versions of USA Network previously operated in certain South American countries (such as Argentina and Brazil); in September 2004, most of these services were renamed under the Universal Channel banner to take advantage of the more well-known brand, and to reduce the awkwardness of a channel branded with the initials of another nation.
Official website
CZ-USA is the US-based subsidiary of Česká zbrojovka Uherský Brod, a Czech firearms manufacturer. Based in Kansas City, Kansas, CZ-USA is responsible for the importation and distribution of CZ products in the United States.
CZ products are currently imported exclusively by CZ-USA, a subsidiary of the parent company, Česká zbrojovka a.s. Uherský Brod (ČZUB) of the Czech Republic. CZ firearms have been available through normal channels in the US since 1991, through such importers/distributors as Bauska, Action Arms and Magnum Research. In 1997 ČZUB recognized the need to control its own destiny in a market as big as the US. Initially based in Oakhurst, California, the CZ-USA headquarters and warehouse facility was moved to Kansas City in January 1998. CZ-USA also owns Dan Wesson Firearms based in Norwich, New York, maker of a variety of pistols, including semi-automatics and revolvers since 1968. Many CZ products sold by CZ-USA are imports from the Česká zbrojovka Uherský Brod factory, but there are several uniquely American CZ-USA products, notably the line of Safari Classics rifles and the 550 Tactical Rifles. All of these products are built by CZ-USA's custom shop in Warsaw, Missouri. The current president of CZ-USA is Alice Poluchova.
Česká zbrojovka official website CZ-USA official website History Of CZ Rifles From The CZ-USA President At The CZ Factory In The Czech Republic on YouTube
According to the legends, Usha was daughter of Banasura, powerful asura king of Sonitpur. Banasura was a great devotee of the Lord Shiva and had 1000 arms. Just like her father Usha was also devote of Shiva and Parvati. Once Usha asked Parvati who her husband would be. Parvati replied "In the moth of Baishakh a person would be appear in your dreams. That would eventually be your husband." One day, Usha saw a young prince in her dream and fell in love with him. That prince was Aniruddha, the grandson of Lord Krishna and son of Pradyumna. Usha's friend Chitralekha, through supernatural powers abducted Aniruddha from the Dwarka and brought him to Usha.When Krishna got to know about it, he came with a huge army and attacked Banasura's kingdom Sonitpur. Bana also attacked Krishna’s army with equal might, but he began to feel powerless in front of Lord Krishna. Thus, he evoked Lord Shiva to take his side. At this point, Shiva joined the battle against Krishna because he had promised protection to Banasura. Krishna himself defeated Shiva with a weapon that put Shiva to sleep. After that Krishna cut Bana's thousand arms systematically. Seeing Krishna releasing his Sudarshan Chakra to sever his head, Shiva aroused from his slumber, approached Krishna and asked him to spare Bana’s life. After that Krishna forgave Bana.After the war, Usha married Anirudha. Later Usha gave birth to a son named Vajra.
According to Shiva Purana, Usha and Annirudha reborn again as Behula and Lakshindar in next life and married each other again.
The story of Aniruddha and Usha (as Okha in Gujarati) is depicted in the 18th century Gujarati Akhyana entitled Okhaharan by Premanand Bhatt.A 1901 Telugu language play titled Usha Parinayam written by Vedam Venkataraya Sastry was based on story of Usha. The play was also taken as a Telugu film in 1961 by Kadaru Nagabhushanam under Rajarajeswari films.
A congress is a formal meeting of the representatives of different countries, constituent states, organizations, trade unions, political parties or other groups. The term originated in Late Middle English to denote an encounter (meeting of adversaries) during battle, from the Latin congressus.In the mid-1770s, the term was chosen by the 13 British colonies for the Continental Congress to emphasize the status of each colony represented there as a self-governing entity. Subsequent to the use of congress as the name for the legislature of the U.S. federal government (beginning in 1789), the term has been adopted by many nations to refer to their national legislatures.
Countries with Congresses and presidential systems: The Congress of Guatemala (Spanish: Congreso de la República) is the unicameral legislature of Guatemala. The National Congress of Honduras (Spanish: Congreso nacional) is the legislative branch of the government of Honduras. The Congress of Mexico (Spanish: Congreso de la Unión) is the legislative branch of Mexican government. The Congress of Paraguay is the bicameral legislature of Paraguay. The Congress of the Argentine Nation (Spanish: Congreso de la Nación Argentina) is the legislative branch of the government of Argentina. The Congress of the Dominican Republic is the bicameral legislature of the Dominican Republic. The Palau National Congress (Palauan: Olbiil era Kelulau) is the bicameral legislative branch of the Republic of Palau. The Congress of the Federated States of Micronesia is the unicameral legislature of the Federated States of Micronesia. The Congress of the Philippines (Filipino: Kongreso ng Pilipinas) is the legislative branch of the Philippine government. The Congress of the Republic of Peru (Spanish: Congreso de la República) is the unicameral legislature of Peru. The Congress of Colombia (Spanish: Congreso de la República) is the bicameral legislature of Colombia. The United States Congress is the bicameral legislative branch of the United States federal government. The National Congress of Bolivia was the national legislature of Bolivia before being replaced by the Plurinational Legislative Assembly. The National Congress of Brazil (Portuguese: Congresso Nacional) is the bicameral legislature of Brazil. The National Congress of Chile (Spanish: Congreso Nacional) is the legislative branch of the government of Chile. The National Congress of Ecuador was the unicameral legislature of Ecuador before being replaced by the National Assembly. France: Although France has a Parliament, the term Congress is used on two circumstances: the Congress of the French Parliament, name used specifically when both houses sit together as a single body, usually at the Palace of Versailles, to vote on revisions to the Constitution, to listen to an address by the President of the French Republic, and, in the past, to elect the President of the Republic the Congress of New Caledonia, a territorial assembly
ICCA Congress & Exhibition
The Continental Congress (1774-1781) was a convention of delegates from the Thirteen Colonies that became the governing body of the United States during the American Revolution. The Congress of the Confederation (1781-1789) was the legislature of the United States under the Articles of Confederation. The National Congress of Belgium was a temporary legislative assembly in 1830, which created a constitution for the new state. The Confederate States Congress of 1861-1865, during the American Civil War.
In France, the Congress of France (congrès) denotes a formal and rarely convened joint session of both houses of Parliament to ratify an amendment to the Constitution or to listen to a speech by the President of the French Republic. Spanish Congress of Deputies (Spanish: Congreso de los Diputados), the lower house of the Cortes Generales, Spain's legislative branch. The legislature of the People's Republic of China is known in English as the National People's Congress. The Congress of People's Deputies of the Soviet Union was the legislature and nominal supreme institution of state power in the Soviet Union from 1989 to 1991. Congress of People's Deputies of Russia, a Russian institution modeled after USSR one, existed in 1990—1993.
Congress is included in the name of several political parties, especially those in former British colonies: Guyana People's National Congress India Indian National Congress All India Trinamool Congress Kerala Congress Nationalist Congress Party Tamil Maanila Congress YSR Congress BSR Congress All India N.R. Congress Lesotho Basotho Congress Party Lesotho Congress for Democracy Lesotho People's Congress Malawi Malawi Congress Party Malaysia Malaysian Indian Congress Namibia Congress of Democrats Pakistan Peoples Revolutionary Congress Pakistan Sudan National Congress (Sudan) Fiji National Congress of Fiji Canary Islands National Congress of the Canaries Nepal Nepali Congress Sierra Leone All People's Congress South Africa African National Congress Congress of the People Pan-Africanist Congress Sri Lanka All Ceylon Tamil Congress Sri Lanka Muslim Congress Swaziland Ngwane National Liberatory Congress Trinidad and Tobago United National Congress Uganda Ugandan People's Congress
Many political parties also have a party congress every few years to make decisions for the party and elect governing bodies. This is sometimes called a political convention.
National Congress of American Indians Iraqi National Congress Congress of Racial Equality Continental Congress 2.0
Congress of Industrial Organizations Trade Union Congress of the Philippines Trades Union Congress
Congress is an alternative name for a large national or international academic conference. For instance, the World Congress on Men's Health [1] is an annual meeting on men's medical issues.
Organizations in some athletic sports, such as bowling, have historically been named "congresses". The predecessors to the United States Bowling Congress, formed in 1995, were the male-only American Bowling Congress founded in 1895, and the female-only Women's International Bowling Congress founded in 1927, which combined in 1995 to form the USBC.
A Chess congress is a chess tournament, in one city, where a large number of contestants gather to play competitive chess over a limited period of time; typically one day to one week.
European affairs events International congress calendar Medical Congresses Around the World
The United States Congress or U.S. Congress is the bicameral legislature of the federal government of the United States and consists of two chambers: the House of Representatives and the Senate. The Congress meets in the United States Capitol in Washington, D.C. Both senators and representatives are chosen through direct election, though vacancies in the Senate may be filled by a governor's appointment. Congress has 535 voting members: 100 senators and 435 representatives, the latter defined by the Reapportionment Act of 1929. In addition, the House of Representatives has six non-voting members, bringing the total membership of the US Congress to 541 or fewer in the case of vacancies. The sitting of a congress is for a two-year term, presently beginning every other January; thus, the current congress is the 116th. Elections are held every even-numbered year on Election Day. The members of the House of Representatives are elected for the two-year term of a congress representing the people of a single constituency, known as a district. Congressional districts are apportioned to states by population using the United States Census results, provided that each state has at least one congressional representative. Each state, regardless of population or size, has two senators. Currently, there are 100 senators representing the 50 states. Each senator is elected at-large in their state for a six-year term, with terms staggered, so every two years approximately one-third of the Senate is up for election. Article One of the United States Constitution requires that members of Congress must be at least 25 years old (House) or 30 years old (Senate), have been a citizen of the United States for seven (House) or nine (Senate) years, and be an inhabitant of the state which they represent. Members in both chambers may stand for re-election. The Congress was created by the Constitution of the United States and first met in 1789, replacing in its legislative function the Congress of the Confederation. Although not legally mandated, in practice since the 19th century, Congress members are typically affiliated with the Republican Party or with the Democratic Party and only rarely with a third party or independents.
Article One of the United States Constitution states, "All legislative Powers herein granted shall be vested in a Congress of the United States, which shall consist of a Senate and House of Representatives." The House and Senate are equal partners in the legislative process—legislation cannot be enacted without the consent of both chambers. However, the Constitution grants each chamber some unique powers. The Senate ratifies treaties and approves presidential appointments while the House initiates revenue-raising bills. The House initiates impeachment cases, while the Senate decides impeachment cases. A two-thirds vote of the Senate is required before an impeached person can be removed from office.The term Congress can also refer to a particular meeting of the legislature. A Congress covers two years; the current one, the 116th Congress, began on January 3, 2019, and will end on January 3, 2021. The Congress starts and ends on the third day of January of every odd-numbered year. Members of the Senate are referred to as senators; members of the House of Representatives are referred to as representatives, congresswomen, or congressmen. Scholar and representative Lee H. Hamilton asserted that the "historic mission of Congress has been to maintain freedom" and insisted it was a "driving force in American government" and a "remarkably resilient institution." Congress is the "heart and soul of our democracy," according to this view, even though legislators rarely achieve the prestige or name recognition of presidents or Supreme Court justices; one wrote that "legislators remain ghosts in America's historical imagination." One analyst argues that it is not a solely reactive institution but has played an active role in shaping government policy and is extraordinarily sensitive to public pressure. Several academics described Congress: Congress reflects us in all our strengths and all our weaknesses. It reflects our regional idiosyncrasies, our ethnic, religious, and racial diversity, our multitude of professions, and our shadings of opinion on everything from the value of war to the war over values. Congress is the government's most representative body ... Congress is essentially charged with reconciling our many points of view on the great public policy issues of the day. Congress is constantly changing and is constantly in flux. In recent times, the American south and west have gained House seats according to demographic changes recorded by the census and includes more minorities and women although both groups are still underrepresented. While power balances among the different parts of government continue to change, the internal structure of Congress is important to understand along with its interactions with so-called intermediary institutions such as political parties, civic associations, interest groups, and the mass media.The Congress of the United States serves two distinct purposes that overlap: local representation to the federal government of a congressional district by representatives and a state's at-large representation to the federal government by senators. Most incumbents seek re-election, and their historical likelihood of winning subsequent elections exceeds 90 percent.The historical records of the House of Representatives and the Senate are maintained by the Center for Legislative Archives, which is a part of the National Archives and Records Administration.Congress is directly responsible for the governing of the District of Columbia, the current seat of the federal government.
The First Continental Congress was a gathering of representatives from twelve of the thirteen colonies of North America. On July 4, 1776, the Second Continental Congress adopted the Declaration of Independence, referring to the new nation as the "United States of America." The Articles of Confederation in 1781 created the Congress of the Confederation, a unicameral body with equal representation among the states in which each state had a veto over most decisions. Congress had executive but not legislative authority, and the federal judiciary was confined to admiralty. and lacked authority to collect taxes, regulate commerce, or enforce laws. Government powerlessness led to the Convention of 1787 which proposed a revised constitution with a two–chamber or bicameral congress. Smaller states argued for equal representation for each state. The two-chamber structure had functioned well in state governments. A compromise plan, the Connecticut Compromise, was adopted with representatives chosen by population (benefiting larger states) and exactly two senators chosen by state governments (benefiting smaller states). The ratified constitution created a federal structure with two overlapping power centers so that each citizen as an individual was subjected to both the power of state government and the national government. To protect against abuse of power, each branch of government—executive, legislative, and judicial—had a separate sphere of authority and could check other branches according to the principle of the separation of powers. Furthermore, there were checks and balances within the legislature since there were two separate chambers. The new government became active in 1789.Political scientist Julian E. Zelizer suggested there were four main congressional eras, with considerable overlap, and included the formative era (1780s–1820s), the partisan era (1830s–1900s), the committee era (1910s–1960s), and the contemporary era (1970s–today).
Federalists and anti-federalists jostled for power in the early years as political parties became pronounced, surprising the Constitution's Founding Fathers of the United States. With the passage of the Constitution and the Bill of Rights, the anti-federalist movement was exhausted. Some activists joined the Anti-Administration Party that James Madison and Thomas Jefferson were forming about 1790–91 to oppose policies of Treasury Secretary Alexander Hamilton; it soon became the Democratic-Republican Party or the Jeffersonian Republican Party and began the era of the First Party System. Thomas Jefferson's election to the presidency marked a peaceful transition of power between the parties in 1800. John Marshall, 4th chief justice of the Supreme Court, empowered the courts by establishing the principle of judicial review in law in the landmark case Marbury v. Madison in 1803, effectively giving the Supreme Court a power to nullify congressional legislation.
These years were marked by growth in the power of political parties. The watershed event was the Civil War which resolved the slavery issue and unified the nation under federal authority, but weakened the power of states' rights. The Gilded Age (1877–1901) was marked by Republican dominance of Congress. During this time, lobbying activity became more intense, particularly during the administration of President Ulysses S. Grant in which influential lobbies advocated for railroad subsidies and tariffs on wool. Immigration and high birth rates swelled the ranks of citizens and the nation grew at a rapid pace. The Progressive Era was characterized by strong party leadership in both houses of Congress as well as calls for reform; sometimes reformers would attack lobbyists as corrupting politics. The position of Speaker of the House became extremely powerful under leaders such as Thomas Reed in 1890 and Joseph Gurney Cannon. The Senate was effectively controlled by a half dozen men.
A system of seniority—in which long-time members of Congress gained more and more power—encouraged politicians of both parties to serve for long terms. Committee chairmen remained influential in both houses until the reforms of the 1970s. Important structural changes included the direct popular election of senators according to the Seventeenth Amendment, ratified on April 8, 1913, with positive effects (senators more sensitive to public opinion) and negative effects (undermining the authority of state governments). Supreme Court decisions based on the Constitution's commerce clause expanded congressional power to regulate the economy. One effect of popular election of senators was to reduce the difference between the House and Senate in terms of their link to the electorate. Lame duck reforms according to the Twentieth Amendment ended the power of defeated and retiring members of Congress to wield influence despite their lack of accountability.The Great Depression ushered in President Franklin Roosevelt and strong control by Democrats and historic New Deal policies. Roosevelt's election in 1932 marked a shift in government power towards the executive branch. Numerous New Deal initiatives came from the White House rather than being initiated by Congress. The Democratic Party controlled both houses of Congress for many years. During this time, Republicans and conservative southern Democrats formed the Conservative Coalition. Democrats maintained control of Congress during World War II. Congress struggled with efficiency in the postwar era partly by reducing the number of standing congressional committees. Southern Democrats became a powerful force in many influential committees although political power alternated between Republicans and Democrats during these years. More complex issues required greater specialization and expertise, such as space flight and atomic energy policy. Senator Joseph McCarthy exploited the fear of communism during the Second Red Scare and conducted televised hearings. In 1960, Democratic candidate John F. Kennedy narrowly won the presidency and power shifted again to the Democrats who dominated both houses of Congress until 1994.
Congress enacted Johnson's Great Society program to fight poverty and hunger. The Watergate Scandal had a powerful effect of waking up a somewhat dormant Congress which investigated presidential wrongdoing and coverups; the scandal "substantially reshaped" relations between the branches of government, suggested political scientist Bruce J. Schulman. Partisanship returned, particularly after 1994; one analyst attributes partisan infighting to slim congressional majorities which discouraged friendly social gatherings in meeting rooms such as the Board of Education. Congress began reasserting its authority. Lobbying became a big factor despite the 1971 Federal Election Campaign Act. Political action committees or PACs could make substantive donations to congressional candidates via such means as soft money contributions. While soft money funds were not given to specific campaigns for candidates, the money often benefited candidates substantially in an indirect way and helped reelect candidates. Reforms such as the 2002 Bipartisan Campaign Reform Act limited campaign donations but did not limit soft money contributions. One source suggests post-Watergate laws amended in 1974 meant to reduce the "influence of wealthy contributors and end payoffs" instead "legitimized PACs" since they "enabled individuals to band together in support of candidates." From 1974 to 1984, PACs grew from 608 to 3,803 and donations leaped from $12.5 million to $120 million along with concern over PAC influence in Congress. In 2009, there were 4,600 business, labor and special-interest PACs including ones for lawyers, electricians, and real estate brokers. From 2007 to 2008, 175 members of Congress received "half or more of their campaign cash" from PACs.From 1970 to 2009, the House expanded delegates, along with their powers and privileges representing U.S. citizens in non-state areas, beginning with representation on committees for Puerto Rico's resident commissioner in 1970. In 1971, a delegate for the District of Columbia was authorized, and in 1972 new delegate positions were established for U.S. Virgin Islands and Guam. 1978 saw an additional delegate for American Samoa, and another for the Commonwealth of the Northern Mariana Islands began in 2009. These six members of Congress enjoy floor privileges to introduce bills and resolutions, and in recent congresses they vote in permanent and select committees, in party caucuses and in joint conferences with the Senate. They have Capitol Hill offices, staff and two annual appointments to each of the four military academies. While their votes are constitutional when Congress authorizes their House Committee of the Whole votes, recent Congresses have not allowed for that, and they cannot vote when the House is meeting as the House of Representatives.In the late 20th century, the media became more important in Congress's work. Analyst Michael Schudson suggested that greater publicity undermined the power of political parties and caused "more roads to open up in Congress for individual representatives to influence decisions." Norman Ornstein suggested that media prominence led to a greater emphasis on the negative and sensational side of Congress, and referred to this as the tabloidization of media coverage. Others saw pressure to squeeze a political position into a thirty-second soundbite. A report characterized Congress in 2013 as being unproductive, gridlocked, and "setting records for futility." In October 2013, with Congress unable to compromise, the government was shut down for several weeks and risked a serious default on debt payments, causing 60% of the public to say they would "fire every member of Congress" including their own representative. One report suggested Congress posed the "biggest risk to the US economy" because of its brinksmanship, "down-to-the-wire budget and debt crises" and "indiscriminate spending cuts," resulting in slowed economic activity and keeping up to two million people unemployed. There has been increasing public dissatisfaction with Congress, with extremely low approval ratings which dropped to 5% in October 2013.
Article I of the Constitution creates and sets forth the structure and most of the powers of Congress. Sections One through Six describe how Congress is elected and gives each House the power to create its own structure. Section Seven lays out the process for creating laws, and Section Eight enumerates numerous powers. Section Nine is a list of powers Congress does not have, and Section Ten enumerates powers of the state, some of which may only be granted by Congress. Constitutional amendments have granted Congress additional powers. Congress also has implied powers derived from the Constitution's Necessary and Proper Clause. Congress has authority over financial and budgetary policy through the enumerated power to "lay and collect Taxes, Duties, Imposts and Excises, to pay the Debts and provide for the common Defence and general Welfare of the United States." There is vast authority over budgets, although analyst Eric Patashnik suggested that much of Congress's power to manage the budget has been lost when the welfare state expanded since "entitlements were institutionally detached from Congress's ordinary legislative routine and rhythm." Another factor leading to less control over the budget was a Keynesian belief that balanced budgets were unnecessary.The Sixteenth Amendment in 1913 extended congressional power of taxation to include income taxes without apportionment among the several States, and without regard to any census or enumeration. The Constitution also grants Congress the exclusive power to appropriate funds, and this power of the purse is one of Congress's primary checks on the executive branch. Congress can borrow money on the credit of the United States, regulate commerce with foreign nations and among the states, and coin money. Generally, both the Senate and the House of Representatives have equal legislative authority, although only the House may originate revenue and appropriation bills. Congress has an important role in national defense, including the exclusive power to declare war, to raise and maintain the armed forces, and to make rules for the military. Some critics charge that the executive branch has usurped Congress's constitutionally defined task of declaring war. While historically presidents initiated the process for going to war, they asked for and received formal war declarations from Congress for the War of 1812, the Mexican–American War, the Spanish–American War, World War I, and World War II, although President Theodore Roosevelt's military move into Panama in 1903 did not get congressional approval. In the early days after the North Korean invasion of 1950, President Truman described the American response as a "police action." According to Time magazine in 1970, "U.S. presidents [had] ordered troops into position or action without a formal congressional declaration a total of 149 times." In 1993, Michael Kinsley wrote that "Congress's war power has become the most flagrantly disregarded provision in the Constitution," and that the "real erosion [of Congress's war power] began after World War II." Disagreement about the extent of congressional versus presidential power regarding war has been present periodically throughout the nation's history.Congress can establish post offices and post roads, issue patents and copyrights, fix standards of weights and measures, establish Courts inferior to the Supreme Court, and "make all Laws which shall be necessary and proper for carrying into Execution the foregoing Powers, and all other Powers vested by this Constitution in the Government of the United States, or in any Department or Officer thereof." Article Four gives Congress the power to admit new states into the Union. One of Congress's foremost non-legislative functions is the power to investigate and oversee the executive branch. Congressional oversight is usually delegated to committees and is facilitated by Congress's subpoena power. Some critics have charged that Congress has in some instances failed to do an adequate job of overseeing the other branches of government. In the Plame affair, critics including Representative Henry A. Waxman charged that Congress was not doing an adequate job of oversight in this case. There have been concerns about congressional oversight of executive actions such as warrantless wiretapping, although others respond that Congress did investigate the legality of presidential decisions. Political scientists Ornstein and Mann suggested that oversight functions do not help members of Congress win reelection. Congress also has the exclusive power of removal, allowing impeachment and removal of the president, federal judges and other federal officers. There have been charges that presidents acting under the doctrine of the unitary executive have assumed important legislative and budgetary powers that should belong to Congress. So-called signing statements are one way in which a president can "tip the balance of power between Congress and the White House a little more in favor of the executive branch," according to one account. Past presidents, including Ronald Reagan, George H. W. Bush, Bill Clinton, and George W. Bush, have made public statements when signing congressional legislation about how they understand a bill or plan to execute it, and commentators, including the American Bar Association, have described this practice as against the spirit of the Constitution. There have been concerns that presidential authority to cope with financial crises is eclipsing the power of Congress. In 2008, George F. Will called the Capitol building a "tomb for the antiquated idea that the legislative branch matters."
The Constitution enumerates the powers of Congress in detail. In addition, other congressional powers have been granted, or confirmed, by constitutional amendments. The Thirteenth (1865), Fourteenth (1868), and Fifteenth Amendments (1870) gave Congress authority to enact legislation to enforce rights of African Americans, including voting rights, due process, and equal protection under the law. Generally militia forces are controlled by state governments, not Congress.
Congress also has implied powers deriving from the Constitution's Necessary and Proper Clause which permit Congress to "make all Laws which shall be necessary and proper for carrying into Execution the foregoing Powers, and all other Powers vested by this Constitution in the Government of the United States, or in any Department or Officer thereof." Broad interpretations of this clause and of the Commerce Clause, the enumerated power to regulate commerce, in rulings such as McCulloch v. Maryland, have effectively widened the scope of Congress's legislative authority far beyond that prescribed in Section Eight.
Constitutional responsibility for the oversight of Washington, D.C., the federal district and national capital, and the U.S. territories of Guam, American Samoa, Puerto Rico, the U.S. Virgin Islands, and the Northern Mariana Islands rests with Congress. The republican form of government in territories is devolved by Congressional statute to the respective territories including direct election of governors, the D.C. mayor and locally elective territorial legislatures.Each territory and Washington, D.C., elect a non-voting delegate to the U.S. House of Representatives as they have throughout Congressional history. They "possess the same powers as other members of the House, except that they may not vote when the House is meeting as the House of Representatives." They are assigned offices and allowances for staff, participate in debate, and appoint constituents to the four military service academies for the Army, Navy, Air Force and Coast Guard.Washington, D.C., citizens alone among U.S. territories have the right to directly vote for the President of the United States, although the Democratic and Republican political parties nominate their presidential candidates at national conventions which include delegates from the five major territories.
Representative Lee H. Hamilton explained how Congress functions within the federal government: To me the key to understanding it is balance. The founders went to great lengths to balance institutions against each other—balancing powers among the three branches: Congress, the president, and the Supreme Court; between the House of Representatives and the Senate; between the federal government and the states; among states of different sizes and regions with different interests; between the powers of government and the rights of citizens, as spelled out in the Bill of Rights ... No one part of government dominates the other. The Constitution provides checks and balances among the three branches of the federal government. Its authors expected the greater power to lie with Congress as described in Article One.The influence of Congress on the presidency has varied from period to period depending on factors such as congressional leadership, presidential political influence, historical circumstances such as war, and individual initiative by members of Congress. The impeachment of Andrew Johnson made the presidency less powerful than Congress for a considerable period afterwards. The 20th and 21st centuries have seen the rise of presidential power under politicians such as Theodore Roosevelt, Woodrow Wilson, Franklin D. Roosevelt, Richard Nixon, Ronald Reagan, and George W. Bush. However, in recent years, Congress has restricted presidential power with laws such as the Congressional Budget and Impoundment Control Act of 1974 and the War Powers Resolution. Nevertheless, the Presidency remains considerably more powerful today than during the 19th century. Executive branch officials are often loath to reveal sensitive information to members of Congress because of concern that information could not be kept secret; in return, knowing they may be in the dark about executive branch activity, congressional officials are more likely to distrust their counterparts in executive agencies. Many government actions require fast coordinated effort by many agencies, and this is a task that Congress is ill-suited for. Congress is slow, open, divided, and not well matched to handle more rapid executive action or do a good job of overseeing such activity, according to one analysis. The Constitution concentrates removal powers in the Congress by empowering and obligating the House of Representatives to impeach both executive and judicial officials for "Treason, Bribery, or other high Crimes and Misdemeanors." Impeachment is a formal accusation of unlawful activity by a civil officer or government official. The Senate is constitutionally empowered and obligated to try all impeachments. A simple majority in the House is required to impeach an official; however, a two-thirds majority in the Senate is required for conviction. A convicted official is automatically removed from office; in addition, the Senate may stipulate that the defendant be banned from holding office in the future. Impeachment proceedings may not inflict more than this; however, a convicted party may face criminal penalties in a normal court of law. In the history of the United States, the House of Representatives has impeached sixteen officials, of whom seven were convicted. Another resigned before the Senate could complete the trial. Only three presidents have ever been impeached: Andrew Johnson in 1868, Bill Clinton in 1999, and Donald Trump in 2019. All three trials ended in acquittal; in Johnson's case, the Senate fell one vote short of the two-thirds majority required for conviction. In 1974, Richard Nixon resigned from office after impeachment proceedings in the House Judiciary Committee indicated he would eventually be removed from office. The Senate has an important check on the executive power by confirming Cabinet officials, judges, and other high officers "by and with the Advice and Consent of the Senate." It confirms most presidential nominees but rejections are not uncommon. Furthermore, treaties negotiated by the President must be ratified by a two-thirds majority vote in the Senate to take effect. As a result, presidential arm-twisting of senators can happen before a key vote; for example, President Obama's secretary of state, Hillary Clinton, urged her former senate colleagues to approve a nuclear arms treaty with Russia in 2010. The House of Representatives has no formal role in either the ratification of treaties or the appointment of federal officials, other than in filling a vacancy in the office of the vice president; in such a case, a majority vote in each House is required to confirm a president's nomination of a vice president.In 1803, the Supreme Court established judicial review of federal legislation in Marbury v. Madison, holding, however, that Congress could not grant unconstitutional power to the Court itself. The Constitution does not explicitly state that the courts may exercise judicial review; however, the notion that courts could declare laws unconstitutional was envisioned by the founding fathers. Alexander Hamilton, for example, mentioned and expounded upon the doctrine in Federalist No. 78. Originalists on the Supreme Court have argued that if the constitution does not say something explicitly it is unconstitutional to infer what it should, might or could have said. Judicial review means that the Supreme Court can nullify a congressional law. It is a huge check by the courts on the legislative authority and limits congressional power substantially. In 1857, for example, the Supreme Court struck down provisions of a congressional act of 1820 in its Dred Scott decision. At the same time, the Supreme Court can extend congressional power through its constitutional interpretations. The congressional inquiry into St. Clair's Defeat of 1791 was the first congressional investigation of the executive branch. Investigations are conducted to gather information on the need for future legislation, to test the effectiveness of laws already passed, and to inquire into the qualifications and performance of members and officials of the other branches. Committees may hold hearings, and, if necessary, compel individuals to testify when investigating issues over which it has the power to legislate by issuing subpoenas. Witnesses who refuse to testify may be cited for contempt of Congress, and those who testify falsely may be charged with perjury. Most committee hearings are open to the public (the House and Senate intelligence committees are the exception); important hearings are widely reported in the mass media and transcripts published a few months afterwards. Congress, in the course of studying possible laws and investigating matters, generates an incredible amount of information in various forms, and can be described as a publisher. Indeed, it publishes House and Senate reports and maintains databases which are updated irregularly with publications in a variety of electronic formats.Congress also plays a role in presidential elections. Both Houses meet in joint session on the sixth day of January following a presidential election to count the electoral votes, and there are procedures to follow if no candidate wins a majority.The main result of congressional activity is the creation of laws, most of which are contained in the United States Code, arranged by subject matter alphabetically under fifty title headings to present the laws "in a concise and usable form."
Congress is split into two chambers—House and Senate—and manages the task of writing national legislation by dividing work into separate committees which specialize in different areas. Some members of Congress are elected by their peers to be officers of these committees. Further, Congress has ancillary organizations such as the Government Accountability Office and the Library of Congress to help provide it with information, and members of Congress have staff and offices to assist them as well. In addition, a vast industry of lobbyists helps members write legislation on behalf of diverse corporate and labor interests.
The committee structure permits members of Congress to study a particular subject intensely. It is neither expected nor possible that a member be an expert on all subject areas before Congress. As time goes by, members develop expertise in particular subjects and their legal aspects. Committees investigate specialized subjects and advise the entire Congress about choices and trade-offs. The choice of specialty may be influenced by the member's constituency, important regional issues, prior background and experience. Senators often choose a different specialty from that of the other senator from their state to prevent overlap. Some committees specialize in running the business of other committees and exert a powerful influence over all legislation; for example, the House Ways and Means Committee has considerable influence over House affairs.
Committees write legislation. While procedures, such as the House discharge petition process, can introduce bills to the House floor and effectively bypass committee input, they are exceedingly difficult to implement without committee action. Committees have power and have been called independent fiefdoms. Legislative, oversight, and internal administrative tasks are divided among about two hundred committees and subcommittees which gather information, evaluate alternatives, and identify problems. They propose solutions for consideration by the full chamber. In addition, they perform the function of oversight by monitoring the executive branch and investigating wrongdoing.
At the start of each two-year session the House elects a speaker who does not normally preside over debates but serves as the majority party's leader. In the Senate, the vice president is the ex officio president of the Senate. In addition, the Senate elects an officer called the president pro tempore. Pro tempore means for the time being and this office is usually held by the most senior member of the Senate's majority party and customarily keeps this position until there is a change in party control. Accordingly, the Senate does not necessarily elect a new president pro tempore at the beginning of a new Congress. In both the House and Senate, the actual presiding officer is generally a junior member of the majority party who is appointed so that new members become acquainted with the rules of the chamber.
The Library of Congress was established by an act of Congress in 1800. It is primarily housed in three buildings on Capitol Hill, but also includes several other sites: the National Library Service for the Blind and Physically Handicapped in Washington, D.C.; the National Audio-Visual Conservation Center in Culpeper, Virginia; a large book storage facility located at Fort Meade, Maryland; and multiple overseas offices. The Library had mostly law books when it was burned by a British raiding party during the War of 1812, but the library's collections were restored and expanded when Congress authorized the purchase of Thomas Jefferson's private library. One of the library's missions is to serve the Congress and its staff as well as the American public. It is the largest library in the world with nearly 150 million items including books, films, maps, photographs, music, manuscripts, graphics, and materials in 470 languages.
The Congressional Research Service, part of the Library of Congress, provides detailed, up-to-date and non-partisan research for senators, representatives, and their staff to help them carry out their official duties. It provides ideas for legislation, helps members analyze a bill, facilitates public hearings, makes reports, consults on matters such as parliamentary procedure, and helps the two chambers resolve disagreements. It has been called the "House's think tank" and has a staff of about 900 employees.
The Congressional Budget Office or CBO is a federal agency which provides economic data to Congress.It was created as an independent non-partisan agency by the Congressional Budget and Impoundment Control Act of 1974. It helps Congress estimate revenue inflows from taxes and helps the budgeting process. It makes projections about such matters as the national debt as well as likely costs of legislation. It prepares an annual Economic and Budget Outlook with a mid-year update and writes An Analysis of the President's Budgetary Proposals for the Senate's Appropriations Committee. The speaker of the House and the Senate's president pro tempore jointly appoint the CBO director for a four-year term.
Lobbyists represent diverse interests and often seek to influence congressional decisions to reflect their clients' needs. Lobby groups and their members sometimes write legislation and whip bills. In 2007, there were approximately 17,000 federal lobbyists in Washington, D.C. They explain to legislators the goals of their organizations. Some lobbyists represent non-profit organizations and work pro bono for issues in which they are personally interested.
Congress has alternated between periods of constructive cooperation and compromise between parties, known as bipartisanship, and periods of deep political polarization and fierce infighting, known as partisanship. The period after the Civil War was marked by partisanship, as is the case today. It is generally easier for committees to reach accord on issues when compromise is possible. Some political scientists speculate that a prolonged period marked by narrow majorities in both chambers of Congress has intensified partisanship in the last few decades, but that an alternation of control of Congress between Democrats and Republicans may lead to greater flexibility in policies, as well as pragmatism and civility within the institution.
A term of Congress is divided into two "sessions," one for each year; Congress has occasionally been called into an extra or special session. A new session commences on January 3 each year, unless Congress decides differently. The Constitution requires Congress meet at least once each year and forbids either house from meeting outside the Capitol without the consent of the other house.
Joint sessions of the United States Congress occur on special occasions that require a concurrent resolution from both House and Senate. These sessions include counting electoral votes after a presidential election and the president's State of the Union address. The constitutionally mandated report, normally given as an annual speech, is modeled on Britain's Speech from the Throne, was written by most presidents after Jefferson but personally delivered as a spoken oration beginning with Wilson in 1913. Joint Sessions and Joint Meetings are traditionally presided over by the speaker of the House, except when counting presidential electoral votes when the vice president (acting as the president of the Senate) presides.
Ideas for legislation can come from members, lobbyists, state legislatures, constituents, legislative counsel, or executive agencies. Anyone can write a bill, but only members of Congress may introduce bills. Most bills are not written by Congress members, but originate from the Executive branch; interest groups often draft bills as well. The usual next step is for the proposal to be passed to a committee for review. A proposal is usually in one of these forms: Bills are laws in the making. A House-originated bill begins with the letters "H.R." for "House of Representatives," followed by a number kept as it progresses. Joint resolutions. There is little difference between a bill and a joint resolution since both are treated similarly; a joint resolution originating from the House, for example, begins "H.J.Res." followed by its number. Concurrent Resolutions affect only both the House and Senate and accordingly are not presented to the president for approval later. In the House, they begin with "H.Con.Res." Simple resolutions concern only the House or only the Senate and begin with "H.Res." or "S.Res."Representatives introduce a bill while the House is in session by placing it in the hopper on the Clerk's desk. It is assigned a number and referred to a committee which studies each bill intensely at this stage. Drafting statutes requires "great skill, knowledge, and experience" and sometimes take a year or more. Sometimes lobbyists write legislation and submit it to a member for introduction. Joint resolutions are the normal way to propose a constitutional amendment or declare war. On the other hand, concurrent resolutions (passed by both houses) and simple resolutions (passed by only one house) do not have the force of law but express the opinion of Congress or regulate procedure. Bills may be introduced by any member of either house. However, the Constitution states, "All Bills for raising Revenue shall originate in the House of Representatives." While the Senate cannot originate revenue and appropriation bills, it has power to amend or reject them. Congress has sought ways to establish appropriate spending levels.Each chamber determines its own internal rules of operation unless specified in the Constitution or prescribed by law. In the House, a Rules Committee guides legislation; in the Senate, a Standing Rules committee is in charge. Each branch has its own traditions; for example, the Senate relies heavily on the practice of getting "unanimous consent" for noncontroversial matters. House and Senate rules can be complex, sometimes requiring a hundred specific steps before a bill can become a law. Members sometimes turn to outside experts to learn about proper Congressional procedures.Each bill goes through several stages in each house including consideration by a committee and advice from the Government Accountability Office. Most legislation is considered by standing committees which have jurisdiction over a particular subject such as Agriculture or Appropriations. The House has twenty standing committees; the Senate has sixteen. Standing committees meet at least once each month. Almost all standing committee meetings for transacting business must be open to the public unless the committee votes, publicly, to close the meeting. A committee might call for public hearings on important bills. Each committee is led by a chair who belongs to the majority party and a ranking member of the minority party. Witnesses and experts can present their case for or against a bill. Then, a bill may go to what is called a mark-up session, where committee members debate the bill's merits and may offer amendments or revisions. Committees may also amend the bill, but the full house holds the power to accept or reject committee amendments. After debate, the committee votes whether it wishes to report the measure to the full house. If a bill is tabled then it is rejected. If amendments are extensive, sometimes a new bill with amendments built in will be submitted as a so-called clean bill with a new number. Both houses have procedures under which committees can be bypassed or overruled but they are rarely used. Generally, members who have been in Congress longer have greater seniority and therefore greater power.A bill which reaches the floor of the full house can be simple or complex and begins with an enacting formula such as "Be it enacted by the Senate and House of Representatives of the United States of America in Congress assembled." Consideration of a bill requires, itself, a rule which is a simple resolution specifying the particulars of debate—time limits, possibility of further amendments, and such. Each side has equal time and members can yield to other members who wish to speak. Sometimes opponents seek to recommit a bill which means to change part of it. Generally, discussion requires a quorum, usually half of the total number of representatives, before discussion can begin, although there are exceptions. The house may debate and amend the bill; the precise procedures used by the House and Senate differ. A final vote on the bill follows. Once a bill is approved by one house, it is sent to the other which may pass, reject, or amend it. For the bill to become law, both houses must agree to identical versions of the bill. If the second house amends the bill, then the differences between the two versions must be reconciled in a conference committee, an ad hoc committee that includes both senators and representatives sometimes by using a reconciliation process to limit budget bills. Both houses use a budget enforcement mechanism informally known as pay-as-you-go or paygo which discourages members from considering acts which increase budget deficits. If both houses agree to the version reported by the conference committee, the bill passes, otherwise it fails. The Constitution specifies that a majority of members, known as a quorum, be present before doing business in each house. However, the rules of each house assume that a quorum is present unless a quorum call demonstrates the contrary and debate often continues despite the lack of a majority. Voting within Congress can take many forms, including systems using lights and bells and electronic voting. Both houses use voice voting to decide most matters in which members shout "aye" or "no" and the presiding officer announces the result. The Constitution, however, requires a recorded vote if demanded by one-fifth of the members present or when voting to override a presidential veto. If the voice vote is unclear or if the matter is controversial, a recorded vote usually happens. The Senate uses roll-call voting, in which a clerk calls out the names of all the senators, each senator stating "aye" or "no" when their name is announced. In the Senate, the Vice President may cast the tie-breaking vote if present when the Senators are equally divided. The House reserves roll-call votes for the most formal matters, as a roll call of all 435 representatives takes quite some time; normally, members vote by using an electronic device. In the case of a tie, the motion in question fails. Most votes in the House are done electronically, allowing members to vote yea or nay or present or open. Members insert a voting ID card and can change their votes during the last five minutes if they choose; in addition, paper ballots are used on some occasions—yea indicated by green and nay by red. One member cannot cast a proxy vote for another. Congressional votes are recorded on an online database.After passage by both houses, a bill is enrolled and sent to the president for approval. The president may sign it making it law or veto it, perhaps returning it to Congress with the president's objections. A vetoed bill can still become law if each house of Congress votes to override the veto with a two-thirds majority. Finally, the president may do nothing—neither signing nor vetoing the bill—and then the bill becomes law automatically after ten days (not counting Sundays) according to the Constitution. But if Congress is adjourned during this period, presidents may veto legislation passed at the end of a congressional session simply by ignoring it; the maneuver is known as a pocket veto, and cannot be overridden by the adjourned Congress.
Senators face reelection every six years, and representatives every two. Reelections encourage candidates to focus their publicity efforts at their home states or districts. Running for reelection can be a grueling process of distant travel and fund-raising which distracts senators and representatives from paying attention to governing, according to some critics. Although others respond that the process is necessary to keep members of Congress in touch with voters. Nevertheless, incumbent members of Congress running for reelection have strong advantages over challengers. They raise more money because donors fund incumbents over challengers, perceiving the former as more likely to win, and donations are vital for winning elections. One critic compared being elected to Congress to receiving life tenure at a university. Another advantage for representatives is the practice of gerrymandering. After each ten-year census, states are allocated representatives based on population, and officials in power can choose how to draw the congressional district boundaries to support candidates from their party. As a result, reelection rates of members of Congress hover around 90 percent, causing some critics to accuse them of being a privileged class. Academics such as Princeton's Stephen Macedo have proposed solutions to fix gerrymandering in the U.S. Both senators and representatives enjoy free mailing privileges, called franking privileges; while these are not intended for electioneering, this rule is often skirted by borderline election-related mailings during campaigns.
In 1971, the cost of running for Congress in Utah was $70,000 but costs have climbed. The biggest expense is television advertisements. Today's races cost more than a million dollars for a House seat, and six million or more for a Senate seat. Since fundraising is vital, "members of Congress are forced to spend ever-increasing hours raising money for their re-election."Nevertheless, the Supreme Court has treated campaign contributions as a free speech issue. Some see money as a good influence in politics since it "enables candidates to communicate with voters." Few members retire from Congress without complaining about how much it costs to campaign for reelection. Critics contend that members of Congress are more likely to attend to the needs of heavy campaign contributors than to ordinary citizens.Elections are influenced by many variables. Some political scientists speculate there is a coattail effect (when a popular president or party position has the effect of reelecting incumbents who win by "riding on the president's coattails"), although there is some evidence that the coattail effect is irregular and possibly declining since the 1950s. Some districts are so heavily Democratic or Republican that they are called a safe seat; any candidate winning the primary will almost always be elected, and these candidates do not need to spend money on advertising. But some races can be competitive when there is no incumbent. If a seat becomes vacant in an open district, then both parties may spend heavily on advertising in these races; in California in 1992, only four of twenty races for House seats were considered highly competitive.
Since members of Congress must advertise heavily on television, this usually involves negative advertising, which smears an opponent's character without focusing on the issues. Negative advertising is seen as effective because "the messages tend to stick." However, these advertisements sour the public on the political process in general as most members of Congress seek to avoid blame. One wrong decision or one damaging television image can mean defeat at the next election, which leads to a culture of risk avoidance, a need to make policy decisions behind closed doors, and concentrating publicity efforts in the members' home districts.
Prominent Founding Fathers writing in The Federalist Papers felt that elections were essential to liberty, that a bond between the people and the representatives was particularly essential, and that "frequent elections are unquestionably the only policy by which this dependence and sympathy can be effectually secured." In 2009, however, few Americans were familiar with leaders of Congress. The percentage of Americans eligible to vote who did, in fact, vote was 63% in 1960, but has been falling since, although there was a slight upward trend in the 2008 election. Public opinion polls asking people if they approve of the job Congress is doing have, in the last few decades, hovered around 25% with some variation. Scholar Julian Zeliger suggested that the "size, messiness, virtues, and vices that make Congress so interesting also create enormous barriers to our understanding the institution ... Unlike the presidency, Congress is difficult to conceptualize." Other scholars suggest that despite the criticism, "Congress is a remarkably resilient institution ... its place in the political process is not threatened ... it is rich in resources" and that most members behave ethically. They contend that "Congress is easy to dislike and often difficult to defend" and this perception is exacerbated because many challengers running for Congress run against Congress, which is an "old form of American politics" that further undermines Congress's reputation with the public: The rough-and-tumble world of legislating is not orderly and civil, human frailties too often taint its membership, and legislative outcomes are often frustrating and ineffective ... Still, we are not exaggerating when we say that Congress is essential to American democracy. We would not have survived as a nation without a Congress that represented the diverse interests of our society, conducted a public debate on the major issues, found compromises to resolve conflicts peacefully, and limited the power of our executive, military, and judicial institutions ... The popularity of Congress ebbs and flows with the public's confidence in government generally ... the legislative process is easy to dislike—it often generates political posturing and grandstanding, it necessarily involves compromise, and it often leaves broken promises in its trail. Also, members of Congress often appear self-serving as they pursue their political careers and represent interests and reflect values that are controversial. Scandals, even when they involve a single member, add to the public's frustration with Congress and have contributed to the institution's low ratings in opinion polls. An additional factor that confounds public perceptions of Congress is that congressional issues are becoming more technical and complex and require expertise in subjects such as science, engineering and economics. As a result, Congress often cedes authority to experts at the executive branch.Since 2006, Congress has dropped 10 points in the Gallup confidence poll with only 9% having "a great deal" or "quite a lot" of confidence in their legislators. Since 2011, Gallup poll has reported Congress's approval rating among Americans at 10% or below three times. Public opinion of Congress plummeted further to 5% in October 2013 after parts of the U.S. government deemed 'nonessential government' shut down.
When the Constitution was ratified in 1787, the ratio of the populations of large states to small states was roughly twelve to one. The Connecticut Compromise gave every state, large and small, an equal vote in the Senate. Since each state has two senators, residents of smaller states have more clout in the Senate than residents of larger states. But since 1787, the population disparity between large and small states has grown; in 2006, for example, California had seventy times the population of Wyoming. Critics, such as constitutional scholar Sanford Levinson, have suggested that the population disparity works against residents of large states and causes a steady redistribution of resources from "large states to small states." However, others argue that the Connecticut Compromise was deliberately intended by the Founding Fathers to construct the Senate so that each state had equal footing not based on population, and contend that the result works well on balance.
A major role for members of Congress is providing services to constituents. Constituents request assistance with problems. Providing services helps members of Congress win votes and elections and can make a difference in close races. Congressional staff can help citizens navigate government bureaucracies. One academic described the complex intertwined relation between lawmakers and constituents as home style.
One way to categorize lawmakers, according to political scientist Richard Fenno, is by their general motivation: Reelection. These are lawmakers who "never met a voter they didn't like" and provide excellent constituent services. Good public policy. Legislators who "burnish a reputation for policy expertise and leadership." Power in the chamber. Lawmakers who spend serious time along the "rail of the House floor or in the Senate cloakroom ministering to the needs of their colleagues." Famous legislator Henry Clay in the mid-19th century was described as an "issue entrepreneur" who looked for issues to serve his ambitions.
Members of Congress enjoy parliamentary privilege, including freedom from arrest in all cases except for treason, felony, and breach of the peace, and freedom of speech in debate. This constitutionally derived immunity applies to members during sessions and when traveling to and from sessions. The term arrest has been interpreted broadly, and includes any detention or delay in the course of law enforcement, including court summons and subpoenas. The rules of the House strictly guard this privilege; a member may not waive the privilege on their own, but must seek the permission of the whole house to do so. Senate rules, however, are less strict and permit individual senators to waive the privilege as they choose.The Constitution guarantees absolute freedom of debate in both houses, providing in the Speech or Debate Clause of the Constitution that "for any Speech or Debate in either House, they shall not be questioned in any other Place." Accordingly, a member of Congress may not be sued in court for slander because of remarks made in either house, although each house has its own rules restricting offensive speeches, and may punish members who transgress.Obstructing the work of Congress is a crime under federal law and is known as contempt of Congress. Each member has the power to cite individuals for contempt but can only issue a contempt citation—the judicial system pursues the matter like a normal criminal case. If convicted in court, an individual found guilty of contempt of Congress may be imprisoned for up to one year.The franking privilege allows members of Congress to send official mail to constituents at government expense. Though they are not permitted to send election materials, borderline material is often sent, especially in the run-up to an election by those in close races. Indeed, some academics consider free mailings as giving incumbents a big advantage over challengers.
From 1789 to 1815, members of Congress received only a daily payment of $6 while in session. Members received an annual salary of $1,500 per year from 1815 to 1817, then a per diem salary of $8 from 1818 to 1855; since then they have received an annual salary, first pegged in 1855 at $3,000. In 1907, salaries were raised to $7,500 per year, the equivalent of $173,000 in 2010. In 2006, members of Congress received a yearly salary of $165,200. Congressional leaders were paid $183,500 per year. The Speaker of the House of Representatives earns $212,100 annually. The salary of the President pro tempore for 2006 was $183,500, equal to that of the majority and minority leaders of the House and Senate. Privileges include having an office and paid staff. In 2008, non-officer members of Congress earned $169,300 annually. Some critics complain congressional pay is high compared with a median American income of $45,113 for men and $35,102 for women. Others have countered that congressional pay is consistent with other branches of government. Another criticism is that members of Congress have access to free or low-cost medical care in the Washington, D.C., area. The petition, "Remove health-care subsidies for Members of Congress and their families," garnered over 1,077,000 signatures on the website Change.org. In January 2014, it was reported that for the first time over half of the members of Congress were millionaires. Congress has been criticized for trying to conceal pay raises by slipping them into a large bill at the last minute. Others have criticized the wealth of members of Congress. Representative Jim Cooper of Tennessee told Harvard professor Lawrence Lessig that a chief problem with Congress was that members focused on lucrative careers as lobbyists after serving––that Congress was a "Farm League for K Street"––instead of on public service.Members elected since 1984 are covered by the Federal Employees Retirement System (FERS). Like other federal employees, congressional retirement is funded through taxes and participants' contributions. Members of Congress under FERS contribute 1.3% of their salary into the FERS retirement plan and pay 6.2% of their salary in Social Security taxes. And like federal employees, members contribute one-third of the cost of health insurance with the government covering the other two-thirds.The size of a congressional pension depends on the years of service and the average of the highest three years of their salary. By law, the starting amount of a member's retirement annuity may not exceed 80% of their final salary. In 2006, the average annual pension for retired senators and representatives under the Civil Service Retirement System (CSRS) was $60,972, while those who retired under FERS, or in combination with CSRS, was $35,952.Members of Congress make fact-finding missions to learn about other countries and stay informed, but these outings can cause controversy if the trip is deemed excessive or unconnected with the task of governing. For example, the Wall Street Journal reported in 2009 that lawmaker trips abroad at taxpayer expense had included spas, $300-per-night extra unused rooms, and shopping excursions. Lawmakers respond that "traveling with spouses compensates for being away from them a lot in Washington" and justify the trips as a way to meet officials in other nations.
Caucuses of the United States Congress Congressional Archives Current members of the United States House of Representatives Current members of the United States Senate Elections in the United States § Congressional elections List of United States Congresses Oath of office § United States Radio and Television Correspondents' Association Term limits in the United States United States Congress Joint Select Committee on Deficit Reduction United States Congressional Baseball Game United States congressional hearing United States presidents and control of Congress
The Indian National Congress (pronunciation ) (INC, often called the Congress Party or simply Congress) is a political party in India with widespread roots. Founded in 1885, it was the first modern nationalist movement to emerge in the British Empire in Asia and Africa. From the late 19th century, and especially after 1920, under the leadership of Mahatma Gandhi, Congress became the principal leader of the Indian independence movement. Congress led India to independence from Great Britain, and powerfully influenced other anti-colonial nationalist movements in the British Empire.Congress is one of the two major political parties in India, along with the Bharatiya Janata Party. Congress is a "big tent" party whose social democratic platform is generally considered in the centre to centre-left of Indian politics. Congress' social policy is based upon the Gandhian principle of Sarvodaya–the lifting up of all sections of society–which involves the improvement of the lives of economically underprivileged and socially marginalised people. On social and economic issues, it advocates social justice, equality, welfare state, along with secular society.After India's independence in 1947, Congress formed the union government of India and many state governments of India. Congress became India's dominant political party; as of 2019, in the 17 general elections since independence, it has won an outright majority on seven occasions and has led the ruling coalition a further three times, heading the central government for more than 54 years. There have been six Congress Prime Ministers, the first being Jawaharlal Nehru (1947–1964), and the most recent Manmohan Singh (2004–2014). Although it did not fare well in the last two general elections in India in 2014 and 2019, it remains one of two major, nationwide, political parties in India, along with the right-wing, Hindu nationalist Bharatiya Janata Party (BJP). In the 2014 general election, Congress had its poorest post-independence general election performance, winning only 44 seats of the 543-member Lok Sabha—the lower house of the Parliament of India. From 2004 to 2014, United Progressive Alliance, a coalition of Congress with several regional parties, formed the Indian government led by Manmohan Singh, the Prime Minister as the head of the coalition government. The leader of the party during the period, Sonia Gandhi has served the longest term as the president of the party. As of July 2019, the party is in power in six legislative assemblies: Punjab, Rajasthan, Chhattisgarh, Jharkhand, Maharashtra (as part of the Maha Vikas Aghadi), and the union territory of Puducherry (in an alliance with Dravida Munnetra Kazhagam).
The Indian National Congress conducted its first session in Bombay from 28–31 December 1885 at the initiative of retired Civil Service officer Allan Octavian Hume. In 1883, Hume had outlined his idea for a body representing Indian interests in an open letter to graduates of the University of Calcutta. Its aim was to obtain a greater share in government for educated Indians, and to create a platform for civic and political dialogue between them and the British Raj. Hume took the initiative, and in March 1885 a notice convening the first meeting of the Indian National Union to be held in Poona the following December was issued. Due to a cholera outbreak there, it was moved to Bombay. Hume organised the first meeting in Bombay with the approval of the Viceroy Lord Dufferin. Umesh Chandra Banerjee was the first president of Congress; the first session was attended by 72 delegates, representing each province of India. Notable representatives included Scottish ICS officer William Wedderburn, Dadabhai Naoroji, Pherozeshah Mehta of the Bombay Presidency Association, Ganesh Vasudeo Joshi of the Poona Sarvajanik Sabha, social reformer and newspaper editor Gopal Ganesh Agarkar, Justice K. T. Telang, N. G. Chandavarkar, Dinshaw Wacha, Behramji Malabari, journalist and activist Gooty Kesava Pillai, and P. Rangaiah Naidu of the Madras Mahajana Sabha. This small elite group, unrepresentative of the Indian masses at the time, functioned more as a stage for elite Indian ambitions than a political party for the first decade of its existence.
At the beginning of the 20th century, Congress' demands became more radical in the face of constant opposition from the British government, and the party decided to advocate in favour of the independence movement because it would allow a new political system in which Congress could be a major party. By 1905, a division opened between the moderates led by Gokhale, who downplayed public agitation, and the new extremists who advocated agitation, and regarded the pursuit of social reform as a distraction from nationalism. Bal Gangadhar Tilak, who tried to mobilise Hindu Indians by appealing to an explicitly Hindu political identity displayed in the annual public Ganapati festivals he inaugurated in western India, was prominent among the extremists.Congress included a number of prominent political figures. Dadabhai Naoroji, a member of the sister Indian National Association, was elected president of the party in 1886 and was the first Indian Member of Parliament in the British House of Commons (1892–1895). Congress also included Bal Gangadhar Tilak, Bipin Chandra Pal, Lala Lajpat Rai, Gopal Krishna Gokhale, and Mohammed Ali Jinnah. Jinnah was a member of the moderate group in the Congress, favouring Hindu–Muslim unity in achieving self-government. Later he became the leader of the Muslim League and instrumental in the creation of Pakistan. Congress was transformed into a mass movement by Surendranath Banerjee during the partition of Bengal in 1905, and the resultant Swadeshi movement.
Mahatma Gandhi returned from South Africa in 1915. With the help of the moderate group led by Ghokhale, Gandhi became president of Congress. After the First World War, the party became associated with Gandhi, who remained its unofficial spiritual leader and icon. He formed an alliance with the Khilafat Movement in 1920 to fight for preservation of the Ottoman Caliphate, and rights for Indians using civil disobedience or satyagraha as the tool for agitation. In 1923, after the deaths of policemen at Chauri Chaura, Gandhi suspended the agitation. In protest, a number of leaders, Chittaranjan Das, Annie Besant, and Motilal Nehru, resigned to set up the Swaraj Party. The Khilafat movement collapsed and Congress was split.The rise of Gandhi's popularity and his satyagraha art of revolution led to support from Sardar Vallabhbhai Patel, Pandit Jawaharlal Nehru, Rajendra Prasad, Khan Mohammad Abbas Khan, Khan Abdul Ghaffar Khan, Chakravarti Rajgopalachari, Anugrah Narayan Sinha, Jayaprakash Narayan, Jivatram Kripalani, and Maulana Abul Kalam Azad. As a result of prevailing nationalism, Gandhi's popularity, and the party's attempts at eradicating caste differences, untouchability, poverty, and religious and ethnic divisions, Congress became a forceful and dominant group. Although its members were predominantly Hindu, it had members from other religions, economic classes, and ethnic and linguistic groups.At the Congress 1929 Lahore session under the presidency of Jawaharlal Nehru, Purna Swaraj (complete independence) was declared as the party's goal, declaring 26 January 1930 as "Purna Swaraj Diwas" (Independence Day). The same year, Srinivas Iyenger was expelled from the party for demanding full independence, not just home rule as demanded by Gandhi. After the passage of the Government of India Act 1935, provincial elections were held in India in the winter of 1936–37 in eleven provinces: Madras, Central Provinces, Bihar, Orissa, United Provinces, Bombay Presidency, Assam, NWFP, Bengal, Punjab, and Sindh. The final results of the elections were declared in february 1937. The Indian National Congress gained power in eight of them - the three exceptions being Bengal, Punjab, and Sindh. The All-India Muslim League failed to form a government in any province. Congress ministries resigned in October and November 1939 in protest against Viceroy Lord Linlithgow's declaration that India was a belligerent in the Second World War without consulting the Indian people.In 1939, Subhas Chandra Bose, the elected president in both 1938 and 1939, resigned from Congress over the selection of the working committee. The party was not the sole representative of the Indian polity, other parties included the Akhil Bharatiya Hindu Mahasabha, and the All India Forward Bloc. The party was an umbrella organisation, sheltering radical socialists, traditionalists, and Hindu and Muslim conservatives. Gandhi expelled all the socialist groupings, including the Congress Socialist Party, the Krishak Praja Party, and the Swarajya Party, along with Subhas Chandra Bose, in 1939. Azad Hind, an Indian provisional government, had been established in Singapore in 1943, and was supported by Japan. Recognised as a legitimate state by only a small number of countries limited solely to Axis powers and their allies, Azad Hind had diplomatic relations with nine countries: Nazi Germany, the Empire of Japan, Italian Social Republic, Independent State of Croatia and Wang Jingwei Government, Thailand, the State of Burma, Manchukuo and the Second Philippine Republic. In 1946, the British tried the Indian soldiers who had fought alongside the Japanese during World War II in the INA trials. In response, Congress helped form the INA Defence Committee, which assembled a legal team to defend the case of the soldiers of the Azad Hind government. The team included several famous lawyers, including Bhulabhai Desai, Asaf Ali, and Jawaharlal Nehru. The same year, Congress members initially supported the sailors who led the Royal Indian Navy mutiny, but they withdrew support at a critical juncture and the mutiny failed.
After Indian independence in 1947, the Indian National Congress became the dominant political party in the country. In 1952, in the first general election held after Independence, the party swept to power in the national parliament and most state legislatures. It held power nationally until 1977, when it was defeated by the Janata coalition. It returned to power in 1980 and ruled until 1989, when it was once again defeated. The party formed the government in 1991 at the head of a coalition, as well as in 2004 and 2009, when it led the United Progressive Alliance. During this period the Congress remained centre-left in its social policies while steadily shifting from a socialist to a neoliberal economic outlook. The Party's rivals at state level have been national parties including the Bharatiya Janata Party (BJP), the Communist Party of India (Marxist) (CPIM), and various regional parties, such as the Telugu Desam Party, Trinamool Congress and Aam Aadmi Party.A post-partition successor to the party survived as the Pakistan National Congress, a party which represented the rights of religious minorities in the state. The party's support was strongest in the Bengali-speaking province of East Pakistan. After the Bangladeshi War of Independence, it became known as the Bangladeshi National Congress, but was dissolved in 1975 by the government.
From 1951 until his death in 1964, Jawaharlal Nehru was the paramount leader of the party. Congress gained power in landslide victories in the general elections of 1951–52, 1957, and 1962. During his tenure, Nehru implemented policies based on import substitution industrialisation, and advocated a mixed economy where the government-controlled public sector co-existed with the private sector. He believed the establishment of basic and heavy industries was fundamental to the development and modernisation of the Indian economy. The Nehru government directed investment primarily into key public sector industries—steel, iron, coal, and power—promoting their development with subsidies and protectionist policies. Nehru embraced secularism, socialistic economic practices based on state-driven industrialisation, and a non-aligned and non-confrontational foreign policy that became typical of the modern Congress Party. The policy of non-alignment during the Cold War meant Nehru received financial and technical support from both the Eastern and Western Blocs to build India's industrial base from nothing.During his period in office, there were four known assassination attempts on Nehru. The first attempt on his life was during partition in 1947 while he was visiting the North-West Frontier Province in a car. The second was by a knife-wielding rickshaw-puller in Maharashtra in 1955. A third attempt happened in Bombay in 1956. The fourth was a failed bombing attempt on railway tracks in Maharashtra in 1961. Despite threats to his life, Nehru despised having excess security personnel around him and did not like his movements to disrupt traffic.In 1964, Nehru died because of an aortic dissection, raising questions about the party's future. After Nehru's death in 1964, the congress party started to face internal crisis. There were differences among the top leadership of the Congress regarding the future of the party which makes lot of issues within the party. This resulted in formation of many congress named parties in Kerala Congress, Orissa Jana Congress, Bangla Congress, Utkal Congress, Bharatiya Kranti Dal, etc. K. Kamaraj became the president of the All India Congress Committee in 1963 during the last year of Nehru's life. Prior to that, he had been the chief minister of Madras state for nine years. Kamraj had also been a member of "the syndicate", a group of right wing leaders within Congress.In 1963 the Congress lost popularity following the defeat in the Indo-Chinese war of 1962.To revitalize the party, Kamraj proposed the Kamaraj Plan to Nehru that encouraged six Congress chief ministers (including himself) and six senior cabinet ministers to resign to take up party work. After Nehru's death in May 1964, Kamaraj was widely credited as the "kingmaker" in Indian politics for ensuring the victory of Lal Bahadur Shastri over Morarji Desai as the successor of Nehru. The Congress was then split into two parties : Indian National Congress(O) and Indian National Congress (R) as a left-wing/right-wing division. Indira Gandhi wanted to use a populist agenda in order to mobilize popular support for the party while Kamraj and Desai stood for a more right-wing agenda.As prime minister, Shastri retained many members of Nehru's Council of Ministers; T. T. Krishnamachari was retained as Finance Minister of India, as was Defence Minister Yashwantrao Chavan. Shastri appointed Swaran Singh to succeed him as External Affairs Minister. Shashtri appointed Indira Gandhi, Jawaharlal Nehru's daughter and former party president, Minister of Information and Broadcasting. Gulzarilal Nanda continued as the Minister of Home Affairs. As Prime Minister, Shastri continued Nehru's policy of non-alignment, but built closer relations with the Soviet Union. In the aftermath of the Sino-Indian War of 1962, and the formation of military ties between China and Pakistan, Shastri's government expanded the defence budget of India's armed forces. He also promoted the White Revolution—a national campaign to increase the production and supply of milk by creating the National Dairy Development Board. The Madras anti-Hindi agitation of 1965 occurred during Shastri's tenure.Shastri became a national hero following victory in the Indo-Pakistani War of 1965. His slogan, "Jai Jawan Jai Kisan" ("Hail the soldier, Hail the farmer"), became very popular during the war. On 11 January 1966, a day after signing the Tashkent Declaration, Shastri died in Tashkent, reportedly of a heart attack; but the circumstances of his death remain mysterious. Indian National Congress (O) was led first by Kamraj and later by Morarji Desai. The "O" stands for organisation/Old Congress. Some people used to it the Original Congress.
After Shastri's death, Congress elected Indira Gandhi as leader over Morarji Desai. Once again, politician K. Kamaraj was instrumental in achieving this result. In 1967, following a poor performance in the general election, Indira Gandhi started moving towards the political left. In mid-1969, she was involved in a dispute with senior party leaders on a number of issues. The two major issues were Gandhi supporting the independent candidate, V. V. Giri, rather than the official Congress party candidate, Neelam Sanjiva Reddy, for the vacant post of the President of India. The second issue was Mrs. Gandhi's abrupt nationalization of the 14 biggest banks in India, which resulted in the resignation of the finance minister, Morarji Desai. Later in the year, the Congress party president, S. Nijalingappa, expelled her from the party for indiscipline. Mrs. Gandhi as a counter-move launched her own faction of the INC. Mrs. Gandhi's faction, called Congress (R), was supported by most of the Congress MPs while the original party had the support of only 65 MPs. It was also known as Congress(R) R stood for Requisition or Ruling. It soon came to be known as the New Congress. In the All India Congress Committee, 446 of its 705 members walked over to Indira's side. This created a belief among Indians that Indira's Congress was the Real Congress (INC-R). After the separation of the two parties, there was also a dispute about the party logo. The "Old Congress" retained the party symbol of a pair of bullocks carrying a yoke while Indira's breakaway faction were given a new symbol of a cow with suckling calf by the Election Commission as the party election symbol. The split occurred when, in 1969, a united opposition under the banner of Samyukt Vidhayak Dal, won control over several states in the Hindi Belt.In the mid-term parliamentary elections held in 1971, the Gandhi-led Congress (R) Party won a landslide victory on a platform of progressive policies such as the elimination of poverty (Garibi Hatao). The policies of the Congress (R) Party under Gandhi before the 1971 elections included proposals to abolish the Privy Purse to former rulers of the Princely states, and the 1969 nationalisation of India's 14 largest banks.The New Congress Party's popular support began to wane in the mid-1970s. From 1975, Gandhi's government grew increasingly more authoritarian and unrest among the opposition grew. On 12 June 1975, the High Court of Allahabad declared Indira Gandhi's election to the Lok Sabha, the lower house of India's parliament, void on the grounds of electoral malpractice. However, Gandhi rejected calls to resign and announced plans to appeal to the Supreme Court. She moved to restore order by ordering the arrest of most of the opposition participating in the unrest. In response to increasing disorder and lawlessness, Gandhi's cabinet and government recommended that President Fakhruddin Ali Ahmed declare a State of Emergency, which he did on 25 June 1975 based on the provisions of Article 352 of the Constitution.During the nineteen-month emergency, widespread oppression and abuse of power by Gandhi's unelected younger son and political heir Sanjay Gandhi and his close associates occurred. This period of oppression ended on 23 January 1977, when Gandhi released all political prisoners and called fresh elections for the Lok Sabha to be held in March. The Emergency officially ended on 23 March 1977. In that month's parliamentary elections, the opposition Janata Party won a landslide victory over Congress, winning 295 seats in the Lok Sabha against Congress' 153. Gandhi lost her seat to her Janata opponent Raj Narain. On 2 January 1978, she and her followers seceded and formed a new opposition party, popularly called Congress (I)—the "I" signifying Indira. During the next year, her new party attracted enough members of the legislature to become the official opposition.In November 1978, Gandhi regained a parliamentary seat. In January 1980, following a landslide victory for Congress (I), she was again elected prime minister. The national election commission declared Congress (I) to be the real Indian National Congress for the 1984 general election.However, the designation I was only dropped in 1996.During Gandhi's new term as prime minister, her youngest son Sanjay died in an aeroplane crash in June 1980. This led her to encourage her elder son Rajiv, who was working as a pilot, to enter politics. Gradually, Indira Gandhi's politics and outlook grew more authoritarian and autocratic, and she became the central figure within the Congress Party. As prime minister, she became known for her political ruthlessness and unprecedented centralization of power.Gandhi's term as prime minister also saw increasing turmoil in Punjab, with demands for Sikh autonomy by Jarnail Singh Bhindranwale and his militant followers. In 1983, they headquartered themselves in the Golden Temple in Amritsar and started accumulating weapons. In June 1984, after several futile negotiations, Gandhi ordered the Indian Army to enter the Golden Temple to establish control over the complex and remove Bhindranwale and his armed followers. This event is known as Operation Blue Star.On 31 October 1984, two of Gandhi's bodyguards, Satwant Singh and Beant Singh, shot her with their service weapons in the garden of the prime minister's residence in response to her authorisation of Operation Blue Star. Gandhi was due to be interviewed by British actor Peter Ustinov, who was filming a documentary for Irish television. Her assassination prompted the 1984 anti-Sikh riots, during which more than 3,000 people were killed.
In 1984, Indira Gandhi's son Rajiv Gandhi became nominal head of Congress, and went on to become prime minister upon her assassination. In December, he led Congress to a landslide victory, where it secured 401 seats in the legislature. His administration took measures to reform the government bureaucracy and liberalise the country's economy. Rajiv Gandhi's attempts to discourage separatist movements in Punjab and Kashmir backfired. After his government became embroiled in several financial scandals, his leadership became increasingly ineffectual. Gandhi was regarded as a non-abrasive person who consulted other party members and refrained from hasty decisions. The Bofors scandal damaged his reputation as an honest politician, but he was posthumously cleared of bribery allegations in 2004. On 21 May 1991, Gandhi was killed by a bomb concealed in a basket of flowers carried by a woman associated with the Tamil Tigers. He was campaigning in Tamil Nadu for upcoming parliamentary elections. In 1998, an Indian court convicted 26 people in the conspiracy to assassinate Gandhi. The conspirators, who consisted of Tamil militants from Sri Lanka and their Indian allies, had sought revenge against Gandhi because the Indian troops he sent to Sri Lanka in 1987 to help enforce a peace accord there had fought with Tamil separatist guerrillas. Rajiv Gandhi was succeeded as party leader by P. V. Narasimha Rao, who was elected prime minister in June 1991. His rise to the prime ministership was politically significant because he was the first holder of the office from South India. His administration oversaw major economic change and experienced several home incidents that affected India's national security. Rao, who held the Industries portfolio, was personally responsible for the dismantling of the Licence Raj, which came under the purview of the Ministry of Commerce and Industry. He is often called the "father of Indian economic reforms".Future prime ministers Atal Bihari Vajpayee and Manmohan Singh continued the economic reform policies begun by Rao's government. Rao accelerated the dismantling of the Licence Raj, reversing the socialist policies of previous governments. He employed Manmohan Singh as his finance minister to begin a historic economic change. With Rao's mandate, Singh launched India's globalisation reforms that involved implementing International Monetary Fund (IMF) policies to prevent India's impending economic collapse. Rao was also referred to as Chanakya for his ability to push tough economic and political legislation through the parliament while he headed a minority government.By 1996, the party's image was suffering from allegations of corruption, and in elections that year, Congress was reduced to 140 seats, its lowest number in the Lok Sabha to that point. Rao later resigned as prime minister and, in September, as party president. He was succeeded as president by Sitaram Kesri, the party's first non-Brahmin leader. During the tenure of both Rao and Kesri, the two leaders conducted internal elections to the Congress working committees and their own posts as party presidents.
The 1998 general election saw Congress win 141 seats in the Lok Sabha, its lowest tally until then. To boost its popularity and improve its performance in the forthcoming election, Congress leaders urged Sonia Gandhi, Rajiv Gandhi's widow, to assume leadership of the party. She had previously declined offers to become actively involved in party affairs, and had stayed away from politics. After her election as party leader, a section of the party that objected to the choice because of her Italian ethnicity broke away and formed the Nationalist Congress Party (NCP), led by Sharad Pawar. The breakaway faction commanded strong support in the state of Maharashtra and limited support elsewhere. The remainder continued to be known as the Indian National Congress. Sonia Gandhi struggled to revive the party in her early years as its president; she was under continuous scrutiny for her foreign birth and lack of political acumen. In the snap elections called by the National Democratic Alliance (NDA) government in 1999, Congress' tally further plummeted to just 114 seats. Although the leadership structure was unaltered as the party campaigned strongly in the assembly elections that followed, Gandhi began to make such strategic changes as abandoning the party's 1998 Pachmarhi resolution of ekla chalo, or "go it alone" policy, and formed alliances with other like-minded parties. In the intervening years, the party was successful at various legislative assembly elections; at one point, Congress ruled 15 states. For the 2004 general election, Congress forged alliances with regional parties including the NCP and the Dravida Munnetra Kazhagam. The party's campaign emphasised social inclusion and the welfare of the common masses—an ideology that Gandhi herself endorsed for Congress during her presidency—with slogans such as Congress ka haath, aam aadmi ke saath ("Congress hand in hand with the common man"), contrasting with the NDA's "India Shining" campaign. The Congress-led United Progressive Alliance (UPA) won 222 seats in the new parliament, defeating the NDA by a substantial margin. With the subsequent support of the communist front, Congress won a majority and formed a new government. Despite massive support from within the party, Gandhi declined the post of prime minister, choosing to appoint Manmohan Singh instead. She remained as party president and headed the National Advisory Council (NAC).During its first term in office, the UPA government passed several social reform bills. These included an employment guarantee bill, the Right to Information Act, and a right to education act. The NAC, as well as the Left Front that supported the government from the outside, were widely seen as being the driving force behind such legislation. The Left Front withdrew its support of the government over disagreements about the U.S.–India Civil Nuclear Agreement. Despite the effective loss of 62 seats in parliament, the government survived the trust vote that followed. In the Lok Sabha elections held soon after, Congress won 207 seats, the highest tally of any party since 1991. The UPA as a whole won 262, enabling it to form a government for the second time. The social welfare policies of the first UPA government, and the perceived divisiveness of the BJP, are broadly credited with the victory.By the 2014 Lok Sabha elections, the party had lost much of its popular support, mainly because of several years of poor economic conditions in the country, and growing discontent over a series of corruption allegations involving government officials, including the 2G spectrum case and the Indian coal allocation scam. Congress won only 44 seats, which was its worst-ever performance in a national election with its vote share dipping below 20% for the first time. Gandhi retired as party president in December 2017, having served for a record nineteen years. She was succeeded by her son Rahul Gandhi, who was elected unopposed in the 2017 Indian National Congress presidential election.Rahul Gandhi resigned from his post after the 2019 Indian general election, due to the party's dismal performance. The INC had managed to win only 52 seats, hence failing to provide an official Leader of the Opposition for a second consecutive term. Following Gandhi's resignation, party leaders began deliberations for a suitable candidate to replace him. The Congress Working Committee met on 10 August to make a final decision on the matter and passed a resolution asking Sonia Gandhi to take over as interim president until a consensus candidate could be picked.
Congress is a social democratic and social liberal party that supports the values of freedom, tolerance, secularism, equality, and individual rights. Its political position is generally considered to be on the centre to centre-left, which involves supporting mixed economy, social security and a system of progressive taxation. Throughout much of the Cold War period, Congress supported a foreign policy of nonalignment that called for India to form ties with both the Western and Eastern Blocs, but to avoid formal alliances with either. US support for Pakistan led the party to endorse a friendship treaty with the Soviet Union in 1971. In 2004, when the Congress-led United Progressive Alliance came to power, its chairperson Sonia Gandhi unexpectedly relinquished the premiership to Manmohan Singh. This Singh-led "UPA I" government executed several key pieces of legislation and projects, including the Rural Health Mission, Unique Identification Authority, the Rural Employment Guarantee scheme, and the Right to Information Act.
The history of economic policy of Congress-led governments can be divided into two periods. The first period lasted from independence, in 1947, to 1991 and put great emphasis on the public sector. The second period began with economic liberalization in 1991. At the beginning of the first period, the Congress prime minister Jawaharlal Nehru implemented policies based on import substitution industrialization and advocated a mixed economy where the government-controlled public sector would co-exist with the private sector. He believed that the establishment of basic and heavy industry was fundamental to the development and modernisation of the Indian economy. The government, therefore, directed investment primarily into key public-sector industries—steel, iron, coal, and power—promoting their development with subsidies and protectionist policies. This period was called the Licence Raj, or Permit Raj, which was the elaborate system of licences, regulations, and accompanying red tape that were required to set up and run businesses in India between 1947 and 1990. The Licence Raj was a result of Nehru and his successors' desire to have a planned economy where all aspects of the economy were controlled by the state, and licences were given to a select few. Up to 80 government agencies had to be satisfied before private companies could produce something; and, if the licence were granted, the government would regulate production. The licence raj system continued under Indira Gandhi.In addition, many key sectors such as banking, steel coal, and oil were nationalized. Under Rajiv Gandhi, small steps were taken to liberalize the economy.In 1991, the new Congress-party government, led by P. V. Narasimha Rao, initiated reforms to avert the impending 1991 economic crisis. The reforms progressed furthest in opening up areas to foreign investment, reforming capital markets, deregulating domestic business, and reforming the trade regime. The goals of Rao's government were to reduce the fiscal deficit, privatize the public sector, and increase investment in infrastructure. Trade reforms and changes in the regulation of foreign direct investment were introduced in order to open India to foreign trade while stabilising external loans. Rao chose Manmohan Singh for the job. Singh, an acclaimed economist and former chairman of the Reserve Bank, played a central role in implementing these reforms. In 2004, Singh became prime minister of the Congress-led UPA government. Singh remained prime minister after the UPA won the 2009 general elections. The UPA government introduced policies aimed at reforming the banking and financial sectors, as well as public sector companies. It also introduced policies aimed at relieving farmers of their debt. In 2005, Singh's government introduced the value added tax, replacing the sales tax. India was able to resist the worst effects of the global economic crisis of 2008. Singh's government continued the Golden Quadrilateral, the Indian highway modernisation program that was initiated by Vajpayee's government.At present, Congress endorses a mixed economy in which the private sector and the state both direct the economy, which has characteristics of both market and planned economies. Congress advocates import substitution industrialisation—the replacement of foreign imports with domestic products. Congress believes the Indian economy should be liberalised to increase the pace of development.
The Congress government oversaw the establishment of many institutions of higher learning, including the All India Institute of Medical Sciences, the Indian Institutes of Technology, the Indian Institutes of Management and the National Institutes of Technology. The National Council of Educational Research and Training (NCERT) established in 1961 as a literary, scientific and charitable Society under the Societies' Registration Act. In 2005, The Congress-led government started the National Rural Health Mission, which employed about 500,000 community health workers. It was praised by economist Jeffrey Sachs. In 2006, it implemented a proposal to reserve 27% of seats in the All India Institute of Medical Studies (AIIMS), the Indian Institutes of Technology (IITs), the Indian Institutes of Management (IIMs), and other central higher education institutions, for Other Backward Classes, which led to the 2006 Indian anti-reservation protests. The Singh government also continued the Sarva Shiksha Abhiyan programme, which includes the introduction and improvement of mid-day school meals and the opening of new schools throughout India, especially in rural areas, to fight illiteracy. During Manmohan Singh's prime-ministership, eight Institutes of Technology were opened in the states of Andhra Pradesh, Bihar, Gujarat, Orissa, Punjab, Madhya Pradesh, Rajasthan, and Himachal Pradesh.
Congress has strengthened anti-terrorism laws with amendments to the Unlawful Activities (Prevention) Act (UAPA). The National Investigation Agency (NIA) was created by the UPA government soon after the November 2008 Mumbai terror attacks, in response to the need for a central agency to combat terrorism. The Unique Identification Authority of India was established in February 2009 to implement the proposed Multipurpose National Identity Card, with the objective of increasing national security.
Congress has continued the foreign policy started by P. V. Narasimha Rao. This includes the peace process with Pakistan, and the exchange of high-level visits by leaders from both countries. The party has tried to end the border dispute with the People's Republic of China through negotiations. Relations with Afghanistan have also been a concern for Congress. During Afghan President Hamid Karzai's visit to New Delhi in August 2008, Manmohan Singh increased the aid package to Afghanistan for the development of schools, health clinics, infrastructure, and defence. India is now one of the single largest aid donors to Afghanistan.When in power between 2004 and 2014, Congress worked on India's relationship with the United States. Prime Minister Manmohan Singh visited the US in July 2005 to negotiate an India–United States Civil Nuclear Agreement. US president George W. Bush visited India in March 2006; during this visit, a nuclear agreement that would give India access to nuclear fuel and technology in exchange for the IAEA inspection of its civil nuclear reactors was proposed. Over two years of negotiations, followed by approval from the IAEA, the Nuclear Suppliers Group and the United States Congress, the agreement was signed on 10 October 2008.Congress' policy has been to cultivate friendly relations with Japan as well as European Union countries including the United Kingdom, France, and Germany. Diplomatic relations with Iran have continued, and negotiations over the Iran-Pakistan-India gas pipeline have taken place. In April 2006, New Delhi hosted an India–Africa summit attended by the leaders of 15 African states. Congress' policy has also been to improve relations with other developing countries, particularly Brazil and South Africa.
Congress was structured in a hierarchical manner by Mahatma Gandhi when he took charge as the president of the party in 1921. The party was a "broad church" during the independence movement; however, Jawarlal Nehru's descendants have turned the party into a "family firm" with hereditary succession. At present, the president and the All India Congress Committee (AICC) are elected by delegates from state and district parties at an annual national conference; in every Indian state and union territory—or pradesh—there is a Pradesh Congress Committee (PCC), which is the state-level unit of the party responsible for directing political campaigns at local and state levels, and assisting the campaigns for parliamentary constituencies. Each PCC has a working committee of twenty members, most of whom are appointed by the party president, the leader of the state party, who is chosen by the national president. Those elected as members of the states' legislative assemblies form the Congress Legislature Parties in the various state assemblies; their chairperson is usually the party's nominee for Chief Ministership. The party is also organised into various committees, and sections; it publishes a daily newspaper, the National Herald. Despite being a party with a structure, Congress under Indira Gandhi did not hold any organizational elections after 1972.The AICC is composed of delegates sent from the PCCs. The delegates elect Congress committees, including the Congress Working Committee, consisting of senior party leaders and office bearers. The AICC takes all important executive and political decisions. Since Indira Gandhi formed Congress (I) in 1978, the President of the Indian National Congress has effectively been the party's national leader, head of the organisation, head of the Working Committee and all chief Congress committees, chief spokesman, and Congress' choice for Prime Minister of India. Constitutionally, the president is elected by the PCCs and members of the AICC; however, this procedure has often been bypassed by the Working Committee, which has elected its own candidate.The Congress Parliamentary Party (CPP) consists of elected MPs in the Lok Sabha and Rajya Sabha. There is also a Congress Legislative Party (CLP) leader in each state. The CLP consists of all Congress Members of the Legislative Assembly (MLAs) in each state. In cases of states where the Congress is single-handedly ruling the government, the CLP leader is the Chief Minister. Other directly affiliated groups include: the National Students Union of India (NSUI), the Indian Youth Congress – the party's youth wing, the Indian National Trade Union Congress, All India Mahila Congress, its women's division, and Congress Seva Dal—its voluntary organisation. All India Congress Minority Department also referred as Minority Congress is the minority wing of the Congress party. It is represented by the Pradesh Congress Minority Department in all the states of India.
Dynasticism is fairly common in many political parties in India, including the Congress party. Six members of the Nehru–Gandhi family have been presidents of the party. The party started being controlled by Indira Gandhi's family during the emergency. This was characterized by servility and sycophancy towards the family which later led to hereditary succession of Rajiv Gandhi as successor after Indira Gandhi's assassination, as well as the party's selection of Sonia Gandhi as Rajiv's successor after his assassination, which she turned down. Since the formation of Congress(I) by Indira Gandhi in 1978, the party president has been from her family except for the period between 1991 and 1998. In the last three elections to the Lok Sabha combined, 37% of Congress party MPs had family members precede them in politics.
Source: Pradesh Congress Committee
The Congress has governed a majority of the period of independent India (for 55 years), whereby Jawaharlal Nehru, Indira Gandhi and Manmohan Singh are country's longest serving Prime Ministers.
A majority of non-Congress prime ministers of India had been members of the Congress party earlier in their careers.
The party during the post-independence era has governed most of the States and union territories of India. As of March 2020, the Congress (INC) is in power in the states of Chhattisgarh, Punjab and Rajasthan, where the party has majority. In Puducherry, it shares power with alliance partner Dravida Munnetra Kazhagam. In Maharashtra, it shares power as a junior ally with alliance partners Nationalist Congress Party, Shiv Sena and other smaller regional parties under the multi-party Maha Vikas Aghadi coalition. In Jharkhand, it shares power as a junior ally with Jharkhand Mukti Morcha.
The Congress has previously been the sole party in power in Delhi, Andhra Pradesh, Meghalaya, Haryana, and Uttarakhand. The Congress has never been in power in Telangana (upon its formation in 2014). In the past, the Congress has also been the party in power in the following states: Madhya Pradesh Karnataka Bihar (with Mahagathbandhan) GoaAs of April 2020, the Congress has Chief Ministers in 5 states and one Union territory: Punjab Rajasthan Chhattisgarh Maharashtra (with Shiv Sena and Nationalist Congress Party) Jharkhand (with Jharkhand Mukti Morcha) PuducherryThe Congress has been out of power for over decade in the following states: Tamil nadu Uttar Pradesh West Bengal Gujarat Odisha
Official website Indian National Congress at Curlie
The Library of Congress (LC) is the research library that officially serves the United States Congress and is the de facto national library of the United States. It is the oldest federal cultural institution in the United States. The library is housed in three buildings on Capitol Hill in Washington, D.C.; it also maintains the National Audio-Visual Conservation Center in Culpeper, Virginia. The library's functions are overseen by the librarian of Congress, and its buildings are maintained by the architect of the Capitol. The Library of Congress is one of the largest libraries in the world. Its "collections are universal, not limited by subject, format, or national boundary, and include research materials from all parts of the world and in more than 450 languages."Congress moved to Washington, D.C., in 1800 after sitting for 11 years in the temporary national capitals in New York City and Philadelphia. In both cities, members of the U.S. Congress had access to the sizable collections of the New York Society Library and the Library Company of Philadelphia. The small Congressional Library was housed in the United States Capitol for most of the 19th century until the early 1890s. Most of the original collection had been destroyed by the British in 1814 during the War of 1812, and the library sought to restore its collection in 1815. They bought Thomas Jefferson's entire personal collection of 6,487 books. After a period of slow growth, another fire struck the library in its Capitol chambers in 1851, again destroying a large amount of the collection, including many of Jefferson's books. After the American Civil War, the Library of Congress grew rapidly in both size and importance, which sparked a campaign to purchase replacement copies for volumes that had been burned. The library received the right of transference of all copyrighted works to deposit two copies of books, maps, illustrations, and diagrams printed in the United States. It also began to build its collections, and its development culminated between 1888 and 1894 with the construction of a separate, extensive library building across the street from the Capitol. The library's primary mission is to research inquiries made by members of Congress, carried out through the Congressional Research Service. The library is open to the public, although only high-ranking government officials and library employees may check out books and materials.
James Madison is credited with the idea of creating a congressional library, first making such a proposition in 1783. The Library of Congress was subsequently established on April 24, 1800, when President John Adams signed an act of Congress providing for the transfer of the seat of government from Philadelphia to the new capital city of Washington. Part of the legislation appropriated $5,000 "for the purchase of such books as may be necessary for the use of Congress ... and for fitting up a suitable apartment for containing them." Books were ordered from London, and the collection consisted of 740 books and three maps which were housed in the new United States Capitol.President Thomas Jefferson played an important role in establishing the structure of the Library of Congress. On January 26, 1802, he signed a bill that allowed the president to appoint the librarian of Congress and establishing a Joint Committee on the Library to regulate and oversee it. The new law also extended borrowing privileges to the president and vice president.The invading British army burned Washington in August 1814 during the War of 1812 and destroyed the Library of Congress and its collection of 3,000 volumes. These volumes had been left in the Senate wing of the Capitol. One of the few congressional volumes to survive was a government account book of receipts and expenditures for 1810. It was taken as a souvenir by British admiral George Cockburn, whose family returned it to the United States government in 1940.Within a month, Thomas Jefferson offered to sell his personal library as a replacement. Congress accepted his offer in January 1815, appropriating $23,950 to purchase his 6,487 books. Some members of the House of Representatives opposed the outright purchase, including New Hampshire representative Daniel Webster who wanted to return "all books of an atheistical, irreligious, and immoral tendency." Jefferson had spent 50 years accumulating a wide variety of books in several languages and on subjects such as philosophy, history, law, religion, architecture, travel, natural sciences, mathematics, studies of classical Greece and Rome, modern inventions, hot air balloons, music, submarines, fossils, agriculture, and meteorology. He had also collected books on topics not normally viewed as part of a legislative library, such as cookbooks. However, he believed that all subjects had a place in the Library of Congress. He remarked: I do not know that it contains any branch of science which Congress would wish to exclude from their collection; there is, in fact, no subject to which a Member of Congress may not have occasion to refer. Jefferson's collection was unique in that it was the working collection of a scholar, not a gentleman's collection for display. With the addition of his collection, which doubled the size of the original library, the Library of Congress was transformed from a specialist's library to a more general one. His original collection was organized into a scheme based on Francis Bacon's organization of knowledge. Specifically, he grouped his books into Memory, Reason, and Imagination, which broke down into 44 more subdivisions. His library included subjects such as philosophy, history, law, religion, and many other topics. The library followed Jefferson's organization scheme until the late 19th century, when librarian Herbert Putnam began work on a more flexible Library of Congress Classification structure that now applies to more than 138 million items. In 1851, a fire destroyed two thirds of the Jefferson collection, with only 2,000 books remaining. By 2008, the librarians of Congress had found replacements for all but 300 of the works that were in Jefferson's original collection.
On December 24, 1851, the largest fire in the library's history destroyed 35,000 books, about two–thirds of the library's collection and two-thirds of Jefferson's original transfer. Congress appropriated $168,700 to replace the lost books in 1852 but not to acquire new materials. This marked the start of a conservative period in the library's administration by librarian John Silva Meehan and joint committee chairman James A. Pearce, who restricted the library's activities. Meehan and Pearce's views about a restricted scope for the Library of Congress reflected those shared by members of Congress. While Meehan was librarian he supported and perpetuated the notion that "the congressional library should play a limited role on the national scene and that its collections, by and large, should emphasize American materials of obvious use to the U.S. Congress." In 1859, Congress transferred the library's public document distribution activities to the Department of the Interior and its international book exchange program to the Department of State.During the 1850s, Smithsonian Institution librarian Charles Coffin Jewett aggressively tried to make the Smithsonian into the United States' national library. His efforts were blocked by Smithsonian secretary Joseph Henry, who advocated a focus on scientific research and publication. To reinforce his intentions for the Smithsonian, Henry established laboratories, developed a robust physical sciences library and started the Smithsonian Contributions to Knowledge, the first of many publications intended to disseminate research results. For Henry, the Library of Congress was the obvious choice as the national library. Unable to resolve the conflict, Henry dismissed Jewett in July 1854. In 1865 the Smithsonian building, also called the Castle due to its Norman architectural style, was devastated by fire and presented Henry an opportunity in regards to the Smithsonian's non-scientific library. Around this time, the Library of Congress was making plans to build and relocate to the new Thomas Jefferson Building, which would be fireproof. Authorized by an act of Congress, he transferred the Smithsonian's non-scientific library of 40,000 volumes to the Library of Congress in 1866.Abraham Lincoln appointed John G. Stephenson as librarian of Congress in 1861 and the appointment is regarded as the most political to date. Stephenson was a physician and spent equal time serving as librarian and as a physician in the Union Army. He could manage this division of interest because he hired Ainsworth Rand Spofford as his assistant. Despite his new job, Stephenson's focus was on non-library affairs; three weeks into his term, he left Washington, D.C. to serve as a volunteer aide-de-camp at the battles of Chancellorsville and Gettysburg during the American Civil War. Stephenson's term as librarian seems to have left little imprint on the library although hiring Spofford, who was left to run the library in his absence, may have been his most significant achievement.
The Library of Congress reasserted itself during the latter half of the 19th century under Librarian Ainsworth Rand Spofford who directed it from 1865 to 1897. He built broad bipartisan support for it as a national library and a legislative resource, aided by an overall expansion of the federal government and a favorable political climate. He began comprehensively collecting Americana and American literature, led the construction of a new building to house the library, and transformed the librarian of Congress position into one of strength and independence. Between 1865 and 1870, Congress appropriated funds for the construction of the Thomas Jefferson Building, placed all copyright registration and deposit activities under the library's control, and restored the international book exchange. The library also acquired the vast libraries of the Smithsonian and of historian Peter Force, strengthening its scientific and Americana collections significantly. By 1876, the Library of Congress had 300,000 volumes and was tied with the Boston Public Library as the nation's largest library. It moved from the Capitol building to its new headquarters in 1897 with more than 840,000 volumes, 40 percent of which had been acquired through copyright deposit. A year before the library's move to its new location, the Joint Library Committee held a session of hearings to assess the condition of the library and plan for its future growth and possible reorganization. Spofford and six experts sent by the American Library Association testified that the library should continue its expansion towards becoming a true national library. Congress more than doubled the library's staff from 42 to 108 based on the hearings, and with the assistance of senators Justin Morrill of Vermont and Daniel W. Voorhees of Indiana, and established new administrative units for all aspects of the collection. Congress also strengthened the office of Librarian of Congress to govern the library and make staff appointments, as well as requiring Senate approval for presidential appointees to the position.
The Library of Congress, spurred by the 1897 reorganization, began to grow and develop more rapidly. Spofford's successor John Russell Young, though only in office for two years, overhauled the library's bureaucracy, used his connections as a former diplomat to acquire more materials from around the world, and established the library's first assistance programs for the blind and physically disabled. Young's successor Herbert Putnam held the office for forty years from 1899 to 1939, entering into the position two years before the library became the first in the United States to hold one million volumes. Putnam focused his efforts on making the library more accessible and useful for the public and for other libraries. He instituted the interlibrary loan service, transforming the Library of Congress into what he referred to as a "library of last resort". Putnam also expanded library access to "scientific investigators and duly qualified individuals" and began publishing primary sources for the benefit of scholars.Putnam's tenure also saw increasing diversity in the library's acquisitions. In 1903, he persuaded President Theodore Roosevelt to transfer by executive order the papers of the Founding Fathers from the State Department to the Library of Congress. Putnam expanded foreign acquisitions as well, including the 1904 purchase of a four-thousand volume library of Indica, the 1906 purchase of G. V. Yudin's eighty-thousand volume Russian library, the 1908 Schatz collection of early opera librettos, and the early 1930s purchase of the Russian Imperial Collection, consisting of 2,600 volumes from the library of the Romanov family on a variety of topics. Collections of Hebraica and Chinese and Japanese works were also acquired. Congress even took the initiative to acquire materials for the library in one occasion, when in 1929 Congressman Ross Collins of Mississippi successfully proposed the $1.5 million purchase of Otto Vollbehr's collection of incunabula, including one of three remaining perfect vellum copies of the Gutenberg Bible. In 1914, Putnam established the Legislative Reference Service as a separative administrative unit of the library. Based in the Progressive era's philosophy of science as a problem-solver, and modeled after successful research branches of state legislatures, the LRS would provide informed answers to Congressional research inquiries on almost any topic. In 1965, Congress passed an act allowing the Library of Congress to establish a trust fund board to accept donations and endowments, giving the library a role as a patron of the arts. The library received the donations and endowments of prominent individuals such as John D. Rockefeller, James B. Wilbur and Archer M. Huntington. Gertrude Clarke Whittall donated five Stradivarius violins to the library and Elizabeth Sprague Coolidge's donations paid for a concert hall within the Library of Congress building and the establishment of an honorarium for the Music Division. A number of chairs and consultantships were established from the donations, the most well-known of which is the Poet Laureate Consultant.The library's expansion eventually filled the library's Main Building, despite shelving expansions in 1910 and 1927, forcing the library to expand into a new structure. Congress acquired nearby land in 1928 and approved construction of the Annex Building (later the John Adams Building) in 1930. Although delayed during the Depression years, it was completed in 1938 and opened to the public in 1939.
When Putnam retired in 1939, President Franklin D. Roosevelt appointed Archibald MacLeish as his successor. Occupying the post from 1939 to 1944 during the height of World War II, MacLeish became the most visible librarian of Congress in the library's history. MacLeish encouraged librarians to oppose totalitarianism on behalf of democracy; dedicated the South Reading Room of the Adams Building to Thomas Jefferson, commissioning artist Ezra Winter to paint four themed murals for the room; and established a "democracy alcove" in the Main Reading Room of the Jefferson Building for important documents such as the Declaration, Constitution and The Federalist Papers. The Library of Congress even assisted during the war effort, ranging from the storage of the Declaration of Independence and the United States Constitution in Fort Knox for safekeeping to researching weather data on the Himalayas for Air Force pilots. MacLeish resigned in 1944 to become Assistant Secretary of State, and President Harry Truman appointed Luther H. Evans as librarian of Congress. Evans, who served until 1953, expanded the library's acquisitions, cataloging and bibliographic services as much as the fiscal-minded Congress would allow, but his primary achievement was the creation of Library of Congress Missions around the world. Missions played a variety of roles in the postwar world: the mission in San Francisco assisted participants in the meeting that established the United Nations, the mission in Europe acquired European publications for the Library of Congress and other American libraries, and the mission in Japan aided in the creation of the National Diet Library.Evans' successor Lawrence Quincy Mumford took over in 1953. Mumford's tenure, lasting until 1974, saw the initiation of the construction of the James Madison Memorial Building, the third Library of Congress building. Mumford directed the library during a period of increased educational spending, the windfall of which allowed the library to devote energies towards establishing new acquisition centers abroad, including in Cairo and New Delhi. In 1967, the library began experimenting with book preservation techniques through a Preservation Office, which grew to become the largest library research and conservation effort in the United States. Mumford's administration also saw the last major public debate about the Library of Congress' role as both a legislative library and a national library. A 1962 memorandum by Douglas Bryant of the Harvard University Library, compiled at the request of Joint Library Committee chairman Claiborne Pell, proposed a number of institutional reforms, including expansion of national activities and services and various organizational changes, all of which would shift the library more towards its national role over its legislative role. Bryant even suggested possibly changing the name of the Library of Congress, which was rebuked by Mumford as "unspeakable violence to tradition". Debate continued within the library community until the Legislative Reorganization Act of 1970 shifted the library back towards its legislative roles, placing greater focus on research for Congress and congressional committees and renaming the Legislative Reference Service to the Congressional Research Service.After Mumford retired in 1974, Gerald Ford appointed Daniel J. Boorstin as librarian. Boorstin's first challenge was the move to the new Madison Building, which took place between 1980 and 1982. The move released pressures on staff and shelf space, allowing Boorstin to focus on other areas of library administration such as acquisitions and collections. Taking advantage of steady budgetary growth, from $116 million in 1975 to over $250 million by 1987, Boorstin actively participated in enhancing ties with scholars, authors, publishers, cultural leaders, and the business community. His active and prolific role changed the post of librarian of Congress so that by the time he retired in 1987, The New York Times called it "perhaps the leading intellectual public position in the nation". President Ronald Reagan nominated James H. Billington as the 13th librarian of Congress in 1987, and the U.S. Senate unanimously confirmed the appointment. Under Billington's leadership, the library doubled the size of its analog collections from 85.5 million items in 1987 to more than 160 million items in 2014. At the same time, it established new programs and employed new technologies to, "get the champagne out of the bottle". These included: American Memory created in 1990, which became The National Digital Library in 1994, providing free access online to digitized American history and culture resources with curatorial explanations for K-12 education. thomas.gov website launched in 1994 to provide free public access to U.S. federal legislative information with ongoing updates; and congress.gov website to provide a state-of-the-art framework for both Congress and the public in 2012; The National Book Festival, founded in 2000 with Laura Bush, has brought more than 1000 authors and a million guests to the National Mall and the Washington Convention Center to celebrate reading. With a major gift from David Rubenstein in 2013, the library also established the Library of Congress Literacy Awards to recognize and support achievements in improving literacy in the U.S. and abroad; The Kluge Center, started with a grant of $60 million from John W. Kluge in 2000 to bring scholars and researchers from around the world to use library resources and to interact with policymakers and the public. It hosts public lectures and scholarly events, provides endowed Kluge fellowships, and awards The Kluge Prize for the Study of Humanity (now worth $1.5 million), the first Nobel-level international prize for lifetime achievement in the humanities and social sciences (subjects not included in the Nobel awards); Open World Leadership Center, established in 2000, administered 23,000 professional exchanges for emerging post-Soviet leaders in Russia, Ukraine, and the other successor states of the former USSR by 2015. Open World began as a Library of Congress project, and later became an independent agency in the legislative branch. The Veterans History Project, congressionally mandated in 2000 to collect, preserve, and make accessible the personal accounts of American war veterans from WWI to the present day; The National Audio-Visual Conservation Center, which opened in 2007 at a 45-acre site in Culpeper, Virginia with the largest private gift ever made to the library (more than $150 million by the Packard Humanities Institute) and $82.1 million additional support from Congress. In 1988, The library also established the National Film Preservation Board, a congressionally mandated National Film Preservation Board to select American films annually for preservation and inclusion in the new National Registry, a collection of American films the library has made available on the Internet for free streaming. By 2015, the librarian had named 650 films to the registry. The films in the collection date from the earliest to ones produced more than ten years ago and they are selected from nominations submitted to the board. The Gershwin Prize for Popular Song, launched in 2007 to honor the work of an artist whose career reflects lifetime achievement in song composition. Winners have included Paul Simon, Stevie Wonder, Paul McCartney, Burt Bacharach and Hal David, Carole King, Billy Joel, and just-named Willie Nelson for November 2015. The library also launched the Living Legend Awards in 2000 to honor artists, activists, filmmakers, and others who have contributed to America's diverse cultural, scientific, and social heritage; The Fiction Prize (now the Library of Congress Prize for American Fiction) started in 2008 to recognize distinguished lifetime achievement in the writing of fiction. The World Digital Library, established in association with UNESCO and 181 partners in 81 countries in 2009, to make online copies of professionally curated primary materials of the world's varied cultures freely available in multiple languages. National Jukebox launched in 2011 to provide streaming free online access to more than 10,000 out-of-print music and spoken word recordings. BARD in 2013, digital talking books mobile app for Braille and Audio Reading Downloads in partnership with the library's National Library Service for the blind and physically handicapped, that enables free downloads of audio and Braille books to mobile devices via the Apple App Store.During Billington's tenure as the 13th librarian of Congress, the library acquired Lafayette's previously inaccessible papers in 1996 from a castle at La Grange, France; and the only copy of the 1507 Waldseemüller world map ("America's birth certificate") in 2003 for permanent display in the library's Thomas Jefferson Building. Using privately raised funds, the Library of Congress reconstructed Thomas Jefferson's original library, which was placed on permanent display in the Jefferson building in 2008. Billington also enlarged and technologically enhanced public spaces of the Jefferson Building into a national exhibition venue, and hosted over 100 exhibitions. These included exhibits on the Vatican Library and the Bibliothèque Nationale de France, several on the Civil War and Lincoln, on African-American culture, on Religion and the founding of the American Republic, the Early Americas (the Kislak Collection became a permanent display), on the global celebration commemorating the 800th anniversary of Magna Carta, and on early American printing featuring the Rubenstein Bay Psalm Book. Onsite access to the Library of Congress was also increased when Billington advocated successfully for an underground connection between the U.S. Capitol Visitors Center and the library in 2008 to increase congressional usage and public tours of the library's Thomas Jefferson Building.Under Billington, the library launched a mass deacidification program in 2001, which has extended the lifespan of almost 4 million volumes and 12 million manuscript sheets; and new collection storage modules at Fort Meade, the first opening in 2002, to preserve and make accessible more than 4 million items from the library's analog collections. Billington established the Library Collections Security Oversight Committee in 1992 to improve protection of collections, and also the Library of Congress Congressional Caucus in 2008 to draw attention to the library's curators and collections. He created the library's first Young Readers Center in the Jefferson Building in 2009, and the first large-scale summer intern (Junior Fellows) program for university students in 1991. Under Billington, the library also sponsored the Gateway to Knowledge in 2010–2011, a mobile exhibition to 90 sites covering all states east of the Mississippi in a specially designed 18-wheel truck, increasing public access to library collections off-site, particularly for rural populations.Billington raised more than half a billion dollars of private support to supplement Congressional appropriations for library collections, programs, and digital outreach. These private funds helped the library to continue its growth and outreach in the face of a 30% decrease in staffing caused mainly by legislative appropriations cutbacks. He created the library's first development office for private fundraising in 1987, and, in 1990, established the James Madison Council, the library's first national private sector donor-support group. In 1987, Billington also asked the GAO to conduct the first library-wide audit, and he created the first Office of the Inspector General at the library to provide regular independent review of library operations. This precedent led to regular annual financial audits, leading to unmodified ("clean") opinions from 1995 onwards.In April 2010, it announced plans to archive all public communication on Twitter, including all communication since Twitter's launch in March 2006. As of 2015, the Twitter archive remains unfinished.Before retiring in 2015, after 28 years of service, Billington had come "under pressure" as librarian of Congress. This followed a Government Accountability Office report which revealed a "work environment lacking central oversight" and faulted Billington for "ignoring repeated calls to hire a chief information officer, as required by law."When Billington announced his plans to retire in 2015, commentator George Weigel described the Library of Congress as "one of the last refuges in Washington of serious bipartisanship and calm, considered conversation," and "one of the world's greatest cultural centers."Carla Hayden was sworn in as the 14th librarian of Congress on September 14, 2016, becoming both the first woman and the first African-American to hold the position.In 2017, the library announced the Librarian-in-Residence program which aims to support the future generation of librarians by giving them opportunity to gain work experience in five different areas of librarianship including: Acquisitions/Collection Development, Cataloging/Metadata, and Collection Preservation.
The collections of the Library of Congress include more than 32 million catalogued books and other print materials in 470 languages; more than 61 million manuscripts; the largest rare book collection in North America, including the rough draft of the Declaration of Independence, a Gutenberg Bible (originating from the Saint Blaise Abbey, Black Forest) (one of only three perfect vellum copies known to exist); over 1 million U.S. government publications; 1 million issues of world newspapers spanning the past three centuries; 33,000 bound newspaper volumes; 500,000 microfilm reels; U.S. and foreign comic books--over 12,000 titles in all, totaling more than 140,000 issues; films; 5.3 million maps; 6 million works of sheet music; 3 million sound recordings; more than 14.7 million prints and photographic images including fine and popular art pieces and architectural drawings; the Betts Stradivarius; and the Cassavetti Stradivarius. The library developed a system of book classification called Library of Congress Classification (LCC), which is used by most US research and university libraries. The library serves as a legal repository for copyright protection and copyright registration, and as the base for the United States Copyright Office. Regardless of whether they register their copyright, all publishers are required to submit two complete copies of their published works to the library—this requirement is known as mandatory deposit. Nearly 15,000 new items published in the U.S. arrive every business day at the library. Contrary to popular belief, however, the library does not retain all of these works in its permanent collection, although it does add an average of 12,000 items per day. Rejected items are used in trades with other libraries around the world, distributed to federal agencies, or donated to schools, communities, and other organizations within the United States. As is true of many similar libraries, the Library of Congress retains copies of every publication in the English language that is deemed significant. The Library of Congress states that its collection fills about 838 miles (1,349 km) of bookshelves, while the British Library reports about 388 miles (624 km) of shelves. The Library of Congress holds more than 167 million items with more than 39 million books and other print materials, against approximately 150 million items with 25 million books for the British Library. A 2000 study by information scientists Peter Lyman and Hal Varian suggested that the amount of uncompressed textual data represented by the 26 million books then in the collection was 10 terabytes.The library also administers the National Library Service for the Blind and Physically Handicapped, an audio book and braille library program provided to more than 766,000 Americans.
The library's first digitization project was called "American Memory." Launched in 1990, it initially planned to choose 160 million objects from its collection to make digitally available on laserdiscs and CDs that would be distributed to schools and libraries. After realizing that this plan would be too expensive and inefficient, and with the rise of the Internet, the library decided to instead make digitized material available over the Internet. This project was made official in the National Digital Library Program (NDLP), created in October 1994. By 1999, the NDLP had succeeded in digitizing over 5 million objects and had a budget of $12 million. The library has kept the "American Memory" name for its public domain website, which today contains 15 million digital objects, comprising over 7 petabytes.American Memory is a source for public domain image resources, as well as audio, video, and archived Web content. Nearly all of the lists of holdings, the catalogs of the library, can be consulted directly on its web site. Librarians all over the world consult these catalogs, through the Web or through other media better suited to their needs, when they need to catalog for their collection a book published in the United States. They use the Library of Congress Control Number to make sure of the exact identity of the book. Digital images are also available at Snapshots of the Past, which provides archival prints.The library has a budget of between $6–8 million each year for digitization, meaning that not all works can be digitized. It makes determinations about what objects to prioritize based on what is especially important to Congress or potentially interesting for the public. The 15 million digitized items represent less than 10% of the library's total 160-million item collection. The library has chosen not to participate in other digital library projects such as Google Books and the Digital Public Library of America, although it has supported the Internet Archive project.
In 1995, the Library of Congress established online archive of the proceedings of the U.S. Congress, THOMAS. The THOMAS website included the full text of proposed legislation, as well as bill summaries and statuses, Congressional Record text, and the Congressional Record Index. The THOMAS system received major updates in 2005 and 2010. A migration to a more modernized Web system, Congress.gov, began in 2012, and the THOMAS system was retired in 2016. Congress.gov is a joint project of the Library of Congress, the House, the Senate and the Government Publishing Office.
The Library of Congress is physically housed in three buildings on Capitol Hill and a conservation center in rural Virginia. The library's Capitol Hill buildings are all connected by underground passageways, so that a library user need pass through security only once in a single visit. The library also has off-site storage facilities for less commonly requested materials.
The Thomas Jefferson Building is located between Independence Avenue and East Capitol Street on First Street SE. It first opened in 1897 as the main building of the library and is the oldest of the three buildings. Known originally as the Library of Congress Building or Main Building, it took its present name on June 13, 1980.
The John Adams Building is located between Independence Avenue and East Capitol Street on 2nd Street SE, the block adjacent to the Jefferson Building. The building was originally known as The Annex to the Main Building, which had run out of space. It opened its doors to the public January 3, 1939.
The James Madison Memorial Building is located between First and Second Streets on Independence Avenue SE. The building was constructed from 1971 to 1976, and serves as the official memorial to President James Madison.The Madison Building is also home to the Mary Pickford Theater, the "motion picture and television reading room" of the Library of Congress. The theater hosts regular free screenings of classic and contemporary movies and television shows.
The Packard Campus for Audio-Visual Conservation is the Library of Congress's newest building, opened in 2007 and located in Culpeper, Virginia. It was constructed out of a former Federal Reserve storage center and Cold War bunker. The campus is designed to act as a single site to store all of the library's movie, television, and sound collections. It is named to honor David Woodley Packard, whose Packard Humanities Institute oversaw design and construction of the facility. The centerpiece of the complex is a reproduction Art Deco movie theater that presents free movie screenings to the public on a semi-weekly basis.
The Library of Congress, through both the librarian of Congress and the Register of Copyrights, is responsible for authorizing exceptions to Section 1201 of Title 17 of the United States Code as part of the Digital Millennium Copyright Act. This process is done every three years, with the Register receiving proposals from the public and acting as an advisor to the librarian, who issues a ruling on what is exempt. After three years have passed, the ruling is no longer valid and a new ruling on exemptions must be made.
The library is open for academic research to anyone with a Reader Identification Card. One may not remove library items from the reading rooms or the library buildings. Most of the library's general collection of books and journals are in the closed stacks of the Jefferson and Adams Buildings; specialized collections of books and other materials are in closed stacks in all three main library buildings, or are stored off-site. Access to the closed stacks is not permitted under any circumstances, except to authorized library staff, and occasionally, to dignitaries. Only the reading room reference collections are on open shelves. Since 1902, American libraries have been able to request books and other items through interlibrary loan from the Library of Congress if these items are not readily available elsewhere. Through this system, the Library of Congress has served as a "library of last resort", according to former librarian of Congress Herbert Putnam. The Library of Congress lends books to other libraries with the stipulation that they be used only inside the borrowing library.
In addition to its library services, the Library of Congress is also actively involved in various standard activities in areas related to bibliographical and search and retrieve standards. Areas of work include MARC standards, Metadata Encoding and Transmission Standard (METS), Metadata Object Description Schema (MODS), Z39.50 and Search/Retrieve Web Service (SRW), and Search/Retrieve via URL (SRU).The Law Library of Congress seeks to further legal scholarship by providing opportunities for scholars and practitioners to conduct significant legal research. Individuals are invited to apply for projects which would further the multi-faceted mission of the law library in serving the U.S. Congress, other governmental agencies, and the public.
Fellows in American Letters of the Library of Congress Gershwin Prize for Popular Song Library of Congress Prize for American Fiction Founder's Day Celebration National Book Festival Mostly Lost Film Identification Workshop
Cecil Hobbs (1943–1971): American scholar of Southeast Asian history, head of the Southern Asia Section of the Orientalia (now Asian) Division of the Library of Congress, a major contributor to scholarship on Asia and the development of South East Asian coverage in American library collections
Documents Expediting Project Federal Research Division Feleky Collection Law Library of Congress Library of Congress Classification Library of Congress Country Studies Library of Congress Living Legend Library of Congress Subject Headings Minerva Initiative National Digital Library Program (NDLP) National Film Registry National Recording Registry United States Senate Library
The Library of Congress website Library of Congress YouTube channel Search the Library of Congress catalog Congress.gov, legislative information Library Of Congress Meeting Notices and Rule Changes from The Federal Register RSS Feed Library of Congress photos on Flickr Outdoor sculpture at the Library of Congress Works by Library of Congress at Project Gutenberg Works by or about Library of Congress at Internet Archive Library of Congress at FamilySearch Research Wiki for genealogists "Congress, Library of" . Encyclopedia Americana. 1920. C-SPAN's Library of Congress documentary and resources The Library of Congress National Library Service (NLS) Video: "Library of Congress in 1968 - Computer Automation"
The Congress of Vienna (French: Congrès de Vienne, German: Wiener Kongress) of 1814–1815 was one of the most important international conferences in European history. It remade Europe after the downfall of French Emperor Napoleon I. It was a meeting of ambassadors of European states chaired by Austrian statesman Klemens von Metternich, and held in Vienna from November 1814 to June 1815. The objective of the Congress was to provide a long-term peace plan for Europe by settling critical issues arising from the French Revolutionary Wars and the Napoleonic Wars. The goal was not simply to restore old boundaries but to resize the main powers so they could balance each other and remain at peace. The leaders were conservatives with little use for republicanism or revolution, both of which threatened to upset the status quo in Europe. France lost all its recent conquests while Prussia, Austria and Russia made major territorial gains. Prussia added smaller German states in the west, Swedish Pomerania, and 60% of the Kingdom of Saxony; Austria gained Venice and much of northern Italy. Russia gained parts of Poland. The new Kingdom of the Netherlands had been created just months before and included formerly Austrian territory that in 1830 became Belgium. The immediate background was Napoleonic France's defeat and surrender in May 1814, which brought an end to 23 years of nearly continuous war. Negotiations continued despite the outbreak of fighting triggered by Napoleon's dramatic return from exile and resumption of power in France during the Hundred Days of March to July 1815. The Congress's "final act" was signed nine days before his final defeat at Waterloo on 18 June 1815. Liberal historians have criticized the Congress for causing the subsequent suppression of the emerging national and liberal movements, and it has been seen as a reactionary movement for the benefit of traditional monarchs. However, others praise it for having created relatively long-term stability and peaceful conditions in most of Europe.In a technical sense, the "Congress of Vienna" was not properly a congress: it never met in plenary session. Instead most of the discussions occurred in informal, face-to-face sessions among the Great Powers of Austria, Britain, France, Russia, and sometimes Prussia, with limited or no participation by other delegates. On the other hand, the Congress was the first occasion in history where, on a continental scale, national representatives came together to formulate treaties instead of relying mostly on messages among the several capitals. The Congress of Vienna settlement formed the framework for European international politics until the outbreak of the First World War in 1914.
The Treaty of Chaumont in 1814 had reaffirmed decisions that had been made already and that would be ratified by the more important Congress of Vienna of 1814–15. They included the establishment of a confederated Germany, the division of Italy into independent states, the restoration of the Bourbon kings of Spain, and the enlargement of the Netherlands to include what in 1830 became modern Belgium. The Treaty of Chaumont became the cornerstone of the European Alliance that formed the balance of power for decades. Other partial settlements had already occurred at the Treaty of Paris between France and the Sixth Coalition, and the Treaty of Kiel that covered issues raised regarding Scandinavia. The Treaty of Paris had determined that a "general congress" should be held in Vienna and that invitations would be issued to "all the Powers engaged on either side in the present war". The opening was scheduled for July 1814.
The Congress functioned through formal meetings such as working groups and official diplomatic functions; however, a large portion of the Congress was conducted informally at salons, banquets, and balls.
The Four Great Powers had previously formed the core of the Sixth Coalition. On the verge of Napoleon's defeat they had outlined their common position in the Treaty of Chaumont (March 1814), and negotiated the Treaty of Paris (1814) with the Bourbons during their restoration: Austria was represented by Prince Metternich, the Foreign Minister, and by his deputy, Baron Johann von Wessenberg. As the Congress's sessions were in Vienna, Emperor Francis was kept closely informed. Britain was represented first by its Foreign Secretary, Viscount Castlereagh; then by the Duke of Wellington, after Castlereagh's return to England in February 1815. In the last weeks it was headed by the Earl of Clancarty, after Wellington left to face Napoleon during the Hundred Days. Tsar Alexander I controlled the Russian delegation which was formally led by the foreign minister, Count Karl Robert Nesselrode. The tsar had two main goals, to gain control of Poland and to promote the peaceful coexistence of European nations. He succeeded in forming the Holy Alliance (1815), based on monarchism and anti-secularism, and formed to combat any threat of revolution or republicanism. Prussia was represented by Prince Karl August von Hardenberg, the Chancellor, and the diplomat and scholar Wilhelm von Humboldt. King Frederick William III of Prussia was also in Vienna, playing his role behind the scenes. France, the "fifth" power, was represented by its foreign minister, Talleyrand, as well as the Minister Plenipotentiary the Duke of Dalberg. Talleyrand had already negotiated the Treaty of Paris (1814) for Louis XVIII of France; the king, however, distrusted him and was also secretly negotiating with Metternich, by mail.
Denmark – Count Niels Rosenkrantz, foreign minister. King Frederick VI was also present in Vienna. The Netherlands – Earl of Clancarty, the British Ambassador at the Dutch court, and Baron Hans von Gagern Switzerland – Every canton had its own delegation. Charles Pictet de Rochemont from Geneva played a prominent role. Kingdom of Sardinia - Marquis Filippo Antonio Asinari di San Marzano. The Papal States – Cardinal Ercole Consalvi Republic of Genoa – Marquise Agostino Pareto, Senator of the Republic. Grand Duchy of Tuscany – Neri Corsini. On German issues, Bavaria – Maximilian Graf von Montgelas Württemberg – Georg Ernst Levin von Wintzingerode Hanover, then in a personal union with the British crown – Georg Graf zu Münster. (King George III had refused to recognize the dissolution of the Holy Roman Empire in 1806 and maintained a separate diplomatic staff as Elector of Hanover to conduct the affairs of the family estate, the Duchy of Brunswick-Lüneburg, until the results of the Congress were concluded establishing the Kingdom of Hanover.) Mecklenburg-Schwerin – Leopold von PlessenVirtually every state in Europe had a delegation in Vienna – more than 200 states and princely houses were represented at the Congress. In addition, there were representatives of cities, corporations, religious organizations (for instance, abbeys) and special interest groups – e.g., a delegation representing German publishers, demanding a copyright law and freedom of the press. The Congress was noted for its lavish entertainment: according to a famous joke it did not move, but danced.
Initially, the representatives of the four victorious powers hoped to exclude the French from serious participation in the negotiations, but Talleyrand skillfully managed to insert himself into "her inner councils" in the first weeks of negotiations. He allied himself to a Committee of Eight lesser powers (including Spain, Sweden, and Portugal) to control the negotiations. Once Talleyrand was able to use this committee to make himself a part of the inner negotiations, he then left it, once again abandoning his allies. The major Allies' indecision on how to conduct their affairs without provoking a united protest from the lesser powers led to the calling of a preliminary conference on protocol, to which Talleyrand and the Marquis of Labrador, Spain's representative, were invited on 30 September 1814.Congress Secretary Friedrich von Gentz reported, "The intervention of Talleyrand and Labrador has hopelessly upset all our plans. Talleyrand protested against the procedure we have adopted and soundly [be]rated us for two hours. It was a scene I shall never forget." The embarrassed representatives of the Allies replied that the document concerning the protocol they had arranged actually meant nothing. "If it means so little, why did you sign it?" snapped Labrador. Talleyrand's policy, directed as much by national as personal ambitions, demanded the close but by no means amicable relationship he had with Labrador, whom Talleyrand regarded with disdain. Labrador later remarked of Talleyrand: "that cripple, unfortunately, is going to Vienna." Talleyrand skirted additional articles suggested by Labrador: he had no intention of handing over the 12,000 afrancesados – Spanish fugitives, sympathetic to France, who had sworn fealty to Joseph Bonaparte, nor the bulk of the documents, paintings, pieces of fine art, and books that had been looted from the archives, palaces, churches and cathedrals of Spain.
The most dangerous topic at the Congress was the Polish-Saxon Crisis. Russia wanted most of Poland, and Prussia wanted all of Saxony, whose king had allied with Napoleon. The tsar would become king of Poland. Austria was fearful this would make Russia much too powerful, a view which was supported by Britain. The result was deadlock, for which Talleyrand proposed a solution: admit France to the inner circle, and France would support Austria and Britain. The three nations signed a secret treaty on 3 January 1815, agreeing to go to war against Russia and Prussia, if necessary, to prevent the Russo-Prussian plan from coming to fruition.When the Tsar heard of the secret treaty he agreed to a compromise that satisfied all parties on 24 October 1815. Russia received most of the Napoleonic Duchy of Warsaw as a "Kingdom of Poland" – called Congress Poland, with the tsar as king ruling it independently of Russia. Russia, however, did not receive the province of Posen (Poznań), which was given to Prussia as the Grand Duchy of Posen, nor Kraków, which became a free city. Furthermore, the tsar was unable to unite the new domain with the parts of Poland that had been incorporated into Russia in the 1790s. Prussia received 60 percent of Saxony-later known as the Province of Saxony, with the remainder returned to King Frederick Augustus I as his Kingdom of Saxony.
The Final Act, embodying all the separate treaties, was signed on 9 June 1815 (a few days before the Battle of Waterloo). Its provisions included: Russia was given most of the Duchy of Warsaw (Poland) and was allowed to keep Finland (which it had annexed from Sweden in 1809 and held until 1917). Prussia was given three-fifths of Saxony, parts of the Duchy of Warsaw (the Grand Duchy of Posen), Danzig, and the Rhineland/Westphalia. A German Confederation of 39 states was created from the previous 300 of the Holy Roman Empire, under the presidency of the Austrian Emperor. Only portions of the territory of Austria and Prussia were included in the Confederation (roughly the same portions that had been within the Holy Roman Empire). The Netherlands and the Southern Netherlands (approx. modern-day Belgium) were united in a monarchy, the United Kingdom of the Netherlands, with the House of Orange-Nassau providing the king (the Eight Articles of London). To compensate for the Orange-Nassau's loss of the Nassau lands to Prussia, the United Kingdom of the Netherlands and the Grand Duchy of Luxembourg were to form a personal union under the House of Orange-Nassau, with Luxembourg (but not the Netherlands) inside the German Confederation. Swedish Pomerania, given to Denmark a year earlier in return for Norway, was ceded by Denmark to Prussia. France received back Guadeloupe from Sweden in return for yearly installments to the Swedish king. The neutrality of the 22 cantons of Switzerland was guaranteed and a federal pact was recommended to them in strong terms. Bienne and the Prince-Bishopric of Basel were incorporated into the Canton of Bern. The Congress also suggested a number of compromises for territorial disputes between cantons to be resolved. Hanover gave up the Duchy of Lauenburg to Denmark, but was enlarged by the addition of former territories of the Bishop of Münster and by the formerly Prussian East Frisia, and made a kingdom. Most of the territorial gains of Bavaria, Württemberg, Baden, Hesse-Darmstadt, and Nassau under the mediatizations of 1801–1806 were recognized. Bavaria also gained control of the Rhenish Palatinate and parts of the Napoleonic Duchy of Würzburg and Grand Duchy of Frankfurt. Hesse-Darmstadt, in exchange for giving up the Duchy of Westphalia to Prussia, received Rhenish Hesse with its capital at Mainz. Austria regained control of the Tyrol and Salzburg; of the former Illyrian Provinces; of Tarnopol district (from Russia); received Lombardy-Venetia in Italy and Ragusa in Dalmatia. Former Austrian territory in Southwest Germany remained under the control of Württemberg and Baden, and the Austrian Netherlands were also not recovered. Ferdinand III was restored as Grand Duke of Tuscany. Archduke Francis IV was acknowledged as the ruler of the Duchy of Modena, Reggio and Mirandola. The Papal States were under the rule of the pope and restored to their former extent, with the exception of Avignon and the Comtat Venaissin, which remained part of France. Britain was confirmed in control of the Cape Colony in Southern Africa; Tobago; Ceylon; and various other colonies in Africa and Asia. Other colonies, most notably the Dutch East Indies and Martinique, were restored to their previous overlords. The King of Sardinia was restored in Piedmont, Nice, and Savoy, and was given control of Genoa (putting an end to the brief proclamation of a restored Republic). The Duchies of Parma, Piacenza and Guastalla were taken from the Queen of Etruria and given to Marie Louise for her lifetime. The Duchy of Lucca was created for the House of Bourbon-Parma, which would have reversionary rights to Parma after the death of Marie Louise. The Bourbon Ferdinand IV, King of Sicily was restored to control of the Kingdom of Naples after Joachim Murat, the king installed by Bonaparte, supported Napoleon in the Hundred Days and started the Neapolitan War by attacking Austria. The slave trade was condemned. Freedom of navigation was guaranteed for many rivers, notably the Rhine and the Danube.The Final Act was signed by representatives of Austria, France, Portugal, Prussia, Russia, Sweden-Norway, and Britain. Spain did not sign the treaty but ratified it in 1817.
The Congress's principal results, apart from its confirmation of France's loss of the territories annexed between 1795 and 1810, which had already been settled by the Treaty of Paris, were the enlargement of Russia, (which gained most of the Duchy of Warsaw) and Prussia, which acquired the district of Poznań, Swedish Pomerania, Westphalia and the northern Rhineland. The consolidation of Germany from the nearly 300 states of the Holy Roman Empire (dissolved in 1806) into a much less complex system of thirty-nine states (4 of which were free cities) was confirmed. These states formed a loose German Confederation under the leadership of Austria.Representatives at the Congress agreed to numerous other territorial changes. By the Treaty of Kiel, Norway had been ceded by the king of Denmark-Norway to the king of Sweden. This sparked the nationalist movement which led to the establishment of the Kingdom of Norway on May 17, 1814 and the subsequent personal Union with Sweden. Austria gained Lombardy-Venetia in Northern Italy, while much of the rest of North-Central Italy went to Habsburg dynasties (the Grand Duchy of Tuscany, the Duchy of Modena, and the Duchy of Parma).The Papal States were restored to the Pope. The Kingdom of Piedmont-Sardinia was restored to its mainland possessions, and also gained control of the Republic of Genoa. In Southern Italy, Napoleon's brother-in-law, Joachim Murat, was originally allowed to retain his Kingdom of Naples, but his support of Napoleon in the Hundred Days led to the restoration of the Bourbon Ferdinand IV to the throne.A large United Kingdom of the Netherlands was formed for the Prince of Orange, including both the old United Provinces and the formerly Austrian-ruled territories in the Southern Netherlands. Other, less important, territorial adjustments included significant territorial gains for the German Kingdoms of Hanover (which gained East Frisia from Prussia and various other territories in Northwest Germany) and Bavaria (which gained the Rhenish Palatinate and territories in Franconia). The Duchy of Lauenburg was transferred from Hanover to Denmark, and Prussia annexed Swedish Pomerania. Switzerland was enlarged, and Swiss neutrality was established. Swiss mercenaries had played a significant role in European wars for a couple of hundred years: the Congress intended to put a stop to these activities permanently.During the wars, Portugal had lost its town of Olivenza to Spain and moved to have it restored. Portugal is historically Britain's oldest ally, and with British support succeeded in having the re-incorporation of Olivenza decreed in Article CV of the General Treaty of the Final Act, which stated that "The Powers, recognizing the justice of the claims of ... Portugal and the Brazils, upon the town of Olivenza, and the other territories ceded to Spain by the Treaty of Badajoz of 1801". Portugal ratified the Final Act in 1815 but Spain would not sign, and this became the most important hold-out against the Congress of Vienna. Deciding in the end that it was better to become part of Europe than to stand alone, Spain finally accepted the Treaty on 7 May 1817; however, Olivenza and its surroundings were never returned to Portuguese control and this issue remains unresolved.The United Kingdom received parts of the West Indies at the expense of the Netherlands and Spain and kept the former Dutch colonies of Ceylon and the Cape Colony as well as Malta and Heligoland. Under the Treaty of Paris (1814) Article VIII France ceded to Britain the islands of "Tobago and Saint Lucia, and of the Isle of France and its dependencies, especially Rodrigues and Les Seychelles", and under the Treaty between Great Britain and Austria, Prussia and Russia, respecting the Ionian Islands (signed in Paris on 5 November 1815), as one of the treaties signed during the Peace of Paris (1815), Britain obtained a protectorate over the United States of the Ionian Islands.
The Congress of Vienna has frequently been criticized by 19th century and more recent historians for ignoring national and liberal impulses, and for imposing a stifling reaction on the Continent. It was an integral part in what became known as the Conservative Order, in which the democracy and civil rights associated with the American and French Revolutions were de-emphasized.In the 20th century, however, many historians came to admire the statesmen at the Congress, whose work prevented another widespread European war for nearly 100 years (1815–1914). Among these is Henry Kissinger, who in 1954 wrote his doctoral dissertation, A World Restored, on it. Historian Mark Jarrett argues that the Congress of Vienna and the Congress System marked "the true beginning of our modern era". He says the Congress System was deliberate conflict management, and was the first genuine attempt to create an international order based upon consensus rather than conflict. "Europe was ready," Jarrett states, "to accept an unprecedented degree of international cooperation in response to the French Revolution." Historian Paul Schroeder argues that the old formulae for "balance of power" were in fact highly destabilizing and predatory. He says the Congress of Vienna avoided them and instead set up rules that produced a stable and benign equilibrium. The Congress of Vienna was the first of a series of international meetings that came to be known as the Concert of Europe, which was an attempt to forge a peaceful balance of power in Europe. It served as a model for later organizations such as the League of Nations in 1919 and the United Nations in 1945. Before the opening of the Paris peace conference of 1918, the British Foreign Office commissioned a history of the Congress of Vienna to serve as an example to its own delegates of how to achieve an equally successful peace. Besides, the main decisions of the Congress were made by the Four Great Powers and not all the countries of Europe could extend their rights at the Congress. The Italian peninsula became a mere "geographical expression" as divided into seven parts: Lombardy–Venetia, Modena, Naples–Sicily, Parma, Piedmont–Sardinia, Tuscany, and the Papal States under the control of different powers. Poland remained partitioned between Russia, Prussia and Austria, with the largest part, the newly created Kingdom of Poland, remaining under Russian control. The arrangements made by the Four Great Powers sought to ensure future disputes would be settled in a manner that would avoid the terrible wars of the previous 20 years. Although the Congress of Vienna preserved the balance of power in Europe, it could not check the spread of revolutionary movements across the continent some 30 years later.
Diplomatic timeline for 1815 Concert of Europe European balance of power Battle of Waterloo International relations of the Great Powers (1814–1919) Treaty of Paris (1814) Paris Peace Conference, 1919
Animated map Europe and nations, 1815–1914 Final Act of the Congress of Vienna Map of Europe in 1815 Congress of Vienna (1814–1815) Search Results at Internet Archive
A congress is a formal meeting of the representatives of different countries, constituent states, organizations, trade unions, political parties or other groups. The term originated in Late Middle English to denote an encounter (meeting of adversaries) during battle, from the Latin congressus.In the mid-1770s, the term was chosen by the 13 British colonies for the Continental Congress to emphasize the status of each colony represented there as a self-governing entity. Subsequent to the use of congress as the name for the legislature of the U.S. federal government (beginning in 1789), the term has been adopted by many nations to refer to their national legislatures.
Countries with Congresses and presidential systems: The Congress of Guatemala (Spanish: Congreso de la República) is the unicameral legislature of Guatemala. The National Congress of Honduras (Spanish: Congreso nacional) is the legislative branch of the government of Honduras. The Congress of Mexico (Spanish: Congreso de la Unión) is the legislative branch of Mexican government. The Congress of Paraguay is the bicameral legislature of Paraguay. The Congress of the Argentine Nation (Spanish: Congreso de la Nación Argentina) is the legislative branch of the government of Argentina. The Congress of the Dominican Republic is the bicameral legislature of the Dominican Republic. The Palau National Congress (Palauan: Olbiil era Kelulau) is the bicameral legislative branch of the Republic of Palau. The Congress of the Federated States of Micronesia is the unicameral legislature of the Federated States of Micronesia. The Congress of the Philippines (Filipino: Kongreso ng Pilipinas) is the legislative branch of the Philippine government. The Congress of the Republic of Peru (Spanish: Congreso de la República) is the unicameral legislature of Peru. The Congress of Colombia (Spanish: Congreso de la República) is the bicameral legislature of Colombia. The United States Congress is the bicameral legislative branch of the United States federal government. The National Congress of Bolivia was the national legislature of Bolivia before being replaced by the Plurinational Legislative Assembly. The National Congress of Brazil (Portuguese: Congresso Nacional) is the bicameral legislature of Brazil. The National Congress of Chile (Spanish: Congreso Nacional) is the legislative branch of the government of Chile. The National Congress of Ecuador was the unicameral legislature of Ecuador before being replaced by the National Assembly. France: Although France has a Parliament, the term Congress is used on two circumstances: the Congress of the French Parliament, name used specifically when both houses sit together as a single body, usually at the Palace of Versailles, to vote on revisions to the Constitution, to listen to an address by the President of the French Republic, and, in the past, to elect the President of the Republic the Congress of New Caledonia, a territorial assembly
ICCA Congress & Exhibition
The Continental Congress (1774-1781) was a convention of delegates from the Thirteen Colonies that became the governing body of the United States during the American Revolution. The Congress of the Confederation (1781-1789) was the legislature of the United States under the Articles of Confederation. The National Congress of Belgium was a temporary legislative assembly in 1830, which created a constitution for the new state. The Confederate States Congress of 1861-1865, during the American Civil War.
In France, the Congress of France (congrès) denotes a formal and rarely convened joint session of both houses of Parliament to ratify an amendment to the Constitution or to listen to a speech by the President of the French Republic. Spanish Congress of Deputies (Spanish: Congreso de los Diputados), the lower house of the Cortes Generales, Spain's legislative branch. The legislature of the People's Republic of China is known in English as the National People's Congress. The Congress of People's Deputies of the Soviet Union was the legislature and nominal supreme institution of state power in the Soviet Union from 1989 to 1991. Congress of People's Deputies of Russia, a Russian institution modeled after USSR one, existed in 1990—1993.
Congress is included in the name of several political parties, especially those in former British colonies: Guyana People's National Congress India Indian National Congress All India Trinamool Congress Kerala Congress Nationalist Congress Party Tamil Maanila Congress YSR Congress BSR Congress All India N.R. Congress Lesotho Basotho Congress Party Lesotho Congress for Democracy Lesotho People's Congress Malawi Malawi Congress Party Malaysia Malaysian Indian Congress Namibia Congress of Democrats Pakistan Peoples Revolutionary Congress Pakistan Sudan National Congress (Sudan) Fiji National Congress of Fiji Canary Islands National Congress of the Canaries Nepal Nepali Congress Sierra Leone All People's Congress South Africa African National Congress Congress of the People Pan-Africanist Congress Sri Lanka All Ceylon Tamil Congress Sri Lanka Muslim Congress Swaziland Ngwane National Liberatory Congress Trinidad and Tobago United National Congress Uganda Ugandan People's Congress
Many political parties also have a party congress every few years to make decisions for the party and elect governing bodies. This is sometimes called a political convention.
National Congress of American Indians Iraqi National Congress Congress of Racial Equality Continental Congress 2.0
Congress of Industrial Organizations Trade Union Congress of the Philippines Trades Union Congress
Congress is an alternative name for a large national or international academic conference. For instance, the World Congress on Men's Health [1] is an annual meeting on men's medical issues.
Organizations in some athletic sports, such as bowling, have historically been named "congresses". The predecessors to the United States Bowling Congress, formed in 1995, were the male-only American Bowling Congress founded in 1895, and the female-only Women's International Bowling Congress founded in 1927, which combined in 1995 to form the USBC.
A Chess congress is a chess tournament, in one city, where a large number of contestants gather to play competitive chess over a limited period of time; typically one day to one week.
European affairs events International congress calendar Medical Congresses Around the World
The Nepali Congress (Nepali: नेपाली कांग्रेस Nepali pronunciation: [neˈpali ˈkaŋɡres]; NC) is a social-democratic political party in Nepal. It is the largest opposition party in the House of Representatives and the National Assembly.The party was formed in 1950 by the merger of Nepali National Congress and Nepal Democratic Congress. Nepali Congress prime ministers led four governments between the fall of the Rana dynasty and the start of the Panchayat era, including the first democratically elected government of Nepal in 1959. In the 2017 elections, NC emerged as the second largest party in the House of Representatives, winning 63 out of 275 seats.
In 1947, Bishweshwar Prasad Koirala, published an appeal for a unified struggle of Nepali people against the Rana regime. The same year, some Nepalese got together in Benaras and formed an organization by the name All Indian Nepali National Congress (Nepali: अखिल भारतीय नेपाली राष्ट्रिय कांग्रेस) where an ad-hoc committee was established. The initial officers were Devi Prasad Sapkota, chairman, Balchandra Sharma, vice-president, Krishna Prasad Bhattarai, general secretary, Gopal Prasad Bhattarai, publicity minister. Its Working Committee included Batuk Prasad Bhattarai, Narayan Prasad Bhattarai and Narendra Regmi. Its coordinator was Bishweshwar Prasad Koirala. Around the same time, Nepalese located in Calcutta formed another organization by the name All Indian Nepali Gorkha Congress (Nepali: अखिल भारतीय गोर्खा कांग्रेस) whose Chairman was Dharma Narayan Pradhan.
Koirala traveled extensively to places such as Benaras, Calcutta, Darjeeling, Assam, Bhaksu and Dehradhun and established contact with the Nepalese there. He met with Ganesh Man Singh during the same period. Nepalese representatives from different areas of Nepal and India organized one session in Calcutta. Koirala, Dilli Raman Regmi Dharma Narayan Pradhan and Dhan Man Singh Pariyar were present. In the same session, dropping Akhil Bharatiya from its name, the organization was named Nepali National Congress. Tanka Prasad Acharya, who was facing a life-sentence in Kathmandu, was made its chairman. The flag was square-shaped with white, blue and red colors in succession, with the moon and the sun in its center. The major four proposals passed by the session were: Assist the Indians in their Independence movement. Support Vietnam struggling for freedom against French colonization. Ask for the immediate release of imprisoned members of the Praja Parishad. Initiate a non-violence movement in Nepal for the establishment of an accountable ruling system.The organization's modus operandi was chosen. The organization attached itself to the civil conscience process in Nepal by establishing Tanka Prasad Acharya as its chairman.
The Nepali Congress Party was formed by the merger of Nepali National Congress and Nepal Democratic Congress. The Nepali National Congress was founded by Matrika Prasad Koirala in Calcutta, India on 25 January 1946. The Nepal Democratic Congress was founded by Subarna Shumsher Rana in Calcutta on 4 August 1948. The two parties merged on 10 April 1950 to form the Nepali Congress and Koirala became its first president. The party called for an armed revolution against the Rana regime. During the Bairgania Conference in Bairgania, Bihar, on 27 September 1950 the Nepali Congress announced an armed revolution against the Rana regime. The president of the party also announced the liquidation of operations in India and that the party would operate only inside Nepal.After King Tribhuvan took refuge inside the Indian Embassy on 6 November 1950. The Congress Liberation Army decided to take this opportunity to launch attacks against the regime before the King "left Nepalese soil". Matrika and Bisheshwor Prasad Koirala and Subarna Shamsher Rana flew to Purnia, Bihar. They called the commanders posted at different locations inside Nepal to prepare for armed strikes near the Nepal-India border.On 11 November 1950, at midnight Birgunj was attacked, and by 12 November it fell to the Nepali Congress and the first "People's Government" was declared. The liberation army was able to control most of the eastern hills of Nepal and the town of Tansen in Palpa. After pressure by the Indian government and the mass movement by the Nepali Congress and other political parties, the Rana government finally submitted to their demands and King Tribhuvan returned to the throne, replacing King Gyanendra, who had been crowned king after King Tribhuvan left for India.
After the fall of the Rana government, the Nepali Congress led three of the five governments formed before the elections. Matrika Prasad Koirala, the first commoner to become Prime Minister, led the government from 1951-1952 and 1953-1955 and Subarna Shamsher Rana led the government from 1958-1959. The much delayed elections were finally held in February 1959 and Bishweshwar Prasad Koirala became the first democratically elected Prime Minister of Nepal after the Nepali Congress won 74 of 109 Parliament seats.
Following a royal coup by King Mahendra in 1960, many leaders of the party, including Koirala, Rana and General Secretary Hora Prasad Joshi, were imprisoned or exiled; others took political refuge in India. Although political parties were prohibited from 1960 to 1989 and remained outlawed during the panchayat system under the aegis of the Associations and Organizations (Control) Act of 1963, the Nepali Congress persisted. The party placed great emphasis on eliminating the feudal economy and building a basis for socioeconomic development. It proposed nationalizing basic industries and instituting progressive taxes on land, urban housing, salaries, profits and foreign investments. While in exile, the Nepali Congress served as the nucleus around which other opposition groups clustered and instigated popular uprisings in the Hill and Terai regions. During this time, the Nepali Congress refused the overtures of a radical faction of the Communist Party of Nepal for a tactical alliance. Although the Nepali Congress demonstrated endurance, defection, factionalism and external pressures weakened it over time. Nevertheless, it continued to be the only organized party to press for democratization. In the 1980 referendum, it supported the multiparty system in opposition to the panchayat system. In 1981, the party boycotted the Rashtriya Panchayat elections and rejected the new government. The death in 1982 of Bishweshwar Prasad Koirala further weakened the party. Although the party boycotted the 1986 elections to the Rastriya Panchayat, its members were allowed to run in the 1987 local elections. In defiance of the demonstration ban, the Nepali Congress organized mass rallies with the communist factions in January 1990 that ultimately triggered the pro-democracy movement.
After the Jana Andolan I, party president Krishna Prasad Bhattarai was invited to form an interim coalition government. In the elections of 1991, the Nepali Congress won 110 of 205 seats but Bhattarai lost his seat and yielded the position of prime minister to Girija Prasad Koirala who held his seat until 1994.During the 1994 elections, the Nepali Congress lost its majority to Communist Party of Nepal (Unified Marxist–Leninist). The CPN (UML) lacked a majority and formed a minority government. After 46 parliamentarians from the CPN (UML) quit to form the Communist Party of Nepal (Marxist–Leninist), the Nepali Congress formed their own government with the Rastriya Prajatantra Party and Nepal Sadbhawana Party. After CPN (UML) offered Lokendra Bahadur Chand the position of prime minister, the Rastriya Prajatantra Party led a government with the CPN (UML). Internal problems within the Rastriya Prajatantra Party caused one faction led by Surya Bahadur Thapa to lead a government with Nepali Congress and Nepal Sadbhawana Party.Girija Prasad Koirala again became the Prime Minister in April 1998, leading a Congress minority government after Rastriya Prajatantra and Nepal Sadbhawana quit the government. Eventually, they got support from the CPN (ML) and after their withdrawal the CPN (UML) and Nepal Sadbhawana. During the 1999 elections, Girija Prasad Koirala stepped aside in favour of Krishna Prasad Bhattarai, who returned as Prime Minister when the Nepali Congress won 111 out of 205 House seats. Bhattarai resigned as prime minister on 16 March 2000 after conflicts between himself and supporters of Girija Prasad Koirala. In the party's first open leadership election, the parliamentarians selected Girija Prasad Koirala as their leader by 69-43 votes over Sher Bahadur Deuba. Accordingly, King Birendra designated Girija Prasad Koirala as prime minister on 20 March.On 8 August 2000, Koirala dismissed the Minister of Water Resources, Khum Bahadur Khadka, for calling for Koirala's resignation. Although Koirala beat back another challenge by Deuba's supporters at a party convention in January 2001, he resigned as Prime Minister on 19 July. Deuba then defeated Secretary General Sushil Koirala, 72–40, for the party leadership and was designated prime minister by the king.In May 2002, the party's disciplinary committee expelled Deuba for failing to consult the party before seeking a parliamentary extension of the country's state of emergency. Deuba's supporters then expelled Koirala at a general convention in June. Deuba registered his faction as the Nepali Congress (Democratic), following a decision by the Election Commission that the Koirala faction held ownership of the name Nepali Congress, taking 40 of the party's lower house representatives with him.
In the months following the King's October 2002 decisions to dissolve the House of Representatives and replace Prime Minister Deuba with Rastriya Prajatantra's Lokendra Bahadur Chand, the party joined the CPN (UML) and other, smaller parties in challenging the constitutionality of the moves. The party played a significant role in the formation of the Seven Party Alliance (SPA), which launched a series of street protests against the King's regression. The Seven Party Alliance had earlier avoided the Communist Party of Nepal (Maoist) (CPN-M and their violent methods, signed a 12-point understanding in Delhi in November 2005. The agreement contained three key commitments: the SPA endorsed CPN-M's fundamental demand for elections to a Constituent Assembly; the Maoists reciprocated with an assurance that they accepted a multi-party political system (SPA's prime concern); the SPA and the Maoists agreed to launch a peaceful mass movement against the monarchy.
On 26 April 2006, the king reinstated the dissolved parliament and formed a small government under the premiership of Girija Prasad Koirala, the president of the Nepali Congress. In November 2006, the government and the CPN-M signed a Comprehensive Peace Accord in India and the Nepalese Civil War formally ended.On 24 September 2007, the Nepali Congress (Democratic) and Nepali Congress unified as a single party with the Constituent Assembly elections looming. Girija Prasad Koirala remained president of the newly unified party. The party placed second—with 110 out of 575 elected seats—in the April 2008 Constituent Assembly election, winning only half as many seats as CPN-M.The party joined the coalition government headed by Madhav Kumar Nepal in May 2009. Girija Prasad Koirala angered some in the party by nominating his daughter Sujata Koirala to be Foreign Minister. In June, in a contested election for leader of the party's parliamentary group, Ram Chandra Poudel defeated Deuba. The 12th General Convention of the party was held in Kathmandu from 17–21 September 2010. The convention elected Sushil Koirala as the party president.After the Constituent Assembly of Nepal was dissolved by Prime Minister Baburam Bhattarai after failure to draft a new constitution before the deadline. In the resulting elections, the party emerged as the largest party winning 196 of the 575 elected seats. Along with CPN (UML), under the leadership of Sushil Koirala, they formed a new coalition government. The country's new constitution was promulgated under his leadership on 20 September 2015.
Sushil Koirala resigned as prime minister on 10 October 2015 after losing support from CPN (UML). Nepali Congress joined the government again on August 2016, after backing Pushpa Kamal Dahal to become prime minister. According to their agreement, Dahal resigned on 24 May 2017 paving the way for Deuba to become prime minister for a fourth time on 6 June 2017.On 22 April 2017, the Akhanda Nepal Party joined the Nepali Congress ahead of the 2017 Nepalese local elections. Nepali Congress won 11,456 seats including 266 mayoral or chairman positions. The party also won mayor posts in Lalitpur and Biratnagar. Ahead of the 2017 legislative and provincial elections, Nepal Loktantrik Forum led by former Nepali Congress leader, Bijay Kumar Gachhadar merged into the party. The party won 63 seats to the House of Representatives becoming the second largest party. The party could win only 23 seats under first past the post and many influential leaders including Ram Chandra Paudel, Ram Sharan Mahat, Bimalendra Nidhi, Krishna Prasad Sitaula and Arjun Narsingh KC lost in their constituencies. The party won 113 seats in provincial assemblies and became the largest opposition in six out of seven provinces. The party won 13 seats in the 2018 National Assembly election. After the National Assembly elections, Deuba resigned as prime minister on 15 February 2018, paving the way for a new government under CPN (UML). The party's under performance in the elections caused many elements inside the party to call for Deuba's resignation. Prakash Man Singh stood against Deuba for the election of the parliamentary party leader, but Deuba won the vote 44-19.
The party was founded on the principle of democracy and socialism. In 1956 the party adopted democratic socialism as its ideology for socio-economic transformation. Its foreign policy orientation was to nonalignment and good relations with India.
As of the 2017 provincial elections, Nepali Congress is the major opposition in all the provinces.
According to the website of Nepali Congress, the following are its sister organizations. Nepal Woman Association. Nepal Tarun Dal. Nepal Students' Union. Nepal Peasants' Union. Nepal Dalit Sangh. Nepal Ex-servicemen's Association. Nepal Prajatantra Senani Sangh. Indigenous Nationality Association of Nepal. Nepal Muslim Sangh. Nepal Tamang Sangh. Nepal National Magar Association. Nepal Thakur Society.
Krishna Prasad Bhattarai Congress Liberation Army Biratnagar Jute Mill Strike
Official website Info on the party from FES
A Member of Congress (MOC) is a person who has been appointed or elected and inducted into an official body called a congress, typically to represent a particular constituency in a legislature. Member of Parliament (MP) is an equivalent term in other, unaffiliated jurisdictions.
In referring to an individual lawmaker in their capacity of serving in the United States Congress, a bicameral legislature, the term Member of Congress is used less often than other terms in the United States. This is because in the United States the word Congress is used as a descriptive term for the collective body of legislators, from both houses of its bicameral federal legislature: the Senate and the House of Representatives. For this reason, and in order to distinguish who is a member of which house, a member of the Senate is typically referred to as Senator (followed by "name" from "state"), and a member of the House of Representatives is usually referred to as Congressman or Congresswoman (followed by "name" from the "number" district of "state"), or Representative ("name" from the "number" district of "state"). Although Senators are members of Congress, they are not normally referred to and addressed as "Congressmen" or "Congresswomen" or "Congresspeople". Members of Congress in both houses are elected by direct popular vote. Senators are elected via a statewide vote and representatives by voters in each congressional district. Congressional districts are apportioned to the states, once every ten years, based on population figures from the most recent nationwide census. Each of the 435 members of the House of Representatives is elected to serve a two-year term representing the people of their district. Each state, regardless of its size, has at least one congressman or congresswoman. Each of the 100 members of the Senate is elected to serve a six-year term representing the people of their state. Each state, regardless of its size, has two senators. Senatorial terms are staggered, so every two years approximately one-third of the Senate is up for election. Each staggered group of one-third of the senators is called a 'class'. No state has both its senators in the same class. One senator is a class above the other, or vice versa.
The United States Congress was created in Article I of the Constitution, where the Founding Fathers laid out the limitations and powers of Congress. Article I grants Congress legislative power and lists the enumerated powers and allows Congress to make laws that are necessary and proper to carry out the enumerated powers. It specifies the election and composition of the House of Representative no s and Senate and the qualifications necessary to serve in each chamber. The Seventeenth Amendment changed how senators were elected. Originally, senators were elected by state legislatures. The Seventeenth Amendment changed this to senators being elected directly by popular vote. Whether or not the federal government or any governmental entity has the right to regulate how many times a congressmen can hold office is a controversial debate.
The 116th United States Congress is the current meeting of the legislative branch of the United States federal government, composed of the Senate and the House of Representatives. It convened in Washington, D.C., on January 3, 2019, and will end on January 3, 2021, during the third and fourth years of the Presidency of Donald Trump. Senators elected to regular terms in 2014 are finishing their terms in this Congress, and House seats were apportioned based on the 2010 Census. In the November 2018 midterm elections, the Democratic Party won a new majority in the House, while the Republican Party increased its majority in the Senate. Consequently, this is the first split Congress since the 113th Congress of 2013–2015, and the first Republican Senate–Democratic House split since the 99th Congress of 1985–1987. This Congress is the youngest incoming class by mean age in the past three cycles and the most demographically diverse ever. Upon joining the Libertarian Party on May 1, 2020, Justin Amash became the first member of Congress to represent a political party other than the Democrats or the Republicans since Rep. William Carney, who served as a Conservative before switching to the Republican Party in 1985. Before joining the Libertarian party, Amash had been serving as an independent since his departure from the Republican party on July 4, 2019.
December 22, 2018 – January 25, 2019: 2018–2019 United States federal government shutdown February 5, 2019: 2019 State of the Union Address was delayed from January 29 due to the partial government shutdown. February 15, 2019: President Trump declared a National Emergency Concerning the Southern Border of the United States. February 27, 2019: Former Trump lawyer Michael Cohen testified before the House Oversight and Reform Committee. March 24, 2019: Special Counsel investigation (2017–2019): U.S. Attorney General William Barr issued a summary letter of special counsel Robert Mueller's report to congress on the investigation into Russian interference in the 2016 presidential election. July 24, 2019: Special Counsel investigation (2017–2019): Special counsel Robert Mueller testified before the House Judiciary and Intelligence committees. September 24, 2019: Impeachment of Donald Trump: House opened an Impeachment inquiry against Donald Trump after a whistleblower alleged the President abused his power in a phone call with the President of Ukraine. December 13, 2019: Impeachment of Donald Trump: House Judiciary Committee approved two impeachment articles. December 18, 2019: Impeachment of Donald Trump: House impeached President Trump. January 16, 2020 – February 5, 2020: Impeachment of Donald Trump: Impeachment trial of Donald Trump February 4, 2020: 2020 State of the Union Address March 11, 2020 – present: COVID-19 pandemic in the United States March 19, 2020 – present: 2020 Congressional insider trading scandal May 26, 2020 – present: Nationwide George Floyd protests August 18, 2020 – present: 2020 United States Postal Service crisis September 30, 2020 – present: White House COVID-19 outbreak October 26, 2020: The Senate confirmed Amy Coney Barrett to the United States Supreme Court.
Resignations and new members are discussed in the "Changes in membership" section below.
Majority Leader: Mitch McConnell Majority Whip: John Thune Republican Conference Chairman: John Barrasso Republican Conference Vice Chairman: Joni Ernst Policy Committee Chairman: Roy Blunt Republican Campaign Committee Chairman: Todd Young Steering Committee Chairman: Mike Lee Chief Deputy Whip: Mike Crapo Deputy Whips: Roy Blunt, Shelley Moore Capito, John Cornyn, Cory Gardner, James Lankford, Martha McSally, Rob Portman, Mitt Romney, Tim Scott, Thom Tillis, Todd Young
Minority Leader/Caucus Chair: Chuck Schumer Minority Whip: Dick Durbin Assistant Leader: Patty Murray Policy Committee Chairwoman: Debbie Stabenow Democratic Caucus Vice Chairs: Mark Warner, Elizabeth Warren Steering Committee Chairwoman: Amy Klobuchar Outreach Chair: Bernie Sanders Policy Committee Vice Chairman: Joe Manchin Democratic Caucus Secretary: Tammy Baldwin Democratic Campaign Committee Chair: Catherine Cortez Masto Chief Deputy Whip: Cory Booker, Jeff Merkley, Brian Schatz
Majority Leader: Steny Hoyer Majority Whip: Jim Clyburn Assistant Leader: Ben Ray Luján Democratic Caucus Chairman: Hakeem Jeffries Democratic Caucus Vice Chairman: Katherine Clark Democratic Campaign Committee Chairwoman: Cheri Bustos Policy and Communications Committee Chairman: David Cicilline Policy and Communications Committee Co-Chairs: Matt Cartwright, Debbie Dingell, Ted Lieu Steering and Policy Committee Co-Chairs: Rosa DeLauro, Barbara Lee, Eric Swalwell Assistant to the Majority Whip: Cedric Richmond Senior Chief Deputy Whips: Jan Schakowsky Chief Deputy Whips: Pete Aguilar, G. K. Butterfield, Henry Cuellar, Dan Kildee, Sheila Jackson Lee, Debbie Wasserman Schultz, Terri Sewell, Peter Welch
Minority Leader: Kevin McCarthy Minority Whip: Steve Scalise Republican Conference Chairwoman: Liz Cheney Republican Conference Vice-Chairman: Mark Walker Republican Conference Secretary: Jason Smith Policy Committee Chairman: Gary Palmer Republican Campaign Committee Chairman: Tom Emmer Chief Deputy Whip: Drew Ferguson
Most members of this Congress are Christian (88.2%), with approximately half being Protestant and 30.5% being Catholic. Jewish membership is 6.4%. Other religions represented include Buddhism, Islam, and Hinduism. One senator says that she is religiously unaffiliated, while the number of members refusing to specify their religious affiliation increased.
There are 101 women in the House, the largest number in history. There are 313 non-Hispanic whites, 56 black, 44 Hispanic, 15 Asian, and 4 American Indian. Seven identify as LGBTQ+. Two Democrats — Alexandria Ocasio-Cortez and Donna Shalala — are the youngest (30) and oldest (78) freshmen women in history. Freshmen Rashida Tlaib (D-MI) and Ilhan Omar (DFL-MN) are the first two Muslim women and freshmen Sharice Davids (D-KS) and Deb Haaland (D-NM) are the first two Native American women elected as well.With the election of Carolyn Maloney as the first woman to chair the House Oversight Committee, women now chair a record six House committees in a single Congress (out of 26 women to ever chair House committees in the history of Congress), including representatives Maxine Waters (Financial Services), Nita Lowey (Appropriations), Zoe Lofgren (Administration), Eddie Bernice Johnson (Science, Space and Technology) and Nydia Velázquez (Small Business), as well as Kathy Castor who chairs the Select Committee on the Climate Crisis. In addition, women chair a record 39 House subcommittees. Lowey and Kay Granger are also the first women to serve as chair and ranking member of the same committee in the same Congress since the defunct Select Committee on the House Beauty Shop, which was chaired and populated entirely by congresswomen during its existence from 1967 to 1977.
The demographics of the 116th U.S. Congress freshmen were more diverse than any previous incoming class.At least 25 new congressional representatives were Hispanic, Native American, or persons of color, and the incoming class included the first Native American women, the first Muslim women, and the two youngest women ever elected. The 116th congress included more women elected to the House than any previous congress.
The numbers refer to their Senate classes. All class 1 seats were contested in the November 2018 elections. In this Congress, class 1 means their term commenced in the current Congress, requiring re-election in 2024; class 2 means their term ends with this Congress, requiring re-election in 2020; and class 3 means their term began in the last Congress, requiring re-election in 2022.
Section contents: Senate, House, Joint Listed by chamber and then alphabetically by committee name, including chair and ranking member.
Also called "elected" or "appointed" officials, there are many employees of the House and Senate whose leaders are included here.
Chaplain: Barry C. Black (Seventh-day Adventist) Historian: Betty Koed Parliamentarian: Elizabeth MacDonough Secretary: Julie E. Adams Sergeant at Arms: Michael C. Stenger Secretary for the Majority: until February 2020: Laura Dove starting February 2020: Robert Duncan Secretary for the Minority: Gary B. Myrick
Chaplain: Patrick J. Conroy (Roman Catholic) Chief Administrative Officer: Phil Kiko Clerk: until February 26, 2019: Karen L. Haas starting February 26, 2019: Cheryl L. Johnson Historian: Matthew Wasniewski Inspector General: Michael Ptasienski Parliamentarian: until September 30, 2020: Thomas J. Wickham Jr. starting September 30, 2020: Jason A. Smith Reading Clerks: Susan Cole and Joseph Novotny Sergeant at Arms: Paul D. Irving
2018 United States elections (elections leading to this Congress) 2018 United States Senate elections 2018 United States House of Representatives elections 2019 United States elections (elections during this Congress) 2019 United States House of Representatives elections 2020 United States elections (elections during this Congress, leading to the next Congress) 2020 United States presidential election 2020 United States Senate elections 2020 United States House of Representatives elections 2010s in United States political history
List of freshman class members of the 116th United States Congress
Official website, via Congress.gov Videos of House of Representatives Sessions for the 116th Congress from C-SPAN Videos of Senate Sessions for the 116th Congress from C-SPAN Videos of Committees from the House and Senate for the 116th Congress C-SPAN Congressional Pictorial Directory for the 116th Congress
This list of members of the United States Congress by wealth includes only the forty richest current members of Congress and displays the difference between assets and liabilities for the member and his or her immediate family, such as a spouse or dependent children. These figures can never be entirely accurate, because the financial disclosure requirements for the United States Congress are approximate by design. The original documents for each member's disclosure are publicly available on a database website, maintained by the Center for Responsive Politics. For 2018, the median net worth of members of Congress was $511,000.
List of richest American politicians List of current members of the Congress of the Philippines by wealth
A virus is a submicroscopic infectious agent that replicates only inside the living cells of an organism. Viruses infect all types of life forms, from animals and plants to microorganisms, including bacteria and archaea. Since Dmitri Ivanovsky's 1892 article describing a non-bacterial pathogen infecting tobacco plants and the discovery of the tobacco mosaic virus by Martinus Beijerinck in 1898, more than 6,000 virus species have been described in detail of the millions of types of viruses in the environment. Viruses are found in almost every ecosystem on Earth and are the most numerous type of biological entity. The study of viruses is known as virology, a subspeciality of microbiology. When infected, a host cell is forced to rapidly produce thousands of identical copies of the original virus. When not inside an infected cell or in the process of infecting a cell, viruses exist in the form of independent particles, or virions, consisting of: (i) the genetic material, i.e., long molecules of DNA or RNA that encode the structure of the proteins by which the virus acts; (ii) a protein coat, the capsid, which surrounds and protects the genetic material; and in some cases (iii) an outside envelope of lipids. The shapes of these virus particles range from simple helical and icosahedral forms to more complex structures. Most virus species have virions too small to be seen with an optical microscope, as they are one-hundredth the size of most bacteria. The origins of viruses in the evolutionary history of life are unclear: some may have evolved from plasmids—pieces of DNA that can move between cells—while others may have evolved from bacteria. In evolution, viruses are an important means of horizontal gene transfer, which increases genetic diversity in a way analogous to sexual reproduction. Viruses are considered by some biologists to be a life form, because they carry genetic material, reproduce, and evolve through natural selection, although they lack the key characteristics, such as cell structure, that are generally considered necessary criteria for life. Because they possess some but not all such qualities, viruses have been described as "organisms at the edge of life", and as self-replicators.Viruses spread in many ways. One transmission pathway is through disease-bearing organisms known as vectors: for example, viruses are often transmitted from plant to plant by insects that feed on plant sap, such as aphids; and viruses in animals can be carried by blood-sucking insects. Influenza viruses are spread by coughing and sneezing. Norovirus and rotavirus, common causes of viral gastroenteritis, are transmitted by the faecal–oral route, passed by hand-to-mouth contact or in food or water. The infectious dose of norovirus required to produce infection in humans is less than 100 particles. HIV is one of several viruses transmitted through sexual contact and by exposure to infected blood. The variety of host cells that a virus can infect is called its "host range". This can be narrow, meaning a virus is capable of infecting few species, or broad, meaning it is capable of infecting many.Viral infections in animals provoke an immune response that usually eliminates the infecting virus. Immune responses can also be produced by vaccines, which confer an artificially acquired immunity to the specific viral infection. Some viruses, including those that cause AIDS, HPV infection, and viral hepatitis, evade these immune responses and result in chronic infections. Several antiviral drugs have been developed.
Louis Pasteur was unable to find a causative agent for rabies and speculated about a pathogen too small to be detected by microscopes. In 1884, the French microbiologist Charles Chamberland invented the Chamberland filter (or Pasteur-Chamberland filter) with pores small enough to remove all bacteria from a solution passed through it. In 1892, the Russian biologist Dmitri Ivanovsky used this filter to study what is now known as the tobacco mosaic virus: crushed leaf extracts from infected tobacco plants remained infectious even after filtration to remove bacteria. Ivanovsky suggested the infection might be caused by a toxin produced by bacteria, but he did not pursue the idea. At the time it was thought that all infectious agents could be retained by filters and grown on a nutrient medium—this was part of the germ theory of disease. In 1898, the Dutch microbiologist Martinus Beijerinck repeated the experiments and became convinced that the filtered solution contained a new form of infectious agent. He observed that the agent multiplied only in cells that were dividing, but as his experiments did not show that it was made of particles, he called it a contagium vivum fluidum (soluble living germ) and reintroduced the word virus. Beijerinck maintained that viruses were liquid in nature, a theory later discredited by Wendell Stanley, who proved they were particulate. In the same year, Friedrich Loeffler and Paul Frosch passed the first animal virus, aphthovirus (the agent of foot-and-mouth disease), through a similar filter.In the early 20th century, the English bacteriologist Frederick Twort discovered a group of viruses that infect bacteria, now called bacteriophages (or commonly 'phages'), and the French-Canadian microbiologist Félix d'Herelle described viruses that, when added to bacteria on an agar plate, would produce areas of dead bacteria. He accurately diluted a suspension of these viruses and discovered that the highest dilutions (lowest virus concentrations), rather than killing all the bacteria, formed discrete areas of dead organisms. Counting these areas and multiplying by the dilution factor allowed him to calculate the number of viruses in the original suspension. Phages were heralded as a potential treatment for diseases such as typhoid and cholera, but their promise was forgotten with the development of penicillin. The development of bacterial resistance to antibiotics has renewed interest in the therapeutic use of bacteriophages.By the end of the 19th century, viruses were defined in terms of their infectivity, their ability to pass filters, and their requirement for living hosts. Viruses had been grown only in plants and animals. In 1906 Ross Granville Harrison invented a method for growing tissue in lymph, and in 1913 E. Steinhardt, C. Israeli, and R.A. Lambert used this method to grow vaccinia virus in fragments of guinea pig corneal tissue. In 1928, H. B. Maitland and M. C. Maitland grew vaccinia virus in suspensions of minced hens' kidneys. Their method was not widely adopted until the 1950s when poliovirus was grown on a large scale for vaccine production.Another breakthrough came in 1931 when the American pathologist Ernest William Goodpasture and Alice Miles Woodruff grew influenza and several other viruses in fertilised chicken eggs. In 1949, John Franklin Enders, Thomas Weller, and Frederick Robbins grew poliovirus in cultured cells from aborted human embryonic tissue, the first virus to be grown without using solid animal tissue or eggs. This work enabled Hilary Koprowski, and then Jonas Salk, to make an effective polio vaccine.The first images of viruses were obtained upon the invention of electron microscopy in 1931 by the German engineers Ernst Ruska and Max Knoll. In 1935, American biochemist and virologist Wendell Meredith Stanley examined the tobacco mosaic virus and found it was mostly made of protein. A short time later, this virus was separated into protein and RNA parts. The tobacco mosaic virus was the first to be crystallised and its structure could, therefore, be elucidated in detail. The first X-ray diffraction pictures of the crystallised virus were obtained by Bernal and Fankuchen in 1941. On the basis of her X-ray crystallographic pictures, Rosalind Franklin discovered the full structure of the virus in 1955. In the same year, Heinz Fraenkel-Conrat and Robley Williams showed that purified tobacco mosaic virus RNA and its protein coat can assemble by themselves to form functional viruses, suggesting that this simple mechanism was probably the means through which viruses were created within their host cells.The second half of the 20th century was the golden age of virus discovery, and most of the documented species of animal, plant, and bacterial viruses were discovered during these years. In 1957 equine arterivirus and the cause of Bovine virus diarrhoea (a pestivirus) were discovered. In 1963 the hepatitis B virus was discovered by Baruch Blumberg, and in 1965 Howard Temin described the first retrovirus. Reverse transcriptase, the enzyme that retroviruses use to make DNA copies of their RNA, was first described in 1970 by Temin and David Baltimore independently. In 1983 Luc Montagnier's team at the Pasteur Institute in France, first isolated the retrovirus now called HIV. In 1989 Michael Houghton's team at Chiron Corporation discovered Hepatitis C.
Viruses are found wherever there is life and have probably existed since living cells first evolved. The origin of viruses is unclear because they do not form fossils, so molecular techniques are used to investigate how they arose. In addition, viral genetic material occasionally integrates into the germline of the host organisms, by which they can be passed on vertically to the offspring of the host for many generations. This provides an invaluable source of information for paleovirologists to trace back ancient viruses that have existed up to millions of years ago. There are three main hypotheses that aim to explain the origins of viruses: Regressive hypothesis Viruses may have once been small cells that parasitised larger cells. Over time, genes not required by their parasitism were lost. The bacteria rickettsia and chlamydia are living cells that, like viruses, can reproduce only inside host cells. They lend support to this hypothesis, as their dependence on parasitism is likely to have caused the loss of genes that enabled them to survive outside a cell. This is also called the 'degeneracy hypothesis', or 'reduction hypothesis'. Cellular origin hypothesis Some viruses may have evolved from bits of DNA or RNA that "escaped" from the genes of a larger organism. The escaped DNA could have come from plasmids (pieces of naked DNA that can move between cells) or transposons (molecules of DNA that replicate and move around to different positions within the genes of the cell). Once called "jumping genes", transposons are examples of mobile genetic elements and could be the origin of some viruses. They were discovered in maize by Barbara McClintock in 1950. This is sometimes called the 'vagrancy hypothesis', or the 'escape hypothesis'.Co-evolution hypothesis This is also called the 'virus-first hypothesis' and proposes that viruses may have evolved from complex molecules of protein and nucleic acid at the same time that cells first appeared on Earth and would have been dependent on cellular life for billions of years. Viroids are molecules of RNA that are not classified as viruses because they lack a protein coat. They have characteristics that are common to several viruses and are often called subviral agents. Viroids are important pathogens of plants. They do not code for proteins but interact with the host cell and use the host machinery for their replication. The hepatitis delta virus of humans has an RNA genome similar to viroids but has a protein coat derived from hepatitis B virus and cannot produce one of its own. It is, therefore, a defective virus. Although hepatitis delta virus genome may replicate independently once inside a host cell, it requires the help of hepatitis B virus to provide a protein coat so that it can be transmitted to new cells. In similar manner, the sputnik virophage is dependent on mimivirus, which infects the protozoan Acanthamoeba castellanii. These viruses, which are dependent on the presence of other virus species in the host cell, are called 'satellites' and may represent evolutionary intermediates of viroids and viruses.In the past, there were problems with all of these hypotheses: the regressive hypothesis did not explain why even the smallest of cellular parasites do not resemble viruses in any way. The escape hypothesis did not explain the complex capsids and other structures on virus particles. The virus-first hypothesis contravened the definition of viruses in that they require host cells. Viruses are now recognised as ancient and as having origins that pre-date the divergence of life into the three domains. This discovery has led modern virologists to reconsider and re-evaluate these three classical hypotheses.The evidence for an ancestral world of RNA cells and computer analysis of viral and host DNA sequences are giving a better understanding of the evolutionary relationships between different viruses and may help identify the ancestors of modern viruses. To date, such analyses have not proved which of these hypotheses is correct. It seems unlikely that all currently known viruses have a common ancestor, and viruses have probably arisen numerous times in the past by one or more mechanisms.
Scientific opinions differ on whether viruses are a form of life or organic structures that interact with living organisms. They have been described as "organisms at the edge of life", since they resemble organisms in that they possess genes, evolve by natural selection, and reproduce by creating multiple copies of themselves through self-assembly. Although they have genes, they do not have a cellular structure, which is often seen as the basic unit of life. Viruses do not have their own metabolism and require a host cell to make new products. They therefore cannot naturally reproduce outside a host cell—although bacterial species such as rickettsia and chlamydia are considered living organisms despite the same limitation. Accepted forms of life use cell division to reproduce, whereas viruses spontaneously assemble within cells. They differ from autonomous growth of crystals as they inherit genetic mutations while being subject to natural selection. Virus self-assembly within host cells has implications for the study of the origin of life, as it lends further credence to the hypothesis that life could have started as self-assembling organic molecules.
Viruses display a wide diversity of shapes and sizes, called 'morphologies'. In general, viruses are much smaller than bacteria. Most viruses that have been studied have a diameter between 20 and 300 nanometres. Some filoviruses have a total length of up to 1400 nm; their diameters are only about 80 nm. Most viruses cannot be seen with an optical microscope, so scanning and transmission electron microscopes are used to visualise them. To increase the contrast between viruses and the background, electron-dense "stains" are used. These are solutions of salts of heavy metals, such as tungsten, that scatter the electrons from regions covered with the stain. When virions are coated with stain (positive staining), fine detail is obscured. Negative staining overcomes this problem by staining the background only.A complete virus particle, known as a virion, consists of nucleic acid surrounded by a protective coat of protein called a capsid. These are formed from identical protein subunits called capsomeres. Viruses can have a lipid "envelope" derived from the host cell membrane. The capsid is made from proteins encoded by the viral genome and its shape serves as the basis for morphological distinction. Virally-coded protein subunits will self-assemble to form a capsid, in general requiring the presence of the virus genome. Complex viruses code for proteins that assist in the construction of their capsid. Proteins associated with nucleic acid are known as nucleoproteins, and the association of viral capsid proteins with viral nucleic acid is called a nucleocapsid. The capsid and entire virus structure can be mechanically (physically) probed through atomic force microscopy. In general, there are four main morphological virus types: Helical These viruses are composed of a single type of capsomere stacked around a central axis to form a helical structure, which may have a central cavity, or tube. This arrangement results in rod-shaped or filamentous virions which can be short and highly rigid, or long and very flexible. The genetic material (typically single-stranded RNA, but ssDNA in some cases) is bound into the protein helix by interactions between the negatively charged nucleic acid and positive charges on the protein. Overall, the length of a helical capsid is related to the length of the nucleic acid contained within it, and the diameter is dependent on the size and arrangement of capsomeres. The well-studied tobacco mosaic virus is an example of a helical virus.Icosahedral Most animal viruses are icosahedral or near-spherical with chiral icosahedral symmetry. A regular icosahedron is the optimum way of forming a closed shell from identical sub-units. The minimum number of identical capsomeres required for each triangular face is 3, which gives 60 for the icosahedron. Many viruses, such as rotavirus, have more than 60 capsomers and appear spherical but they retain this symmetry. To achieve this, the capsomeres at the apices are surrounded by five other capsomeres and are called pentons. Capsomeres on the triangular faces are surrounded by six others and are called hexons. Hexons are in essence flat and pentons, which form the 12 vertices, are curved. The same protein may act as the subunit of both the pentamers and hexamers or they may be composed of different proteins.Prolate This is an icosahedron elongated along the fivefold axis and is a common arrangement of the heads of bacteriophages. This structure is composed of a cylinder with a cap at either end.Enveloped Some species of virus envelop themselves in a modified form of one of the cell membranes, either the outer membrane surrounding an infected host cell or internal membranes such as nuclear membrane or endoplasmic reticulum, thus gaining an outer lipid bilayer known as a viral envelope. This membrane is studded with proteins coded for by the viral genome and host genome; the lipid membrane itself and any carbohydrates present originate entirely from the host. Influenza virus, HIV (which causes AIDS), and severe acute respiratory syndrome coronavirus 2 (which causes COVID-19) use this strategy. Most enveloped viruses are dependent on the envelope for their infectivity.Complex These viruses possess a capsid that is neither purely helical nor purely icosahedral, and that may possess extra structures such as protein tails or a complex outer wall. Some bacteriophages, such as Enterobacteria phage T4, have a complex structure consisting of an icosahedral head bound to a helical tail, which may have a hexagonal base plate with protruding protein tail fibres. This tail structure acts like a molecular syringe, attaching to the bacterial host and then injecting the viral genome into the cell.The poxviruses are large, complex viruses that have an unusual morphology. The viral genome is associated with proteins within a central disc structure known as a nucleoid. The nucleoid is surrounded by a membrane and two lateral bodies of unknown function. The virus has an outer envelope with a thick layer of protein studded over its surface. The whole virion is slightly pleomorphic, ranging from ovoid to brick-shaped.
Mimivirus is one of the largest characterised viruses, with a capsid diameter of 400 nm. Protein filaments measuring 100 nm project from the surface. The capsid appears hexagonal under an electron microscope, therefore the capsid is probably icosahedral. In 2011, researchers discovered the largest then known virus in samples of water collected from the ocean floor off the coast of Las Cruces, Chile. Provisionally named Megavirus chilensis, it can be seen with a basic optical microscope. In 2013, the Pandoravirus genus was discovered in Chile and Australia, and has genomes about twice as large as Megavirus and Mimivirus. All giant viruses have dsDNA genomes and they are classified into several families: Mimiviridae, Pithoviridae, Pandoraviridae, Phycodnaviridae, and the Mollivirus genus.Some viruses that infect Archaea have complex structures unrelated to any other form of virus, with a wide variety of unusual shapes, ranging from spindle-shaped structures to viruses that resemble hooked rods, teardrops or even bottles. Other archaeal viruses resemble the tailed bacteriophages, and can have multiple tail structures.
An enormous variety of genomic structures can be seen among viral species; as a group, they contain more structural genomic diversity than plants, animals, archaea, or bacteria. There are millions of different types of viruses, although fewer than 7,000 types have been described in detail. As of September 2015, the NCBI Virus genome database has more than 75,000 complete genome sequences, but there are doubtlessly many more to be discovered.A virus has either a DNA or an RNA genome and is called a DNA virus or an RNA virus, respectively. The vast majority of viruses have RNA genomes. Plant viruses tend to have single-stranded RNA genomes and bacteriophages tend to have double-stranded DNA genomes.Viral genomes are circular, as in the polyomaviruses, or linear, as in the adenoviruses. The type of nucleic acid is irrelevant to the shape of the genome. Among RNA viruses and certain DNA viruses, the genome is often divided up into separate parts, in which case it is called segmented. For RNA viruses, each segment often codes for only one protein and they are usually found together in one capsid. All segments are not required to be in the same virion for the virus to be infectious, as demonstrated by brome mosaic virus and several other plant viruses.A viral genome, irrespective of nucleic acid type, is almost always either single-stranded or double-stranded. Single-stranded genomes consist of an unpaired nucleic acid, analogous to one-half of a ladder split down the middle. Double-stranded genomes consist of two complementary paired nucleic acids, analogous to a ladder. The virus particles of some virus families, such as those belonging to the Hepadnaviridae, contain a genome that is partially double-stranded and partially single-stranded.For most viruses with RNA genomes and some with single-stranded DNA genomes, the single strands are said to be either positive-sense (called the 'plus-strand') or negative-sense (called the 'minus-strand'), depending on if they are complementary to the viral messenger RNA (mRNA). Positive-sense viral RNA is in the same sense as viral mRNA and thus at least a part of it can be immediately translated by the host cell. Negative-sense viral RNA is complementary to mRNA and thus must be converted to positive-sense RNA by an RNA-dependent RNA polymerase before translation. DNA nomenclature for viruses with single-sense genomic ssDNA is similar to RNA nomenclature, in that positive-strand viral ssDNA is identical in sequence to the viral mRNA and is thus a coding strand, while negative-strand viral ssDNA is complementary to the viral mRNA and is thus a template strand. Several types of ssDNA and ssRNA viruses have genomes that are ambisense in that transcription can occur off both strands in a double-stranded replicative intermediate. Examples include geminiviruses, which are ssDNA plant viruses and arenaviruses, which are ssRNA viruses of animals.
Genome size varies greatly between species. The smallest—the ssDNA circoviruses, family Circoviridae—code for only two proteins and have a genome size of only two kilobases; the largest—the pandoraviruses—have genome sizes of around two megabases which code for about 2500 proteins. Virus genes rarely have introns and often are arranged in the genome so that they overlap.In general, RNA viruses have smaller genome sizes than DNA viruses because of a higher error-rate when replicating, and have a maximum upper size limit. Beyond this, errors when replicating render the virus useless or uncompetitive. To compensate, RNA viruses often have segmented genomes—the genome is split into smaller molecules—thus reducing the chance that an error in a single-component genome will incapacitate the entire genome. In contrast, DNA viruses generally have larger genomes because of the high fidelity of their replication enzymes. Single-strand DNA viruses are an exception to this rule, as mutation rates for these genomes can approach the extreme of the ssRNA virus case.
Viruses undergo genetic change by several mechanisms. These include a process called antigenic drift where individual bases in the DNA or RNA mutate to other bases. Most of these point mutations are "silent"—they do not change the protein that the gene encodes—but others can confer evolutionary advantages such as resistance to antiviral drugs. Antigenic shift occurs when there is a major change in the genome of the virus. This can be a result of recombination or reassortment. When this happens with influenza viruses, pandemics might result. RNA viruses often exist as quasispecies or swarms of viruses of the same species but with slightly different genome nucleoside sequences. Such quasispecies are a prime target for natural selection.Segmented genomes confer evolutionary advantages; different strains of a virus with a segmented genome can shuffle and combine genes and produce progeny viruses (or offspring) that have unique characteristics. This is called reassortment or 'viral sex'.Genetic recombination is the process by which a strand of DNA is broken and then joined to the end of a different DNA molecule. This can occur when viruses infect cells simultaneously and studies of viral evolution have shown that recombination has been rampant in the species studied. Recombination is common to both RNA and DNA viruses.
Viral populations do not grow through cell division, because they are acellular. Instead, they use the machinery and metabolism of a host cell to produce multiple copies of themselves, and they assemble in the cell. When infected, the host cell is forced to rapidly produce thousands of identical copies of the original virus.Their life cycle differs greatly between species, but there are six basic stages in their life cycle:Attachment is a specific binding between viral capsid proteins and specific receptors on the host cellular surface. This specificity determines the host range and type of host cell of a virus. For example, HIV infects a limited range of human leucocytes. This is because its surface protein, gp120, specifically interacts with the CD4 molecule—a chemokine receptor—which is most commonly found on the surface of CD4+ T-Cells. This mechanism has evolved to favour those viruses that infect only cells in which they are capable of replication. Attachment to the receptor can induce the viral envelope protein to undergo changes that result in the fusion of viral and cellular membranes, or changes of non-enveloped virus surface proteins that allow the virus to enter.Penetration or viral entry follows attachment: Virions enter the host cell through receptor-mediated endocytosis or membrane fusion. The infection of plant and fungal cells is different from that of animal cells. Plants have a rigid cell wall made of cellulose, and fungi one of chitin, so most viruses can get inside these cells only after trauma to the cell wall. Nearly all plant viruses (such as tobacco mosaic virus) can also move directly from cell to cell, in the form of single-stranded nucleoprotein complexes, through pores called plasmodesmata. Bacteria, like plants, have strong cell walls that a virus must breach to infect the cell. Given that bacterial cell walls are much thinner than plant cell walls due to their much smaller size, some viruses have evolved mechanisms that inject their genome into the bacterial cell across the cell wall, while the viral capsid remains outside.Uncoating is a process in which the viral capsid is removed: This may be by degradation by viral enzymes or host enzymes or by simple dissociation; the end-result is the releasing of the viral genomic nucleic acid.Replication of viruses involves primarily multiplication of the genome. Replication involves synthesis of viral messenger RNA (mRNA) from "early" genes (with exceptions for positive sense RNA viruses), viral protein synthesis, possible assembly of viral proteins, then viral genome replication mediated by early or regulatory protein expression. This may be followed, for complex viruses with larger genomes, by one or more further rounds of mRNA synthesis: "late" gene expression is, in general, of structural or virion proteins.Assembly – Following the structure-mediated self-assembly of the virus particles, some modification of the proteins often occurs. In viruses such as HIV, this modification (sometimes called maturation) occurs after the virus has been released from the host cell.Release – Viruses can be released from the host cell by lysis, a process that kills the cell by bursting its membrane and cell wall if present: this is a feature of many bacterial and some animal viruses. Some viruses undergo a lysogenic cycle where the viral genome is incorporated by genetic recombination into a specific place in the host's chromosome. The viral genome is then known as a "provirus" or, in the case of bacteriophages a "prophage". Whenever the host divides, the viral genome is also replicated. The viral genome is mostly silent within the host. At some point, the provirus or prophage may give rise to active virus, which may lyse the host cells. Enveloped viruses (e.g., HIV) typically are released from the host cell by budding. During this process the virus acquires its envelope, which is a modified piece of the host's plasma or other, internal membrane.
The genetic material within virus particles, and the method by which the material is replicated, varies considerably between different types of viruses. DNA viruses The genome replication of most DNA viruses takes place in the cell's nucleus. If the cell has the appropriate receptor on its surface, these viruses enter the cell either by direct fusion with the cell membrane (e.g., herpesviruses) or—more usually—by receptor-mediated endocytosis. Most DNA viruses are entirely dependent on the host cell's DNA and RNA synthesising machinery, and RNA processing machinery. Viruses with larger genomes may encode much of this machinery themselves. In eukaryotes the viral genome must cross the cell's nuclear membrane to access this machinery, while in bacteria it need only enter the cell.RNA viruses Replication of RNA viruses usually takes place in the cytoplasm. RNA viruses can be placed into four different groups depending on their modes of replication. The polarity (whether or not it can be used directly by ribosomes to make proteins) of single-stranded RNA viruses largely determines the replicative mechanism; the other major criterion is whether the genetic material is single-stranded or double-stranded. All RNA viruses use their own RNA replicase enzymes to create copies of their genomes.Reverse transcribing viruses Reverse transcribing viruses have ssRNA (Retroviridae, Metaviridae, Pseudoviridae) or dsDNA (Caulimoviridae, and Hepadnaviridae) in their particles. Reverse transcribing viruses with RNA genomes (retroviruses) use a DNA intermediate to replicate, whereas those with DNA genomes (pararetroviruses) use an RNA intermediate during genome replication. Both types use a reverse transcriptase, or RNA-dependent DNA polymerase enzyme, to carry out the nucleic acid conversion. Retroviruses integrate the DNA produced by reverse transcription into the host genome as a provirus as a part of the replication process; pararetroviruses do not, although integrated genome copies of especially plant pararetroviruses can give rise to infectious virus. They are susceptible to antiviral drugs that inhibit the reverse transcriptase enzyme, e.g. zidovudine and lamivudine. An example of the first type is HIV, which is a retrovirus. Examples of the second type are the Hepadnaviridae, which includes Hepatitis B virus.
The range of structural and biochemical effects that viruses have on the host cell is extensive. These are called 'cytopathic effects'. Most virus infections eventually result in the death of the host cell. The causes of death include cell lysis, alterations to the cell's surface membrane and apoptosis. Often cell death is caused by cessation of its normal activities because of suppression by virus-specific proteins, not all of which are components of the virus particle. The distinction between cytopathic and harmless is gradual. Some viruses, such as Epstein–Barr virus, can cause cells to proliferate without causing malignancy, while others, such as papillomaviruses, are established causes of cancer.
Some viruses cause no apparent changes to the infected cell. Cells in which the virus is latent and inactive show few signs of infection and often function normally. This causes persistent infections and the virus is often dormant for many months or years. This is often the case with herpes viruses.
Viruses are by far the most abundant biological entities on Earth and they outnumber all the others put together. They infect all types of cellular life including animals, plants, bacteria and fungi. Different types of viruses can infect only a limited range of hosts and many are species-specific. Some, such as smallpox virus for example, can infect only one species—in this case humans, and are said to have a narrow host range. Other viruses, such as rabies virus, can infect different species of mammals and are said to have a broad range. The viruses that infect plants are harmless to animals, and most viruses that infect other animals are harmless to humans. The host range of some bacteriophages is limited to a single strain of bacteria and they can be used to trace the source of outbreaks of infections by a method called phage typing. The complete set of viruses in an organism or habitat is called the virome; for example, all human viruses constitute the human virome.
Classification seeks to describe the diversity of viruses by naming and grouping them on the basis of similarities. In 1962, André Lwoff, Robert Horne, and Paul Tournier were the first to develop a means of virus classification, based on the Linnaean hierarchical system. This system based classification on phylum, class, order, family, genus, and species. Viruses were grouped according to their shared properties (not those of their hosts) and the type of nucleic acid forming their genomes. In 1966, the International Committee on Taxonomy of Viruses (ICTV) was formed. The system proposed by Lwoff, Horne and Tournier was initially not accepted by the ICTV because the small genome size of viruses and their high rate of mutation made it difficult to determine their ancestry beyond order. As such, the Baltimore classification system has come to be used to supplement the more traditional hierarchy. Starting in 2018, the ICTV began to acknowledge deeper evolutionary relationships between viruses that have been discovered over time and adopted a 15-rank classification system ranging from realm to species.
Examples of common human diseases caused by viruses include the common cold, influenza, chickenpox, and cold sores. Many serious diseases such as rabies, Ebola virus disease, AIDS (HIV), avian influenza, and SARS are caused by viruses. The relative ability of viruses to cause disease is described in terms of virulence. Other diseases are under investigation to discover if they have a virus as the causative agent, such as the possible connection between human herpesvirus 6 (HHV6) and neurological diseases such as multiple sclerosis and chronic fatigue syndrome. There is controversy over whether the bornavirus, previously thought to cause neurological diseases in horses, could be responsible for psychiatric illnesses in humans.Viruses have different mechanisms by which they produce disease in an organism, which depends largely on the viral species. Mechanisms at the cellular level primarily include cell lysis, the breaking open and subsequent death of the cell. In multicellular organisms, if enough cells die, the whole organism will start to suffer the effects. Although viruses cause disruption of healthy homeostasis, resulting in disease, they may exist relatively harmlessly within an organism. An example would include the ability of the herpes simplex virus, which causes cold sores, to remain in a dormant state within the human body. This is called latency and is a characteristic of the herpes viruses, including Epstein–Barr virus, which causes glandular fever, and varicella zoster virus, which causes chickenpox and shingles. Most people have been infected with at least one of these types of herpes virus. These latent viruses might sometimes be beneficial, as the presence of the virus can increase immunity against bacterial pathogens, such as Yersinia pestis.Some viruses can cause lifelong or chronic infections, where the viruses continue to replicate in the body despite the host's defence mechanisms. This is common in hepatitis B virus and hepatitis C virus infections. People chronically infected are known as carriers, as they serve as reservoirs of infectious virus. In populations with a high proportion of carriers, the disease is said to be endemic.
Viral epidemiology is the branch of medical science that deals with the transmission and control of virus infections in humans. Transmission of viruses can be vertical, which means from mother to child, or horizontal, which means from person to person. Examples of vertical transmission include hepatitis B virus and HIV, where the baby is born already infected with the virus. Another, more rare, example is the varicella zoster virus, which, although causing relatively mild infections in children and adults, can be fatal to the foetus and newborn baby.Horizontal transmission is the most common mechanism of spread of viruses in populations. Horizontal transmission can occur when body fluids are exchanged during sexual activity, by exchange of saliva or when contaminated food or water is ingested. It can also occur when aerosols containing viruses are inhaled or by insect vectors such as when infected mosquitoes penetrate the skin of a host. Most types of viruses are restricted to just one or two of these mechanisms and they are referred to as "respiratory viruses" or "enteric viruses" and so forth. The rate or speed of transmission of viral infections depends on factors that include population density, the number of susceptible individuals, (i.e., those not immune), the quality of healthcare and the weather.Epidemiology is used to break the chain of infection in populations during outbreaks of viral diseases. Control measures are used that are based on knowledge of how the virus is transmitted. It is important to find the source, or sources, of the outbreak and to identify the virus. Once the virus has been identified, the chain of transmission can sometimes be broken by vaccines. When vaccines are not available, sanitation and disinfection can be effective. Often, infected people are isolated from the rest of the community, and those that have been exposed to the virus are placed in quarantine. To control the outbreak of foot-and-mouth disease in cattle in Britain in 2001, thousands of cattle were slaughtered. Most viral infections of humans and other animals have incubation periods during which the infection causes no signs or symptoms. Incubation periods for viral diseases range from a few days to weeks, but are known for most infections. Somewhat overlapping, but mainly following the incubation period, there is a period of communicability—a time when an infected individual or animal is contagious and can infect another person or animal. This, too, is known for many viral infections, and knowledge of the length of both periods is important in the control of outbreaks. When outbreaks cause an unusually high proportion of cases in a population, community, or region, they are called epidemics. If outbreaks spread worldwide, they are called pandemics.
A pandemic is a worldwide epidemic. The 1918 flu pandemic, which lasted until 1919, was a category 5 influenza pandemic caused by an unusually severe and deadly influenza A virus. The victims were often healthy young adults, in contrast to most influenza outbreaks, which predominantly affect juvenile, elderly, or otherwise-weakened patients. Older estimates say it killed 40–50 million people, while more recent research suggests that it may have killed as many as 100 million people, or 5% of the world's population in 1918.Although viral pandemics are rare events, HIV—which evolved from viruses found in monkeys and chimpanzees—has been pandemic since at least the 1980s. During the 20th century there were four pandemics caused by influenza virus and those that occurred in 1918, 1957 and 1968 were severe. Most researchers believe that HIV originated in sub-Saharan Africa during the 20th century; it is now a pandemic, with an estimated 37.9 million people now living with the disease worldwide. There were about 770,000 deaths from AIDS in 2018. The Joint United Nations Programme on HIV/AIDS (UNAIDS) and the World Health Organization (WHO) estimate that AIDS has killed more than 25 million people since it was first recognised on 5 June 1981, making it one of the most destructive epidemics in recorded history. In 2007 there were 2.7 million new HIV infections and 2 million HIV-related deaths. Several highly lethal viral pathogens are members of the Filoviridae. Filoviruses are filament-like viruses that cause viral hemorrhagic fever, and include ebolaviruses and marburgviruses. Marburg virus, first discovered in 1967, attracted widespread press attention in April 2005 for an outbreak in Angola. Ebola virus disease has also caused intermittent outbreaks with high mortality rates since 1976 when it was first identified. The worst and most recent one is the 2013–2016 West Africa epidemic.With the exception of smallpox, most pandemics are caused by newly evolved viruses. These "emergent" viruses are usually mutants of less harmful viruses that have circulated previously either in humans or other animals.Severe acute respiratory syndrome (SARS) and Middle East respiratory syndrome (MERS) are caused by new types of coronaviruses. Other coronaviruses are known to cause mild infections in humans, so the virulence and rapid spread of SARS infections—that by July 2003 had caused around 8,000 cases and 800 deaths—was unexpected and most countries were not prepared.A related coronavirus emerged in Wuhan, China in November 2019 and spread rapidly around the world. Thought to have originated in bats and subsequently named severe acute respiratory syndrome coronavirus 2, infections with the virus caused a pandemic in 2020. Unprecedented restrictions in peacetime have been placed on international travel, and curfews imposed in several major cities worldwide.
Viruses are an established cause of cancer in humans and other species. Viral cancers occur only in a minority of infected persons (or animals). Cancer viruses come from a range of virus families, including both RNA and DNA viruses, and so there is no single type of "oncovirus" (an obsolete term originally used for acutely transforming retroviruses). The development of cancer is determined by a variety of factors such as host immunity and mutations in the host. Viruses accepted to cause human cancers include some genotypes of human papillomavirus, hepatitis B virus, hepatitis C virus, Epstein–Barr virus, Kaposi's sarcoma-associated herpesvirus and human T-lymphotropic virus. The most recently discovered human cancer virus is a polyomavirus (Merkel cell polyomavirus) that causes most cases of a rare form of skin cancer called Merkel cell carcinoma. Hepatitis viruses can develop into a chronic viral infection that leads to liver cancer. Infection by human T-lymphotropic virus can lead to tropical spastic paraparesis and adult T-cell leukaemia. Human papillomaviruses are an established cause of cancers of cervix, skin, anus, and penis. Within the Herpesviridae, Kaposi's sarcoma-associated herpesvirus causes Kaposi's sarcoma and body-cavity lymphoma, and Epstein–Barr virus causes Burkitt's lymphoma, Hodgkin's lymphoma, B lymphoproliferative disorder, and nasopharyngeal carcinoma. Merkel cell polyomavirus closely related to SV40 and mouse polyomaviruses that have been used as animal models for cancer viruses for over 50 years.
The body's first line of defence against viruses is the innate immune system. This comprises cells and other mechanisms that defend the host from infection in a non-specific manner. This means that the cells of the innate system recognise, and respond to, pathogens in a generic way, but, unlike the adaptive immune system, it does not confer long-lasting or protective immunity to the host.RNA interference is an important innate defence against viruses. Many viruses have a replication strategy that involves double-stranded RNA (dsRNA). When such a virus infects a cell, it releases its RNA molecule or molecules, which immediately bind to a protein complex called a dicer that cuts the RNA into smaller pieces. A biochemical pathway—the RISC complex—is activated, which ensures cell survival by degrading the viral mRNA. Rotaviruses have evolved to avoid this defence mechanism by not uncoating fully inside the cell, and releasing newly produced mRNA through pores in the particle's inner capsid. Their genomic dsRNA remains protected inside the core of the virion.When the adaptive immune system of a vertebrate encounters a virus, it produces specific antibodies that bind to the virus and often render it non-infectious. This is called humoral immunity. Two types of antibodies are important. The first, called IgM, is highly effective at neutralising viruses but is produced by the cells of the immune system only for a few weeks. The second, called IgG, is produced indefinitely. The presence of IgM in the blood of the host is used to test for acute infection, whereas IgG indicates an infection sometime in the past. IgG antibody is measured when tests for immunity are carried out.Antibodies can continue to be an effective defence mechanism even after viruses have managed to gain entry to the host cell. A protein that is in cells, called TRIM21, can attach to the antibodies on the surface of the virus particle. This primes the subsequent destruction of the virus by the enzymes of the cell's proteosome system. A second defence of vertebrates against viruses is called cell-mediated immunity and involves immune cells known as T cells. The body's cells constantly display short fragments of their proteins on the cell's surface, and, if a T cell recognises a suspicious viral fragment there, the host cell is destroyed by 'killer T' cells and the virus-specific T-cells proliferate. Cells such as the macrophage are specialists at this antigen presentation. The production of interferon is an important host defence mechanism. This is a hormone produced by the body when viruses are present. Its role in immunity is complex; it eventually stops the viruses from reproducing by killing the infected cell and its close neighbours.Not all virus infections produce a protective immune response in this way. HIV evades the immune system by constantly changing the amino acid sequence of the proteins on the surface of the virion. This is known as "escape mutation" as the viral epitopes escape recognition by the host immune response. These persistent viruses evade immune control by sequestration, blockade of antigen presentation, cytokine resistance, evasion of natural killer cell activities, escape from apoptosis, and antigenic shift. Other viruses, called 'neurotropic viruses', are disseminated by neural spread where the immune system may be unable to reach them.
Because viruses use vital metabolic pathways within host cells to replicate, they are difficult to eliminate without using drugs that cause toxic effects to host cells in general. The most effective medical approaches to viral diseases are vaccinations to provide immunity to infection, and antiviral drugs that selectively interfere with viral replication.
Vaccination is a cheap and effective way of preventing infections by viruses. Vaccines were used to prevent viral infections long before the discovery of the actual viruses. Their use has resulted in a dramatic decline in morbidity (illness) and mortality (death) associated with viral infections such as polio, measles, mumps and rubella. Smallpox infections have been eradicated. Vaccines are available to prevent over thirteen viral infections of humans, and more are used to prevent viral infections of animals. Vaccines can consist of live-attenuated or killed viruses, or viral proteins (antigens). Live vaccines contain weakened forms of the virus, which do not cause the disease but, nonetheless, confer immunity. Such viruses are called attenuated. Live vaccines can be dangerous when given to people with a weak immunity (who are described as immunocompromised), because in these people, the weakened virus can cause the original disease. Biotechnology and genetic engineering techniques are used to produce subunit vaccines. These vaccines use only the capsid proteins of the virus. Hepatitis B vaccine is an example of this type of vaccine. Subunit vaccines are safe for immunocompromised patients because they cannot cause the disease. The yellow fever virus vaccine, a live-attenuated strain called 17D, is probably the safest and most effective vaccine ever generated.
Antiviral drugs are often nucleoside analogues (fake DNA building-blocks), which viruses mistakenly incorporate into their genomes during replication. The life-cycle of the virus is then halted because the newly synthesised DNA is inactive. This is because these analogues lack the hydroxyl groups, which, along with phosphorus atoms, link together to form the strong "backbone" of the DNA molecule. This is called DNA chain termination. Examples of nucleoside analogues are aciclovir for Herpes simplex virus infections and lamivudine for HIV and hepatitis B virus infections. Aciclovir is one of the oldest and most frequently prescribed antiviral drugs. Other antiviral drugs in use target different stages of the viral life cycle. HIV is dependent on a proteolytic enzyme called the HIV-1 protease for it to become fully infectious. There is a large class of drugs called protease inhibitors that inactivate this enzyme.Hepatitis C is caused by an RNA virus. In 80% of people infected, the disease is chronic, and without treatment, they are infected for the remainder of their lives. There is now an effective treatment that uses the nucleoside analogue drug ribavirin combined with interferon. The treatment of chronic carriers of the hepatitis B virus by using a similar strategy using lamivudine has been developed.
Viruses infect all cellular life and, although viruses occur universally, each cellular species has its own specific range that often infect only that species. Some viruses, called satellites, can replicate only within cells that have already been infected by another virus.
Viruses are important pathogens of livestock. Diseases such as foot-and-mouth disease and bluetongue are caused by viruses. Companion animals such as cats, dogs, and horses, if not vaccinated, are susceptible to serious viral infections. Canine parvovirus is caused by a small DNA virus and infections are often fatal in pups. Like all invertebrates, the honey bee is susceptible to many viral infections. Most viruses co-exist harmlessly in their host and cause no signs or symptoms of disease.
There are many types of plant virus, but often they cause only a loss of yield, and it is not economically viable to try to control them. Plant viruses are often spread from plant to plant by organisms, known as vectors. These are usually insects, but some fungi, nematode worms, and single-celled organisms have been shown to be vectors. When control of plant virus infections is considered economical, for perennial fruits, for example, efforts are concentrated on killing the vectors and removing alternate hosts such as weeds. Plant viruses cannot infect humans and other animals because they can reproduce only in living plant cells.Originally from Peru, the potato has become a staple crop worldwide. The potato virus Y causes disease in potatoes and related species including tomatoes and peppers. In the 1980s, this virus acquired economical importance when it proved difficult to control in seed potato crops. Transmitted by aphids, this virus can reduce crop yields by up to 80 per cent, causing significant losses to potato yields.Plants have elaborate and effective defence mechanisms against viruses. One of the most effective is the presence of so-called resistance (R) genes. Each R gene confers resistance to a particular virus by triggering localised areas of cell death around the infected cell, which can often be seen with the unaided eye as large spots. This stops the infection from spreading. RNA interference is also an effective defence in plants. When they are infected, plants often produce natural disinfectants that kill viruses, such as salicylic acid, nitric oxide, and reactive oxygen molecules.Plant virus particles or virus-like particles (VLPs) have applications in both biotechnology and nanotechnology. The capsids of most plant viruses are simple and robust structures and can be produced in large quantities either by the infection of plants or by expression in a variety of heterologous systems. Plant virus particles can be modified genetically and chemically to encapsulate foreign material and can be incorporated into supramolecular structures for use in biotechnology.
Bacteriophages are a common and diverse group of viruses and are the most abundant biological entity in aquatic environments—there are up to ten times more of these viruses in the oceans than there are bacteria, reaching levels of 250,000,000 bacteriophages per millilitre of seawater. These viruses infect specific bacteria by binding to surface receptor molecules and then entering the cell. Within a short amount of time, in some cases just minutes, bacterial polymerase starts translating viral mRNA into protein. These proteins go on to become either new virions within the cell, helper proteins, which help assembly of new virions, or proteins involved in cell lysis. Viral enzymes aid in the breakdown of the cell membrane, and, in the case of the T4 phage, in just over twenty minutes after injection over three hundred phages could be released.The major way bacteria defend themselves from bacteriophages is by producing enzymes that destroy foreign DNA. These enzymes, called restriction endonucleases, cut up the viral DNA that bacteriophages inject into bacterial cells. Bacteria also contain a system that uses CRISPR sequences to retain fragments of the genomes of viruses that the bacteria have come into contact with in the past, which allows them to block the virus's replication through a form of RNA interference. This genetic system provides bacteria with acquired immunity to infection.
Some viruses replicate within archaea: these are double-stranded DNA viruses with unusual and sometimes unique shapes. These viruses have been studied in most detail in the thermophilic archaea, particularly the orders Sulfolobales and Thermoproteales. Defences against these viruses involve RNA interference from repetitive DNA sequences within archaean genomes that are related to the genes of the viruses. Most archaea have CRISPR–Cas systems as an adaptive defence against viruses. These enable archaea to retain sections of viral DNA, which are then used to target and eliminate subsequent infections by the virus using a process similar to RNA interference.
Viruses are the most abundant biological entity in aquatic environments There are about ten million of them in a teaspoon of seawater. Most of these viruses are bacteriophages infecting heterotrophic bacteria and cyanophages infecting cyanobacteria and they are essential to the regulation of saltwater and freshwater ecosystems. Bacteriophages are harmless to plants and animals, and are essential to the regulation of marine and freshwater ecosystems are important mortality agents of phytoplankton, the base of the foodchain in aquatic environments. They infect and destroy bacteria in aquatic microbial communities, and are one of the most important mechanisms of recycling carbon and nutrient cycling in marine environments. The organic molecules released from the dead bacterial cells stimulate fresh bacterial and algal growth, in a process known as the viral shunt. In particular, lysis of bacteria by viruses has been shown to enhance nitrogen cycling and stimulate phytoplankton growth. Viral activity may also affect the biological pump, the process whereby carbon is sequestered in the deep ocean.Microorganisms constitute more than 90% of the biomass in the sea. It is estimated that viruses kill approximately 20% of this biomass each day and that there are 10 to 15 times as many viruses in the oceans as there are bacteria and archaea. Viruses are also major agents responsible for the destruction of phytoplankton including harmful algal blooms, The number of viruses in the oceans decreases further offshore and deeper into the water, where there are fewer host organisms.In January 2018, scientists reported that 800 million viruses, mainly of marine origin, are deposited daily from the Earth's atmosphere onto every square meter of the planet's surface, as the result of a global atmospheric stream of viruses, circulating above the weather system but below the altitude of usual airline travel, distributing viruses around the planet.Like any organism, marine mammals are susceptible to viral infections. In 1988 and 2002, thousands of harbour seals were killed in Europe by phocine distemper virus. Many other viruses, including caliciviruses, herpesviruses, adenoviruses and parvoviruses, circulate in marine mammal populations.
Viruses are an important natural means of transferring genes between different species, which increases genetic diversity and drives evolution. It is thought that viruses played a central role in early evolution, before the diversification of the last universal common ancestor into bacteria, archaea and eukaryotes. Viruses are still one of the largest reservoirs of unexplored genetic diversity on Earth.
Viruses are important to the study of molecular and cell biology as they provide simple systems that can be used to manipulate and investigate the functions of cells. The study and use of viruses have provided valuable information about aspects of cell biology. For example, viruses have been useful in the study of genetics and helped our understanding of the basic mechanisms of molecular genetics, such as DNA replication, transcription, RNA processing, translation, protein transport, and immunology. Geneticists often use viruses as vectors to introduce genes into cells that they are studying. This is useful for making the cell produce a foreign substance, or to study the effect of introducing a new gene into the genome. In a similar fashion, virotherapy uses viruses as vectors to treat various diseases, as they can specifically target cells and DNA. It shows promising use in the treatment of cancer and in gene therapy. Eastern European scientists have used phage therapy as an alternative to antibiotics for some time, and interest in this approach is increasing, because of the high level of antibiotic resistance now found in some pathogenic bacteria. The expression of heterologous proteins by viruses is the basis of several manufacturing processes that are currently being used for the production of various proteins such as vaccine antigens and antibodies. Industrial processes have been recently developed using viral vectors and a number of pharmaceutical proteins are currently in pre-clinical and clinical trials.
Virotherapy involves the use of genetically modified viruses to treat diseases. Viruses have been modified by scientists to reproduce in cancer cells and destroy them but not infect healthy cells. Talimogene laherparepvec (T-VEC), for example, is a modified herpes simplex virus that has had a gene, which is required for viruses to replicate in healthy cells, deleted and replaced with a human gene (GM-CSF) that stimulates immunity. When this virus infects cancer cells, it destroys them and in doing so the presence the GM-CSF gene attracts dendritic cells from the surrounding tissues of the body. The dendritic cells process the dead cancer cells and present components of them to other cells of the immune system. Having completed successful clinical trials, the virus gained approval for the treatment of melanoma in late 2015. Viruses that have been reprogrammed to kill cancer cells are called oncolytic viruses.
Current trends in nanotechnology promise to make much more versatile use of viruses. From the viewpoint of a materials scientist, viruses can be regarded as organic nanoparticles. Their surface carries specific tools that enable them to cross the barriers of their host cells. The size and shape of viruses and the number and nature of the functional groups on their surface is precisely defined. As such, viruses are commonly used in materials science as scaffolds for covalently linked surface modifications. A particular quality of viruses is that they can be tailored by directed evolution. The powerful techniques developed by life sciences are becoming the basis of engineering approaches towards nanomaterials, opening a wide range of applications far beyond biology and medicine.Because of their size, shape, and well-defined chemical structures, viruses have been used as templates for organising materials on the nanoscale. Recent examples include work at the Naval Research Laboratory in Washington, D.C., using Cowpea mosaic virus (CPMV) particles to amplify signals in DNA microarray based sensors. In this application, the virus particles separate the fluorescent dyes used for signalling to prevent the formation of non-fluorescent dimers that act as quenchers. Another example is the use of CPMV as a nanoscale breadboard for molecular electronics.
Many viruses can be synthesised de novo ("from scratch") and the first synthetic virus was created in 2002. Although somewhat of a misconception, it is not the actual virus that is synthesised, but rather its DNA genome (in case of a DNA virus), or a cDNA copy of its genome (in case of RNA viruses). For many virus families the naked synthetic DNA or RNA (once enzymatically converted back from the synthetic cDNA) is infectious when introduced into a cell. That is, they contain all the necessary information to produce new viruses. This technology is now being used to investigate novel vaccine strategies. The ability to synthesise viruses has far-reaching consequences, since viruses can no longer be regarded as extinct, as long as the information of their genome sequence is known and permissive cells are available. As of November 2017, the full-length genome sequences of 7454 different viruses, including smallpox, are publicly available in an online database maintained by the National Institutes of Health.
The ability of viruses to cause devastating epidemics in human societies has led to the concern that viruses could be weaponised for biological warfare. Further concern was raised by the successful recreation of the infamous 1918 influenza virus in a laboratory. Smallpox virus devastated numerous societies throughout history before its eradication. There are only two centres in the world authorised by the WHO to keep stocks of smallpox virus: the State Research Center of Virology and Biotechnology VECTOR in Russia and the Centers for Disease Control and Prevention in the United States. It may be used as a weapon, as the vaccine for smallpox sometimes had severe side-effects, it is no longer used routinely in any country. Thus, much of the modern human population has almost no established resistance to smallpox and would be vulnerable to the virus.
Media related to Viruses at Wikimedia Commons Data related to Virus at Wikispecies ViralZone A Swiss Institute of Bioinformatics resource for all viral families, providing general molecular and epidemiological information
A computer virus is a type of computer program that, when executed, replicates itself by modifying other computer programs and inserting its own code. When this replication succeeds, the affected areas are then said to be "infected" with a computer virus.Computer viruses cause billions of dollars' worth of economic damage each year,In 1989 The ADAPSO Software Industry Division published Dealing With Electronic Vandalism, in which they followed the risk of data loss by "the added risk of losing customer confidence."In response, free, open-source antivirus tools have been developed, and an industry of antivirus software has cropped up, selling or freely distributing virus protection to users of various operating systems.
Virus writers use social engineering deceptions and exploit detailed knowledge of security vulnerabilities to initially infect systems and to spread the virus. The vast majority of viruses target systems running Microsoft Windows, employing a variety of mechanisms to infect new hosts, and often using complex anti-detection/stealth strategies to evade antivirus software. Motives for creating viruses can include seeking profit (e.g., with ransomware), desire to send a political message, personal amusement, to demonstrate that a vulnerability exists in software, for sabotage and denial of service, or simply because they wish to explore cybersecurity issues, artificial life and evolutionary algorithms.Damage is due to causing system failure, corrupting data, wasting computer resources, increasing maintenance costs or stealing personal information. Even though no antivirus software can uncover all computer viruses (especially new ones), computer security researchers are actively searching for new ways to enable antivirus solutions to more effectively detect emerging viruses, before they become widely distributed.
The term "virus" is also misused by extension to refer to other types of malware. "Malware" encompasses computer viruses along with many other forms of malicious software, such as computer "worms", ransomware, spyware, adware, trojan horses, keyloggers, rootkits, bootkits, malicious Browser Helper Object (BHOs), and other malicious software. The majority of active malware threats are trojan horse programs or computer worms rather than computer viruses. The term computer virus, coined by Fred Cohen in 1985, is a misnomer. Viruses often perform some type of harmful activity on infected host computers, such as acquisition of hard disk space or central processing unit (CPU) time, accessing and stealing private information (e.g., credit card numbers, debit card numbers, phone numbers, names, email addresses, passwords, bank information, house addresses, etc.), corrupting data, displaying political, humorous or threatening messages on the user's screen, spamming their e-mail contacts, logging their keystrokes, or even rendering the computer useless. However, not all viruses carry a destructive "payload" and attempt to hide themselves—the defining characteristic of viruses is that they are self-replicating computer programs that modify other software without user consent by injecting themselves into the said programs, similar to a biological virus which replicates within living cells.
The first academic work on the theory of self-replicating computer programs was done in 1949 by John von Neumann who gave lectures at the University of Illinois about the "Theory and Organization of Complicated Automata". The work of von Neumann was later published as the "Theory of self-reproducing automata". In his essay von Neumann described how a computer program could be designed to reproduce itself. Von Neumann's design for a self-reproducing computer program is considered the world's first computer virus, and he is considered to be the theoretical "father" of computer virology. In 1972, Veith Risak directly building on von Neumann's work on self-replication, published his article "Selbstreproduzierende Automaten mit minimaler Informationsübertragung" (Self-reproducing automata with minimal information exchange). The article describes a fully functional virus written in assembler programming language for a SIEMENS 4004/35 computer system. In 1980 Jürgen Kraus wrote his diplom thesis "Selbstreproduktion bei Programmen" (Self-reproduction of programs) at the University of Dortmund. In his work Kraus postulated that computer programs can behave in a way similar to biological viruses.
The first known description of a self-reproducing program in fiction is in the 1970 short story The Scarred Man by Gregory Benford which describes a computer program called VIRUS which, when installed on a computer with telephone modem dialing capability, randomly dials phone numbers until it hits a modem that is answered by another computer, and then attempts to program the answering computer with its own program, so that the second computer will also begin dialing random numbers, in search of yet another computer to program. The program rapidly spreads exponentially through susceptible computers and can only be countered by a second program called VACCINE.The idea was explored further in two 1972 novels, When HARLIE Was One by David Gerrold and The Terminal Man by Michael Crichton, and became a major theme of the 1975 novel The Shockwave Rider by John Brunner.The 1973 Michael Crichton sci-fi movie Westworld made an early mention of the concept of a computer virus, being a central plot theme that causes androids to run amok. Alan Oppenheimer's character summarizes the problem by stating that "...there's a clear pattern here which suggests an analogy to an infectious disease process, spreading from one...area to the next." To which the replies are stated: "Perhaps there are superficial similarities to disease" and, "I must confess I find it difficult to believe in a disease of machinery."
The Creeper virus was first detected on ARPANET, the forerunner of the Internet, in the early 1970s. Creeper was an experimental self-replicating program written by Bob Thomas at BBN Technologies in 1971. Creeper used the ARPANET to infect DEC PDP-10 computers running the TENEX operating system. Creeper gained access via the ARPANET and copied itself to the remote system where the message, "I'm the creeper, catch me if you can!" was displayed. The Reaper program was created to delete Creeper.In 1982, a program called "Elk Cloner" was the first personal computer virus to appear "in the wild"—that is, outside the single computer or [computer] lab where it was created. Written in 1981 by Richard Skrenta, a ninth grader at Mount Lebanon High School near Pittsburgh, it attached itself to the Apple DOS 3.3 operating system and spread via floppy disk. On its 50th use the Elk Cloner virus would be activated, infecting the personal computer and displaying a short poem beginning "Elk Cloner: The program with a personality." In 1984 Fred Cohen from the University of Southern California wrote his paper "Computer Viruses – Theory and Experiments". It was the first paper to explicitly call a self-reproducing program a "virus", a term introduced by Cohen's mentor Leonard Adleman. In 1987, Fred Cohen published a demonstration that there is no algorithm that can perfectly detect all possible viruses. Fred Cohen's theoretical compression virus was an example of a virus which was not malicious software (malware), but was putatively benevolent (well-intentioned). However, antivirus professionals do not accept the concept of "benevolent viruses", as any desired function can be implemented without involving a virus (automatic compression, for instance, is available under Windows at the choice of the user). Any virus will by definition make unauthorised changes to a computer, which is undesirable even if no damage is done or intended. On page one of Dr Solomon's Virus Encyclopaedia, the undesirability of viruses, even those that do nothing but reproduce, is thoroughly explained.An article that describes "useful virus functionalities" was published by J. B. Gunn under the title "Use of virus functions to provide a virtual APL interpreter under user control" in 1984. The first IBM PC virus in the "wild" was a boot sector virus dubbed (c)Brain, created in 1986 by Amjad Farooq Alvi and Basit Farooq Alvi in Lahore, Pakistan, reportedly to deter unauthorized copying of the software they had written. The first virus to specifically target Microsoft Windows, WinVir was discovered in April 1992, two years after the release of Windows 3.0. The virus did not contain any Windows API calls, instead relying on DOS interrupts. A few years later, in February 1996, Australian hackers from the virus-writing crew VLAD created the Bizatch virus (also known as "Boza" virus), which was the first known virus to target Windows 95. In late 1997 the encrypted, memory-resident stealth virus Win32.Cabanas was released—the first known virus that targeted Windows NT (it was also able to infect Windows 3.0 and Windows 9x hosts).Even home computers were affected by viruses. The first one to appear on the Commodore Amiga was a boot sector virus called SCA virus, which was detected in November 1987.
A viable computer virus must contain a search routine, which locates new files or new disks that are worthwhile targets for infection. Secondly, every computer virus must contain a routine to copy itself into the program which the search routine locates. The three main virus parts are: Infection mechanism (also called 'infection vector'): This is how the virus spreads or propagates. A virus typically has a search routine, which locates new files or new disks for infection. Trigger: Also known as a logic bomb, this is the compiled version that could be activated any time within an executable file when the virus is run that determines the event or condition for the malicious "payload" to be activated or delivered such as a particular date, a particular time, particular presence of another program, capacity of the disk exceeding some limit, or a double-click that opens a particular file. Payload: The "payload" is the actual body or data which carries out the malicious purpose of the virus. Payload activity might be noticeable (e.g., because it causes the system to slow down or "freeze"), as most of the time the "payload" itself is the harmful activity, or some times non-destructive but distributive, which is called virus hoax.
Virus phases is the life cycle of the computer virus, described by using an analogy to biology. This life cycle can be divided into four phases: Dormant phase: The virus program is idle during this stage. The virus program has managed to access the target user's computer or software, but during this stage, the virus does not take any action. The virus will eventually be activated by the "trigger" which states which event will execute the virus. Not all viruses have this stage. Propagation phase: The virus starts propagating, which is multiplying and replicating itself. The virus places a copy of itself into other programs or into certain system areas on the disk. The copy may not be identical to the propagating version; viruses often "morph" or change to evade detection by IT professionals and anti-virus software. Each infected program will now contain a clone of the virus, which will itself enter a propagation phase. Triggering phase: A dormant virus moves into this phase when it is activated, and will now perform the function for which it was intended. The triggering phase can be caused by a variety of system events, including a count of the number of times that this copy of the virus has made copies of itself. The trigger may occur when an employee is terminated from their employment or after a set period of time has elapsed, in order to reduce suspicion. Execution phase: This is the actual work of the virus, where the "payload" will be released. It can be destructive such as deleting files on disk, crashing the system, or corrupting files or relatively harmless such as popping up humorous or political messages on screen.
Many common applications, such as Microsoft Outlook and Microsoft Word, allow macro programs to be embedded in documents or emails, so that the programs may be run automatically when the document is opened. A macro virus (or "document virus") is a virus that is written in a macro language and embedded into these documents so that when users open the file, the virus code is executed, and can infect the user's computer. This is one of the reasons that it is dangerous to open unexpected or suspicious attachments in e-mails. While not opening attachments in e-mails from unknown persons or organizations can help to reduce the likelihood of contracting a virus, in some cases, the virus is designed so that the e-mail appears to be from a reputable organization (e.g., a major bank or credit card company).
Email viruses are viruses that intentionally, rather than accidentally, uses the email system to spread. While virus infected files may be accidentally sent as email attachments, email viruses are aware of email system functions. They generally target a specific type of email system (Microsoft Outlook is the most commonly used), harvest email addresses from various sources, and may append copies of themselves to all email sent, or may generate email messages containing copies of themselves as attachments.
To avoid detection by users, some viruses employ different kinds of deception. Some old viruses, especially on the DOS platform, make sure that the "last modified" date of a host file stays the same when the file is infected by the virus. This approach does not fool antivirus software, however, especially those which maintain and date cyclic redundancy checks on file changes. Some viruses can infect files without increasing their sizes or damaging the files. They accomplish this by overwriting unused areas of executable files. These are called cavity viruses. For example, the CIH virus, or Chernobyl Virus, infects Portable Executable files. Because those files have many empty gaps, the virus, which was 1 KB in length, did not add to the size of the file. Some viruses try to avoid detection by killing the tasks associated with antivirus software before it can detect them (for example, Conficker). In the 2010s, as computers and operating systems grow larger and more complex, old hiding techniques need to be updated or replaced. Defending a computer against viruses may demand that a file system migrate towards detailed and explicit permission for every kind of file access.
While some kinds of antivirus software employ various techniques to counter stealth mechanisms, once the infection occurs any recourse to "clean" the system is unreliable. In Microsoft Windows operating systems, the NTFS file system is proprietary. This leaves antivirus software a little alternative but to send a "read" request to Windows files that handle such requests. Some viruses trick antivirus software by intercepting its requests to the operating system. A virus can hide by intercepting the request to read the infected file, handling the request itself, and returning an uninfected version of the file to the antivirus software. The interception can occur by code injection of the actual operating system files that would handle the read request. Thus, an antivirus software attempting to detect the virus will either not be permitted to read the infected file, or, the "read" request will be served with the uninfected version of the same file.The only reliable method to avoid "stealth" viruses is to "reboot" from a medium that is known to be "clear". Security software can then be used to check the dormant operating system files. Most security software relies on virus signatures, or they employ heuristics. Security software may also use a database of file "hashes" for Windows OS files, so the security software can identify altered files, and request Windows installation media to replace them with authentic versions. In older versions of Windows, file cryptographic hash functions of Windows OS files stored in Windows—to allow file integrity/authenticity to be checked—could be overwritten so that the System File Checker would report that altered system files are authentic, so using file hashes to scan for altered files would not always guarantee finding an infection.
Most modern antivirus programs try to find virus-patterns inside ordinary programs by scanning them for so-called virus signatures. Unfortunately, the term is misleading, in that viruses do not possess unique signatures in the way that human beings do. Such a virus "signature" is merely a sequence of bytes that an antivirus program looks for because it is known to be part of the virus. A better term would be "search strings". Different antivirus programs will employ different search strings, and indeed different search methods, when identifying viruses. If a virus scanner finds such a pattern in a file, it will perform other checks to make sure that it has found the virus, and not merely a coincidental sequence in an innocent file, before it notifies the user that the file is infected. The user can then delete, or (in some cases) "clean" or "heal" the infected file. Some viruses employ techniques that make detection by means of signatures difficult but probably not impossible. These viruses modify their code on each infection. That is, each infected file contains a different variant of the virus.
One method of evading signature detection is to use simple encryption to encipher (encode) the body of the virus, leaving only the encryption module and a static cryptographic key in cleartext which does not change from one infection to the next. In this case, the virus consists of a small decrypting module and an encrypted copy of the virus code. If the virus is encrypted with a different key for each infected file, the only part of the virus that remains constant is the decrypting module, which would (for example) be appended to the end. In this case, a virus scanner cannot directly detect the virus using signatures, but it can still detect the decrypting module, which still makes indirect detection of the virus possible. Since these would be symmetric keys, stored on the infected host, it is entirely possible to decrypt the final virus, but this is probably not required, since self-modifying code is such a rarity that finding some may be reason enough for virus scanners to at least "flag" the file as suspicious. An old but compact way will be the use of arithmetic operation like addition or subtraction and the use of logical conditions such as XORing, where each byte in a virus is with a constant so that the exclusive-or operation had only to be repeated for decryption. It is suspicious for a code to modify itself, so the code to do the encryption/decryption may be part of the signature in many virus definitions. A simpler older approach did not use a key, where the encryption consisted only of operations with no parameters, like incrementing and decrementing, bitwise rotation, arithmetic negation, and logical NOT. Some viruses, called polymorphic viruses, will employ a means of encryption inside an executable in which the virus is encrypted under certain events, such as the virus scanner being disabled for updates or the computer being rebooted. This is called cryptovirology. At said times, the executable will decrypt the virus and execute its hidden runtimes, infecting the computer and sometimes disabling the antivirus software.
Polymorphic code was the first technique that posed a serious threat to virus scanners. Just like regular encrypted viruses, a polymorphic virus infects files with an encrypted copy of itself, which is decoded by a decryption module. In the case of polymorphic viruses, however, this decryption module is also modified on each infection. A well-written polymorphic virus therefore has no parts which remain identical between infections, making it very difficult to detect directly using "signatures". Antivirus software can detect it by decrypting the viruses using an emulator, or by statistical pattern analysis of the encrypted virus body. To enable polymorphic code, the virus has to have a polymorphic engine (also called "mutating engine" or "mutation engine") somewhere in its encrypted body. See polymorphic code for technical detail on how such engines operate.Some viruses employ polymorphic code in a way that constrains the mutation rate of the virus significantly. For example, a virus can be programmed to mutate only slightly over time, or it can be programmed to refrain from mutating when it infects a file on a computer that already contains copies of the virus. The advantage of using such slow polymorphic code is that it makes it more difficult for antivirus professionals and investigators to obtain representative samples of the virus, because "bait" files that are infected in one run will typically contain identical or similar samples of the virus. This will make it more likely that the detection by the virus scanner will be unreliable, and that some instances of the virus may be able to avoid detection.
To avoid being detected by emulation, some viruses rewrite themselves completely each time they are to infect new executables. Viruses that utilize this technique are said to be in metamorphic code. To enable metamorphism, a "metamorphic engine" is needed. A metamorphic virus is usually very large and complex. For example, W32/Simile consisted of over 14,000 lines of assembly language code, 90% of which is part of the metamorphic engine.
As software is often designed with security features to prevent unauthorized use of system resources, many viruses must exploit and manipulate security bugs, which are security defects in a system or application software, to spread themselves and infect other computers. Software development strategies that produce large numbers of "bugs" will generally also produce potential exploitable "holes" or "entrances" for the virus.
To replicate itself, a virus must be permitted to execute code and write to memory. For this reason, many viruses attach themselves to executable files that may be part of legitimate programs (see code injection). If a user attempts to launch an infected program, the virus' code may be executed simultaneously. In operating systems that use file extensions to determine program associations (such as Microsoft Windows), the extensions may be hidden from the user by default. This makes it possible to create a file that is of a different type than it appears to the user. For example, an executable may be created and named "picture.png.exe", in which the user sees only "picture.png" and therefore assumes that this file is a digital image and most likely is safe, yet when opened, it runs the executable on the client machine. Viruses may be installed on removable media, such as flash drives. The drives may be left in a parking lot of a government building or other target, with the hopes that curious users will insert the drive into a computer. In a 2015 experiment, researchers at the University of Michigan found that 45–98 percent of users would plug in a flash drive of unknown origin.
The vast majority of viruses target systems running Microsoft Windows. This is due to Microsoft's large market share of desktop computer users. The diversity of software systems on a network limits the destructive potential of viruses and malware. Open-source operating systems such as Linux allow users to choose from a variety of desktop environments, packaging tools, etc., which means that malicious code targeting any of these systems will only affect a subset of all users. Many Windows users are running the same set of applications, enabling viruses to rapidly spread among Microsoft Windows systems by targeting the same exploits on large numbers of hosts.While Linux and Unix in general have always natively prevented normal users from making changes to the operating system environment without permission, Windows users are generally not prevented from making these changes, meaning that viruses can easily gain control of the entire system on Windows hosts. This difference has continued partly due to the widespread use of administrator accounts in contemporary versions like Windows XP. In 1997, researchers created and released a virus for Linux—known as "Bliss". Bliss, however, requires that the user run it explicitly, and it can only infect programs that the user has the access to modify. Unlike Windows users, most Unix users do not log in as an administrator, or "root user", except to install or configure software; as a result, even if a user ran the virus, it could not harm their operating system. The Bliss virus never became widespread, and remains chiefly a research curiosity. Its creator later posted the source code to Usenet, allowing researchers to see how it worked.
Many users install antivirus software that can detect and eliminate known viruses when the computer attempts to download or run the executable file (which may be distributed as an email attachment, or on USB flash drives, for example). Some antivirus software blocks known malicious websites that attempt to install malware. Antivirus software does not change the underlying capability of hosts to transmit viruses. Users must update their software regularly to patch security vulnerabilities ("holes"). Antivirus software also needs to be regularly updated to recognize the latest threats. This is because malicious hackers and other individuals are always creating new viruses. The German AV-TEST Institute publishes evaluations of antivirus software for Windows and Android.Examples of Microsoft Windows anti virus and anti-malware software include the optional Microsoft Security Essentials (for Windows XP, Vista and Windows 7) for real-time protection, the Windows Malicious Software Removal Tool (now included with Windows (Security) Updates on "Patch Tuesday", the second Tuesday of each month), and Windows Defender (an optional download in the case of Windows XP). Additionally, several capable antivirus software programs are available for free download from the Internet (usually restricted to non-commercial use). Some such free programs are almost as good as commercial competitors. Common security vulnerabilities are assigned CVE IDs and listed in the US National Vulnerability Database. Secunia PSI is an example of software, free for personal use, that will check a PC for vulnerable out-of-date software, and attempt to update it. Ransomware and phishing scam alerts appear as press releases on the Internet Crime Complaint Center noticeboard. Ransomware is a virus that posts a message on the user's screen saying that the screen or system will remain locked or unusable until a ransom payment is made. Phishing is a deception in which the malicious individual pretends to be a friend, computer security expert, or other benevolent individual, with the goal of convincing the targeted individual to reveal passwords or other personal information. Other commonly used preventive measures include timely operating system updates, software updates, careful Internet browsing (avoiding shady websites), and installation of only trusted software. Certain browsers flag sites that have been reported to Google and that have been confirmed as hosting malware by Google.There are two common methods that an antivirus software application uses to detect viruses, as described in the antivirus software article. The first, and by far the most common method of virus detection is using a list of virus signature definitions. This works by examining the content of the computer's memory (its Random Access Memory (RAM), and boot sectors) and the files stored on fixed or removable drives (hard drives, floppy drives, or USB flash drives), and comparing those files against a database of known virus "signatures". Virus signatures are just strings of code that are used to identify individual viruses; for each virus, the antivirus designer tries to choose a unique signature string that will not be found in a legitimate program. Different antivirus programs use different "signatures" to identify viruses. The disadvantage of this detection method is that users are only protected from viruses that are detected by signatures in their most recent virus definition update, and not protected from new viruses (see "zero-day attack").A second method to find viruses is to use a heuristic algorithm based on common virus behaviors. This method can detect new viruses for which antivirus security firms have yet to define a "signature", but it also gives rise to more false positives than using signatures. False positives can be disruptive, especially in a commercial environment, because it may lead to a company instructing staff not to use the company computer system until IT services have checked the system for viruses. This can slow down productivity for regular workers.
Many websites run by antivirus software companies provide free online virus scanning, with limited "cleaning" facilities (after all, the purpose of the websites is to sell antivirus products and services). Some websites—like Google subsidiary VirusTotal.com—allow users to upload one or more suspicious files to be scanned and checked by one or more antivirus programs in one operation. Additionally, several capable antivirus software programs are available for free download from the Internet (usually restricted to non-commercial use). Microsoft offers an optional free antivirus utility called Microsoft Security Essentials, a Windows Malicious Software Removal Tool that is updated as part of the regular Windows update regime, and an older optional anti-malware (malware removal) tool Windows Defender that has been upgraded to an antivirus product in Windows 8. Some viruses disable System Restore and other important Windows tools such as Task Manager and CMD. An example of a virus that does this is CiaDoor. Many such viruses can be removed by rebooting the computer, entering Windows "safe mode" with networking, and then using system tools or Microsoft Safety Scanner. System Restore on Windows Me, Windows XP, Windows Vista and Windows 7 can restore the registry and critical system files to a previous checkpoint. Often a virus will cause a system to "hang" or "freeze", and a subsequent hard reboot will render a system restore point from the same day corrupted. Restore points from previous days should work, provided the virus is not designed to corrupt the restore files and does not exist in previous restore points.
Microsoft's System File Checker (improved in Windows 7 and later) can be used to check for, and repair, corrupted system files. Restoring an earlier "clean" (virus-free) copy of the entire partition from a cloned disk, a disk image, or a backup copy is one solution—restoring an earlier backup disk "image" is relatively simple to do, usually removes any malware, and may be faster than "disinfecting" the computer—or reinstalling and reconfiguring the operating system and programs from scratch, as described below, then restoring user preferences. Reinstalling the operating system is another approach to virus removal. It may be possible to recover copies of essential user data by booting from a live CD, or connecting the hard drive to another computer and booting from the second computer's operating system, taking great care not to infect that computer by executing any infected programs on the original drive. The original hard drive can then be reformatted and the OS and all programs installed from original media. Once the system has been restored, precautions must be taken to avoid reinfection from any restored executable files.
Before computer networks became widespread, most viruses spread on removable media, particularly floppy disks. In the early days of the personal computer, many users regularly exchanged information and programs on floppies. Some viruses spread by infecting programs stored on these disks, while others installed themselves into the disk boot sector, ensuring that they would be run when the user booted the computer from the disk, usually inadvertently. Personal computers of the era would attempt to boot first from a floppy if one had been left in the drive. Until floppy disks fell out of use, this was the most successful infection strategy and boot sector viruses were the most common in the "wild" for many years. Traditional computer viruses emerged in the 1980s, driven by the spread of personal computers and the resultant increase in bulletin board system (BBS), modem use, and software sharing. Bulletin board–driven software sharing contributed directly to the spread of Trojan horse programs, and viruses were written to infect popularly traded software. Shareware and bootleg software were equally common vectors for viruses on BBSs. Viruses can increase their chances of spreading to other computers by infecting files on a network file system or a file system that is accessed by other computers.Macro viruses have become common since the mid-1990s. Most of these viruses are written in the scripting languages for Microsoft programs such as Microsoft Word and Microsoft Excel and spread throughout Microsoft Office by infecting documents and spreadsheets. Since Word and Excel were also available for Mac OS, most could also spread to Macintosh computers. Although most of these viruses did not have the ability to send infected email messages, those viruses which did take advantage of the Microsoft Outlook Component Object Model (COM) interface. Some old versions of Microsoft Word allow macros to replicate themselves with additional blank lines. If two macro viruses simultaneously infect a document, the combination of the two, if also self-replicating, can appear as a "mating" of the two and would likely be detected as a virus unique from the "parents".A virus may also send a web address link as an instant message to all the contacts (e.g., friends and colleagues' e-mail addresses) stored on an infected machine. If the recipient, thinking the link is from a friend (a trusted source) follows the link to the website, the virus hosted at the site may be able to infect this new computer and continue propagating. Viruses that spread using cross-site scripting were first reported in 2002, and were academically demonstrated in 2005. There have been multiple instances of the cross-site scripting viruses in the "wild", exploiting websites such as MySpace (with the Samy worm) and Yahoo!.
Botnet Comparison of computer viruses Computer fraud and abuse act Computer insecurity Crimeware Core Wars – an early computer game featuring virus-like competitors Cryptovirology Keystroke logging Malware Spam (electronic) Technical support scam – unsolicited phone calls from a fake "tech support" person, claiming that the computer has a virus or other problems Trojan horse (computing) Virus hoax Windows 7 File Recovery Windows Action Center (Security Center) Zombie (computer science)
Knowledge is a familiarity, awareness, or understanding of someone or something, such as facts (propositional knowledge), skills (procedural knowledge), or objects (acquaintance knowledge). By most accounts, knowledge can be acquired in many different ways and from many sources, including but not limited to perception, reason, memory, testimony, scientific inquiry, education, and practice. The philosophical study of knowledge is called epistemology. The term "knowledge" can refer to a theoretical or practical understanding of a subject. It can be implicit (as with practical skill or expertise) or explicit (as with the theoretical understanding of a subject); formal or informal; systematic or particular. The philosopher Plato famously pointed out the need for a distinction between knowledge and true belief in the Theaetetus, leading many to attribute to him a definition of knowledge as "justified true belief". The difficulties with this definition raised by the Gettier problem have been the subject of extensive debate in epistemology for more than half a century.
The eventual demarcation of philosophy from science was made possible by the notion that philosophy's core was "theory of knowledge," a theory distinct from the sciences because it was their foundation... Without this idea of a "theory of knowledge," it is hard to imagine what "philosophy" could have been in the age of modern science. Knowledge is the primary subject of the field of epistemology, which studies what we know, how we come to know it, and what it means to know something.The definition of knowledge is a matter of ongoing debate among epistemologists. The classical definition, described but not ultimately endorsed by Plato, specifies that a statement must meet three criteria in order to be considered knowledge: it must be justified, true, and believed. Epistemologists today generally agree that these conditions are not sufficient, as various Gettier cases are thought to demonstrate. There are a number of alternative definitions which have been proposed, including Robert Nozick's proposal that all instances of knowledge must 'track the truth' and Simon Blackburn's proposal that those who have a justified true belief 'through a defect, flaw, or failure' fail to have knowledge. Richard Kirkham suggests that our definition of knowledge requires that the evidence for the belief necessitates its truth.In contrast to this approach, Ludwig Wittgenstein observed, following Moore's paradox, that one can say "He believes it, but it isn't so," but not "He knows it, but it isn't so." He goes on to argue that these do not correspond to distinct mental states, but rather to distinct ways of talking about conviction. What is different here is not the mental state of the speaker, but the activity in which they are engaged. For example, on this account, to know that the kettle is boiling is not to be in a particular state of mind, but to perform a particular task with the statement that the kettle is boiling. Wittgenstein sought to bypass the difficulty of definition by looking to the way "knowledge" is used in natural languages. He saw knowledge as a case of a family resemblance. Following this idea, "knowledge" has been reconstructed as a cluster concept that points out relevant features but that is not adequately captured by any definition.
“Self-knowledge” usually refers to a person's knowledge of their own sensations, thoughts, beliefs, and other mental states. A number of questions regarding self-knowledge have been the subject of extensive debates in philosophy, including whether self-knowledge differs from other types of knowledge, whether we have privileged self-knowledge compared to knowledge of other minds, and the nature of our acquaintance with ourselves. David Hume famously expressed skepticism about whether we could ever have self-knowledge over and above our immediate awareness of a "bundle of perceptions", which was part of his broader skepticism about personal identity.
We generally assume that knowledge is more valuable than mere true belief. If so, what is the explanation? A formulation of the value problem in epistemology first occurs in Plato's Meno. Socrates points out to Meno that a man who knew the way to Larissa could lead others there correctly. But so, too, could a man who had true beliefs about how to get there, even if he had not gone there or had any knowledge of Larissa. Socrates says that it seems that both knowledge and true opinion can guide action. Meno then wonders why knowledge is valued more than true belief and why knowledge and true belief are different. Socrates responds that knowledge is more valuable than mere true belief because it is tethered or justified. Justification, or working out the reason for a true belief, locks down true belief.The problem is to identify what (if anything) makes knowledge more valuable than mere true belief, or that makes knowledge more valuable than a mere minimal conjunction of its components, such as justification, safety, sensitivity, statistical likelihood, and anti-Gettier conditions, on a particular analysis of knowledge that conceives of knowledge as divided into components (to which knowledge-first epistemological theories, which posit knowledge as fundamental, are notable exceptions). The value problem re-emerged in the philosophical literature on epistemology in the twenty-first century following the rise of virtue epistemology in the 1980s, partly because of the obvious link to the concept of value in ethics.In contemporary philosophy, epistemologists including Ernest Sosa, John Greco, Jonathan Kvanvig, Linda Zagzebski, and Duncan Pritchard have defended virtue epistemology as a solution to the value problem. They argue that epistemology should also evaluate the "properties" of people as epistemic agents (i.e. intellectual virtues), rather than merely the properties of propositions and propositional mental attitudes.
The development of the scientific method has made a significant contribution to how knowledge of the physical world and its phenomena is acquired. To be termed scientific, a method of inquiry must be based on gathering observable and measurable evidence subject to specific principles of reasoning and experimentation. The scientific method consists of the collection of data through observation and experimentation, and the formulation and testing of hypotheses. Science, and the nature of scientific knowledge have also become the subject of philosophy. As science itself has developed, scientific knowledge now includes a broader usage in the soft sciences such as biology and the social sciences – discussed elsewhere as meta-epistemology, or genetic epistemology, and to some extent related to "theory of cognitive development". Note that "epistemology" is the study of knowledge and how it is acquired. Science is "the process used everyday to logically complete thoughts through inference of facts determined by calculated experiments." Sir Francis Bacon was critical in the historical development of the scientific method; his works established and popularized an inductive methodology for scientific inquiry. His famous aphorism, "knowledge is power", is found in the Meditations Sacrae (1597).Until recent times, at least in the Western tradition, it was simply taken for granted that knowledge was something possessed only by humans – and probably adult humans at that. Sometimes the notion might stretch to Society-as-such, as in (e. g.) "the knowledge possessed by the Coptic culture" (as opposed to its individual members), but that was not assured either. Nor was it usual to consider unconscious knowledge in any systematic way until this approach was popularized by Freud.Other biological domains where "knowledge" might be said to reside, include: (iii) the immune system, and (iv) in the DNA of the genetic code. See the list of four "epistemological domains": Popper, (1975); and Traill (2008: Table S, p. 31) – also references by both to Niels Jerne. Such considerations seem to call for a separate definition of "knowledge" to cover the biological systems. For biologists, knowledge must be usefully available to the system, though that system need not be conscious. Thus the criteria seem to be: The system should apparently be dynamic and self-organizing (unlike a mere book on its own). The knowledge must constitute some sort of representation of "the outside world", or ways of dealing with it (directly or indirectly). Some way must exist for the system to access this information quickly enough for it to be useful.Those who use the phrase "scientific knowledge" don't necessary claim to certainty, since scientists will never be absolutely certain when they are correct and when they are not. It is thus an irony of proper scientific method that one must doubt even when correct, in the hopes that this practice will lead to greater convergence on the truth in general.
Situated knowledge is knowledge specific to a particular situation. It was used by Donna Haraway as an extension of the feminist approaches of "successor science" suggested by Sandra Harding, one which "offers a more adequate, richer, better account of a world, in order to live in it well and in critical, reflexive relation to our own as well as others' practices of domination and the unequal parts of privilege and oppression that makes up all positions." This situation partially transforms science into a narrative, which Arturo Escobar explains as, "neither fictions nor supposed facts." This narrative of situation is historical textures woven of fact and fiction, and as Escobar explains further, "even the most neutral scientific domains are narratives in this sense," insisting that rather than a purpose dismissing science as a trivial matter of contingency, "it is to treat (this narrative) in the most serious way, without succumbing to its mystification as 'the truth' or to the ironic skepticism common to many critiques."Haraway's argument stems from the limitations of the human perception, as well as the overemphasis of the sense of vision in science. According to Haraway, vision in science has been, "used to signify a leap out of the marked body and into a conquering gaze from nowhere." This is the "gaze that mythically inscribes all the marked bodies, that makes the unmarked category claim the power to see and not be seen, to represent while escaping representation." This causes a limitation of views in the position of science itself as a potential player in the creation of knowledge, resulting in a position of "modest witness". This is what Haraway terms a "god trick", or the aforementioned representation while escaping representation. In order to avoid this, "Haraway perpetuates a tradition of thought which emphasizes the importance of the subject in terms of both ethical and political accountability".Some methods of generating knowledge, such as trial and error, or learning from experience, tend to create highly situational knowledge. Situational knowledge is often embedded in language, culture, or traditions. This integration of situational knowledge is an allusion to the community, and its attempts at collecting subjective perspectives into an embodiment "of views from somewhere." Knowledge is also said to be related to the capacity of acknowledgement in human beings.Even though Haraway's arguments are largely based on feminist studies, this idea of different worlds, as well as the skeptic stance of situated knowledge is present in the main arguments of post-structuralism. Fundamentally, both argue the contingency of knowledge on the presence of history; power, and geography, as well as the rejection of universal rules or laws or elementary structures; and the idea of power as an inherited trait of objectification.
One discipline of epistemology focuses on partial knowledge. In most cases, it is not possible to understand an information domain exhaustively; our knowledge is always incomplete or partial. Most real problems have to be solved by taking advantage of a partial understanding of the problem context and problem data, unlike the typical math problems one might solve at school, where all data is given and one is given a complete understanding of formulas necessary to solve them.This idea is also present in the concept of bounded rationality which assumes that in real life situations people often have a limited amount of information and make decisions accordingly.
Outline of knowledge – guide to the subject of knowledge presented as a tree structured list of its subtopics. Outline of human intelligence - list of subtopics in tree structure Analytic-synthetic distinction Descriptive knowledge Epistemic modal logic Inductive inference Inductive probability Intelligence Metaknowledge Philosophical skepticism Procedural knowledge Society for the Diffusion of Useful Knowledge
Epistemology ( (listen); from Greek ἐπιστήμη, epistēmē 'knowledge', and -logy) is the branch of philosophy concerned with knowledge. Epistemologists study the nature of knowledge, epistemic justification, the rationality of belief, and various related issues. Epistemology is considered one of the four main branches of philosophy, along with ethics, logic, and metaphysics.Debates in epistemology are generally clustered around four core areas: The philosophical analysis of the nature of knowledge and the conditions required for a belief to constitute knowledge, such as truth and justification Potential sources of knowledge and justified belief, such as perception, reason, memory, and testimony The structure of a body of knowledge or justified belief, including whether all justified beliefs must be derived from justified foundational beliefs or whether justification requires only a coherent set of beliefs Philosophical skepticism, which questions the possibility of knowledge, and related problems, such as whether skepticism poses a threat to our ordinary knowledge claims and whether it is possible to refute skeptical argumentsIn these debates and others, epistemology aims to answer questions such as "What do we know?", "What does it mean to say that we know something?", "What makes justified beliefs justified?", and "How do we know that we know?".
The word epistemology is derived from the ancient Greek epistēmē, meaning "knowledge", and the suffix -logia, meaning "logical discourse" (derived from the Greek word logos meaning "discourse"). The word's appearance in English was predated by the German term Wissenschaftslehre (literally, theory of science), which was introduced by philosophers Johann Fichte and Bernard Bolzano in the late 18th century. The word "epistemology" first appeared in 1847, in a review in New York's Eclectic Magazine. It was first used as a translation of the word Wissenschaftslehre as it appears in a philosophical novel by German author Jean Paul: The title of one of the principal works of Fichte is ′Wissenschaftslehre,′ which, after the analogy of technology ... we render epistemology. The word "epistemology" was properly introduced into Anglophone philosophical literature by Scottish philosopher James Frederick Ferrier in 1854, who used it in his Institutes of Metaphysics: This section of the science is properly termed the Epistemology—the doctrine or theory of knowing, just as ontology is the science of being... It answers the general question, ‘What is knowing and the known?’—or more shortly, ‘What is knowledge?’ It is important to note that the French term épistémologie is used with a different and far narrower meaning than the English term "epistemology", being used by French philosophers to refer solely to philosophy of science. For instance, Émile Meyerson opened his Identity and Reality, written in 1908, with the remark that the word 'is becoming current' as equivalent to 'the philosophy of the sciences.'
The concept of "epistemology" as a distinct field of inquiry predates the introduction of the term into the lexicon of philosophy. John Locke, for instance, described his efforts in Essay Concerning Human Understanding (1689) as an inquiry "into the original, certainty, and extent of human knowledge, together with the grounds and degrees of belief, opinion, and assent". According to Brett Warren, the character Epistemon in King James VI of Scotland's Daemonologie (1591) "was meant to be a personification of [what would later come to be] known as 'epistemology': the investigation into the differences of a justified belief versus its opinion." While it was not until the modern era that epistemology was first recognized as a distinct philosophical discipline which addresses a well-defined set of questions, almost every major historical philosopher has considered questions about what we know and how we know it. Among the Ancient Greek philosophers, Plato distinguished between inquiry regarding what we know and inquiry regarding what exists, particularly in the Republic, the Theaetetus, and the Meno. A number of important epistemological concerns also appeared in the works of Aristotle.During the subsequent Hellenistic period, philosophical schools began to appear which had a greater focus on epistemological questions, often in the form of philosophical skepticism. For instance, the Pyrrhonian skepticism of Pyrrho and Sextus Empiricus held that eudaimonia (flourishing, happiness, or "the good life") could be attained through the application of epoché (suspension of judgment) regarding all non-evident matters. Pyrrhonism was particularly concerned with undermining the epistemological dogmas of Stoicism and Epicureanism. The other major school of Hellenistic skepticism was Academic skepticism, most notably defended by Carneades and Arcesilaus, which predominated in the Platonic Academy for almost two centuries.In ancient India the Ajñana school of ancient Indian philosophy promoted skepticism. Ajñana was a Śramaṇa movement and a major rival of early Buddhism, Jainism and the Ājīvika school. They held that it was impossible to obtain knowledge of metaphysical nature or ascertain the truth value of philosophical propositions; and even if knowledge was possible, it was useless and disadvantageous for final salvation. They were specialized in refutation without propagating any positive doctrine of their own. After the ancient philosophical era but before the modern philosophical era, a number of Medieval philosophers also engaged with epistemological questions at length. Most notable among the Medievals for their contributions to epistemology were Thomas Aquinas, John Duns Scotus, and William of Ockham.Epistemology largely came to the fore in philosophy during the early modern period, which historians of philosophy traditionally divide up into a dispute between empiricists (including John Locke, David Hume, and George Berkeley) and rationalists (including René Descartes, Baruch Spinoza, and Gottfried Leibniz). The debate between them has often been framed using the question of whether knowledge comes primarily from sensory experience (empiricism), or whether a significant portion of our knowledge is derived entirely from our faculty of reason (rationalism). According to some scholars, this dispute was resolved in the late 18th century by Immanuel Kant, whose transcendental idealism famously made room for the view that "though all our knowledge begins with experience, it by no means follows that all [knowledge] arises out of experience". While the 19th century saw a decline in interest in epistemological issues, it came back to the forefront with the Vienna Circle and the development of analytic philosophy. There are a number of different methods that scholars use when trying to understand the relationship between historical epistemology and contemporary epistemology. One of the most contentious questions is this: "Should we assume that the problems of epistemology are perennial, and that trying to reconstruct and evaluate Plato’s or Hume’s or Kant’s arguments is meaningful for current debates, too?" Similarly, there is also a question of whether contemporary philosophers should aim to rationally reconstruct and evaluate historical views in epistemology, or to merely describe them. Barry Stroud claims that doing epistemology competently requires the historical study of past attempts to find philosophical understanding of the nature and scope of human knowledge. He argues that since inquiry may progress over time, we may not realize how different the questions that contemporary epistemologists ask are from questions asked at various different points in the history of philosophy.
Nearly all debates in epistemology are in some way related to knowledge. Most generally, "knowledge" is a familiarity, awareness, or understanding of someone or something, which might include facts (propositional knowledge), skills (procedural knowledge), or objects (acquaintance knowledge). Philosophers tend to draw an important distinction between three different senses of "knowing" something: "knowing that" (knowing the truth of propositions), "knowing how" (understanding how to perform certain actions), and "knowing by acquaintance" (directly perceiving an object, being familiar with it, or otherwise coming into contact with it). Epistemology is primarily concerned with the first of these forms of knowledge, propositional knowledge. All three senses of "knowing" can be seen in our ordinary use of the word. In mathematics, you can know that 2 + 2 = 4, but there is also knowing how to add two numbers, and knowing a person (e.g., knowing other persons, or knowing oneself), place (e.g., one's hometown), thing (e.g., cars), or activity (e.g., addition). While these distinctions are not explicit in English, they are explicitly made in other languages, including French, Portuguese, Spanish, Romanian, German and Dutch (although some languages related to English have been said to retain these verbs, such as Scots). The theoretical interpretation and significance of these linguistic issues remains controversial. In his paper On Denoting and his later book Problems of Philosophy, Bertrand Russell brought a great deal of attention to the distinction between "knowledge by description" and "knowledge by acquaintance". Gilbert Ryle is similarly credited with bringing more attention to the distinction between knowing how and knowing that in The Concept of Mind. In Personal Knowledge, Michael Polanyi argues for the epistemological relevance of knowledge how and knowledge that; using the example of the act of balance involved in riding a bicycle, he suggests that the theoretical knowledge of the physics involved in maintaining a state of balance cannot substitute for the practical knowledge of how to ride, and that it is important to understand how both are established and grounded. This position is essentially Ryle's, who argued that a failure to acknowledge the distinction between "knowledge that" and "knowledge how" leads to infinite regress.
One of the most important distinctions in epistemology is between what can be known a priori (independently of experience) and what can be known a posteriori (through experience). The terms may be roughly defined as follows: A priori knowledge is knowledge that is known independently of experience (that is, it is non-empirical, or arrived at before experience, usually by reason). It will henceforth be acquired through anything that is independent from experience. A posteriori knowledge is knowledge that is known by experience (that is, it is empirical, or arrived at through experience).Views that emphasize the importance of a priori knowledge are generally classified as rationalist. Views that emphasize the importance of a posteriori knowledge are generally classified as empiricist.
One of the core concepts in epistemology is belief. A belief is an attitude that a person holds regarding anything that they take to be true. For instance, to believe that snow is white is comparable to accepting the truth of the proposition "snow is white". Beliefs can be occurrent (e.g. a person actively thinking "snow is white"), or they can be dispositional (e.g. a person who if asked about the color of snow would assert "snow is white"). While there is not universal agreement about the nature of belief, most contemporary philosophers hold the view that a disposition to express belief B qualifies as holding the belief B. There are various different ways that contemporary philosophers have tried to describe beliefs, including as representations of ways that the world could be (Jerry Fodor), as dispositions to act as if certain things are true (Roderick Chisholm), as interpretive schemes for making sense of someone's actions (Daniel Dennett and Donald Davidson), or as mental states that fill a particular function (Hilary Putnam). Some have also attempted to offer significant revisions to our notion of belief, including eliminativists about belief who argue that there is no phenomenon in the natural world which corresponds to our folk psychological concept of belief (Paul Churchland) and formal epistemologists who aim to replace our bivalent notion of belief ("either I have a belief or I don't have a belief") with the more permissive, probabilistic notion of credence ("there is an entire spectrum of degrees of belief, not a simple dichotomy between belief and non-belief").While belief plays a significant role in epistemological debates surrounding knowledge and justification, it also has many other philosophical debates in its own right. Notable debates include: "What is the rational way to revise one's beliefs when presented with various sorts of evidence?"; "Is the content of our beliefs entirely determined by our mental states, or do the relevant facts have any bearing on our beliefs (e.g. if I believe that I'm holding a glass of water, is the non-mental fact that water is H2O part of the content of that belief)?"; "How fine-grained or coarse-grained are our beliefs?"; and "Must it be possible for a belief to be expressible in language, or are there non-linguistic beliefs?".
Truth is the property of being in accord with facts or reality. On most views, truth is the correspondence of language or thought to a mind-independent world. This is called the correspondence theory of truth. Among philosophers who think that it is possible to analyze the conditions necessary for knowledge, virtually all of them accept that truth is such a condition. There is much less agreement about the extent to which a knower must know why something is true in order to know. On such views, something being known implies that it is true. However, this should not be confused for the more contentious view that one must know that one knows in order to know (the KK principle).Epistemologists disagree about whether belief is the only truth-bearer. Other common suggestions for things that can bear the property of being true include propositions, sentences, thoughts, utterances, and judgments. Plato, in his Gorgias, argues that belief is the most commonly invoked truth-bearer.Many of the debates regarding truth are at the crossroads of epistemology and logic. Some contemporary debates regarding truth include: How do we define truth? Is it even possible to give an informative definition of truth? What things are truth-bearers and are therefore capable of being true or false? Are truth and falsity bivalent, or are there other truth values? What are the criteria of truth that allow us to identify it and to distinguish it from falsity? What role does truth play in constituting knowledge? And is truth absolute, or is it merely relative to one's perspective?
As the term "justification" is used in epistemology, a belief is justified if one has good reason for holding it. Loosely speaking, justification is the reason that someone holds a rationally admissible belief, on the assumption that it is a good reason for holding it. Sources of justification might include perceptual experience (the evidence of the senses), reason, and authoritative testimony, among others. Importantly however, a belief being justified does not guarantee that the belief is true, since a person could be justified in forming beliefs based on very convincing evidence that was nonetheless deceiving. In Plato's Theaetetus, Socrates considers a number of theories as to what knowledge is, first excluding merely true belief as an adequate account. For example, an ill person with no medical training, but with a generally optimistic attitude, might believe that he will recover from his illness quickly. Nevertheless, even if this belief turned out to be true, the patient would not have known that he would get well since his belief lacked justification. The last account that Plato considers is that knowledge is true belief "with an account" that explains or defines it in some way. According to Edmund Gettier, the view that Plato is describing here is that knowledge is justified true belief. The truth of this view would entail that in order to know that a given proposition is true, one must not only believe the relevant true proposition, but must also have a good reason for doing so. One implication of this would be that no one would gain knowledge just by believing something that happened to be true.Edmund Gettier's famous 1963 paper, "Is Justified True Belief Knowledge?", popularized the claim that the definition of knowledge as justified true belief had been widely accepted throughout the history of philosophy. The extent to which this is true is highly contentious, since Plato himself disavowed the "justified true belief" view at the end of the Theaetetus. Regardless of the accuracy of the claim, Gettier's paper produced major widespread discussion which completely reoriented epistemology in the second half of the 20th century, with a newfound focus on trying to provide an airtight definition of knowledge by adjusting or replacing the "justified true belief" view. Today there is still little consensus about whether any set of conditions succeeds in providing a set of necessary and sufficient conditions for knowledge, and many contemporary epistemologists have come to the conclusion that no such exception-free definition is possible. However, even if justification fails as a condition for knowledge as some philosophers claim, the question of whether or not a person has good reasons for holding a particular belief in a particular set of circumstances remains a topic of interest to contemporary epistemology, and is unavoidably linked to questions about rationality.
A central debate about the nature of justification is a debate between epistemological externalists on the one hand, and epistemological internalists on the other. While epistemic externalism first arose in attempts to overcome the Gettier problem, it has flourished in the time since as an alternative way of conceiving of epistemic justification. The initial development of epistemic externalism is often attributed to Alvin Goldman, although numerous other philosophers have worked on the topic in the time since.Externalists hold that factors deemed "external", meaning outside of the psychological states of those who gain knowledge, can be conditions of justification. For example, an externalist response to the Gettier problem is to say that for a justified true belief to count as knowledge, there must be a link or dependency between the belief and the state of the external world. Usually this is understood to be a causal link. Such causation, to the extent that it is "outside" the mind, would count as an external, knowledge-yielding condition. Internalists, on the other hand, assert that all knowledge-yielding conditions are within the psychological states of those who gain knowledge. Though unfamiliar with the internalist/externalist debate himself, many point to René Descartes as an early example of the internalist path to justification. He wrote that, because the only method by which we perceive the external world is through our senses, and that, because the senses are not infallible, we should not consider our concept of knowledge infallible. The only way to find anything that could be described as "indubitably true", he advocates, would be to see things "clearly and distinctly". He argued that if there is an omnipotent, good being who made the world, then it's reasonable to believe that people are made with the ability to know. However, this does not mean that man's ability to know is perfect. God gave man the ability to know but not with omniscience. Descartes said that man must use his capacities for knowledge correctly and carefully through methodological doubt.The dictum "Cogito ergo sum" (I think, therefore I am) is also commonly associated with Descartes' theory. In his own methodological doubt—doubting everything he previously knew so he could start from a blank slate—the first thing that he could not logically bring himself to doubt was his own existence: "I do not exist" would be a contradiction in terms. The act of saying that one does not exist assumes that someone must be making the statement in the first place. Descartes could doubt his senses, his body, and the world around him—but he could not deny his own existence, because he was able to doubt and must exist to manifest that doubt. Even if some "evil genius" were deceiving him, he would have to exist to be deceived. This one sure point provided him with what he called his Archimedean point, in order to further develop his foundation for knowledge. Simply put, Descartes' epistemological justification depended on his indubitable belief in his own existence and his clear and distinct knowledge of God.
Edmund Gettier is best known for his 1963 paper entitled "Is Justified True Belief Knowledge?", which called into question the common conception of knowledge as justified true belief. In just two and a half pages, Gettier argued that there are situations in which one's belief may be justified and true, yet fail to count as knowledge. That is, Gettier contended that while justified belief in a true proposition is necessary for that proposition to be known, it is not sufficient. According to Gettier, there are certain circumstances in which one does not have knowledge, even when all of the above conditions are met. Gettier proposed two thought experiments, which have become known as Gettier cases, as counterexamples to the classical account of knowledge. One of the cases involves two men, Smith and Jones, who are awaiting the results of their applications for the same job. Each man has ten coins in his pocket. Smith has excellent reasons to believe that Jones will get the job (the head of the company told him); and furthermore, Smith knows that Jones has ten coins in his pocket (he recently counted them). From this Smith infers: "The man who will get the job has ten coins in his pocket." However, Smith is unaware that he also has ten coins in his own pocket. Furthermore, it turns out that Smith, not Jones, is going to get the job. While Smith has strong evidence to believe that Jones will get the job, he is wrong. Smith therefore has a justified true belief that the man who will get the job has ten coins in his pocket; however, according to Gettier, Smith does not know that the man who will get the job has ten coins in his pocket, because Smith's belief is "...true by virtue of the number of coins in Jones's pocket, while Smith does not know how many coins are in Smith's pocket, and bases his belief... on a count of the coins in Jones's pocket, whom he falsely believes to be the man who will get the job." These cases fail to be knowledge because the subject's belief is justified, but only happens to be true by virtue of luck. In other words, he made the correct choice (believing that the man who will get the job has ten coins in his pocket) for the wrong reasons. Gettier then goes on to offer a second similar case, providing the means by which the specifics of his examples can be generalized into a broader problem for defining knowledge in terms of justified true belief. There have been various notable responses to the Gettier problem. Typically, they have involved substantial attempts to provide a new definition of knowledge that is not susceptible to Gettier-style objections, either by providing an additional fourth condition that justified true beliefs must meet to constitute knowledge, or proposing a completely new set of necessary and sufficient conditions for knowledge. While there have been far too many published responses for all of them to be mentioned, some of the most notable responses are discussed below.
Reliabilism has been a significant line of response to the Gettier problem among philosophers, originating with work by Alvin Goldman in the 1960s. According to reliabilism, a belief is justified (or otherwise supported in such a way as to count towards knowledge) only if it is produced by processes that typically yield a sufficiently high ratio of true to false beliefs. In other words, this theory states that a true belief counts as knowledge only if it is produced by a reliable belief-forming process. Examples of reliable processes include standard perceptual processes, remembering, good reasoning, and introspection.One commonly discussed challenge for reliabilism is the case of Henry and the barn façades. In this thought experiment, a man, Henry, is driving along and sees a number of buildings that resemble barns. Based on his perception of one of these, he concludes that he is looking at a barn. While he is indeed looking at a barn, it turns out that all of the other barn-like buildings he saw were façades. According to the challenge, Henry does not know that he has seen a barn, despite his belief being true, and despite his belief having been formed on the basis of a reliable process (i.e. his vision), since he only acquired his reliably formed true belief by accident. In other words, since he could have just as easily been looking at a barn façade and formed a false belief, the reliability of perception in general does not mean that his belief wasn't merely formed luckily, and this luck seems to preclude him from knowledge.
One less common response to the Gettier problem is defended by Richard Kirkham, who has argued that the only definition of knowledge that could ever be immune to all counterexamples is the infallibilist definition. To qualify as an item of knowledge, goes the theory, a belief must not only be true and justified, the justification of the belief must necessitate its truth. In other words, the justification for the belief must be infallible. While infallibilism is indeed an internally coherent response to the Gettier problem, it is incompatible with our everyday knowledge ascriptions. For instance, as the Cartesian skeptic will point out, all of my perceptual experiences are compatible with a skeptical scenario in which I am completely deceived about the existence of the external world, in which case most (if not all) of my beliefs would be false. The typical conclusion to draw from this is that it is possible to doubt most (if not all) of my everyday beliefs, meaning that if I am indeed justified in holding those beliefs, that justification is not infallible. For the justification to be infallable, my reasons for holding my everyday beliefs would need to completely exclude the possibility that those beliefs were false. Consequently, if a belief must be infallibly justified in order to constitute knowledge, then it must be the case that we are mistaken in most (if not all) instances in which we claim to have knowledge in everyday situations. While it is indeed possible to bite the bullet and accept this conclusion, most philosophers find it implausible to suggest that we know nothing or almost nothing, and therefore reject the infallibilist response as collapsing into radical skepticism.
Another possible candidate for the fourth condition of knowledge is indefeasibility. Defeasibility theory maintains that there should be no overriding or defeating truths for the reasons that justify one's belief. For example, suppose that person S believes he saw Tom Grabit steal a book from the library and uses this to justify the claim that Tom Grabit stole a book from the library. A possible defeater or overriding proposition for such a claim could be a true proposition like, "Tom Grabit's identical twin Sam is currently in the same town as Tom." When no defeaters of one's justification exist, a subject would be epistemologically justified. In a similar vein, the Indian philosopher B.K. Matilal drew on the Navya-Nyāya fallibilist tradition to respond to the Gettier problem. Nyaya theory distinguishes between know p and know that one knows p—these are different events, with different causal conditions. The second level is a sort of implicit inference that usually follows immediately the episode of knowing p (knowledge simpliciter). The Gettier case is examined by referring to a view of Gangesha Upadhyaya (late 12th century), who takes any true belief to be knowledge; thus a true belief acquired through a wrong route may just be regarded as knowledge simpliciter on this view. The question of justification arises only at the second level, when one considers the knowledge-hood of the acquired belief. Initially, there is lack of uncertainty, so it becomes a true belief. But at the very next moment, when the hearer is about to embark upon the venture of knowing whether he knows p, doubts may arise. "If, in some Gettier-like cases, I am wrong in my inference about the knowledge-hood of the given occurrent belief (for the evidence may be pseudo-evidence), then I am mistaken about the truth of my belief—and this is in accordance with Nyaya fallibilism: not all knowledge-claims can be sustained."
Robert Nozick has offered a definition of knowledge according to which S knows that P if and only if: P is true; S believes that P; if P were false, S would not believe that P; if P were true, S would believe that P.Nozick argues that the third of these conditions serves to address cases of the sort described by Gettier. Nozick further claims this condition addresses a case of the sort described by D.M. Armstrong: A father believes his daughter is innocent of committing a particular crime, both because of faith in his baby girl and (now) because he has seen presented in the courtroom a conclusive demonstration of his daughter's innocence. His belief via the method of the courtroom satisfies the four subjunctive conditions, but his faith-based belief does not. If his daughter were guilty, he would still believe her innocence, on the basis of faith in his daughter; this would violate the third condition. The British philosopher Simon Blackburn has criticized this formulation by suggesting that we do not want to accept as knowledge beliefs which, while they "track the truth" (as Nozick's account requires), are not held for appropriate reasons. He says that "we do not want to award the title of knowing something to someone who is only meeting the conditions through a defect, flaw, or failure, compared with someone else who is not meeting the conditions." In addition to this, externalist accounts of knowledge, such as Nozick's, are often forced to reject closure in cases where it is intuitively valid. An account similar to Nozick's has also been offered by Fred Dretske, although his view focuses more on relevant alternatives that might have obtained if things had turned out differently. Views of both the Nozick variety and the Dretske variety have faced serious problems suggested by Saul Kripke.
Timothy Williamson has advanced a theory of knowledge according to which knowledge is not justified true belief plus some extra condition(s), but primary. In his book Knowledge and its Limits, Williamson argues that the concept of knowledge cannot be broken down into a set of other concepts through analysis—instead, it is sui generis. Thus, according to Williamson, justification, truth, and belief are necessary but not sufficient for knowledge. Williamson is also known for being one of the only philosophers who take knowledge to be a mental state; most epistemologists assert that belief (as opposed to knowledge) is a mental state. As such, Williamson's claim has been seen to be highly counterintuitive.
In an earlier paper that predates his development of reliabilism, Alvin Goldman writes in his "Causal Theory of Knowing" that knowledge requires a causal link between the truth of a proposition and the belief in that proposition. A similar view has also been defended by Hilary Kornblith in Knowledge and its Place in Nature, although his view is meant to capture an empirical scientific conception of knowledge, not an analysis of the everyday concept "knowledge". Kornblith, in turn, takes himself to be elaborating on the naturalized epistemology framework first suggested by W.V.O. Quine.
We generally assume that knowledge is more valuable than mere true belief. If so, what is the explanation? A formulation of the value problem in epistemology first occurs in Plato's Meno. Socrates points out to Meno that a man who knew the way to Larissa could lead others there correctly. But so, too, could a man who had true beliefs about how to get there, even if he had not gone there or had any knowledge of Larissa. Socrates says that it seems that both knowledge and true opinion can guide action. Meno then wonders why knowledge is valued more than true belief and why knowledge and true belief are different. Socrates responds that knowledge is more valuable than mere true belief because it is tethered or justified. Justification, or working out the reason for a true belief, locks down true belief.The problem is to identify what (if anything) makes knowledge more valuable than mere true belief, or that makes knowledge more valuable than a mere minimal conjunction of its components, such as justification, safety, sensitivity, statistical likelihood, and anti-Gettier conditions, on a particular analysis of knowledge that conceives of knowledge as divided into components (to which knowledge-first epistemological theories, which posit knowledge as fundamental, are notable exceptions). The value problem re-emerged in the philosophical literature on epistemology in the twenty-first century following the rise of virtue epistemology in the 1980s, partly because of the obvious link to the concept of value in ethics.
In contemporary philosophy, epistemologists including Ernest Sosa, John Greco, Jonathan Kvanvig, Linda Zagzebski, and Duncan Pritchard have defended virtue epistemology as a solution to the value problem. They argue that epistemology should also evaluate the "properties" of people as epistemic agents (i.e. intellectual virtues), rather than merely the properties of propositions and propositional mental attitudes. The value problem has been presented as an argument against epistemic reliabilism by Linda Zagzebski, Wayne Riggs, and Richard Swinburne, among others. Zagzebski analogizes the value of knowledge to the value of espresso produced by an espresso maker: "The liquid in this cup is not improved by the fact that it comes from a reliable espresso maker. If the espresso tastes good, it makes no difference if it comes from an unreliable machine." For Zagzebski, the value of knowledge deflates to the value of mere true belief. She assumes that reliability in itself has no value or disvalue, but Goldman and Olsson disagree. They point out that Zagzebski's conclusion rests on the assumption of veritism: all that matters is the acquisition of true belief. To the contrary, they argue that a reliable process for acquiring a true belief adds value to the mere true belief by making it more likely that future beliefs of a similar kind will be true. By analogy, having a reliable espresso maker that produced a good cup of espresso would be more valuable than having an unreliable one that luckily produced a good cup because the reliable one would more likely produce good future cups compared to the unreliable one. The value problem is important to assessing the adequacy of theories of knowledge that conceive of knowledge as consisting of true belief and other components. According to Kvanvig, an adequate account of knowledge should resist counterexamples and allow an explanation of the value of knowledge over mere true belief. Should a theory of knowledge fail to do so, it would prove inadequate.One of the more influential responses to the problem is that knowledge is not particularly valuable and is not what ought to be the main focus of epistemology. Instead, epistemologists ought to focus on other mental states, such as understanding. Advocates of virtue epistemology have argued that the value of knowledge comes from an internal relationship between the knower and the mental state of believing.
There are many proposed sources of knowledge and justified belief which we take to be actual sources of knowledge in our everyday lives. Some of the most commonly discussed include perception, reason, memory, and testimony.
As mentioned above, epistemologists draw a distinction between what can be known a priori (independently of experience) and what can only be known a posteriori (through experience). Much of what we call a priori knowledge is thought to be attained through reason alone, as featured prominently in rationalism. This might also include a non-rational faculty of intuition, as defended by proponents of innatism. In contrast, a posteriori knowledge is derived entirely through experience or as a result of experience, as emphasized in empiricism. This also includes cases where knowledge can be traced back to an earlier experience, as in memory or testimony.A way to look at the difference between the two is through an example. Bruce Russell gives two propositions in which the reader decides which one he believes more. Option A: All crows are birds. Option B: All crows are black. If you believe option A, then you are a priori justified in believing it because you don't have to see a crow to know it's a bird. If you believe in option B, then you are posteriori justified to believe it because you have seen many crows therefore knowing they are black. He goes on to say that it doesn't matter if the statement is true or not, only that if you believe in one or the other that matters.The idea of a priori knowledge is that it is based on intuition or rational insights. Laurence BonJour says in his article "The Structure of Empirical Knowledge", that a "rational insight is an immediate, non-inferential grasp, apprehension or 'seeing' that some proposition is necessarily true." (3) Going back to the crow example, by Laurence BonJour's definition the reason you would believe in option A is because you have an immediate knowledge that a crow is a bird, without ever experiencing one. Evolutionary psychology takes a novel approach to the problem. It says that there is an innate predisposition for certain types of learning. "Only small parts of the brain resemble a tabula rasa; this is true even for human beings. The remainder is more like an exposed negative waiting to be dipped into a developer fluid".
Immanuel Kant, in his Critique of Pure Reason, drew a distinction between "analytic" and "synthetic" propositions. He contended that some propositions are such that we can know they are true just by understanding their meaning. For example, consider, "My father's brother is my uncle." We can know it is true solely by virtue of our understanding in what its terms mean. Philosophers call such propositions analytic". Synthetic propositions, on the other hand, have distinct subjects and predicates. An example would be, "My father's brother has black hair." Kant stated that all mathematical and scientific statements are analytic priori propositions because they are necessarily true but our knowledge about the attributes of the mathematical or physical subjects we can only get by logical inference. While this distinction is first and foremost about meaning and is therefore most relevant to the philosophy of language, the distinction has significant epistemological consequences, seen most prominently in the works of the logical positivists. In particular, if the set of propositions which can only be known a posteriori is coextensive with the set of propositions which are synthetically true, and if the set of propositions which can be known a priori is coextensive with the set of propositions which are analytically true (or in other words, which are true by definition), then there can only be two kinds of successful inquiry: Logico-mathematical inquiry, which investigates what is true by definition, and empirical inquiry, which investigates what is true in the world. Most notably, this would exclude the possibility that branches of philosophy like metaphysics could ever provide informative accounts of what actually exists.The American philosopher Willard Van Orman Quine, in his paper "Two Dogmas of Empiricism", famously challenged the analytic-synthetic distinction, arguing that the boundary between the two is too blurry to provide a clear division between propositions that are true by definition and propositions that are not. While some contemporary philosophers take themselves to have offered more sustainable accounts of the distinction that are not vulnerable to Quine's objections, there is no consensus about whether or not these succeed.
Science is often considered to be a refined, formalized, systematic, institutionalized form of the pursuit and acquisition of empirical knowledge. As such, the philosophy of science may be viewed variously as an application of the principles of epistemology or as a foundation for epistemological inquiry.
The regress problem (also known as Agrippa's Trilemma) is the problem of providing a complete logical foundation for human knowledge. The traditional way of supporting a rational argument is to appeal to other rational arguments, typically using chains of reason and rules of logic. A classic example that goes back to Aristotle is deducing that Socrates is mortal. We have a logical rule that says All humans are mortal and an assertion that Socrates is human and we deduce that Socrates is mortal. In this example how do we know that Socrates is human? Presumably we apply other rules such as: All born from human females are human. Which then leaves open the question how do we know that all born from humans are human? This is the regress problem: how can we eventually terminate a logical argument with some statement(s) that do not require further justification but can still be considered rational and justified? As John Pollock stated: ... to justify a belief one must appeal to a further justified belief. This means that one of two things can be the case. Either there are some beliefs that we can be justified for holding, without being able to justify them on the basis of any other belief, or else for each justified belief there is an infinite regress of (potential) justification [the nebula theory]. On this theory there is no rock bottom of justification. Justification just meanders in and out through our network of beliefs, stopping nowhere. The apparent impossibility of completing an infinite chain of reasoning is thought by some to support skepticism. It is also the impetus for Descartes' famous dictum: I think, therefore I am. Descartes was looking for some logical statement that could be true without appeal to other statements.
Many epistemologists studying justification have attempted to argue for various types of chains of reasoning that can escape the regress problem.
Foundationalists respond to the regress problem by asserting that certain "foundations" or "basic beliefs" support other beliefs but do not themselves require justification from other beliefs. These beliefs might be justified because they are self-evident, infallible, or derive from reliable cognitive mechanisms. Perception, memory, and a priori intuition are often considered possible examples of basic beliefs. The chief criticism of foundationalism is that if a belief is not supported by other beliefs, accepting it may be arbitrary or unjustified.
Another response to the regress problem is coherentism, which is the rejection of the assumption that the regress proceeds according to a pattern of linear justification. To avoid the charge of circularity, coherentists hold that an individual belief is justified circularly by the way it fits together (coheres) with the rest of the belief system of which it is a part. This theory has the advantage of avoiding the infinite regress without claiming special, possibly arbitrary status for some particular class of beliefs. Yet, since a system can be coherent while also being wrong, coherentists face the difficulty of ensuring that the whole system corresponds to reality. Additionally, most logicians agree that any argument that is circular is, at best, only trivially valid. That is, to be illuminating, arguments must operate with information from multiple premises, not simply conclude by reiterating a premise. Nigel Warburton writes in Thinking from A to Z that "[c]ircular arguments are not invalid; in other words, from a logical point of view there is nothing intrinsically wrong with them. However, they are, when viciously circular, spectacularly uninformative."
An alternative resolution to the regress problem is known as "infinitism". Infinitists take the infinite series to be merely potential, in the sense that an individual may have indefinitely many reasons available to them, without having consciously thought through all of these reasons when the need arises. This position is motivated in part by the desire to avoid what is seen as the arbitrariness and circularity of its chief competitors, foundationalism and coherentism. The most prominent defense of infinitism has been given by Peter Klein.
An intermediate position, known as "foundherentism", is advanced by Susan Haack. Foundherentism is meant to unify foundationalism and coherentism. Haack explains the view by using a crossword puzzle as an analogy. Whereas, for example, infinitists regard the regress of reasons as taking the form of a single line that continues indefinitely, Haack has argued that chains of properly justified beliefs look more like a crossword puzzle, with various different lines mutually supporting each other. Thus, Haack's view leaves room for both chains of beliefs that are "vertical" (terminating in foundational beliefs) and chains that are "horizontal" (deriving their justification from coherence with beliefs that are also members of foundationalist chains of belief).
Epistemic skepticism questions whether knowledge is possible at all. Generally speaking, skeptics argue that knowledge requires certainty, and that most or all of our beliefs are fallible (meaning that our grounds for holding them always, or almost always, fall short of certainty), which would together entail that knowledge is always or almost always impossible for us. Characterizing knowledge as strong or weak is dependent on a person's viewpoint and their characterization of knowledge. Much of modern epistemology is derived from attempts to better understand and address philosophical skepticism.
One of the oldest forms of epistemic skepticism can be found in Agrippa's trilemma (named after the Pyrrhonist philosopher Agrippa the Skeptic) which demonstrates that certainty can not be achieved with regard to beliefs. Pyrrhonism dates back to Pyrrho of Elis from the 4th century BCE, although most of what we know about Pyrrhonism today is from the surviving works of Sextus Empiricus. Pyrrhonists claim that for any argument for a non-evident proposition, an equally convincing argument for a contradictory proposition can be produced. Pyrrhonists do not dogmatically deny the possibility of knowledge, but instead point out that beliefs about non-evident matters cannot be substantiated.
The Cartesian evil demon problem, first raised by René Descartes, supposes that our sensory impressions may be controlled by some external power rather than the result of ordinary veridical perception. In such a scenario, nothing we sense would actually exist, but would instead be mere illusion. As a result, we would never be able to know anything about the world, since we would be systematically deceived about everything. The conclusion often drawn from evil demon skepticism is that even if we are not completely deceived, all of the information provided by our senses is still compatible with skeptical scenarios in which we are completely deceived, and that we must therefore either be able to exclude the possibility of deception or else must deny the possibility of infallible knowledge (that is, knowledge which is completely certain) beyond our immediate sensory impressions. While the view that no beliefs are beyond doubt other than our immediate sensory impressions is often ascribed to Descartes, he in fact thought that we can exclude the possibility that we are systematically deceived, although his reasons for thinking this are based on a highly contentious ontological argument for the existence of a benevolent God who would not allow such deception to occur.
Epistemological skepticism can be classified as either "mitigated" or "unmitigated" skepticism. Mitigated skepticism rejects "strong" or "strict" knowledge claims but does approve weaker ones, which can be considered "virtual knowledge", but only with regard to justified beliefs. Unmitigated skepticism rejects claims of both virtual and strong knowledge. Characterizing knowledge as strong, weak, virtual or genuine can be determined differently depending on a person's viewpoint as well as their characterization of knowledge. Some of the most notable attempts to respond to unmitigated skepticism include direct realism, disjunctivism, common sense philosophy, pragmatism, fideism, and fictionalism.
Empiricism is a view in the theory of knowledge which focuses on the role of experience, especially experience based on perceptual observations by the senses, in the generation of knowledge. Certain forms exempt disciplines such as mathematics and logic from these requirements.There are many variants of empiricism, including British empiricism, logical empiricism, phenomenalism, and some versions of common sense philosophy. Most forms of empiricism give epistemologically privileged status to sensory impressions or sense data, although this plays out very differently in different cases. Some of the most famous historical empiricists include John Locke, David Hume, George Berkeley, Francis Bacon, John Stuart Mill, Rudolf Carnap, and Bertrand Russell.
Rationalism is the epistemological view that reason is the chief source of knowledge and the main determinant of what constitutes knowledge. More broadly, it can also refer to any view which appeals to reason as a source of knowledge or justification. Rationalism is one of the two classical views in epistemology, the other being empiricism. Rationalists claim that the mind, through the use of reason, can directly grasp certain truths in various domains, including logic, mathematics, ethics, and metaphysics. Rationalist views can range from modest views in mathematics and logic (such as that of Gottlob Frege) to ambitious metaphysical systems (such as that of Baruch Spinoza). Some of the most famous rationalists include Plato, René Descartes, Baruch Spinoza, and Gottfried Leibniz.
Skepticism is a position that questions the possibility of human knowledge, either in particular domains or on a general level. Skepticism does not refer to any one specific school of philosophy, but is rather a thread that runs through many epistemological debates. Ancient Greek skepticism began during the Hellenistic period in philosophy, which featured both Pyrrhonism (notably defended by Pyrrho and Sextus Empiricus) and Academic skepticism (notably defended by Arcesilaus and Carneades). Among ancient Indian philosophers, skepticism was notably defended by the Ajñana school and in the Buddhist Madhyamika tradition. In modern philosophy, René Descartes' famous inquiry into mind and body began as an exercise in skepticism, in which he started by trying to doubt all purported cases of knowledge in order to search for something that was known with absolute certainty.
Pragmatism is an empiricist epistemology formulated by Charles Sanders Peirce, William James, and John Dewey, which understands truth as that which is practically applicable in the world. Pragmatists often treat "truth" as the final outcome of ideal scientific inquiry, meaning that something cannot be true unless it is potentially observable. Peirce formulates the maxim: 'Consider what effects, that might conceivably have practical bearings, we conceive the object of our conception to have. Then, our conception of these effects is the whole of our conception of the object.' This suggests that we are to analyse ideas and objects in the world for their practical value. This is in contrast to any correspondence theory of truth that holds that what is true is what corresponds to an external reality. William James suggests that through a pragmatist epistemology, theories "become instruments, not answers to enigmas in which we can rest."Contemporary versions of pragmatism have been most notably developed by Richard Rorty and Hilary Putnam. Rorty proposed that values were historically contingent and dependent upon their utility within a given historical period, Contemporary philosophers working in pragmatism are called neopragmatists, and also include Nicholas Rescher, Robert Brandom, Susan Haack, and Cornel West.
In certain respects an intellectual descendant of pragmatism, naturalized epistemology considers the evolutionary role of knowledge for agents living and evolving in the world. It de-emphasizes the questions around justification and truth, and instead asks, empirically, how reliable beliefs are formed and the role that evolution played in the development of such processes. It suggests a more empirical approach to the subject as a whole, leaving behind philosophical definitions and consistency arguments, and instead using psychological methods to study and understand how "knowledge" is actually formed and is used in the natural world. As such, it does not attempt to answer the analytic questions of traditional epistemology, but rather replace them with new empirical ones.Naturalized epistemology was first proposed in "Epistemology Naturalized", a seminal paper by W.V.O. Quine. A less radical view has been defended by Hilary Kornblith in Knowledge and its Place in Nature, in which he seeks to turn epistemology towards empirical investigation without completely abandoning traditional epistemic concepts.
Feminist epistemology is a subfield of epistemology which applies feminist theory to epistemological questions. It began to emerge as a distinct subfield in the 20th century. Prominent feminist epistemologists include Miranda Fricker (who developed the concept of epistemic injustice), Donna Haraway (who first proposed the concept of situated knowledge), Sandra Harding, and Elizabeth Anderson. Harding proposes that feminist epistemology can be broken into three distinct categories: Feminist empiricism, standpoint epistemology, and postmodern epistemology. Feminist epistemology has also played a significant role in the development of many debates in social epistemology.
Epistemic relativism is the view that what is true, rational, or justified for one person need not be true, rational, or justified for another person. Epistemic relativists therefore assert that while there are relative facts about truth, rationality, justification, and so on, there is no perspective-independent fact of the matter. Note that this is distinct from epistemic contextualism, which holds that the meaning of epistemic terms vary across contexts (e.g. "I know" might mean something different in everyday contexts and skeptical contexts). In contrast, epistemic relativism holds that the relevant facts vary, not just linguistic meaning. Relativism about truth may also be a form of ontological relativism, insofar as relativists about truth hold that facts about what exists vary based on perspective.
Constructivism is a view in philosophy according to which all "knowledge is a compilation of human-made constructions", "not the neutral discovery of an objective truth". Whereas objectivism is concerned with the "object of our knowledge", constructivism emphasizes "how we construct knowledge". Constructivism proposes new definitions for knowledge and truth, which emphasize intersubjectivity rather than objectivity, and viability rather than truth. The constructivist point of view is in many ways comparable to certain forms of pragmatism.
Idealism is a broad term referring to both an ontological view about the world being in some sense mind-dependent and a corresponding epistemological view that everything we know can be reduced to mental phenomena. First and foremost, "idealism" is a metaphysical doctrine. As an epistemological doctrine, idealism shares a great deal with both empiricism and rationalism. Some of the most famous empiricists have been classified as idealists (particularly Berkeley), and yet the subjectivism inherent to idealism also resembles that of Descartes in many respects. Many idealists believe that knowledge is primarily (at least in some areas) acquired by a priori processes, or that it is innate—for example, in the form of concepts not derived from experience. The relevant theoretical concepts may purportedly be part of the structure of the human mind (as in Kant's theory of transcendental idealism), or they may be said to exist independently of the mind (as in Plato's theory of Forms). Some of the most famous forms of idealism include transcendental idealism (developed by Immanuel Kant), subjective idealism (developed by George Berkeley), and absolute idealism (developed by Georg Wilhelm Friedrich Hegel and Friedrich Schelling).
Indian schools of philosophy, such as the Hindu Nyaya and Carvaka schools, and the Jain and Buddhist philosophical schools, developed an epistemological tradition independently of the Western philosophical tradition called "pramana". Pramana can be translated as "instrument of knowledge" and refers to various means or sources of knowledge that Indian philosophers held to be reliable. Each school of Indian philosophy had their own theories about which pramanas were valid means to knowledge and which were unreliable (and why). A Vedic text, Taittirīya Āraṇyaka (c. 9th–6th centuries BCE), lists "four means of attaining correct knowledge": smṛti ("tradition" or "scripture"), pratyakṣa ("perception"), aitihya ("communication by one who is expert", or "tradition"), and anumāna ("reasoning" or "inference").In the Indian traditions, the most widely discussed pramanas are: Pratyakṣa (perception), Anumāṇa (inference), Upamāṇa (comparison and analogy), Arthāpatti (postulation, derivation from circumstances), Anupalabdi (non-perception, negative/cognitive proof) and Śabda (word, testimony of past or present reliable experts). While the Nyaya school (beginning with the Nyāya Sūtras of Gotama, between 6th-century BCE and 2nd-century CE) were a proponent of realism and supported four pramanas (perception, inference, comparison/analogy and testimony), the Buddhist epistemologists (Dignaga and Dharmakirti) generally accepted only perception and inference. The Carvaka school of materialists only accepted the pramana of perception, and hence were among the first empiricists in the Indian traditions. Another school, the Ajñana, included notable proponents of philosophical skepticism. The theory of knowledge of the Buddha in the early Buddhist texts has been interpreted as a form of pragmatism as well as a form of correspondence theory. Likewise, the Buddhist philosopher Dharmakirti has been interpreted both as holding a form of pragmatism or correspondence theory for his view that what is true is what has effective power (arthakriya). The Buddhist Madhyamika school's theory of emptiness (shunyata) meanwhile has been interpreted as a form of philosophical skepticism.The main contribution to epistemology by the Jains has been their theory of "many sided-ness" or "multi-perspectivism" (Anekantavada), which says that since the world is multifaceted, any single viewpoint is limited (naya – a partial standpoint). This has been interpreted as a kind of pluralism or perspectivism. According to Jain epistemology, none of the pramanas gives absolute or perfect knowledge since they are each limited points of view.
Social epistemology deals with questions about knowledge in contexts where our knowledge attributions cannot be explained by simply examining individuals in isolation from one another, meaning that the scope of our knowledge attributions must be widened to include broader social contexts. It also explores the ways in which interpersonal beliefs can be justified in social contexts. The most common topics discussed in contemporary social epistemology are testimony, which deals with the conditions under which a belief "x is true" which resulted from being told "x is true" constitutes knowledge; peer disagreement, which deals with when and how I should revise my beliefs in light of other people holding beliefs that contradict mine; and group epistemology, which deals with what it means to attribute knowledge to groups rather than individuals, and when group knowledge attributions are appropriate.
Formal epistemology uses formal tools and methods from decision theory, logic, probability theory and computability theory to model and reason about issues of epistemological interest. Work in this area spans several academic fields, including philosophy, computer science, economics, and statistics. The focus of formal epistemology has tended to differ somewhat from that of traditional epistemology, with topics like uncertainty, induction, and belief revision garnering more attention than the analysis of knowledge, skepticism, and issues with justification.
Metaepistemology is the metaphilosophical study of the methods, aims, and subject matter of epistemology. In general, metaepistemology aims to better understand our first-order epistemological inquiry. Some goals of metaepistemology are identifying inaccurate assumptions made in epistemological debates and determining whether the questions asked in mainline epistemology are the right epistemological questions to be asking.
Works cited
Stanford Encyclopedia of Philosophy articlesEpistemology by Matthias Steup. Bayesian Epistemology by William Talbott. Evolutionary Epistemology by Michael Bradie & William Harms. Feminist Epistemology and Philosophy of Science by Elizabeth Anderson. Naturalized Epistemology by Richard Feldman. Social Epistemology by Alvin Goldman. Virtue Epistemology by John Greco. Knowledge How by Jeremy Fantl.Internet Encyclopedia of Philosophy articles"Epistemology". Internet Encyclopedia of Philosophy. "Coherentism". Internet Encyclopedia of Philosophy. "Contextualism in Epistemology". Internet Encyclopedia of Philosophy. "Epistemic Circularity". Internet Encyclopedia of Philosophy. "Epistemic Justification". Internet Encyclopedia of Philosophy. "Epistemology of Perception". Internet Encyclopedia of Philosophy. "Ethnoepistemology". Internet Encyclopedia of Philosophy. "Evolutionary Epistemology". Internet Encyclopedia of Philosophy. "Fallibilism". Internet Encyclopedia of Philosophy. "Feminist Epistemology". Internet Encyclopedia of Philosophy. "Infinitism in Epistemology". Internet Encyclopedia of Philosophy. "Internalism and Externalism in Epistemology". Internet Encyclopedia of Philosophy. "Moral Epistemology". Internet Encyclopedia of Philosophy. "Naturalistic Epistemology". Internet Encyclopedia of Philosophy. "Virtue Epistemology". Internet Encyclopedia of Philosophy. "Understanding in Epistemology". Internet Encyclopedia of Philosophy.Encyclopedia BritannicaEpistemology by Avrum Stroll and A.P. MartinichOther linksThe London Philosophy Study Guide offers many suggestions on what to read, depending on the student's familiarity with the subject: Epistemology & Methodology Epistemology at PhilPapers Knowledge-How at Philpapers Epistemology at the Indiana Philosophy Ontology Project What Is Epistemology? – a brief introduction to the topic by Keith DeRose. Justified True Belief and Critical Rationalism by Mathew Toll Epistemology Introduction, Part 1 and Part 2 by Paul Newall at the Galilean Library. Teaching Theory of Knowledge (1986) – Marjorie Clay (ed.), an electronic publication from The Council for Philosophical Studies. An Introduction to Epistemology by Paul Newall, aimed at beginners. A short film about epistemology, for beginners on YouTube
Knowledge management (KM) is the process of creating, sharing, using and managing the knowledge and information of an organization. It refers to a multidisciplinary approach to achieve organisational objectives by making the best use of knowledge.An established discipline since 1991, KM includes courses taught in the fields of business administration, information systems, management, library, and information sciences. Other fields may contribute to KM research, including information and media, computer science, public health and public policy. Several universities offer dedicated master's degrees in knowledge management. Many large companies, public institutions and non-profit organisations have resources dedicated to internal KM efforts, often as a part of their business strategy, IT, or human resource management departments. Several consulting companies provide advice regarding KM to these organizations.Knowledge management efforts typically focus on organisational objectives such as improved performance, competitive advantage, innovation, the sharing of lessons learned, integration and continuous improvement of the organisation. These efforts overlap with organisational learning and may be distinguished from that by a greater focus on the management of knowledge as a strategic asset and on encouraging the sharing of knowledge. KM is an enabler of organizational learning.
Knowledge management efforts have a long history, including on-the-job discussions, formal apprenticeship, discussion forums, corporate libraries, professional training, and mentoring programs. With increased use of computers in the second half of the 20th century, specific adaptations of technologies such as knowledge bases, expert systems, information repositories, group decision support systems, intranets, and computer-supported cooperative work have been introduced to further enhance such efforts.In 1999, the term personal knowledge management was introduced; it refers to the management of knowledge at the individual level.In the enterprise, early collections of case studies recognised the importance of knowledge management dimensions of strategy, process and measurement. Key lessons learned include people and the cultural norms which influence their behaviors are the most critical resources for successful knowledge creation, dissemination and application; cognitive, social and organisational learning processes are essential to the success of a knowledge management strategy; and measurement, benchmarking and incentives are essential to accelerate the learning process and to drive cultural change. In short, knowledge management programs can yield impressive benefits to individuals and organisations if they are purposeful, concrete and action-orientated.
KM emerged as a scientific discipline in the early 1990s. It was initially supported by individual practitioners, when Skandia hired Leif Edvinsson of Sweden as the world's first Chief Knowledge Officer (CKO). Hubert Saint-Onge (formerly of CIBC, Canada), started investigating KM long before that. The objective of CKOs is to manage and maximise the intangible assets of their organizations. Gradually, CKOs became interested in practical and theoretical aspects of KM, and the new research field was formed. The KM idea has been taken up by academics, such as Ikujiro Nonaka (Hitotsubashi University), Hirotaka Takeuchi (Hitotsubashi University), Thomas H. Davenport (Babson College) and Baruch Lev (New York University).In 2001, Thomas A. Stewart, former editor at Fortune magazine and subsequently the editor of Harvard Business Review, published a cover story highlighting the importance of intellectual capital in organizations. The KM discipline has been gradually moving towards academic maturity. First, is a trend toward higher cooperation among academics; single-author publications are less common. Second, the role of practitioners has changed. Their contribution to academic research declined from 30% of overall contributions up to 2002, to only 10% by 2009. Third, the number of academic knowledge management journals has been steadily growing, currently reaching 27 outlets.Multiple KM disciplines exist; approaches vary by author and school. As the discipline matured, academic debates increased regarding theory and practice, including: Techno-centric with a focus on technology, ideally those that enhance knowledge sharing and creation. Organisational with a focus on how an organisation can be designed to facilitate knowledge processes best. Ecological with a focus on the interaction of people, identity, knowledge, and environmental factors as a complex adaptive system akin to a natural ecosystem.Regardless of the school of thought, core components of KM roughly include people/culture, processes/structure and technology. The details depend on the perspective. KM perspectives include: community of practice social network analysis intellectual capital information theory complexity science constructivismThe practical relevance of academic research in KM has been questioned with action research suggested as having more relevance and the need to translate the findings presented in academic journals to a practice.
Different frameworks for distinguishing between different 'types of' knowledge exist. One proposed framework for categorising the dimensions of knowledge distinguishes tacit knowledge and explicit knowledge. Tacit knowledge represents internalised knowledge that an individual may not be consciously aware of, such as to accomplish particular tasks. At the opposite end of the spectrum, explicit knowledge represents knowledge that the individual holds consciously in mental focus, in a form that can easily be communicated to others. Ikujiro Nonaka proposed a model (SECI, for Socialisation, Externalisation, Combination, Internalisation) which considers a spiraling interaction between explicit knowledge and tacit knowledge. In this model, knowledge follows a cycle in which implicit knowledge is 'extracted' to become explicit knowledge, and explicit knowledge is 're-internalised' into implicit knowledge.Hayes and Walsham (2003) describe knowledge and knowledge management as two different perspectives. The content perspective suggests that knowledge is easily stored; because it may be codified, while the relational perspective recognises the contextual and relational aspects of knowledge which can make knowledge difficult to share outside the specific context in which it is developed.Early research suggested that KM needs to convert internalised tacit knowledge into explicit knowledge to share it, and the same effort must permit individuals to internalise and make personally meaningful any codified knowledge retrieved from the KM effort.Subsequent research suggested that a distinction between tacit knowledge and explicit knowledge represented an oversimplification and that the notion of explicit knowledge is self-contradictory. Specifically, for knowledge to be made explicit, it must be translated into information (i.e., symbols outside our heads). More recently, together with Georg von Krogh and Sven Voelpel, Nonaka returned to his earlier work in an attempt to move the debate about knowledge conversion forward.A second proposed framework for categorising knowledge dimensions distinguishes embedded knowledge of a system outside a human individual (e.g., an information system may have knowledge embedded into its design) from embodied knowledge representing a learned capability of a human body's nervous and endocrine systems.A third proposed framework distinguishes between the exploratory creation of "new knowledge" (i.e., innovation) vs. the transfer or exploitation of "established knowledge" within a group, organisation, or community. Collaborative environments such as communities of practice or the use of social computing tools can be used for both knowledge creation and transfer.
Knowledge may be accessed at three stages: before, during, or after KM-related activities. Organisations have tried knowledge capture incentives, including making content submission mandatory and incorporating rewards into performance measurement plans. Considerable controversy exists over whether such incentives work and no consensus has emerged.One strategy to KM involves actively managing knowledge (push strategy). In such an instance, individuals strive to explicitly encode their knowledge into a shared knowledge repository, such as a database, as well as retrieving knowledge they need that other individuals have provided (codification).Another strategy involves individuals making knowledge requests of experts associated with a particular subject on an ad hoc basis (pull strategy). In such an instance, expert individual(s) provide insights to requestor (personalisation).Hansen et al. defined the two strategies. Codification focuses on collecting and storing codified knowledge in electronic databases to make it accessible. Codification can therefore refer to both tacit and explicit knowledge. In contrast, personalisation encourages individuals to share their knowledge directly. Information technology plays a less important role, as it only facilitates communication and knowledge sharing. Other knowledge management strategies and instruments for companies include: Knowledge sharing (fostering a culture that encourages the sharing of information, based on the concept that knowledge is not irrevocable and should be shared and updated to remain relevant) Make knowledge-sharing a key role in employees' job description Inter-project knowledge transfer Intra-organisational knowledge sharing Inter-organisational knowledge sharing Knowledge retention also known as Knowledge Continuation: activiities addressing the challenge of knowledge loss as a result of people leaving Mapping knowledge competencies, roles and identifying current or future predicted gaps. Defining for each chosen role the main knowledge that should be retained, and building rituals in which the knowledge is documented or transferred on, from the day they start their job. Transfer of knowledge and information prior to employee departure by means of sharing documents, shadowing, mentoring, and more, Proximity & architecture (the physical situation of employees can be either conducive or obstructive to knowledge sharing) Storytelling (as a means of transferring tacit knowledge) Cross-project learning After-action reviews Knowledge mapping (a map of knowledge repositories within a company accessible by all) Communities of practice Expert directories (to enable knowledge seeker to reach to the experts) Expert systems (knowledge seeker responds to one or more specific questions to reach knowledge in a repository) Best practice transfer Knowledge fairs Competency-based management (systematic evaluation and planning of knowledge related competences of individual organisation members) Master–apprentice relationship, Mentor-mentee relationship, job shadowing Collaborative software technologies (wikis, shared bookmarking, blogs, social software, etc.) Knowledge repositories (databases, bookmarking engines, etc.) Measuring and reporting intellectual capital (a way of making explicit knowledge for companies) Knowledge brokers (some organisational members take on responsibility for a specific "field" and act as first reference on a specific subject) Knowledge farming (Using note-taking software to cultivate a knowledge graph, part of Knowledge Agriculture)
Multiple motivations lead organisations to undertake KM. Typical considerations include: Making available increased knowledge content in the development and provision of products and services Achieving shorter development cycles Facilitating and managing innovation and organisational learning Leveraging expertises across the organisation Increasing network connectivity between internal and external individuals Managing business environments and allowing employees to obtain relevant insights and ideas appropriate to their work Solving intractable or wicked problems Managing intellectual capital and assets in the workforce (such as the expertise and know-how possessed by key individuals or stored in repositories)
Knowledge management (KM) technology can be categorised: Groupware—Software that facilitates collaboration and sharing of organisational information. Such applications provide tools for threaded discussions, document sharing, organisation-wide uniform email, and other collaboration-related features. Workflow systems—Systems that allow the representation of processes associated with the creation, use and maintenance of organisational knowledge, such as the process to create and utilise forms and documents. Content management and document management systems—Software systems that automate the process of creating web content and/or documents. Roles such as editors, graphic designers, writers and producers can be explicitly modeled along with the tasks in the process and validation criteria. Commercial vendors started either to support documents or to support web content but as the Internet grew these functions merged and vendors now perform both functions. Enterprise portals—Software that aggregates information across the entire organisation or for groups such as project teams. eLearning—Software that enables organisations to create customised training and education. This can include lesson plans, monitoring progress and online classes. Planning and scheduling software—Software that automates schedule creation and maintenance. The planning aspect can integrate with project management software. Telepresence—Software that enables individuals to have virtual "face-to-face" meetings without assembling at one location. Videoconferencing is the most obvious example. Semantic technology such as ontologies—Systems that encode meaning alongside data to give machines the ability to extract and infer information.These categories overlap. Workflow, for example, is a significant aspect of a content or document management systems, most of which have tools for developing enterprise portals.Proprietary KM technology products such as Lotus Notes defined proprietary formats for email, documents, forms, etc. The Internet drove most vendors to adopt Internet formats. Open-source and freeware tools for the creation of blogs and wikis now enable capabilities that used to require expensive commercial tools.KM is driving the adoption of tools that enable organisations to work at the semantic level, as part of the Semantic Web. Some commentators have argued that after many years the Semantic Web has failed to see widespread adoption, while other commentators have argued that it has been a success.
Just like knowledge transfer and knowledge sharing, the term "knowledge barriers" is not a uniformly defined term and differs in its meaning depending on the author. Knowledge barriers can be associated with high costs for both companies and individuals.
Knowledge management at Curlie
In Christianity, the word of knowledge is a spiritual gift listed in 1 Corinthians 12:8. It has been associated with the ability to teach the faith, but also with forms of revelation similar to prophecy. It is closely related to another spiritual gift, the word of wisdom.
Throughout church history, this gift has often been viewed as a teaching gift and connected with being able to understand scriptural truth. The Catholic Encyclopedia defines it as "the grace of propounding the Faith effectively, of bringing home to the minds and hearts of the listener with Divine persuasiveness, the hidden mysteries and the moral precepts of Christianity".Among Pentecostal and some Charismatic Christians, the word of knowledge is often defined as the ability of one person to know what God is currently doing or intends to do in the life of another person. It can also be defined as knowing the secrets of another person's heart. Through this revelation, it is believed that God encourages the faith of the believer to receive the healing or comfort that God offers. For example, in a public gathering, a person who claims to have the gift of knowledge may describe a medical problem (such as syphilis or trench foot) and ask anyone suffering from the described problem to identify themselves and receive an effective prayer for healing. According to this definition, the word of knowledge is a form of revelation similar to prophecy or a type of discernment.
Knowledge Network, also branded as British Columbia's Knowledge Network, is a Canadian publicly funded educational cable television network serving the province of British Columbia. It is owned by the Knowledge Network Corporation, a Crown corporation of the Government of British Columbia, and began broadcasting on January 12, 1981. Rudy Buttignol is president and CEO of British Columbia's Knowledge Network.Knowledge Network’s broadcast licence is for satellite-to-cable programming. The network is available on the Bell Satellite TV satellite service, on channel 268, on Shaw Direct channel 354, and on TELUS Optik TV channel 117. It has also been broadcast over-the-air in remote locations throughout British Columbia, with these repeater sites being operated by local volunteers in the few areas of the province where cable television is not available. The network used the callsign CKNO, although the transmitters were assigned numeric callsigns with the prefix "CH" due to being low-powered.Knowledge receives funding both from the British Columbia government and from public donations. The station supports lifelong learning for children and adults by providing quality, commercial-free programming through its broadcast channel, websites and apps. Knowledge Network also invests in documentaries and children’s programs produced by independent filmmakers and helps to develop skills within the independent production community.
Knowledge Network is British Columbia's public educational broadcaster and is required to be distributed as part of the basic cable service in British Columbia. When Knowledge first signed on in 1981, its broadcast schedule originally ran from 9:00 a.m. to 11:00 p.m. In later years, it broadcast from 7:00 a.m.-12 a.m. until July 2007, when programming hours were expanded to 6:00 a.m.-1:00 a.m. In late 2007, Knowledge Network began changing its logo from the green tree to its new wordmark logo, and as of June 2008 the green tree logo has been removed. The channel is currently a 24-hour broadcaster. The network obtains an average of 1.5 million viewers, or over one-third of British Columbians per week. Currently, within the province, the station holds the number one position on weekday mornings for kids age two to six. Also, it has experienced an increase in viewers age 29 to 49 for its prime time programs. In its programming, Knowledge Network covers a range of topics including politics, history and culture, arts and music, health, parenting, and science. It has a children's block, Knowledge Kids, that features characters Luna, Chip and Inkie.With funding from the provincial government and over 40,000 individual donors, Knowledge Network acquires and commissions over 750 hours of original programming per year. In 2011, Knowledge Network acquired Shaw Media's stake in the children's television service BBC Kids, and converted it into a commercial-free service.Knowledge Network launched an HD feed on September 25, 2013. It became available to customers of Shaw Cable and Shaw Direct on October 8, 2013.
List of programs broadcast by Knowledge Network
Official website "Knowledge Network’s CEO weighs in on the importance of public broadcasting." Global Civic Policy Society Westland – A television series on environmental issues aired on the Knowledge Network from 1984 to 2007 - UBC Library Digital Collections
The following outline is provided as an overview of and topical guide to knowledge: Knowledge – familiarity with someone or something, which can include facts, information, descriptions, and/or skills acquired through experience or education. It can refer to the theoretical or practical understanding of a subject. It can be implicit (as with practical skill or expertise) or explicit (as with the theoretical understanding of a subject); and it can be more or less formal or systematic.
A priori and a posteriori knowledge – these terms are used with respect to reasoning (epistemology) to distinguish necessary conclusions from first premises... A priori knowledge or justification – knowledge that is independent of experience, as with mathematics (3+2=5), tautologies ("All bachelors are unmarried"), and deduction from pure reason (e.g., ontological proofs). A posteriori knowledge or justification – knowledge dependent on experience or empirical evidence, as with most aspects of science and personal knowledge. Descriptive knowledge – also called declarative knowledge or propositional knowledge, it is the type of knowledge that is, by its very nature, expressed in declarative sentences or indicative propositions (e.g., "Albert is fat", or "It is raining"). This is distinguished from what is commonly known as "know-how" or procedural knowledge (the knowledge of how, and especially how best, to perform some task), and "knowing of", or knowledge by acquaintance (the knowledge of something's existence). Experience – knowledge or mastery of an event or subject gained through involvement in or exposure to it.Empirical evidence – also referred to as empirical data, empirical knowledge, and sense experience, it is a collective term for the knowledge or source of knowledge acquired by means of the senses, particularly by observation and experimentation. After Immanuel Kant, it is common in philosophy to call the knowledge thus gained a posteriori knowledge. This is contrasted with a priori knowledge, the knowledge accessible from pure reason alone. Experiential knowledge – Explicit knowledge – knowledge that can be readily articulated, codified, accessed and verbalized. It can be easily transmitted to others. Most forms of explicit knowledge can be stored in certain media. The information contained in encyclopedias and textbooks are good examples of explicit knowledge. Extelligence – term coined by Ian Stewart and Jack Cohen in their 1997 book Figments of Reality. They define it as the cultural capital that is available to us in the form of external media (e.g. tribal legends, folklore, nursery rhymes, books, videotapes, CD-ROMs, etc.). Knowledge by acquaintance – according to Bertrand Russell, knowledge by acquaintance is obtained through a direct causal (experience-based) interaction between a person and the object that person is perceiving. Sense-data from that object are the only things that people can ever become acquainted with; they can never truly KNOW the physical object itself. The distinction between "knowledge by acquaintance" and "knowledge by description" was promoted by Russell (notably in his 1905 paper On Denoting). Russell was extremely critical of the equivocal nature of the word "know", and believed that the equivocation arose from a failure to distinguish between the two fundamentally different types of knowledge. Libre knowledge – knowledge released in such a way that users are free to read, listen to, watch, or otherwise experience it; to learn from or with it; to copy, adapt and use it for any purpose; and to share the work (unchanged or modified). Whilst shared tacit knowledge is regarded as implicitly libre, (explicit) libre knowledge is defined as a generalisation of the libre software definition. Procedural knowledge – also known as imperative knowledge, it is the knowledge exercised in the performance of some task. Commonly referred to as "knowing-how" and opposed to "knowing-that" (descriptive knowledge). Tacit knowledge – kind of knowledge that is difficult to transfer to another person by means of writing it down or verbalizing it. For example, that London is in the United Kingdom is a piece of explicit knowledge that can be written down, transmitted, and understood by a recipient. However, the ability to speak a language, knead dough, play a musical instrument or design and use complex equipment requires all sorts of knowledge that is not always known explicitly, even by expert practitioners, and which is difficult or impossible to explicitly transfer to other users.
Common knowledge – knowledge that is known by everyone or nearly everyone, usually with reference to the community in which the term is used. Customer knowledge – knowledge for, about, or from customers. Domain knowledge – valid knowledge used to refer to an area of human endeavour, an autonomous computer activity, or other specialized discipline. Foundational knowledge – the knowledge necessary for understanding or usefully applying further knowledge in a field. General knowledge – information that has been accumulated over time through various mediums. This definition excludes highly specialized learning that can only be obtained with extensive training and information confined to a single medium. General knowledge is an important component of crystallized intelligence and is strongly associated with general intelligence, and with openness to experience. Metaknowledge – knowledge about knowledge. Bibliographies are a form of metaknowledge. Patterns within scientific literature is another. Mutual knowledge – Information known by all participatory agents Self-knowledge – information that an individual draws upon when finding an answer to the question "What am I like?". Traditional knowledge – knowledge systems embedded in the cultural traditions of regional, indigenous, or local communities. Traditional knowledge includes types of knowledge about traditional technologies of subsistence (e.g. tools and techniques for hunting or agriculture), midwifery, ethnobotany and ecological knowledge, traditional medicine, celestial navigation, ethnoastronomy, the climate, and others. These kinds of knowledge, crucial for subsistence and survival, are generally based on accumulations of empirical observation and on interaction with the environment. Traditional ecological knowledge –
Taxonomies – Types of subject taxonomies Document classification – Library classification – Taxonomy for search engines – Specific taxonomies of knowledge Figurative System of Human Knowledge – Propædia – first of three parts of the 15th edition of Encyclopædia Britannica, presenting its Outline of Knowledge. Tree of Knowledge System –
Academic disciplines – branch of knowledge that is taught and researched as part of higher education. A scholar's discipline is commonly defined and recognized by the university faculties and learned societies to which he or she belongs and the academic journals in which he or she publishes research. However, no formal criteria exist for defining an academic discipline. Body of knowledge (BOK) – specialized term in knowledge representation meaning the set of concepts, terms and activities that make up a professional domain, as defined by the relevant learned society or professional association. Curriculi – plural of curriculum, which means the totality of student experiments that occur in the educational process. The term often refers specifically to a planned sequence of instruction, or to a view of planned student's experiences in terms of the educator's or school's instructional goals. Curricula may be tightly standardized, or may include a high level of instructor or learner autonomy. Many countries have national curricula in primary and secondary education, such as the United Kingdom's National Curriculum. Encyclopedias – type of reference work or compendium holding a comprehensive summary of information from either all branches of knowledge or a particular branch of knowledge. Encyclopedias are divided into articles or entries, which are usually accessed alphabetically by article name. Encyclopedia entries are longer and more detailed than those in most dictionaries. Generally speaking, unlike dictionary entries, which focus on linguistic information about words, encyclopedia articles focus on factual information concerning the subject for which the article is named. Knowledge base – Personal knowledge base – Knowledge commons – Libraries – a library is a collection of sources of information and similar resources, made accessible to a defined community for reference or borrowing. It provides physical or digital access to material, and may be a physical building or room, or a virtual space, or both. A library's collection can include books, periodicals, newspapers, manuscripts, films, maps, prints, documents, microform, CDs, cassettes, videotapes, DVDs, Blu-ray Discs, e-books, audiobooks, databases, and other formats. Libraries range in size from a few shelves of books to several million items.
Specific BOKs (bodies of knowledge, in the context of the knowledge representation field) A Guide to the Business Analysis Body of Knowledge Canadian IT Body of Knowledge Civil Engineering Body of Knowledge Common Body of Knowledge Enterprise Architecture Body of Knowledge Geographic Information Science and Technology Body of Knowledge Project Management Body of Knowledge Software Engineering Body of Knowledge Data Management Body of Knowledge Specific encyclopedias Bibliography of encyclopedias List of encyclopedias by branch of knowledge List of encyclopedias by language List of historical encyclopedias List of online encyclopedias Wikipedia – largest encyclopedia in the world. It is a free, web-based, collaborative, multilingual encyclopedia project supported by the non-profit Wikimedia Foundation. Its more than 20 million articles (over 5.04 million in English) have been written collaboratively by volunteers around the world. Almost all of its articles can be edited by anyone with access to the site, and it has about 100,000 regularly active contributors. Specific knowledge bases Knowledge Vault – knowledge base created by Google. As of 2014, it contained 1.6 billion facts which had been collated automatically from the Internet.
Epistemology – philosophy of knowledge. It is the study of knowledge and justified belief. It questions what knowledge is and how it can be acquired, and the extent to which knowledge pertinent to any given subject or entity can be acquired. Much of the debate in this field has focused on the philosophical analysis of the nature of knowledge and how it relates to connected notions such as truth, belief, and justification. DIKW pyramid – theoretical model of the relationship between data, information, knowledge, and wisdom Knowledge neglect – failure to apply knowledge Theory of knowledge (IB course) – a course related to epistemology
Knowledge management – Chief knowledge officer – Knowledge balance sheet – Knowledge ecosystem – Knowledge mobilization – Knowledge organization (effort) – Knowledge organization system – Knowledge organization (company or agency) – Knowledge transfer – Knowledge worker –
Methods of obtaining knowledge – Exploration – Space exploration – Revelation – Research – Scientific method – Experimentation – Learning – Autodidactism – self-education; act of self-directed learning about a subject or subjects in which one has had little to no formal education. Reading – Studying – Knowledge building – Knowledge building communities –
Knowledge can be stored in: Books – Knowledge bases – Ontology – formal naming and definition of the types, properties, and interrelationships of the entities that really or fundamentally exist for a particular domain of discourse. Commonsense knowledge base – database containing all the general knowledge that most people possess, represented in a way that it is available to artificial intelligence programs that use natural language or make inferences about the ordinary world. Knowledge graph – another name for ontology Knowledge representation (AI) – Body of knowledge (BOK) – complete set of concepts, terms and activities that make up a professional domain, as defined by the relevant learned society or professional association Libraries – Memory –
Knowledge retrieval – Stored knowledge can be retrieved by: Knowledge engine Wolfram Alpha – computational knowledge engine or answer engine developed by Wolfram Research Knowledge Engine (Wikimedia Foundation) – search engine project by the Wikimedia Foundation Google Search – powered by: Google Knowledge Graph – knowledge base used by Google to enhance its search engine's search results with semantic search information gathered from a wide variety of sources Knowledge discovery – Reading –
Imparting knowledge means spreading or disseminating knowledge to others. Communication – purposeful activity of information exchange between two or more participants in order to convey or receive the intended meanings through a shared system of signs and semiotic rules. The basic steps of communication are the forming of communicative intent, message composition, message encoding, transmission of signal, reception of signal, message decoding and interpretation of the message by the recipient. Examples of methods of communication used to impart knowledge include: Writing and Publishing. Education – process of facilitating learning. Educational methods: Storytelling – Discussion – Teaching – Training – Directed research – Knowledge sharing – activity through which knowledge (namely, information, skills, or expertise) is exchanged among people, friends, families, communities (for example, Wikipedia), or organizations.Knowledge café – Knowledge transfer – Knowledge translation –
Intellectual capital – Knowledge broker – Knowledge Economic Index – Knowledge economy – Knowledge gap hypothesis – Knowledge market – Knowledge services – Knowledge spillover – Knowledge value – Monopolies of knowledge –
Access to Knowledge movement – Knowledge assessment methodology – Knowledge society – Local knowledge problem – Open access – Berlin Declaration on Open Access to Knowledge in the Sciences and Humanities – Scientia potentia est – Latin for "knowledge is power". The Cost of Knowledge protest – World Brain –
Knowledge-based systems – Knowledge acquisition – Knowledge base – Knowledge engineering – Knowledge engineer – Knowledge extraction – Knowledge level – Knowledge level modeling – Knowledge modeling – Knowledge management (see above) –
The world's knowledge (knowledge possessed by human civilization) Humanities and arts Classics History Literature Performing arts Dance Music Theatre Philosophy Religion Visual arts Media type Painting Social sciences Anthropology Economics Trade Education Geography Law Jurisprudence Linguistics / Language Political science Psychology Sociology Science Natural Sciences Astronomy Biology Chemistry Earth Sciences Mathematics Physics Engineering / Technology Aerospace engineering Biotechnology / Biological engineering Biomedical engineering Chemical engineering Civil engineering Computer science / Computer engineering Electrical engineering Electronics engineering Environmental engineering Industrial engineering Marine engineering / Naval architecture Materials science and engineering Mechanical engineering Nuclear science and engineering Healthcare sciences Dentistry and oral health Medicine and surgery Veterinary medicine / Veterinary surgery
Institute of Knowledge Transfer – International Society for Knowledge Organization – Open Knowledge International –
A Guide for the Perplexed – critique of materialist scientism and an exploration of the nature and organization of knowledge. By E. F. Schumacher. Knowledge and Its Limits –
Electronic Journal of Knowledge Management – Journal of Information & Knowledge Management – Journal of Information Science – Journal of Knowledge Management – Journal of Knowledge Management Practice – Journal of Web Semantics – Knowledge Management Research & Practice –
Knowledge workers are workers whose main capital is knowledge. Examples include programmers, physicians, pharmacists, architects, engineers, scientists, design thinkers, public accountants, lawyers, and academics, and any other white-collar workers, whose line of work requires one to "think for a living".
Knowledge work can be differentiated from other forms of work by its emphasis on "non-routine" problem solving that requires a combination of convergent and divergent thinking. But despite the amount of research and literature on knowledge work, there is no succinct definition of the term.Mosco and McKercher (2007) outline various viewpoints on the matter. They first point to the most narrow and defined definition of knowledge work, such as Florida's view of it as specifically, "the direct manipulation of symbols to create an original knowledge product, or to add obvious value to an existing one", which limits the definition of knowledge work to mainly creative work. They then contrast this view of knowledge work with the notably broader view which includes the handling and distribution of information, arguing that workers who play a role in the handling and distribution of information add real value to the field, despite not necessarily contributing a creative element. Thirdly, one might consider a definition of knowledge work which includes, "all workers involved in the chain of producing and distributing knowledge products", which allows for a very broad and inclusive categorization of knowledge workers. It should thus be acknowledged that the term "knowledge worker" can be quite broad in its meaning, and is not always definitive in who it refers to. Knowledge workers spend 38% of their time searching for information. They are also often displaced from their bosses, working in various departments and time zones or from remote sites such as home offices and airport lounges. As businesses increase their dependence on information technology, the number of fields in which knowledge workers must operate has expanded dramatically.Even though they sometimes are called "gold collars", because of their high salaries, as well as because of their relative independence in controlling the process of their own work, current research shows that they are also more prone to burnout, and very close normative control from organizations they work for, unlike regular workers.Managing knowledge workers can be a difficult task. Most knowledge workers prefer some level of autonomy, and do not like being overseen or managed. Those who manage knowledge workers are often knowledge workers themselves, or have been in the past. Projects must be carefully considered before assigning to a knowledge worker, as their interest and goals will affect the quality of the completed project. Knowledge workers must be treated as individuals. Loo ( 2017) using empirical findings from knowledge workers of two sectors – advertising and IT software sectors – and from three developed countries – England, Japan and Singapore – investigated a specific type of knowledge workers – the creative knowledge workers - as opposed to the generic ones as indicated above. The findings from the analysed empirical data offer a complex picture of this type of work in the knowledge economy where workers use a combination of creativity, abilities, talents, skills, and knowledge towards the eventual production of products and services. This investigation (Loo, 2017) identified a definition of creative knowledge work from four specific roles of copywriting, creative directing, software programming, and systems programme managing in advertising and IT software. The manner in which each of the creative applications is applied is dependent on the role(s) of the creative workers. This type of work includes a complex combination of skill sets or ‘creative knowledge work (ckw) capacities.’ "Creative knowledge workers use a combination of creative applications to perform their functions/roles in the knowledge economy including anticipatory imagination, problem solving, problem seeking, and generating ideas and aesthetic sensibilities" (Loo, 2017, p. 138). Taking aesthetic sensibility as an example, for a creative director, it is a visual imagery whether still or moving via a camera lens and for a software programmer, it is the innovative technical expertise in which the software is written. Other sector-related creative applications include an emotional connection in the advertising sector and the power of expression and sensitivity in the IT software sector. Terms such as ‘general sponge,’ ‘social chameleon,’ and ‘in tune with the zeitgeist’ were identified which the creative knowledge workers used to identify emotionally with their potential audience in ad making. From the IT software perspective, creative knowledge workers used a ‘sensitivity’ creative application to ascertain business intelligence and as a measurement of information, the software worker might obtain from various parties (Loo, 2017). Creative workers also require abilities and aptitudes. Passion for one's job was generic to the roles investigated in the two sectors and for copywriters, this passion was identified with fun, enjoyment, and happiness in carrying out the role alongside attributes such as honesty (regarding the product), confidence, and patience in finding the appropriate copy. As with the other roles, a creative worker in software programming requires team working and interpersonal skills in order to communicate effectively with those from other disciplinary backgrounds and training. As regards the managerial roles of creative directing and systems programme managing, the abilities to create a vision for the job in hand, to convince, strategize, execute, and plan towards the eventual completion of the given task (such as a campaign or a software) are necessary capacities (Loo, 2017). Linking these abilities and capacities are collaborative ways of working, which the findings from this study have identified. The two modes of working ranged from individual to collaborative where a worker might be doing either or both depending on the specific activity. The abilities to traverse between these two work modes alongside the relevant creative application are part of the complexity of this style of working. Creative workers also require an understanding of various forms of knowledge (Loo, 2017). These are related to disciplines such as those from the humanities (e.g., literature), and the creative arts such as painting and music (e.g., popular and classical varieties). Creative knowledge workers also require technical-related knowledge such as mathematics and computer sciences (e.g., software engineering) and physical sciences (e.g., physics) though there are distinctions in the two sectors. In the IT software sector, technical knowledge of software languages is especially significant for programmers as ascertained in the findings. However, the degree of technical expertise may be less for a programme manager, as only knowledge of the relevant software language is necessary to understand the issues for communicating with the team of developers and testers. The technical know-how for a creative director relates only to the understanding of the possibilities of technologies (such as graphics and typography) in order to capitalise on the technical wizardry. The technical specialists are then required to execute the creative director's vision. The above types of disciplinary knowledge may appear in explicit formats, which can be learnt from formal programmes at teaching institutions such as higher education and professional institutions alongside other skills and abilities relating to presentation, communication, and team working. As ascertained in the findings, there was other non-disciplinary knowledge, which was not explicit but tacit in nature. Interviewees mentioned tacit experiences from their past work and life experiences, which they used to draw upon in performing their creative knowledge work. This form of knowledge was harnessed collectively as a team (of an advertising campaign or a software programme). This collaborative approach to working, especially with roles such as creative directing and software programme managing, requires tacit knowledge of the strengths and weaknesses and the needs and wants of the related team members (knowledge of psychology). This form of working may occur within the organisation, as a stand-alone group for a specific project in the organisation, or as a sub-contracted team outside the organisation. Within this role, creative knowledge workers may perform their activities individually and/or collectively as part of their contribution to the project. The findings also brought out some characteristics of collaborative working such as the varieties of stakeholders such as sub-contracted groups, and the indirect relationships between clients, workers (of an ad agency), and consumers (Loo, 2017).
The term is often incorrectly said to have been first coined by Peter Drucker in The Landmarks of Tomorrow (1959), which was his first use of the term 'knowledge work' but not 'knowledge worker'. Drucker first coined the term 'knowledge worker' in The Effective Executive in 1966. Later, in 1999, he suggested that "the most valuable asset of a 21st-century institution, whether business or non-business, will be its knowledge workers and their productivity."Paul Alfred Weiss (1960) said that "knowledge grows like organisms, with data serving as food to be assimilated rather than merely stored". Popper (1963) stated there is always an increasing need for knowledge to grow and progress continually, whether tacit (Polanyi, 1976) or explicit. Toffler (1990) observed that typical knowledge workers (especially R&D scientists and engineers) in the age of knowledge economy must have some system at their disposal to create, process and enhance their own knowledge. In some cases they would also need to manage the knowledge of their co-workers. Nonaka (1991) described knowledge as the fuel for innovation, but was concerned that many managers failed to understand how knowledge could be leveraged. Companies are more like living organisms than machines, he argued, and most viewed knowledge as a static input to the corporate machine. Nonaka advocated a view of knowledge as renewable and changing, and that knowledge workers were the agents for that change. Knowledge-creating companies, he believed, should be focused primarily on the task of innovation. This laid the foundation for the new practice of knowledge management, or "KM", which evolved in the 1990s to support knowledge workers with standard tools and processes. Savage (1995) describes a knowledge-focus as the third wave of human socio-economic development. The first wave was the Agricultural Age with wealth defined as ownership of land. In the second wave, the Industrial Age, wealth was based on ownership of Capital, i.e. factories. In the Knowledge Age, wealth is based upon the ownership of knowledge and the ability to use that knowledge to create or improve goods and services. Product improvements include cost, durability, suitability, timeliness of delivery, and security. Using data, in the Knowledge Age, 2% of the working population will work on the land, 10% will work in Industry and the rest will be knowledge workers.
Davenport (2005) says that the rise of knowledge work has actually been foreseen for years. He points to the fact that Fritz Machlup did a lot of the early work on both knowledge as well as knowledge work roles and as early as 1958 stated that the sector was growing much faster than the rest of the economy with knowledge workers making up almost a third of the workforce in the United States. "According to the Organization for Economic Co-operation and Development (1981), by the beginning of the 1970s around 40 percent of the working population in the USA and Canada were classified to the information sector, whereas in most other OECD countries the figures were still considerably lower."As of 2016, an average of 1.9 million knowledge worker positions had been added every year since 1980, more than any other type of role.Tapscott (2006) sees a strong, on-going linkage between knowledge workers and innovation, but the pace and manner of interaction have become more advanced. He describes social media tools on the internet that now drive more powerful forms of collaboration. Knowledge workers engage in ‘’peer-to-peer’’ knowledge sharing across organizational and company boundaries, forming networks of expertise. Some of these are open to the public. While he echoes concern over copyright and intellectual property law being challenged in the marketplace, he feels strongly that businesses must engage in collaboration to survive. He sees on-going alliance of public (government) and private (commercial) teams to solve problems, referencing the open source Linux operating system along with the Human Genome Project as examples where knowledge is being freely exchanged, with commercial value being realized. Palmer (2014) researched knowledge worker productivity and work patterns. Part of this research has involved the analysis of how an average knowledge worker spends their day. He notes that effective and efficient knowledge work relies on the smooth navigation of unstructured processes and the elaboration of custom and one-off procedures. "As we move to the 21st century business model, the focus must be on equipping knowledge workers with tools and infrastructure that enable communication and information sharing, such as networking, e-mail, content management and increasingly, social media." Palmer points to the emergence of Adaptive Case Management (also known as Dynamic or Advanced case management) representing the paradigm shift triggered by the appearance from adapting business practices to the design of IT systems, to building systems that reflect how work is actually performed. Due to the rapid global expansion of information-based transactions and interactions being conducted via the Internet, there has been an ever-increasing demand for a workforce that is capable of performing these activities. Knowledge Workers are now estimated to outnumber all other workers in North America by at least a four to one margin.While knowledge worker roles overlap heavily with professions that require college degrees, the comprehensive nature of knowledge work in today's connected workplace requires virtually all workers to obtain these skills at some level. To that end, the public education and community college systems have become increasingly focused on lifelong learning to ensure students receive skills necessary to be productive knowledge workers in the 21st century. Many of the knowledge workers currently entering the workforce are from the generation X demographic. These new knowledge workers value lifelong learning over lifelong employment. "They seek employability over employment [and] value career over self-reliance" (Elsdon and Iyer, 1999). Where baby boomers are proficient in specified knowledge regarding a specific firm, generation X knowledge workers acquire knowledge from many firms and take that knowledge with them from company to company (2002).
Knowledge workers bring benefits to organizations in a variety of ways. These include: These knowledge worker contributions are in contrast with activities that they would typically not be asked to perform, including: transaction processing routine tasks simple prioritization of workThere is a set of transitional tasks which include roles that are seemingly routine, but that require deeper technology, product, or customer knowledge to fulfill the function. These include: providing technical or customer support handling unique customer issues addressing open-ended inquiriesGenerally, if the knowledge can be retained, knowledge worker contributions will serve to expand the knowledge assets of a company. While it can be difficult to measure, this increases the overall value of its intellectual capital. In cases where the knowledge assets have commercial or monetary value, companies may create patents around their assets, at which point the material becomes restricted intellectual property. In these knowledge-intensive situations, knowledge workers play a direct, vital role in increasing the financial value of a company. They can do this by finding solutions on how they can find new ways to make profits. This can also be related with market and research. Davenport (2005) says that even if knowledge workers are not a majority of all workers, they do have the most influence on their economies. He adds that companies with a high volume of knowledge workers are the most successful and fastest growing in leading economies including the United States. Reinhardt et al.'s (2011) review of current literature shows that the roles of knowledge workers across the workforce are incredibly diverse. In two empirical studies they have "proposed a new way of classifying the roles of knowledge workers and the knowledge actions they perform during their daily work." The typology of knowledge worker roles suggested by them are "controller, helper, learner, linker, networker, organizer, retriever, sharer, solver, and tracker":
Drucker (1966) defines six factors for knowledge worker productivity: Knowledge worker productivity demands that we ask the question: "What is the task?" It demands that we impose the responsibility for their productivity on the individual knowledge workers themselves. Knowledge workers have to manage themselves. Continuing innovation has to be part of the work, the task and the responsibility of knowledge workers. Knowledge work requires continuous learning on the part of the knowledge worker, but equally continuous teaching on the part of the knowledge worker. Productivity of the knowledge worker is not — at least not primarily — a matter of the quantity of output. Quality is at least as important. Finally, knowledge worker productivity requires that the knowledge worker is both seen and treated as an "asset" rather than a "cost." It requires that knowledge workers want to work for the organization in preference to all other opportunities.The theory of Human Interaction Management asserts that there are 5 principles characterizing effective knowledge work: Build effective teams Communicate in a structured way Create, share and maintain knowledge Align your time with strategic goals Negotiate next steps as you workAnother, more recent breakdown of knowledge work (author unknown) shows activity that ranges from tasks performed by individual knowledge workers to global social networks. This framework spans every class of knowledge work that is being or is likely to be undertaken. There are seven levels or scales of knowledge work, with references for each are cited. Knowledge work (e.g., writing, analyzing, advising) is performed by subject-matter specialists in all areas of an organization. Although knowledge work began with the origins of writing and counting, it was first identified as a category of work by Drucker (1973). Knowledge functions (e.g., capturing, organizing, and providing access to knowledge) are performed by technical staff, to support knowledge processes projects. Knowledge functions date from c. 450 BC, with the Library of Alexandria, but their modern roots can be linked to the emergence of information management in the 1970s. Knowledge processes (preserving, sharing, integration) are performed by professional groups, as part of a knowledge management program. Knowledge processes have evolved in concert with general-purpose technologies, such as the printing press, mail delivery, the telegraph, telephone networks, and the Internet. Knowledge management programs link the generation of knowledge (e.g., from science, synthesis, or learning) with its use (e.g., policy analysis, reporting, program management) as well as facilitating organizational learning and adaptation in a knowledge organization. Knowledge management emerged as a discipline in the 1990s (Leonard, 1995). Knowledge organizations transfer outputs (content, products, services, and solutions), in the form of knowledge services, to enable external use. The concept of knowledge organizations emerged in the 1990s. Knowledge services support other organizational services, yield sector outcomes, and result in benefits for citizens in the context of knowledge markets. Knowledge services emerged as a subject in the 2000s. Social media networks enable knowledge organizations to co-produce knowledge outputs by leveraging their internal capacity with massive social networks. Social networking emerged in the 2000sThe hierarchy ranges from the effort of individual specialists, through technical activity, professional projects, and management programs, to organizational strategy, knowledge markets, and global-scale networking. This framework is useful for positioning the myriad types of knowledge work relative to each other and within the context of organizations, markets, and the global knowledge economy. It also provides a useful context for planning, developing, and implementing knowledge management projects. Loo (2017) investigates how a particular group - creative knowledge workers – carries out their jobs and learns within it. Using empirical data from advertising and software development in England, Japan and Singapore, it develops a new conceptual framework to analyse the complexities of creative knowledge work. The framework draws from four disciplines of business and management, economics, sociology and psychology (Loo, 2017, p. 59). Focusing uniquely on the human element of working in the knowledge economy, Loo explores the real world of how people work in this emerging phenomenon and examines the relationships between knowledge and creative dimensions to provide new frameworks for learning and working. This research identified three levels of creative knowledge applications. They relate to intra-sectoral approaches, inter-sectoral approaches (where jobs require different styles of work depending on the sectors), and changes in culture/practices in the sectors. With the intra-sectoral work, they refer to the roles and functions of specific jobs in each of the two sectors of advertising (e.g. copywriting and creative directing) and software development (e.g. software developing and software programme managing). With the inter-sectoral work, it may include software programme managers having different functions when working in different organisations – e.g. a computer software company and a multinational financial organisation. With the last type of creative working, it may include aspects such as the culture of ‘good practice’ in technical problem-solving and the ‘power of expression’ in software programming. All the three types of micro-level of creative knowledge work offer a highly contextualized understanding of how these workers operate in the knowledge economy. This approach is different from that taken by Zuboff (1988), Drucker (1993), Nonaka and Takeuchi (1995) and Reich (2001) who sought to provide a more generic understanding (Loo, 2017). Finally, complex creative knowledge work needs a supportive environment. One such environment relates to the supporting technical base. Based on the findings, information, communications and electronic technologies (ICET) are viewed as an organisational tool, a source of ideas (such as the Internet), and a way of modelling a concept. It may also be applied to inter-sectoral activities such as software for cross-disciplinary applications. This organisational tool enables creative knowledge workers to devote their energies to multi-faceted activities such as analysis of huge data sets and the enabling of new jobs such as webpage designing. ICET enables workers to spend more time on advanced activities, which leads to the intensification of creative applications. Lastly, it was noted from the findings that a supportive environment focused on training, work environment, and education (Loo, 2017 Loo, S. (2017) Creative Working in the Knowledge Economy. Abingdon: Routledge).
How to think like a knowledge worker (UNPAN) Personal knowledge balance sheets for knowledge workers
The Archaeology of Knowledge (L’archéologie du savoir, 1969) by Michel Foucault, is a treatise about the methodology and historiography of the systems of thought (epistemes) and of knowledge (discursive formations) which follow rules that operate beneath the consciousness of the subject men and women, and which define a conceptual system of possibility that determines the boundaries of language and thought used in a given time and domain. The archaeology of knowledge is the analytical method that Foucault used in Madness and Civilization: A History of Insanity in the Age of Reason (1961), The Birth of the Clinic: An Archaeology of Medical Perception (1963), and The Order of Things: An Archaeology of the Human Sciences (1966).
The contemporary study of the History of Ideas concerns the transitions between historical world-views, but ultimately depends upon narrative continuities that break down under close inspection. The history of ideas marks points of discontinuity between broadly defined modes of knowledge, but those modes of knowledge exist are not discrete structures among the complex relations of historical discourse. Discourses emerge and transform according to a complex set of relationships (discursive and institutional) defined by discontinuities and unified themes.An énoncé (statement) is a discourse, a way of speaking; the methodology studies only the “things said” as emergences and transformations, without speculation about the collective meaning of the statements of the things said. A statement is the set of rules that makes an expression — a phrase, a proposition, an act of speech — into meaningful discourse, and is conceptually different from signification; thus, the expression “The gold mountain is in California” is discursively meaningless if it is unrelated to the geographic reality of California. Therefore, the function of existence is necessary for an énoncé (statement) to have a discursive meaning.As a set of rules, the statement has special meaning in the archaeology of knowledge, because it is the rules that render an expression discursively meaningful, while the syntax and the semantics are additional rules that make an expression significative, are additional rules. The structures of syntax and the structures of semantics are insufficient to determine the discursive meaning of an expression; whether or not an expression complies with the rules of discursive meaning, a grammatically correct sentence might lack discursive meaning; inversely, a grammatically incorrect sentence might be discursively meaningful; even meaningless letters can possess discursive meaning, e.g. QWERTY identifies a type of keyboard layout for typewriters and computers.The meaning of an expression depends upon the conditions in which the expression emerges and exists within the discourse of a field or the discourse of a discipline; the discursive meaning of an expression is determined by the statements that precede and follow it. To wit, the énoncés (statements) constitute a network of rules that establish which expressions are discursively meaningful; the rules are the preconditions for signifying propositions, utterances, and acts of speech to have discursive meaning. The analysis then deals with the organized dispersion of statements, discursive formations, and Foucault reiterates that the outlined archaeology of knowledge is one possible method of historical analysis.
The philosopher Gilles Deleuze describes The Archaeology of Knowledge as, "the most decisive step yet taken in the theory-practice of multiplicities."
Foucauldian discourse analysis
Knowledge Channel is a Philippine pay television channel owned by ABS-CBN that consists of educational and informative programs. The channel is available on Sky Cable, Sky Direct and other cable providers. It was also previously available on ABS-CBN TVplus (until June 30, 2020), following an alias cease-and-desist order issued by the National Telecommunications Commission in connection with the expiration of ABS-CBN franchise.
Elvira "Rina" M. López-Bautista established the non-profit organization Sky Foundation, Inc. in June 1999; it would later be renamed the Knowledge Channel Foundation, Inc. (KCFI) in 2003. The Knowledge Channel began broadcasting on November 6, 1999 with 18 hours of educational programs on cable television that supplemented the curriculum of the Department of Education, Culture and Sports (later the Department of Education). The foundation entered into a 10-year memorandum of agreement (MoA) with the department, with the latter proclaiming the channel as required viewing in all public schools in 2000 and has since then renewed its partnership with another 10 year MoA signed in November 2009. KCFI's first two original programs are Kasaysayan TV and Pamana.As of 2019, Rina Bautista is the President and Executive Director of KCFI, with Oscar M. López as Chairman and Carlo Katigbak as Vice-President.On August 2020, the channel operates at 24 hours with the launch of School at Home.
Official website Knowledge Channel on Facebook Knowledge Channel on Twitter Knowledge Channel's channel on YouTube
For Unlawful Carnal Knowledge is the ninth studio album by American hard rock band Van Halen. It was released on June 18, 1991, on Warner Bros. Records and is the third to feature vocalist Sammy Hagar. It debuted at number 1 on the Billboard 200 album chart and maintained the position for three weeks. The album marked the reconciliation of the band with producer Ted Templeman, whom they had not worked with since 1984, when David Lee Roth was still lead singer.
All tracks are written by Eddie Van Halen, Michael Anthony, Sammy Hagar and Alex Van Halen.
Billboard (United States)
Grammy Award
The Buddha (also known as Siddhartha Gotama or Siddhārtha Gautama) was a philosopher, mendicant, meditator, spiritual teacher, and religious leader who lived in Ancient India (c. 5th to 4th century BCE). He is revered as the founder of the world religion of Buddhism, and worshiped by most Buddhist schools as the Enlightened One who has transcended Karma and escaped the cycle of birth and rebirth. He taught for around 45 years and built a large following, both monastic and lay. His teaching is based on his insight into duḥkha (typically translated as "suffering") and the end of dukkha – the state called Nibbāna or Nirvana. The Buddha was born into an aristocratic family, in the Shakya clan but eventually renounced lay life. According to Buddhist tradition, after several years of mendicancy, meditation, and asceticism, he awakened to understand the mechanism which keeps people trapped in the cycle of rebirth. The Buddha then traveled throughout the Ganges plain teaching and building a religious community. The Buddha taught a middle way between sensual indulgence and the severe asceticism found in the Indian śramaṇa movement. He taught a spiritual path that included ethical training and meditative practices such as jhana and mindfulness. The Buddha also critiqued the practices of Brahmin priests, such as animal sacrifice. A couple of centuries after his death he came to be known by the title Buddha, which means "Awakened One" or "Enlightened One". Gautama's teachings were compiled by the Buddhist community in the Suttas, which contain his discourses, and the Vinaya, his codes for monastic practice. These were passed down in Middle-Indo Aryan dialects through an oral tradition. Later generations composed additional texts, such as systematic treatises known as Abhidharma, biographies of the Buddha, collections of stories about the Buddha's past lives known as Jataka tales, and additional discourses, i.e, the Mahayana sutras.
Scholars are hesitant to make unqualified claims about the historical facts of the Buddha's life. Most people accept that the Buddha lived, taught, and founded a monastic order during the Mahajanapada era during the reign of Bimbisara (c. 558 – c. 491 BCE, or c. 400 BCE), the ruler of the Magadha empire, and died during the early years of the reign of Ajatasatru, who was the successor of Bimbisara, thus making him a younger contemporary of Mahavira, the Jain tirthankara. While the general sequence of "birth, maturity, renunciation, search, awakening and liberation, teaching, death" is widely accepted, there is less consensus on the veracity of many details contained in traditional biographies.The times of Gautama's birth and death are uncertain. Most historians in the early 20th century dated his lifetime as c. 563 BCE to 483 BCE. Within the Eastern Buddhist tradition of China, Vietnam, Korea and Japan, the traditional date for the death of the Buddha was 949 B.C. According to the Ka-tan system of time calculation in the Kalachakra tradition, Buddha is believed to have died about 833 BCE. More recently his death is dated later, between 411 and 400 BCE, while at a symposium on this question held in 1988, the majority of those who presented definite opinions gave dates within 20 years either side of 400 BCE for the Buddha's death. These alternative chronologies, however, have not been accepted by all historians.
According to the Buddhist tradition, Gautama was born in Lumbini, now in modern-day Nepal, and raised in Kapilvastu, which may have been either in what is present-day Tilaurakot, Nepal or Piprahwa, India. According to Buddhist tradition, he obtained his enlightenment in Bodh Gaya, gave his first sermon in Sarnath, and died in Kushinagar. One of Gautama's usual names was "Sakamuni" or "Sakyamunī" ("Sage of the Shakyas"). This and the evidence of the early texts suggests that he was born into the Shakya clan, a community that was on the periphery, both geographically and culturally, of the eastern Indian subcontinent in the 5th century BCE. The community was either a small republic, or an oligarchy. His father was an elected chieftain, or oligarch. Bronkhorst calls this eastern culture Greater Magadha and notes that "Buddhism and Jainism arose in a culture which was recognized as being non-Vedic".The Shakyas were an eastern sub-Himalayan ethnic group who were considered outside of the Āryāvarta and of ‘mixed origin’ (saṃkīrṇa-yonayaḥ, possibly part Aryan and part indigenous). The laws of Manu treats them as being non Aryan. As noted by Levman, "The Baudhāyana-dharmaśāstra (1.1.2.13–4) lists all the tribes of Magadha as being outside the pale of the Āryāvarta; and just visiting them required a purificatory sacrifice as expiation" (In Manu 10.11, 22). This is confirmed by the Ambaṭṭha Sutta, where the Sakyans are said to be "rough-spoken", "of menial origin" and criticised because "they do not honour, respect, esteem, revere or pay homage to Brahmans." Some of the non-Vedic practices of this tribe included incest (marrying their sisters), the worship of trees, tree spirits and nagas. According to Levman "while the Sakyans’ rough speech and Munda ancestors do not prove that they spoke a non-Indo-Aryan language, there is a lot of other evidence suggesting that they were indeed a separate ethnic (and probably linguistic) group." Christopher I. Beckwith identifies the Shakyas as Scythians.Apart from the Vedic Brahmins, the Buddha's lifetime coincided with the flourishing of influential Śramaṇa schools of thought like Ājīvika, Cārvāka, Jainism, and Ajñana. Brahmajala Sutta records sixty-two such schools of thought. In this context, a śramaṇa refers to one who labors, toils, or exerts themselves (for some higher or religious purpose). It was also the age of influential thinkers like Mahavira, Pūraṇa Kassapa, Makkhali Gosāla, Ajita Kesakambalī, Pakudha Kaccāyana, and Sañjaya Belaṭṭhaputta, as recorded in Samaññaphala Sutta, whose viewpoints the Buddha most certainly must have been acquainted with. Indeed, Sariputta and Moggallāna, two of the foremost disciples of the Buddha, were formerly the foremost disciples of Sañjaya Belaṭṭhaputta, the sceptic; and the Pali canon frequently depicts Buddha engaging in debate with the adherents of rival schools of thought. There is also philological evidence to suggest that the two masters, Alara Kalama and Uddaka Ramaputta, were indeed historical figures and they most probably taught Buddha two different forms of meditative techniques. Thus, Buddha was just one of the many śramaṇa philosophers of that time. In an era where holiness of person was judged by their level of asceticism, Buddha was a reformist within the śramaṇa movement, rather than a reactionary against Vedic Brahminism.Historically, the life of the Buddha also coincided with the Achaemenid conquest of the Indus Valley during the rule of Darius I from about 517/516 BCE. This Achaemenid occupation of the areas of Gandhara and Sindh, which lasted about two centuries, was accompanied by the introduction of Achaemenid religions, reformed Mazdaism or early Zoroastrianism, to which Buddhism might have in part reacted. In particular, the ideas of the Buddha may have partly consisted of a rejection of the "absolutist" or "perfectionist" ideas contained in these Achaemenid religions.
No written records about Gautama were found from his lifetime or from the one or two centuries thereafter. But from the middle of the 3rd century BCE, several Edicts of Ashoka (reigned c. 269–232 BCE) mention the Buddha, and particularly Ashoka's Lumbini pillar inscription commemorates the Emperor's pilgrimage to Lumbini as the Buddha's birthplace, calling him the Buddha Shakyamuni (Brahmi script: 𑀩𑀼𑀥 𑀲𑀓𑁆𑀬𑀫𑀼𑀦𑀻 Bu-dha Sa-kya-mu-nī, "Buddha, Sage of the Shakyas"). Another one of his edicts (Minor Rock Edict No. 3) mentions the titles of several Dhamma texts (in Buddhism, "dhamma" is another word for "dharma"), establishing the existence of a written Buddhist tradition at least by the time of the Maurya era. These texts may be the precursor of the Pāli Canon. "Sakamuni" is also mentioned in the reliefs of Bharhut, dated to circa 100 BCE, in relation with his illumination and the Bodhi tree, with the inscription Bhagavato Sakamunino Bodho ("The illumination of the Blessed Sakamuni").The oldest surviving Buddhist manuscripts are the Gandhāran Buddhist texts, found in Afghanistan and written in Gāndhārī, they date from the first century BCE to the third century CE.On the basis of philological evidence, Indologist and Pali expert Oskar von Hinüber says that some of the Pali suttas have retained very archaic place-names, syntax, and historical data from close to the Buddha's lifetime, including the Mahāparinibbāṇa Sutta which contains a detailed account of the Buddha's final days. Hinüber proposes a composition date of no later than 350–320 BCE for this text, which would allow for a "true historical memory" of the events approximately 60 years prior if the Short Chronology for the Buddha's lifetime is accepted (but he also points out that such a text was originally intended more as hagiography than as an exact historical record of events).John S. Strong sees certain biographical fragments in the canonical texts preserved in Pali, as well as Chinese, Tibetan and Sanskrit as the earliest material. These include texts such as the “Discourse on the Noble Quest” (Pali: Ariyapariyesana-sutta) and its parallels in other languages.
The sources which present a complete picture of the life of Siddhārtha Gautama are a variety of different, and sometimes conflicting, traditional biographies. These include the Buddhacarita, Lalitavistara Sūtra, Mahāvastu, and the Nidānakathā. Of these, the Buddhacarita is the earliest full biography, an epic poem written by the poet Aśvaghoṣa in the first century CE. The Lalitavistara Sūtra is the next oldest biography, a Mahāyāna/Sarvāstivāda biography dating to the 3rd century CE. The Mahāvastu from the Mahāsāṃghika Lokottaravāda tradition is another major biography, composed incrementally until perhaps the 4th century CE. The Dharmaguptaka biography of the Buddha is the most exhaustive, and is entitled the Abhiniṣkramaṇa Sūtra, and various Chinese translations of this date between the 3rd and 6th century CE. The Nidānakathā is from the Theravada tradition in Sri Lanka and was composed in the 5th century by Buddhaghoṣa.The earlier canonical sources include the Ariyapariyesana Sutta (MN 26), the Mahāparinibbāṇa Sutta (DN 16), the Mahāsaccaka-sutta (MN 36), the Mahapadana Sutta (DN 14), and the Achariyabhuta Sutta (MN 123), which include selective accounts that may be older, but are not full biographies. The Jātaka tales retell previous lives of Gautama as a bodhisattva, and the first collection of these can be dated among the earliest Buddhist texts. The Mahāpadāna Sutta and Achariyabhuta Sutta both recount miraculous events surrounding Gautama's birth, such as the bodhisattva's descent from the Tuṣita Heaven into his mother's womb.
In the earliest Buddhist texts, the nikāyas and āgamas, the Buddha is not depicted as possessing omniscience (sabbaññu) nor is he depicted as being an eternal transcendent (lokottara) being. According to Bhikkhu Analayo, ideas of the Buddha's omniscience (along with an increasing tendency to deify him and his biography) are found only later, in the Mahayana sutras and later Pali commentaries or texts such as the Mahāvastu. In the Sandaka Sutta, the Buddha's disciple Ananda outlines an argument against the claims of teachers who say they are all knowing while in the Tevijjavacchagotta Sutta the Buddha himself states that he has never made a claim to being omniscient, instead he claimed to have the "higher knowledges" (abhijñā). The earliest biographical material from the Pali Nikayas focuses on the Buddha's life as a śramaṇa, his search for enlightenment under various teachers such as Alara Kalama and his forty-five-year career as a teacher.Traditional biographies of Gautama often include numerous miracles, omens, and supernatural events. The character of the Buddha in these traditional biographies is often that of a fully transcendent (Skt. lokottara) and perfected being who is unencumbered by the mundane world. In the Mahāvastu, over the course of many lives, Gautama is said to have developed supramundane abilities including: a painless birth conceived without intercourse; no need for sleep, food, medicine, or bathing, although engaging in such "in conformity with the world"; omniscience, and the ability to "suppress karma". As noted by Andrew Skilton, the Buddha was often described as being superhuman, including descriptions of him having the 32 major and 80 minor marks of a "great man," and the idea that the Buddha could live for as long as an aeon if he wished (see DN 16).The ancient Indians were generally unconcerned with chronologies, being more focused on philosophy. Buddhist texts reflect this tendency, providing a clearer picture of what Gautama may have taught than of the dates of the events in his life. These texts contain descriptions of the culture and daily life of ancient India which can be corroborated from the Jain scriptures, and make the Buddha's time the earliest period in Indian history for which significant accounts exist. British author Karen Armstrong writes that although there is very little information that can be considered historically sound, we can be reasonably confident that Siddhārtha Gautama did exist as a historical figure. Michael Carrithers goes a bit further by stating that the most general outline of "birth, maturity, renunciation, search, awakening and liberation, teaching, death" must be true.
Legendary biographies like the Pali Buddhavaṃsa and the Sanskrit Jātakamālā depict the Buddha's (referred to as "bodhisattva" before his awakening) career as spanning hundreds of lifetimes before his last birth as Gautama. Many stories of these previous lives are depicted in the Jatakas. The format of a Jataka typically begins by telling a story in the present which is then explained by a story of someone's previous life.Besides imbuing the pre-Buddhist past with a deep karmic history, the Jatakas also serve to explain the bodhisattva's (the Buddha-to-be) path to Buddhahood. In biographies like the Buddhavaṃsa, this path is described as long and arduous, taking "four incalculable ages" (asamkheyyas).In these legendary biographies, the bodhisattva goes through many different births (animal and human), is inspired by his meeting of past Buddhas, and then makes a series of resolves or vows (pranidhana) to become a Buddha himself. Then he begins to receive predictions by past Buddhas. One of the most popular of these stories is his meeting with Dipankara Buddha, who gives the bodhisattva a prediction of future Buddhahood.Another theme found in the Pali Jataka Commentary (Jātakaṭṭhakathā) and the Sanskrit Jātakamālā is how the Buddha-to-be had to practice several "perfections" (pāramitā) to reach Buddhahood. The Jatakas also sometimes depict negative actions done in previous lives by the bodhisattva, which explain difficulties he experienced in his final life as Gautama.
The Buddhist tradition regards Lumbini, in present-day Nepal to be the birthplace of the Buddha. He grew up in Kapilavastu. The exact site of ancient Kapilavastu is unknown. It may have been either Piprahwa, Uttar Pradesh, in present-day India, or Tilaurakot, in present-day Nepal. Both places belonged to the Sakya territory, and are located only 15 miles (24 km) apart.The earliest Buddhist sources state that the Buddha was born to an aristocratic Kshatriya (Pali: khattiya) family called Gotama (Sanskrit: Gautama), who were part of the Shakyas, a tribe of rice-farmers living near the modern border of India and Nepal. the son of Śuddhodana, "an elected chief of the Shakya clan", whose capital was Kapilavastu, and who were later annexed by the growing Kingdom of Kosala during the Buddha's lifetime. Gautama was the family name. According to later biographies such as the Mahavastu and the Lalitavistara, his mother, Maya (Māyādevī), Suddhodana's wife, was a Koliyan princess. Legend has it that, on the night Siddhartha was conceived, Queen Maya dreamt that a white elephant with six white tusks entered her right side, and ten months later Siddhartha was born. As was the Shakya tradition, when his mother Queen Maya became pregnant, she left Kapilavastu for her father's kingdom to give birth. However, her son is said to have been born on the way, at Lumbini, in a garden beneath a sal tree. The early Buddhist texts contain very little information about the birth and youth of Gotama Buddha. Later biographies developed a dramatic narrative about the life of the young Gotama as a prince and his existential troubles. They also depict his father Śuddhodana as a hereditary monarch of the Suryavansha (Solar dynasty) of Ikṣvāku (Pāli: Okkāka). This is unlikely however, as many scholars think that Śuddhodana was merely a Shakya aristocrat (khattiya), and that the Shakya republic was not a hereditary monarchy. Indeed, the more egalitarian gana-sangha form of government, as a political alternative to Indian monarchies, may have influenced the development of the śramanic Jain and Buddhist sanghas, where monarchies tended toward Vedic Brahmanism.The day of the Buddha's birth is widely celebrated in Theravada countries as Vesak. Buddha's Birthday is called Buddha Purnima in Nepal, Bangladesh, and India as he is believed to have been born on a full moon day. According to later biographical legends, during the birth celebrations, the hermit seer Asita journeyed from his mountain abode, analyzed the child for the "32 marks of a great man" and then announced that he would either become a great king (chakravartin) or a great religious leader. Suddhodana held a naming ceremony on the fifth day and invited eight Brahmin scholars to read the future. All gave similar predictions. Kondañña, the youngest, and later to be the first arhat other than the Buddha, was reputed to be the only one who unequivocally predicted that Siddhartha would become a Buddha.Early texts suggest that Gautama was not familiar with the dominant religious teachings of his time until he left on his religious quest, which is said to have been motivated by existential concern for the human condition. According to the early Buddhist Texts of several schools, and numerous post-canonical accounts, Gotama had a wife, Yasodhara, and a son, named Rāhula. Besides this, the Buddha in the early texts reports that "'I lived a spoilt, a very spoilt life, monks (in my parents’ home)."The legendary biographies like the Lalitavistara also tell stories of young Gotama's great martial skill, which was put to the test in various contests against other Shakyan youths.
While the earliest sources merely depict Gotama seeking a higher spiritual goal and becoming an ascetic or sramana after being disillusioned with lay life, the later legendary biographies tell a more elaborate dramatic story about how he became a mendicant.The earliest accounts of the Buddha's spiritual quest is found in texts such as the Pali Ariyapariyesanā-sutta ("The discourse on the noble quest," MN 26) and its Chinese parallel at MĀ 204. These texts report that what led to Gautama's renunciation was the thought that his life was subject to old age, disease and death and that there might be something better (i.e. liberation, nirvana). The early texts also depict the Buddha's explanation for becoming a sramana as follows: "The household life, this place of impurity, is narrow - the samana life is the free open air. It is not easy for a householder to lead the perfected, utterly pure and perfect holy life." MN 26, MĀ 204, the Dharmaguptaka Vinaya and the Mahāvastu all agree that his mother and father opposed his decision and "wept with tearful faces" when he decided to leave. Legendary biographies also tell the story of how Gautama left his palace to see the outside world for the first time and how he was shocked by his encounter with human suffering. The legendary biographies depict Gautama's father as shielding him from religious teachings and from knowledge of human suffering, so that he would become a great king instead of a great religious leader. In the Nidanakatha (5th century CE), Gautama is said to have seen an old man. When his charioteer Chandaka explained to him that all people grew old, the prince went on further trips beyond the palace. On these he encountered a diseased man, a decaying corpse, and an ascetic that inspired him. This story of the "four sights" seems to be adapted from an earlier account in the Digha Nikaya (DN 14.2) which instead depicts the young life of a previous Buddha, Vipassi.The legendary biographies depict Gautama's departure from his palace as follows. Shortly after seeing the four sights, Gautama woke up at night and saw his female servants lying in unattractive, corpse-like poses, which shocked him. Therefore, he discovered what he would later understand more deeply during his enlightenment: suffering and the end of suffering. Moved by all the things he had experienced, he decided to leave the palace in the middle of the night against the will of his father, to live the life of a wandering ascetic. Accompanied by Chandaka and riding his horse Kanthaka, Gautama leaves the palace, leaving behind his son Rahula and Yaśodhara. He traveled to the river Anomiya, and cut off his hair. Leaving his servant and horse behind, he journeyed into the woods and changed into monk's robes there, though in some other versions of the story, he received the robes from a Brahma deity at Anomiya.According to the legendary biographies, when the ascetic Gautama first went to Rajagaha (present-day Rajgir) to beg for alms in the streets, King Bimbisara of Magadha learned of his quest, and offered him a share of his kingdom. Gautama rejected the offer but promised to visit his kingdom first, upon attaining enlightenment.
All sources agree that the ascetic Gautama practised under two teachers of yogic meditation. According to MN 26 and its Chinese parallel at MĀ 204, after having mastered the teaching of Ārāḍa Kālāma (Pali: Alara Kalama), who taught a meditation attainment called "the sphere of nothingness", he was asked by Ārāḍa to become an equal leader of their spiritual community. However, Gautama felt unsatisfied by the practice because it "does not lead to revulsion, to dispassion, to cessation, to calm, to knowledge, to awakening, to Nibbana", and moved on to become a student of Udraka Rāmaputra (Pali: Udaka Ramaputta). With him, he achieved high levels of meditative consciousness (called "The Sphere of Neither Perception nor Non-Perception") and was again asked to join his teacher. But, once more, he was not satisfied for the same reasons as before, and moved on.Majjhima Nikaya 4 also mentions that Gautama lived in "remote jungle thickets" during his years of spiritual striving and had to overcome the fear that he felt while living in the forests. After leaving his meditation teachers, Gotama then practiced ascetic techniques. An account of these practices can be seen in the Mahāsaccaka-sutta (MN 36) and its various parallels (which according to Analayo include some Sanskrit fragments, an individual Chinese translation, a sutra of the Ekottarika-āgama as well as sections of the Lalitavistara and the Mahāvastu). The ascetic techniques described in the early texts include very minimal food intake, different forms of breath control, and forceful mind control. The texts report that he became so emaciated that his bones became visible through his skin.According to other early Buddhist texts, after realising that meditative dhyana was the right path to awakening, Gautama discovered "the Middle Way"—a path of moderation away from the extremes of self-indulgence and self-mortification, or the Noble Eightfold Path. His break with asceticism is said to have led his five companions to abandon him, since they believed that he had abandoned his search and become undisciplined. One popular story tells of how he accepted milk and rice pudding from a village girl named Sujata. Following his decision to stop extreme ascetic practices, MĀ 204 and other parallel early texts report that Gautama sat down to meditate with the determination not to get up until full awakening (sammā-sambodhi) had been reached. This event was said to have occurred under a pipal tree—known as "the Bodhi tree"—in Bodh Gaya, Bihar.Likewise, the Mahāsaccaka-sutta and most of its parallels agree that after taking asceticism to its extremes, the Buddha realized that this had not helped him reach awakening. At this point, he remembered a previous meditative experience he had as a child sitting under a tree while his father worked. This memory leads him to understand that dhyana (meditation) is the path to awakening, and the texts then depict the Buddha achieving all four dhyanas, followed by the "three higher knowledges" (tevijja) culminating in awakening. Gautama thus became known as the Buddha or "Awakened One". The title indicates that unlike most people who are "asleep", a Buddha is understood as having "woken up" to the true nature of reality and sees the world 'as it is' (yatha-bhutam). A Buddha has achieved liberation (vimutti), also called Nirvana, which is seen as the extinguishing of the "fires" of desire, hatred, and ignorance, that keep the cycle of suffering and rebirth going. According to various early texts like the Mahāsaccaka-sutta, and the Samaññaphala Sutta, a Buddha has achieved three higher knowledges: Remembering one's former abodes (i.e. past lives), the "Divine eye" (dibba-cakkhu), which allows the knowing of others' karmic destinations and the "extinction of mental intoxicants" (āsavakkhaya).According to some texts from the Pali canon, at the time of his awakening he realised complete insight into the Four Noble Truths, thereby attaining liberation from samsara, the endless cycle of rebirth. As reported by various texts from the Pali Canon, the Buddha sat for seven days under the bodhi tree "feeling the bliss of deliverance." The Pali texts also report that he continued to meditate and contemplated various aspects of the Dharma while living by the River Nairañjanā, such as Dependent Origination, the Five Spiritual Faculties and Suffering.The legendary biographies like the Mahavastu and the Lalitavistara depict an attempt by Mara, the Lord of the desire realm, to prevent the Buddha's nirvana. He does so by sending his daughters to seduce the Buddha, by asserting his superiority and by assaulting him with armies of monsters. However the Buddha is unfazed and calls on the earth (or in some versions of the legend, the earth goddess) as witness to his superiority by touching the ground before entering meditation. Other miracles and magical events are also depicted.
According to MN 26, immediately after his awakening, the Buddha hesitated on whether or not he should teach the Dharma to others. He was concerned that humans were so overpowered by ignorance, greed, and hatred that they could never recognise the path, which is "subtle, deep and hard to grasp." However, the god Brahmā Sahampati convinced him, arguing that at least some "with little dust in their eyes" will understand it. The Buddha relented and agreed to teach. According to Analayo, the Chinese parallel to MN 26, MĀ 204, does not contain this story, but this event does appear in other parallel texts, such as in an Ekottarika-āgama discourse, in the Catusparisat-sūtra, and in the Lalitavistara.According to MN 26 and MĀ 204, after deciding to teach, the Buddha initially intended to visit his former teachers, Alara Kalama and Udaka Ramaputta, to teach them his insights, but they had already died, so he decided to visit his five former companions. MN 26 and MĀ 204 both report that on his way to Vārānasī (Benares), he met another wanderer, called Ājīvika Upaka in MN 26. The Buddha proclaimed that he had achieved full awakening, but Upaka was not convinced and "took a different path".MN 26 and MĀ 204 continue with the Buddha reaching the Deer Park (Sarnath) (Mrigadāva, also called Rishipatana, "site where the ashes of the ascetics fell") near Vārānasī , where he met the group of five ascetics and was able to convince them that he had indeed reached full awakening. According to MĀ 204 (but not MN 26), as well as the Theravāda Vinaya, an Ekottarika-āgama text, the Dharmaguptaka Vinaya, the Mahīśāsaka Vinaya, and the Mahāvastu, the Buddha then taught them the "first sermon", also known as the "Benares sermon", i.e. the teaching of "the noble eightfold path as the middle path aloof from the two extremes of sensual indulgence and self-mortification." The Pali text reports that after the first sermon, the ascetic Koṇḍañña (Kaundinya) became the first arahant (liberated being) and the first Buddhist bhikkhu or monastic. The Buddha then continued to teach the other ascetics and they formed the first saṅgha: the company of Buddhist monks. Various sources such as the Mahāvastu, the Mahākhandhaka of the Theravāda Vinaya and the Catusparisat-sūtra also mention that the Buddha taught them his second discourse, about the characteristic of "not-self" (Anātmalakṣaṇa Sūtra), at this time or five days later. After hearing this second sermon the four remaining ascetics also reached the status of arahant. The Theravāda Vinaya and the Catusparisat-sūtra also speak of the conversion of Yasa, a local guild master, and his friends and family, who were some of the first laypersons to be converted and to enter the Buddhist community. The conversion of three brothers named Kassapa followed, who brought with them five hundred converts who had previously been "matted hair ascetics," and whose spiritual practice was related to fire sacrifices. According to the Theravāda Vinaya, the Buddha then stopped at the Gayasisa hill near Gaya and delivered his third discourse, the Ādittapariyāya Sutta (The Discourse on Fire), in which he taught that everything in the world is inflamed by passions and only those who follow the Eightfold path can be liberated.At the end of the rainy season, when the Buddha's community had grown to around sixty awakened monks, he instructed them to wander on their own, teach and ordain people into the community, for the "welfare and benefit" of the world.
For the remaining 40 or 45 years of his life, the Buddha is said to have traveled in the Gangetic Plain, in what is now Uttar Pradesh, Bihar, and southern Nepal, teaching a diverse range of people: from nobles to servants, ascetics and householders, murderers such as Angulimala, and cannibals such as Alavaka. According to Schumann, the Buddha's wanderings ranged from "Kosambi on the Yamuna (25 km south-west of Allahabad )", to Campa (40 km east of Bhagalpur)" and from "Kapilavatthu (95 km north-west of Gorakhpur) to Uruvela (south of Gaya)." This covers an area of 600 by 300 km. His sangha enjoyed the patronage of the kings of Kosala and Magadha and he thus spent a lot of time in their respective capitals, Savatthi and Rajagaha.Although the Buddha's language remains unknown, it is likely that he taught in one or more of a variety of closely related Middle Indo-Aryan dialects, of which Pali may be a standardisation. The sangha traveled through the subcontinent, expounding the Dharma. This continued throughout the year, except during the four months of the Vassa rainy season when ascetics of all religions rarely traveled. One reason was that it was more difficult to do so without causing harm to flora and animal life. The health of the ascetics might have been a concern as well. At this time of year, the sangha would retreat to monasteries, public parks or forests, where people would come to them. The first vassana was spent at Varanasi when the sangha was formed. According to the Pali texts, shortly after the formation of the sangha, the Buddha traveled to Rajagaha, capital of Magadha, and met with King Bimbisara, who gifted a bamboo grove park to the sangha.The Buddha's sangha continued to grow during his initial travels in north India. The early texts tell the story of how the Buddha's chief disciples, Sāriputta and Mahāmoggallāna, who were both students of the skeptic sramana Sañjaya Belaṭṭhiputta, were converted by Assaji. They also tell of how the Buddha's son, Rahula, joined his father as a bhikkhu when the Buddha visited his old home, Kapilavastu. Over time, other Shakyans joined the order as bhikkhus, such as Buddha's cousin Ananda, Anuruddha, Upali the barber, the Buddha's half-brother Nanda and Devadatta. Meanwhile, the Buddha's father Suddhodana heard his son's teaching, converted to Buddhism and became a stream-enterer. The early texts also mention an important lay disciple, the merchant Anāthapiṇḍika, who became a strong lay supporter of the Buddha early on. He is said to have gifted Jeta's grove (Jetavana) to the sangha at great expense (the Theravada Vinaya speaks of thousands of gold coins).
The formation of a parallel order of female monastics (bhikkhunī) was another important part of the growth of the Buddha's community. As noted by Analayo's comparative study of this topic, there are various versions of this event depicted in the different early Buddhist texts.According to all the major versions surveyed by Analayo, Mahāprajāpatī Gautamī, Buddha's step-mother, is initially turned down by the Buddha after requesting ordination for her and some other women. Mahāprajāpatī and her followers then shave their hair, don robes and begin following the Buddha on his travels. The Buddha is eventually convinced by Ānanda to grant ordination to Mahāprajāpatī on her acceptance of eight conditions called gurudharmas which focus on the relationship between the new order of nuns and the monks.According to Analayo, the only argument common to all the versions that Ananda uses to convince the Buddha is that women have the same ability to reach all stages of awakening. Analayo also notes that some modern scholars have questioned the authenticity of the eight gurudharmas in their present form due to various inconsistencies. He holds that the historicity of the current lists of eight is doubtful, but that they may have been based on earlier injunctions by the Buddha. Analayo also notes that various passages indicate that the reason for the Buddha's hesitation to ordain women was the danger that the life of a wandering sramana posed for women that were not under the protection of their male family members (such as dangers of sexual assault and abduction). Due to this, the gurudharma injunctions may have been a way to place "the newly founded order of nuns in a relationship to its male counterparts that resembles as much as possible the protection a laywoman could expect from her male relatives."
According to J.S. Strong, after the first 20 years of his teaching career, the Buddha seems to have slowly settled in Sravasti, the capital of the Kingdom of Kosala, spending most of his later years in this city.As the sangha grew in size, the need for a standardized set of monastic rules arose and the Buddha seems to have developed a set of regulations for the sangha. These are preserved in various texts called "Pratimoksa" which were recited by the community every fortnight. The Pratimoksa includes general ethical precepts, as well as rules regarding the essentials of monastic life, such as bowls and robes.In his later years, the Buddha's fame grew and he was invited to important royal events, such as the inauguration of the new council hall of the Shakyans (as seen in MN 53) and the inauguration of a new palace by Prince Bodhi (as depicted in MN 85). The early texts also speak of how during the Buddha's old age, the kingdom of Magadha was usurped by a new king, Ajatasattu, who overthrew his father Bimbisara. According to the Samaññaphala Sutta, the new king spoke with different ascetic teachers and eventually took refuge in the Buddha. However, Jain sources also claim his allegiance, and it is likely he supported various religious groups, not just the Buddha's sangha exclusively.As the Buddha continued to travel and teach, he also came into contact with members of other śrāmana sects. There is evidence from the early texts that the Buddha encountered some of these figures and critiqued their doctrines. The Samaññaphala Sutta identifies six such sects.The early texts also depict the elderly Buddha as suffering from back pain. Several texts depict him delegating teachings to his chief disciples since his body now needed more rest. However, the Buddha continued teaching well into his old age. One of the most troubling events during the Buddha's old age was Devadatta's schism. Early sources speak of how the Buddha's cousin, Devadatta, attempted to take over leadership of the order and then left the sangha with several Buddhist monks and formed a rival sect. This sect is said to have also been supported by King Ajatasattu. The Pali texts also depict Devadatta as plotting to kill the Buddha, but these plans all fail. They also depict the Buddha as sending his two chief disciples (Sariputta and Moggallana) to this schismatic community in order to convince the monks who left with Devadatta to return.All the major early Buddhist Vinaya texts depict Devadatta as a divisive figure who attempted to split the Buddhist community, but they disagree on what issues he disagreed with the Buddha on. The Sthavira texts generally focus on "five points" which are seen as excessive ascetic practices, while the Mahāsaṅghika Vinaya speaks of a more comprehensive disagreement, which has Devadatta alter the discourses as well as monastic discipline.At around the same time of Devadatta's schism, there was also war between Ajatasattu's Kingdom of Magadha, and Kosala, led by an elderly king Pasenadi. Ajatasattu seems to have been victorious, a turn of events the Buddha is reported to have regretted.
The main narrative of the Buddha's last days, death and the events following his death is contained in the Mahaparinibbana Sutta (DN 16) and its various parallels in Sanskrit, Chinese, and Tibetan. According to Analayo, these include the Chinese Dirgha Agama 2, "Sanskrit fragments of the Mahaparinirvanasutra", and "three discourses preserved as individual translations in Chinese".The Mahaparinibbana sutta depicts the Buddha's last year as a time of war. It begins with Ajatasattu's decision to make war on the Vajjian federation, leading him to send a minister to ask the Buddha for advice. The Buddha responds by saying that the Vajjians can be expected to prosper as long as they do seven things, and he then applies these seven principles to the Buddhist Sangha, showing that he is concerned about its future welfare. The Buddha says that the Sangha will prosper as long as they "hold regular and frequent assemblies, meet in harmony, do not change the rules of training, honor their superiors who were ordained before them, do not fall prey to worldly desires, remain devoted to forest hermitages, and preserve their personal mindfulness." He then gives further lists of important virtues to be upheld by the Sangha.The early texts also depict how the Buddha's two chief disciples, Sariputta and Moggallana, died just before the Buddha's death. The Mahaparinibbana depicts the Buddha as experiencing illness during the last months of his life but initially recovering. It also depicts him as stating that he cannot promote anyone to be his successor. When Ānanda requested this, the Mahaparinibbana records his response as follows: Ananda, why does the Order of monks expect this of me? I have taught the Dhamma, making no distinction of “inner” and “ outer”: the Tathagata has no “teacher’s fist” (in which certain truths are held back). If there is anyone who thinks: “I shall take charge of the Order”, or “the Order is under my leadership”, such a person would have to make arrangements about the Order. The Tathagata does not think in such terms. Why should the Tathagata make arrangements for the Order? I am now old, worn out . . . I have reached the term of life, I am turning eighty years of age. Just as an old cart is made to go by being held together with straps, so the Tathagata's body is kept going by being bandaged up . . . Therefore, Ananda, you should live as islands unto yourselves, being your own refuge, seeking no other refuge; with the Dhamma as an island, with the Dhamma as your refuge, seeking no other refuge. . . Those monks who in my time or afterwards live thus, seeking an island and a refuge in themselves and in the Dhamma and nowhere else, these zealous ones are truly my monks and will overcome the darkness (of rebirth). After traveling and teaching some more, the Buddha ate his last meal, which he had received as an offering from a blacksmith named Cunda. Falling violently ill, Buddha instructed his attendant Ānanda to convince Cunda that the meal eaten at his place had nothing to do with his death and that his meal would be a source of the greatest merit as it provided the last meal for a Buddha. Bhikkhu and von Hinüber argue that the Buddha died of mesenteric infarction, a symptom of old age, rather than food poisoning.The precise contents of the Buddha's final meal are not clear, due to variant scriptural traditions and ambiguity over the translation of certain significant terms. The Theravada tradition generally believes that the Buddha was offered some kind of pork, while the Mahayana tradition believes that the Buddha consumed some sort of truffle or other mushroom. These may reflect the different traditional views on Buddhist vegetarianism and the precepts for monks and nuns. Modern scholars also disagree on this topic, arguing both for pig's flesh or some kind of plant or mushroom that pigs like to eat. Whatever the case, none of the sources which mention the last meal attribute the Buddha's sickness to the meal itself.As per the Mahaparinibbana sutta, after the meal with Cunda, the Buddha and his companions continued traveling until he was too weak to continue and had to stop at Kushinagar, where Ānanda had a resting place prepared in a grove of Sala trees. After announcing to the sangha at large that he would soon be passing away to final Nirvana, the Buddha ordained one last novice into the order personally, his name was Subhadda. He then repeated his final instructions to the sangha, which was that the Dhamma and Vinaya was to be their teacher after his death. Then he asked if anyone had any doubts about the teaching, but nobody did. The Buddha's final words are reported to have been: "All saṅkhāras decay. Strive for the goal with diligence (appamāda)" (Pali: 'vayadhammā saṅkhārā appamādena sampādethā').He then entered his final meditation and died, reaching what is known as parinirvana (final nirvana, the end of rebirth and suffering achieved after the death of the body). The Mahaparinibbana reports that in his final meditation he entered the four dhyanas consecutively, then the four immaterial attainments and finally the meditative dwelling known as nirodha-samāpatti, before returning to the fourth dhyana right at the moment of death.
According to the Mahaparinibbana sutta, the Mallians of Kushinagar spent the days following the Buddha's death honoring his body with flowers, music and scents. The sangha waited until the eminent elder Mahākassapa arrived to pay his respects before cremating the body.The Buddha's body was then cremated and the remains, including his bones, were kept as relics and they were distributed among various north Indian kingdoms like Magadha, Shakya and Koliya. These relics were placed in monuments or mounds called stupas, a common funerary practice at the time. Centuries later they would be exhumed and enshrined by Ashoka into many new stupas around the Mauryan realm. Many supernatural legends surround the history of alleged relics as they accompanied the spread of Buddhism and gave legitimacy to rulers. According to various Buddhist sources, the First Buddhist Council was held shortly after the Buddha's death to collect, recite and memorize the teachings. Mahākassapa was chosen by the sangha to be the chairman of the council. However, the historicity of the traditional accounts of the first council is disputed by modern scholars.
One method to obtain information on the oldest core of Buddhism is to compare the oldest versions of the Pali Canon and other texts, such as the surviving portions of Sarvastivada, Mulasarvastivada, Mahisasaka, Dharmaguptaka, and the Chinese Agamas. The reliability of these sources, and the possibility of drawing out a core of oldest teachings, is a matter of dispute. According to Tilmann Vetter, inconsistencies remain, and other methods must be applied to resolve those inconsistencies.According to Lambert Schmithausen, there are three positions held by modern scholars of Buddhism: "Stress on the fundamental homogeneity and substantial authenticity of at least a considerable part of the Nikayic materials." "Scepticism with regard to the possibility of retrieving the doctrine of earliest Buddhism." "Cautious optimism in this respect."Regarding their attribution to the historical Buddha Gautama "Sakyamuni", scholars such as Richard Gombrich, Akira Hirakawa, Alexander Wynne and A.K. Warder hold that these Early Buddhist Texts contain material that could possibly be traced to this figure.
According to scholars of Indology such as Richard Gombrich, the Buddha's teachings on Karma and Rebirth are a development of pre-Buddhist themes that can be found in Jain and Brahmanical sources, like the Brihadaranyaka Upanishad. Likewise, samsara, the idea that we are trapped in cycle of rebirth and that we should seek liberation from this through non-harming (ahimsa) and spiritual practices, pre-dates the Buddha and was likely taught in early Jainism.In various texts, the Buddha is depicted as having studied under two named teachers, Āḷāra Kālāma and Uddaka Rāmaputta. According to Alexander Wynne, these were yogis who taught doctrines and practices similar to those in the Upanishads.The Buddha's tribe of origin, the Shakyas, also seem to have had non-Vedic religious practices which influenced Buddhism, such as the veneration of trees and sacred groves, and the worship of tree spirits (yakkhas) and serpent beings (nagas). They also seem to have built burial mounds called stupas.Tree veneration remains important in Buddhism today, particularly in the practice of venerating Bodhi trees. Likewise, yakkas and nagas have remained important figures in Buddhist religious practices and mythology.In the Early Buddhist Texts, the Buddha also references Brahmanical devices. For example, in Samyutta Nikaya 111, Majjhima Nikaya 92 and Vinaya i 246 of the Pali Canon, the Buddha praises the Agnihotra as the foremost sacrifice and the Gayatri mantra as the foremost meter.The Buddhist teaching of the three marks of existence may also reflect Upanishadic or other influences according to K.R. Norman.According to Johannes Bronkhorst, the "meditation without breath and reduced intake of food" which the Buddha practiced before his awakening are forms of asceticism which are similar to Jain practices.The Buddhist practice called Brahma-vihara may have also originated from a Brahmanic term; but its usage may have been common in the sramana traditions.
The Early Buddhist Texts present many teachings and practices which may have been taught by the historical Buddha. These include basic doctrines such as Dependent Origination, the Middle Way, the Five Aggregates, the Three unwholesome roots, the Four Noble Truths and the Eightfold Path. According to N. Ross Reat, all of these doctrines are shared by the Theravada Pali texts and the Mahasamghika school's Śālistamba Sūtra.A recent study by Bhikkhu Analayo concludes that the Theravada Majjhima Nikaya and Sarvastivada Madhyama Agama contain mostly the same major doctrines. Likewise, Richard Salomon has written that the doctrines found in the Gandharan Manuscripts are "consistent with non-Mahayana Buddhism, which survives today in the Theravada school of Sri Lanka and Southeast Asia, but which in ancient times was represented by eighteen separate schools."These basic teachings such as the Four Noble Truths tend to be widely accepted as basic doctrines in all major schools of Buddhism, as seen in ecumenical documents such as the Basic points unifying Theravāda and Mahāyāna.
In the early Buddhist texts, the Buddha critiques the Brahmanical religion and social system on certain key points. The Brahmin caste held that the Vedas were eternal revealed (sruti) texts. The Buddha, on the other hand, did not accept that these texts had any divine authority or value.The Buddha also did not see the Brahmanical rites and practices as useful for spiritual advancement. For example, in the Udāna, the Buddha points out that ritual bathing does not lead to purity, only "truth and morality" lead to purity. He especially critiqued animal sacrifice as taught in Vedas. The Buddha contrasted his teachings, which were taught openly to all people, with that of the Brahmins', who kept their mantras secret.He also critiqued numerous other Brahmanical practices, such astrology, divination, fortune-telling, and so on (as seen in the Tevijja sutta and the Kutadanta sutta).The Buddha also attacked the Brahmins' claims of superior birth and the idea that different castes and bloodlines were inherently pure or impure, noble or ignoble.In the Vasettha sutta the Buddha argues that the main difference among humans is not birth but their actions and occupations. According to the Buddha, one is a "Brahmin" (i.e. divine, like Brahma) only to the extent that one has cultivated virtue. Because of this the early texts report that he proclaimed: "Not by birth one is a Brahman, not by birth one is a non-Brahman; - by moral action one is a Brahman"The Aggañña Sutta explains all classes or varnas can be good or bad and gives a sociological explanation for how they arose, against the Brahmanical idea that they are divinely ordained. According to Kancha Ilaiah, the Buddha posed the first contract theory of society. The Buddha's teaching then is a single universal moral law, one Dharma valid for everybody, which is opposed to the Brahmanic ethic founded on “one’s own duty” (svadharma) which depends on caste. Because of this, all castes including untouchables were welcome in the Buddhist order and when someone joined, they renounced all caste affiliation.
The early Buddhist texts present the Buddha's worldview as focused on understanding the nature of dukkha, which is seen as the fundamental problem of life. Dukkha refers to all kinds of suffering, unease, frustration, and dissatisfaction that sentient beings experience. At the core of the Buddha's analysis of dukkha is the fact that everything we experience is impermanent, unstable and thus unreliable.A common presentation of the core structure of Buddha's teaching found in the early texts is that of the Four Noble Truths. This teaching is most famously presented in the Dhammacakkappavattana Sutta ("The discourse on the turning of the Dharma wheel") and its many parallels. The basic outline of the four truths is as follows: There is dukkha. There are causes and conditions for the arising of dukkha. Various conditions are outlined in the early texts, such as craving (taṇhā), but the three most basic ones are greed, aversion and delusion. If the conditions for dukkha cease, dukkha also ceases. This is "Nirvana" (literally 'blowing out' or 'extinguishing'). There is path to follow that leads to Nirvana.According to Bhikkhu Analayo, the four truths schema appears to be based "on an analogy with Indian medical diagnosis" (with the form: "disease, pathogen, health, cure") and this comparison is "explicitly made in several early Buddhist texts".In another Pali sutta, the Buddha outlines how "eight worldly conditions", "keep the world turning around...Gain and loss, fame and disrepute, praise and blame, pleasure and pain." He then explains how the difference between a noble (arya) person and an uninstructed worldling is that a noble person reflects on and understands the impermanence of these conditions.The Buddha's analysis of existence includes an understanding that karma and rebirth are part of life. According to the Buddha, the constant cycle of dying and being reborn (i.e. saṃsāra) according to one's karma is just dukkha and the ultimate spiritual goal should be liberation from this cycle. According to the Pali suttas, the Buddha stated that "this saṃsāra is without discoverable beginning. A first point is not discerned of beings roaming and wandering on hindered by ignorance and fettered by craving."The Buddha's teaching of karma differed to that of the Jains and Brahmins, in that on his view, karma is primarily mental intention (as opposed to mainly physical action or ritual acts). The Buddha is reported to have said "By karma I mean intention." Richard Gombrich summarizes the Buddha's view of karma as follows: "all thoughts, words, and deeds derive their moral value, positive or negative, from the intention behind them."For the Buddha, our karmic acts also affected the rebirth process in a positive or negative way. This was seen as an impersonal natural law similar to how certain seeds produce certain plants and fruits (in fact, the result of a karmic act was called its "fruit" by the Buddha). However, it is important to note that the Buddha did not hold that everything that happens is the result of karma alone. In fact when the Buddha was asked to state the causes of pain and pleasure he listed various physical and environmental causes alongside karma.
In the early texts, the process of the arising of dukkha is most thoroughly explained by the Buddha through the teaching of Dependent Origination. At its most basic level, Dependent Origination is an empirical teaching on the nature of phenomena which says that nothing is experienced independently of its conditions.The most basic formulation of Dependent Origination is given in the early texts as: 'It being thus, this comes about' (Pali: evam sati idam hoti). This can be taken to mean that certain phenomena only arise when there are other phenomena present (example: when there is craving, suffering arises), and so, one can say that their arising is "dependent" on other phenomena. In other words, nothing in experience exists without a cause.In numerous early texts, this basic principle is expanded with a list of phenomena that are said to be conditionally dependent. These phenomena are supposed to provide an analysis of the cycle of dukkha as experienced by sentient beings. The philosopher Mark Siderits has outlined the basic idea of the Buddha's teaching of Dependent Origination of dukkha as follows: given the existence of a fully functioning assemblage of psycho-physical elements (the parts that make up a sentient being), ignorance concerning the three characteristics of sentient existence—suffering, impermanence and non-self—will lead, in the course of normal interactions with the environment, to appropriation (the identification of certain elements as ‘I’ and ‘mine’). This leads in turn to the formation of attachments, in the form of desire and aversion, and the strengthening of ignorance concerning the true nature of sentient existence. These ensure future rebirth, and thus future instances of old age, disease and death, in a potentially unending cycle. The Buddha saw his analysis of Dependent Origination as a "Middle Way" between "eternalism" (sassatavada, the idea that some essence exists eternally) and "annihilationism" (ucchedavada, the idea that we go completely out of existence at death). This middle way is basically the view that, conventionally speaking, persons are just a causal series of impermanent psycho-physical elements.
Closely connected to the idea that experience is dependently originated is the Buddha's teaching that there is no independent or permanent self (Sanskrit: atman, Pali: atta).Due to this view which (termed anatta), the Buddha's teaching was opposed to all soul theories of his time, including the Jain theory of a "jiva" ("life monad") and the Brahmanical theories of atman and purusha. All of these theories held that there was an eternal unchanging essence to a person which transmigrated from life to life.While Brahminical teachers affirmed atman theories in an attempt to answer the question of what really exists ultimately, the Buddha saw this question as not being useful, as illustrated in the parable of the poisoned arrow.For the Buddha's contemporaries, the atman was also seen to be the unchanging constant which was separate from all changing experiences and the inner controller in a person. The Buddha instead held that all things in the world of our experience are transient and that there is no unchanging part to a person. According to Richard Gombrich, the Buddha's position is simply that "everything is process". However, this anti-essentialist view still includes an understanding of continuity through rebirth, it is just the rebirth of a process (karma), not an essence like the atman.Perhaps the most important way the Buddha analyzed individual experience in the early texts was by way of the five 'aggregates' or 'groups' (khandha) of physical and mental processes. The Buddha's arguments against an unchanging self rely on these five aggregate schema, as can be seen in the Pali Anattalakkhaṇa Sutta (and its parallels in Gandhari and Chinese).According to the early texts, the Buddha argued that because we have no ultimate control over any of the psycho-physical processes that make up a person, there cannot be an "inner controller" with command over them. Also, since they are all impermanent, one cannot regard any of the psycho-physical processes as an unchanging self. Even mental processes such as consciousness and will (cetana) are seen as being dependently originated and impermanent and thus do not qualify as a self (atman).As noted by Gombrich, in the early texts the Buddha teaches that all five aggregates, including consciousness (viññana, which was held by Brahmins to be eternal), arise dependent on causes. That is, existence is based on processes that are subject to dependent origination. He compared samsaric existence to a fire, which is dynamic and requires fuel (the khandas, literally: "heaps") in order to keep burning.As Rupert Gethin explains, for the Buddha: I am a complex flow of physical and mental phenomena, but peel away these phenomena and look behind them and one just does not find a constant self that one can call one's own. My sense of self is both logically and emotionally just a label that I impose on these physical and mental phenomena in consequence of their connectedness. The Buddha saw the belief in a self as arising from our grasping at and identifying with the various changing phenomena, as well as from ignorance about how things really are. Furthermore, the Buddha held that we experience suffering because we hold on to erroneous self views.
As noted by Bhikkhu Bodhi, the Buddha as depicted in the Pali suttas does not exclusively teach a world transcending goal, but also teaches laypersons how to achieve worldly happiness (sukha).According to Bodhi, the "most comprehensive" of the suttas that focus on how to live as a layperson is the Sigālovāda Sutta (DN 31). This sutta outlines how a layperson behaves towards six basic social relationships: "parents and children, teacher and pupils, husband and wife, friend and friend, employer and workers, lay follower and religious guides." This Pali text also has parallels in Chinese and in Sanskrit fragments.In another sutta (Dīghajāṇu Sutta, AN 8.54) the Buddha teaches two types of happiness. First, there is the happiness visible in this very life. The Buddha states that four things lead to this happiness: "The accomplishment of persistent effort, the accomplishment of protection, good friendship, and balanced living." Similarly, in several other suttas, the Buddha teaches on how to improve family relationships, particularly on the importance of filial love and gratitude as well as marital well-being.Regarding the happiness of the next life, the Buddha (in the Dīghajāṇu Sutta) states that the virtues which lead to a good rebirth are: faith (in the Buddha and the teachings), moral discipline, especially keeping the five precepts, generosity, and wisdom (knowledge of the arising and passing of things).According to the Buddha of the suttas then, achieving a good rebirth is based on cultivating wholesome or skillful (kusala) karma, which leads to a good result, and avoiding unwholesome (akusala) karma. A common list of good karmas taught by the Buddha is the list of ten courses of action (kammapatha) as outlined in MN 41 Saleyyaka Sutta (and its Chinese parallel in SĀ 1042).Good karma is also termed merit (puñña), and the Buddha outlines three bases of meritorious actions: giving, moral discipline and meditation (as seen in AN 8:36).
Liberation (vimutti) from the ignorance and grasping which create suffering is not easily achieved because all beings have deeply entrenched habits (termed āsavas, often translated as "influxes" or "defilements") that keep them trapped in samsara. Because of this, the Buddha taught a path (marga) of training to undo such habits. This path taught by the Buddha is depicted in the early texts (most famously in the Pali Dhammacakkappavattana Sutta and its numerous parallel texts) as a "Middle Way" between sensual indulgence on one hand and mortification of the body on the other.One of the most common formulations of the path to liberation in the earliest Buddhist texts is the Noble Eightfold Path. There is also an alternative formulation with ten elements which is also very commonly taught in the early texts.According to Gethin, another common summary of the path to awakening wisely used in the early texts is "abandoning the hindrances, practice of the four establishments of mindfulness and development of the awakening factors."The early texts also contain many different presentations of the Buddha's path to liberation aside from the Eightfold Path. According to Rupert Gethin, in the Nikayas and Agamas, the Buddha's path is mainly presented in a cumulative and gradual "step by step" process, such as that outlined in the Samaññaphala Sutta. Early texts that outline the graduated path include the Cula-Hatthipadopama-sutta (MN 27, with Chinese parallel at MĀ 146) and the Tevijja Sutta (DN 13, with Chinese parallel at DĀ 26 and a fragmentary Sanskrit parallel entitled the Vāsiṣṭha-sūtra). Other early texts like the Upanisa sutta (SN 12.23), present the path as reversions of the process of Dependent Origination.Some common practices which are shared by most of these early presentations of the path include sila (ethical training), restraint of the senses (indriyasamvara), mindfulness and clear awareness (sati-sampajañña) and the practice of jhana (meditative absorption). Mental development (citta bhāvanā) was central to the Buddha's spiritual path as depicted in the earliest texts and this included meditative practices. Regarding the training of right view and sense restraint, the Buddha taught that it was important to reflect on the dangers or drawbacks (adinava) of sensual pleasures. Various suttas discuss the different drawbacks of sensuality. In the Potaliya Sutta (MN 54) sensual pleasures are said by the Buddha to be a cause of conflict for all humans beings. They are said to be unable to satisfy one's craving, like a clean meatless bone given to a dog. Sensuality is also compared to a torch held against the wind, since it burns the person holding on to it. According to the Buddha, there is "a delight apart from sensual pleasures, apart from unwholesome states, which surpasses even divine bliss." The Buddha thus taught that one should take delight in the higher spiritual pleasures instead of sensual pleasure. This is explained with the simile the leper, who cauterizes his skin with fire to get relief from the pain of leprosy, but after he is cured, avoids the same flames he used to enjoy before (see MN 75, Magandiya Sutta).Numerous scholars such as Vetter have written on the centrality of the practice of dhyāna to the teaching of the Buddha. It is the training of the mind, commonly translated as meditation, to withdraw the mind from the automatic responses to sense-impressions, and leading to a "state of perfect equanimity and awareness (upekkhā-sati-parisuddhi)." Dhyana is preceded and supported by various aspects of the path such as seclusion and sense restraint.Another important mental training in the early texts is the practice of mindfulness (sati), which was mainly taught using the schemas of the "Four Ways of Mindfulness" (Satipatthana, as taught in the Pali Satipatthana Sutta and its various parallel texts) and the sixteen elements of "Mindfulness of Breath" (Anapanasati, as taught in the Anapanasati Sutta and its various parallels).Because getting others to practice the path was the central goal of the Buddha's message, the early texts depict the Buddha as refusing to answer certain metaphysical questions which his contemporaries were preoccupied with, (such as "is the world eternal?"). This is because he did not see these questions as being useful on the path and as not being "connected to the goal".
The early Buddhist texts depict the Buddha as promoting the life of a homeless and celibate "sramana", or mendicant, as the ideal way of life for the practice of the path. He taught that mendicants or "beggars" (bhikkhus) were supposed to give up all possessions and to own just a begging bowl and three robes. As part of the Buddha's monastic discipline, they were also supposed to rely on the wider lay community for the basic necessities (mainly food, clothing, and lodging).The Buddha's teachings on monastic discipline were preserved in the various Vinaya collections of the different early schools.Buddhist monastics, which included both monks and nuns, were supposed to beg for their food, were not allowed to store up food or eat after noon and they were not allowed to use gold, silver or any valuables.
The early texts depict the Buddha as giving a deflationary account of the importance of politics to human life. Politics is inevitable and is probably even necessary and helpful, but it is also a tremendous waste of time and effort, as well as being a prime temptation to allow ego to run rampant. Buddhist political theory denies that people have a moral duty to engage in politics except to a very minimal degree (pay the taxes, obey the laws, maybe vote in the elections), and it actively portrays engagement in politics and the pursuit of enlightenment as being conflicting paths in life.In the Aggañña Sutta, the Buddha teaches a history of how monarchy arose which according to Matthew J. Moore is "closely analogous to a social contract." The Aggañña Sutta also provides a social explanation of how different classes arose, in contrast to the Vedic views on social caste.Other early texts like the Cakkavatti-Sīhanāda Sutta and the Mahāsudassana Sutta focus on the figure of the righteous wheel turning leader (Cakkavatti). This ideal leader is one who promotes Dharma through his governance. He can only achieve his status through moral purity and must promote morality and Dharma to maintain his position. According to the Cakkavatti-Sīhanāda Sutta, the key duties of a Cakkavatti are: "establish guard, ward, and protection according to Dhamma for your own household, your troops, your nobles, and vassals, for Brahmins and householders, town and country folk, ascetics and Brahmins, for beasts and birds. let no crime prevail in your kingdom, and to those who are in need, give property.” The sutta explains the injunction to give to the needy by telling how a line of wheel-turning monarchs falls because they fail to give to the needy, and thus the kingdom falls into infighting as poverty increases, which then leads to stealing and violence.In the Mahāparinibbāna Sutta, the Buddha outlines several principles that he promoted among the Vajjian tribal federation, which had a quasi-republican form of government. He taught them to “hold regular and frequent assemblies”, live in harmony and maintain their traditions. The Buddha then goes on to promote a similar kind of republican style of government among the Buddhist Sangha, where all monks had equal rights to attend open meetings and there would be no single leader, since The Buddha also chose not to appoint one. Some scholars have argued that this fact signals that the Buddha preferred a republican form of government, while others disagree with this position.
Numerous scholars of early Buddhism argue that most of the teachings found in the Early Buddhist texts date back to the Buddha himself. One of these is Richard Gombrich, who argues that since the content of the earliest texts “presents such originality, intelligence, grandeur and—most relevantly—coherence...it is hard to see it as a composite work." Thus he concludes they are "the work of one genius."Peter Harvey also agrees that “much” of the Pali Canon “must derive from his [the Buddha’s] teachings.” Likewise, A. K. Warder has written that “there is no evidence to suggest that it [the shared teaching of the early schools] was formulated by anyone other than the Buddha and his immediate followers.”Furthermore, Alexander Wynne argues that "the internal evidence of the early Buddhist literature proves its historical authenticity."However, other scholars of Buddhist studies have disagreed with the mostly positive view that the early Buddhist texts reflect the teachings of the historical Buddha. For example, Edward Conze argued that the attempts of European scholars to reconstruct the original teachings of the Buddha were “all mere guesswork.”Other scholars argue that some teachings contained in the early texts are the authentic teachings of the Buddha, but not others. For example, according to Tilmann Vetter, the earliest core of the Buddhist teachings is the meditative practice of dhyāna. Vetter argues that "liberating insight" became an essential feature of the Buddhist tradition at a later date. He posits that the Fourth Noble Truths, the Eightfold path and Dependent Origination, which are commonly seen as essential to Buddhism, are later formulations which form part of the explanatory framework of this "liberating insight".Lambert Schmithausen similarly argues that the mention of the four noble truths as constituting "liberating insight", which is attained after mastering the four dhyānas, is a later addition. Also, according to Johannes Bronkhorst, the four truths may not have been formulated in earliest Buddhism, and did not serve in earliest Buddhism as a description of "liberating insight".
Various Buddhist texts attribute to the Buddha a series of extraordinary physical characteristics, known as "the 32 Signs of the Great Man" (Skt. mahāpuruṣa lakṣaṇa). According to Analayo, when they first appear in the Buddhist texts, these physical marks were initially held to be imperceptible to the ordinary person, and required special training to detect. Later though, they are depicted as being visible by regular people and as inspiring faith in the Buddha.These characteristics are described in the Digha Nikaya's Lakkhaṇa Sutta (D, I:142).
Some Hindus regard Gautama as the 9th avatar of Vishnu. However, Buddha's teachings deny the authority of the Vedas and the concepts of Brahman-Atman. Consequently Buddhism is generally classified as a nāstika school (heterodox, literally "It is not so") in contrast to the six orthodox schools of Hinduism. In Sikhism, Buddha is mentioned as the 23rd avatar of Vishnu in the Chaubis Avtar, a composition in Dasam Granth traditionally and historically attributed to Guru Gobind Singh.Classical Sunni scholar Tabari reports that Buddhist idols were brought from Afghanistan to Baghdad in the ninth century. Such idols had been sold in Buddhist temples next to a mosque in Bukhara, but he does not further discuss the role of Buddha. According to the works on Buddhism by Al-Biruni (973–after 1050), views regarding the exact identity of Buddha was diverse. Accordingly, some regarded him as the divine incarnate, others as an apostle of the angels or as an Ifrit and others as an apostle of God sent to human race. By the 12th century, al-Shahrastani even compared Buddha to Khidr, described as an ideal human. Ibn Nadim, who was also familiar with Manichean teachings, even identifies Buddha as a prophet, who taught a religion to "banish Satan", although not mention it explicitly. However, most Classical scholars described Buddha in theistic terms, that is apart from Islamic teachings.Nevertheless the Buddha is regarded as a prophet by the minority Ahmadiyya sect, generally considered deviant and rejected as apostate by mainstream Islam. Some early Chinese Taoist-Buddhists thought the Buddha to be a reincarnation of Laozi.Disciples of the Cao Đài religion worship the Buddha as a major religious teacher. His image can be found in both their Holy See and on the home altar. He is revealed during communication with Divine Beings as son of their Supreme Being (God the Father) together with other major religious teachers and founders like Jesus, Laozi, and Confucius.The Christian Saint Josaphat is based on the Buddha. The name comes from the Sanskrit Bodhisattva via Arabic Būdhasaf and Georgian Iodasaph. The only story in which St. Josaphat appears, Barlaam and Josaphat, is based on the life of the Buddha. Josaphat was included in earlier editions of the Roman Martyrology (feast day 27 November)—though not in the Roman Missal—and in the Eastern Orthodox Church liturgical calendar (26 August). In the ancient Gnostic sect of Manichaeism, the Buddha is listed among the prophets who preached the word of God before Mani.In the Baháʼí Faith, Buddha is regarded as one of the Manifestations of God.
Some of the earliest artistic depictions of the Buddha found at Bharhut and Sanchi are aniconic and symbolic. During this early aniconic period, the Buddha is depicted by other objects or symbols, such as an empty throne, a riderless horse, footprints, a Dharma wheel or a Bodhi tree. The art at Sanchi also depicts the Jataka narratives of the Buddha in his past lives.Other styles of Indian Buddhist art depict the Buddha in human form, either standing, sitting crossed legged (often in the Lotus Pose) or laying down on one side. Iconic representations of the Buddha became particularly popular and widespread after the first century CE. Some of these depictions of the Buddha, particularly those of Gandharan Buddhism and Central Asian Buddhism, were influenced by Hellenistic art, a style known as Greco-Buddhist art.These various Indian and Central Asian styles would then go on to influence the art of East Asian Buddhist Buddha images, as well as those of Southeast Asian Theravada Buddhism.
Films Little Buddha, a 1994 film by Bernardo Bertolucci Prem Sanyas, a 1925 silent film, directed by Franz Osten and Himansu RaiTelevisionBuddha, a 2013 mythological drama on Zee TV The Buddha 2010 PBS documentary by award-winning filmmaker David Grubin and narrated by Richard GereLiteratureThe Light of Asia, an 1879 epic poem by Edwin Arnold Buddha, a manga series that ran from 1972 to 1983 by Osamu Tezuka Siddhartha novel by Hermann Hesse, written in German in 1922 Lord of Light, a novel by Roger Zelazny depicts a man in a far future Earth Colony who takes on the name and teachings of the Buddha Creation, a 1981 novel by Gore Vidal, includes the Buddha as one of the religious figures that the main character encountersMusicKaruna Nadee, a 2010 oratorio by Dinesh Subasinghe The Light of Asia, an 1886 oratorio by Dudley Buck
List of founders of religious traditions Buddha (title) Gautama Buddha in Hinduism
Works by Buddha at Project Gutenberg Works by or about Buddha at Internet Archive Works by or about Siddhārtha Gautama at Internet Archive Works by or about Shakyamuni at Internet Archive Works by Gautama Buddha at LibriVox (public domain audiobooks) Buddha on In Our Time at the BBC A sketch of the Buddha's Life What Was The Buddha Like? by Ven S. Dhammika Parables and Stories of Buddha Who was the Buddha? Buddhism for Beginners
In Buddhism, Buddha (), "awakened one," is a title for someone who is awake, and has attained nirvana and Buddhahood. The title is most commonly used for Gautama Buddha, the founder of Buddhism, who is often simply known as "the Buddha". Buddhahood (Sanskrit: buddhatva; Pali: buddhatta or buddhabhāva; Chinese: 成佛) is the condition or rank of a buddha "awakened one". This highest spiritual state of being is also termed Samyaksaṃbodhi (Full complete Awakening). The title is also used for other beings who have achieved bodhi (awakening), such as the other human Buddhas who achieved enlightenment before Gautama, the five celestial Buddhas worshiped primarily in Mahayana, and the bodhisattva named Maitreya, who will achieve enlightenment in the future and succeed Gautama Buddha as the supreme Buddha of the world. The goal of Mahayana's bodhisattva path is complete Buddhahood, so that one may benefit all sentient beings by teaching them the path of cessation of dukkha. Mahayana theory contrasts this with the goal of the Theravada path, where the most common goal is individual arhatship.
Buddhahood is the state of an awakened being, who, having found the path of cessation of dukkha ("suffering", as created by attachment to desires and distorted perception and thinking) is in the state of "No-more-Learning".There is a broad spectrum of opinion on the universality and method of attainment of Buddhahood, depending on Gautama Buddha's teachings that a school of Buddhism emphasizes. The level to which this manifestation requires ascetic practices varies from none at all to an absolute requirement, dependent on doctrine. Mahayana Buddhism emphasizes the bodhisattva ideal instead of the Arhat. In Theravada Buddhism, Buddha refers to one who has become awake through their own efforts and insight, without a teacher to point out the dharma (Sanskrit; Pali dhamma; "right way of living"). A samyaksambuddha re-discovered the truths and the path to awakening and teaches these to others after his awakening. A pratyekabuddha also reaches Nirvana through his own efforts, but does not teach the dharma to others. An arhat needs to follow the teaching of a Buddha to attain Nirvana, but can also preach the dharma after attaining Nirvana. In one instance the term buddha is also used in Theravada to refer to all who attain Nirvana, using the term Sāvakabuddha to designate an arhat, someone who depends on the teachings of a Buddha to attain Nirvana. In this broader sense it is equivalent to the arhat. The Tathagatagarba and Buddha-nature doctrines of Mahayana Buddhism consider Buddhahood to be a universal and innate property of absolute wisdom. This wisdom is revealed in a person's current lifetime through Buddhist practice, without any specific relinquishment of pleasures or "earthly desires". Buddhists do not consider Gautama to have been the only Buddha. The Pāli Canon refers to many previous ones (see list of the named Buddhas), while the Mahayana tradition additionally has many Buddhas of celestial origin (see Amitābha or Vairocana as examples. For lists of many thousands of Buddha names see Taishō Tripiṭaka numbers 439–448).
The various Buddhist schools hold some varying interpretations on the nature of Buddha (see below).
All Buddhist traditions hold that a Buddha is fully awakened and has completely purified his mind of the three poisons of craving, aversion and ignorance. A Buddha is no longer bound by saṃsāra, and has ended the suffering which unawakened people experience in life. Most schools of Buddhism have also held that the Buddha was omniscient. However, the early texts contain explicit repudiations of making this claim of the Buddha.
According to Buddhist texts, upon reaching Buddhahood each Buddha must perform ten acts during his life to complete his duty as a Buddha. A Buddha must predict that another person will attain Buddhahood in the future. A Buddha must inspire somebody else to strive for Buddhahood. A Buddha must convert all whom he must convert A Buddha must live at least three-quarters of his potential lifespan. A Buddha must have clearly defined what are good deeds and what are evil deeds. A Buddha must appoint two of his disciples as his chief disciples. A Buddha must descend from Tavatimsa Heaven after teaching his mother. A Buddha must hold an assembly at Lake Anavatapta. A Buddha must bring his parents to the Dhamma. A Buddha must have performed the great Miracle at Savatthi.
In the early Buddhist schools, the Mahāsāṃghika branch regarded the buddhas as being characterized primarily by their supramundane nature. The Mahāsāṃghikas advocated the transcendental and supramundane nature of the buddhas and bodhisattvas, and the fallibility of arhats. Of the 48 special theses attributed by the Samayabhedoparacanacakra to the Mahāsāṃghika Ekavyāvahārika, Lokottaravāda, and the Kukkuṭika, 20 points concern the supramundane nature of buddhas and bodhisattvas. According to the Samayabhedoparacanacakra, these four groups held that the Buddha is able to know all dharmas in a single moment of the mind. Yao Zhihua writes: In their view, the Buddha is equipped with the following supernatural qualities: transcendence (lokottara), lack of defilements, all of his utterances preaching his teaching, expounding all his teachings in a single utterance, all of his sayings being true, his physical body being limitless, his power (prabhāva) being limitless, the length of his life being limitless, never tiring of enlightening sentient beings and awakening pure faith in them, having no sleep or dreams, no pause in answering a question, and always in meditation (samādhi). A doctrine ascribed to the Mahāsāṃghikas is, "The power of the tathāgatas is unlimited, and the life of the buddhas is unlimited." According to Guang Xing, two main aspects of the Buddha can be seen in Mahāsāṃghika teachings: the true Buddha who is omniscient and omnipotent, and the manifested forms through which he liberates sentient beings through skillful means. For the Mahāsaṃghikas, the historical Gautama Buddha was one of these transformation bodies (Skt. nirmāṇakāya), while the essential real Buddha is equated with the Dharmakāya.As in Mahāyāna traditions, the Mahāsāṃghikas held the doctrine of the existence of many contemporaneous buddhas throughout the ten directions. In the Mahāsāṃghika Lokānuvartana Sūtra, it is stated, "The Buddha knows all the dharmas of the countless buddhas of the ten directions." It is also stated, "All buddhas have one body, the body of the Dharma." The concept of many bodhisattvas simultaneously working toward buddhahood is also found among the Mahāsāṃghika tradition, and further evidence of this is given in the Samayabhedoparacanacakra, which describes the doctrines of the Mahāsāṃghikas.
In the earliest strata of Pali Buddhist texts, especially in the first four Nikayas, only the following seven Buddhas, The Seven Buddhas of Antiquity (Saptatathāgata), are explicitly mentioned and named: Vipassī Sikhī Vessabhū Kakusandha Koṇāgamana Kasyapa GautamaOne sutta called Cakkavatti-Sīhanāda Sutta from an early Buddhist text called the Digha Nikaya also mentions that following the Seven Buddhas of Antiquity, a Buddha named Maitreya is predicted to arise in the world.However, according to a text in the Theravada Buddhist tradition from a later strata (between 1st and 2nd century BCE) called the Buddhavamsa, twenty-one more Buddhas were added to the list of seven names in the early texts. Theravada tradition maintains that there can be up to five Buddhas in a kalpa or world age and that the current kalpa has had four Buddhas, with the current Buddha, Gotama, being the fourth and the future Buddha Metteyya being the fifth and final Buddha of the kalpa. This would make the current aeon a bhadrakalpa (fortunate aeon). In some Sanskrit and northern Buddhist traditions however, a bhadrakalpa has up to 1,000 Buddhas, with the Buddhas Gotama and Metteyya also being the fourth and fifth Buddhas of the kalpa respectively.According to the Theravada tradition, of the seven Buddhas named in the early Buddhist texts four are from the current kalpa and three are from past ones. Vipassī (lived ninety-one kalpas ago) Sikhī (lived thirty-one kalpas ago) Vessabhū (lived thirty-one kalpas ago in the same kalpa as Sikhī) Kakusandha (the first Buddha of the current bhadrakalpa) Koṇāgamana (the second Buddha of the current bhadrakalpa) Kassapa (the third Buddha of the current bhadrakalpa) Gautama (the fourth and present Buddha of the current bhadrakalpa) The Koṇāgamana Buddha, is mentioned in a 3rd-century BCE inscription by Ashoka at Nigali Sagar, in today's Nepal. There is an Ashoka pillar at the site today. Ashoka's inscription in the Brahmi script is on the fragment of the pillar still partly buried in the ground. The inscription made when Emperor Asoka at Nigali Sagar in 249 BCE records his visit, the enlargement of a stupa dedicated to the Kanakamuni Buddha, and the erection of a pillar.According to Xuanzang, Koṇāgamana's relics were held in a stupa in Nigali Sagar, in what is now Kapilvastu District in southern Nepal.The historical Buddha, Gautama, also called Sakyamuni ("Sage of the Shakyas), is mentioned epigraphically on the Pillar of Ashoka at Rummindei (Lumbini in modern Nepal). The Brahmi script inscription on the pillar gives evidence that Ashoka, emperor of the Maurya Empire, visited the place in 3rd-century BCE and identified it as the birth-place of the Buddha. When King Devandmpriya Priyadarsin had been anointed twenty years, he came himself and worshipped (this spot) because the Buddha Shakyamuni was born here. (He) both caused to be made a stone bearing a horse (?) and caused a stone pillar to be set up, (in order to show) that the Blessed One was born here. (He) made the village of Lummini free of taxes, and paying (only) an eighth share (of the produce).
The Pali literature of the Theravāda tradition includes tales of 29 Buddhas. In countries where Theravāda Buddhism is practiced by the majority of people, such as Sri Lanka, Cambodia, Laos, Myanmar, Thailand, it is customary for Buddhists to hold elaborate festivals, especially during the fair weather season, paying homage to the 29 Buddhas described in the Buddhavamsa. The Buddhavamsa is a text which describes the life of Gautama Buddha and the 27 Buddhas who preceded him, along with the future Metteyya Buddha. The Buddhavamsa is part of the Khuddaka Nikāya, which in turn is part of the Sutta Piṭaka. The Sutta Piṭaka is one of three main sections of the Pāli Canon. The first three of these Buddhas—Taṇhaṅkara, Medhaṅkara, and Saraṇaṅkara—lived before the time of Dīpankara Buddha. The fourth Buddha, Dīpankara, is especially important, as he was the Buddha who gave niyatha vivarana (prediction of future Buddhahood) to the Brahmin youth who would in the distant future become the bodhisattva Gautama Buddha. After Dīpankara, 25 more noble people (ariya-puggala) would attain enlightenment before Gautama, the historical Buddha. Many Buddhists also pay homage to the future (and 29th) Buddha, Metteyya. According to Buddhist scripture, Metteya will be a successor of Gautama who will appear on Earth, achieve complete enlightenment, and teach the pure Dharma. The prophecy of the arrival of Metteyya is found in the canonical literature of all Buddhist sects (Theravada, Mahayana, and Vajrayana), and is accepted by most Buddhists as a statement about an event that will take place when the Dharma will have been forgotten on Jambudvipa (the terrestrial realm, where ordinary human beings live).
Mahayana Buddhists venerate numerous Buddhas, that are not found in early Buddhism or in Theravada Buddhism. They are generally seen as living in other realms, known as Buddhafields or Pure Lands. They are sometimes called "celestial Buddhas", since they are not from this earth. Some of the key Mahayana Buddhas are: Akshobhya ("the Imperturbable") Amitābha (Amida Buddha, "Infinite Light"), principal Buddha of Pure Land Buddhism Amoghasiddhi (“Infallible Success”) Bhaisajyaguru (also known as Medicine Buddha) Ratnasambhava ("Jewel Born") Vairocana ("the Illuminator") Lokesvararaja
In Tantric Buddhism (Vajrayana), one finds some of the same Mahayana Buddhas along with other Buddha figures which are unique to Vajrayana. There are five primary Buddhas known as the "Five Tathagathas": Vairocana, Aksobhya, Ratnasambhava, Amitābha, and Amoghasiddhi. Each is associated with a different consort, direction, aggregate (or, aspect of the personality), emotion, element, color, symbol, and mount. Buddhist Tantra also includes several female Buddhas, such as Tara, the most popular female Buddha in Tibetan Buddhism, who comes in many forms and colors. In the tantras, there are various fierce deities which are tantric forms of the Buddhas. These may be fierce (Tibetan: trowo, Sanskrit: krodha) Buddha forms or semi-fierce, and may appear in sexual union with a female Buddha or as a "solitary hero". The Herukas (Tb. khrag 'thung, lit. "blood drinker") are enlightened masculine beings who adopt fierce forms to help beings. They include Yamantaka, Cakrasamvara, Hevajra, Mahākāla, and Vajrakilaya. Dakinis (Tb. khandroma, "sky-goer") are their feminine counterparts, sometimes depicted with a heruka and sometimes as independent deities. The most prevalent wrathful dakinis are Vajrayogini, Vajravārāhī, Nairatmya, and Kurukullā. Buddhist mythology overlapped with Hindu mythology. Akshobhya, for example, acquires a fierce Tantric form that is reminiscent of the fierce form of the Hindu god Shiva; in this form he became known by the Buddhist names Heruka, Hevajra, or Samvara. He is known in Japan in this guise as Fudō (“Imperturbable”). The Indian god Bhairava, a fierce bull-headed divinity, was adopted by Tantric Buddhists as Vajrabhairava. Also called Yamantaka (“Slayer of Death”) and identified as the fierce expression of the gentle Manjushri, he was accorded quasi-buddha rank. There is also the idea of the Adi-Buddha, the "first Buddha" to attain Buddhahood. Variously named as Vajradhara, Samantabhadra and Vairocana, the first Buddha is also associated with the concept of Dharmakaya. Some historical figures are also seen as Buddhas, such as the Buddhist philosopher Nagarjuna, Tibetan historical figures like Padmasambhava, and Tsongkhapa.
Buddhas are frequently represented in the form of statues and paintings. Commonly seen designs include: The Seated Buddha The Reclining Buddha The Standing Buddha Hotei or Budai, the obese Laughing Buddha, usually seen in China (This figure is believed to be a representation of a medieval Chinese monk who is associated with Maitreya, the future Buddha, and is therefore technically not a Buddha image.) the Emaciated Buddha, which shows Siddhartha Gautama during his extreme ascetic practice of starvation.The Buddha statue shown calling for rain is a pose common in Laos.
Most depictions of Buddha contain a certain number of markings, which are considered the signs of his enlightenment. These signs vary regionally, but two are common: a protuberance on the top of the head (denoting superb mental acuity) long earlobes (denoting superb perception)In the Pāli Canon, there is frequent mention of a list of thirty-two physical characteristics of the Buddha.
The poses and hand-gestures of these statues, known respectively as asanas and mudras, are significant to their overall meaning. The popularity of any particular mudra or asana tends to be region-specific, such as the Vajra (or Chi Ken-in) mudra, which is popular in Japan and Korea but rarely seen in India. Others are more common; for example, the Varada (Wish Granting) mudra is common among standing statues of the Buddha, particularly when coupled with the Abhaya (Fearlessness and Protection) mudra.
List of bodhisattvas Ten Bodhisattas Thirty-five Confession Buddhas Praises to the Twenty-One Taras Bhadrakalpikasutra List of Buddha claimants Glossary of Buddhism Buddha-nature Enlightenment in Buddhism Eternal Buddha Physical characteristics of the Buddha
BuddhaNet
Buddhism (, US: ) is the world's fourth-largest religion with over 520 million followers, or over 7% of the global population, known as Buddhists.Buddhism encompasses a variety of traditions, beliefs and spiritual practices largely based on original teachings attributed to the Buddha and resulting interpreted philosophies. It originated in ancient India as a Sramana tradition sometime between the 6th and 4th centuries BCE, spreading through much of Asia. Two major extant branches of Buddhism are generally recognized by scholars: Theravāda (Pali: "The School of the Elders") and Mahāyāna (Sanskrit: "The Great Vehicle"). Most Buddhist traditions share the goal of overcoming suffering and the cycle of death and rebirth, either by the attainment of Nirvana or through the path of Buddhahood. Buddhist schools vary in their interpretation of the path to liberation, the relative importance and canonicity assigned to the various Buddhist texts, and their specific teachings and practices. Widely observed practices include taking refuge in the Buddha, the Dharma and the Sangha, observance of moral precepts, Buddhist monasticism, Buddhist meditation, and the cultivation of the Paramitas (perfections, or virtues). Theravada Buddhism has a widespread following in Sri Lanka and Southeast Asia such as Cambodia, Laos, Myanmar and Thailand. Mahayana, which includes the traditions of Pure Land, Zen, Nichiren Buddhism, Shingon and Tiantai (Tendai), is found throughout East Asia. Vajrayana, a body of teachings attributed to Indian adepts, may be viewed as a separate branch or as an aspect of Mahayana Buddhism. Tibetan Buddhism, which preserves the Vajrayana teachings of eighth-century India, is practised in the countries of the Himalayan region, Mongolia, and Kalmykia.
Buddhism is an Indian religion founded on the teachings of a mendicant and spiritual teacher called "the Buddha" ("the Awakened One", c. 5th to 4th century BCE). Early texts have the Buddha's family name as "Gautama" (Pali: Gotama). The details of Buddha's life are mentioned in many Early Buddhist Texts but are inconsistent, and his social background and life details are difficult to prove, the precise dates are uncertain.The evidence of the early texts suggests that Siddharta Gautama was born in Lumbini and grew up in Kapilavastu, a town in the Ganges Plain, near the modern Nepal–India border, and that he spent his life in what is now modern Bihar and Uttar Pradesh. Some hagiographic legends state that his father was a king named Suddhodana, his mother was Queen Maya, and he was born in Lumbini. However, scholars such as Richard Gombrich consider this a dubious claim because a combination of evidence suggests he was born in the Shakya community, which was governed by a small oligarchy or republic-like council where there were no ranks but where seniority mattered instead. Some of the stories about Buddha, his life, his teachings, and claims about the society he grew up in may have been invented and interpolated at a later time into the Buddhist texts.According to early texts such as the Pali Ariyapariyesanā-sutta ("The discourse on the noble quest," MN 26) and its Chinese parallel at MĀ 204, Gautama was moved by the suffering (dukkha) of life and death, and its endless repetition due to rebirth. He thus set out on a quest to find liberation from suffering (also known as "nirvana"). Early texts and biographies state that Gautama first studied under two teachers of meditation, namely Alara Kalama (Sanskrit: Arada Kalama) and Uddaka Ramaputta (Sanskrit: Udraka Ramaputra), learning meditation and philosophy, particularly the meditative attainment of "the sphere of nothingness" from the former, and "the sphere of neither perception nor non-perception" from the latter.Finding these teachings to be insufficient to attain his goal, he turned to the practice of severe asceticism, which included a strict fasting regime and various forms of breath control. This too fell short of attaining his goal, and then he turned to the meditative practice of dhyana. He famously sat in meditation under a Ficus religiosa tree now called the Bodhi Tree in the town of Bodh Gaya and attained "Awakening" (Bodhi).According to various early texts like the Mahāsaccaka-sutta, and the Samaññaphala Sutta, on awakening, the Buddha gained insight into the workings of karma and his former lives, as well as achieving the ending of the mental defilements (asavas), the ending of suffering, and the end of rebirth in saṃsāra. This event also brought certainty about the Middle Way as the right path of spiritual practice to end suffering. As a fully enlightened Buddha, he attracted followers and founded a Sangha (monastic order). He spent the rest of his life teaching the Dharma he had discovered, and then died, achieving "final nirvana," at the age of 80 in Kushinagar, India.Buddha's teachings were propagated by his followers, which in the last centuries of the 1st millennium BCE became various Buddhist schools of thought, each with its own basket of texts containing different interpretations and authentic teachings of the Buddha; these over time evolved into many traditions of which the more well known and widespread in the modern era are Theravada, Mahayana and Vajrayana Buddhism.
The Four Truths express the basic orientation of Buddhism: we crave and cling to impermanent states and things, which is dukkha, "incapable of satisfying" and painful. This keeps us caught in saṃsāra, the endless cycle of repeated rebirth, dukkha and dying again. But there is a way to liberation from this endless cycle to the state of nirvana, namely following the Noble Eightfold Path.The truth of dukkha is the basic insight that life in this mundane world, with its clinging and craving to impermanent states and things is dukkha, and unsatisfactory. Dukkha can be translated as "incapable of satisfying," "the unsatisfactory nature and the general insecurity of all conditioned phenomena"; or "painful." Dukkha is most commonly translated as "suffering," but this is inaccurate, since it refers not to episodic suffering, but to the intrinsically unsatisfactory nature of temporary states and things, including pleasant but temporary experiences. We expect happiness from states and things which are impermanent, and therefore cannot attain real happiness. In Buddhism, dukkha is one of the three marks of existence, along with impermanence and anattā (non-self). Buddhism, like other major Indian religions, asserts that everything is impermanent (anicca), but, unlike them, also asserts that there is no permanent self or soul in living beings (anattā). The ignorance or misperception (avijjā) that anything is permanent or that there is self in any being is considered a wrong understanding, and the primary source of clinging and dukkha.Dukkha arises when we crave (Pali: taṇhā) and cling to these changing phenomena. The clinging and craving produces karma, which ties us to samsara, the round of death and rebirth. Craving includes kama-tanha, craving for sense-pleasures; bhava-tanha, craving to continue the cycle of life and death, including rebirth; and vibhava-tanha, craving to not experience the world and painful feelings.Dukkha ceases, or can be confined, when craving and clinging cease or are confined. This also means that no more karma is being produced, and rebirth ends. Cessation is nirvana, "blowing out," and peace of mind.By following the Buddhist path to moksha, liberation, one starts to disengage from craving and clinging to impermanent states and things. The term "path" is usually taken to mean the Noble Eightfold Path, but other versions of "the path" can also be found in the Nikayas. The Theravada tradition regards insight into the four truths as liberating in itself.
Saṃsāra means "wandering" or "world", with the connotation of cyclic, circuitous change. It refers to the theory of rebirth and "cyclicality of all life, matter, existence", a fundamental assumption of Buddhism, as with all major Indian religions. Samsara in Buddhism is considered to be dukkha, unsatisfactory and painful, perpetuated by desire and avidya (ignorance), and the resulting karma.The theory of rebirths, and realms in which these rebirths can occur, is extensively developed in Buddhism, in particular Tibetan Buddhism with its wheel of existence (Bhavacakra) doctrine. Liberation from this cycle of existence, nirvana, has been the foundation and the most important historical justification of Buddhism.The later Buddhist texts assert that rebirth can occur in six realms of existence, namely three good realms (heavenly, demi-god, human) and three evil realms (animal, hungry ghosts, hellish). Samsara ends if a person attains nirvana, the "blowing out" of the desires and the gaining of true insight into impermanence and non-self reality.
Rebirth refers to a process whereby beings go through a succession of lifetimes as one of many possible forms of sentient life, each running from conception to death. In Buddhist thought, this rebirth does not involve any soul, because of its doctrine of anattā (Sanskrit: anātman, no-self doctrine) which rejects the concepts of a permanent self or an unchanging, eternal soul, as it is called in Hinduism and Christianity. According to Buddhism there ultimately is no such thing as a self in any being or any essence in any thing.The Buddhist traditions have traditionally disagreed on what it is in a person that is reborn, as well as how quickly the rebirth occurs after each death. Some Buddhist traditions assert that "no self" doctrine means that there is no perduring self, but there is avacya (inexpressible) self which migrates from one life to another. The majority of Buddhist traditions, in contrast, assert that vijñāna (a person's consciousness) though evolving, exists as a continuum and is the mechanistic basis of what undergoes rebirth, rebecoming and redeath. The rebirth depends on the merit or demerit gained by one's karma, as well as that accrued on one's behalf by a family member.Each rebirth takes place within one of five realms according to Theravadins, or six according to other schools – heavenly, demi-gods, humans, animals, hungry ghosts and hellish.In East Asian and Tibetan Buddhism, rebirth is not instantaneous, and there is an intermediate state (Tibetan "bardo") between one life and the next. The orthodox Theravada position rejects the wait, and asserts that rebirth of a being is immediate. However there are passages in the Samyutta Nikaya of the Pali Canon that seem to lend support to the idea that the Buddha taught about an intermediate stage between one life and the next.
The cessation of the kleshas and the attainment of nirvana (nibbāna), with which the cycle of rebirth ends, has been the primary and the soteriological goal of the Buddhist path for monastic life since the time of the Buddha. The term "path" is usually taken to mean the Noble Eightfold Path, but other versions of "the path" can also be found in the Nikayas. In some passages in the Pali Canon, a distinction is being made between right knowledge or insight (sammā-ñāṇa), and right liberation or release (sammā-vimutti), as the means to attain cessation and liberation.Nirvana literally means "blowing out, quenching, becoming extinguished". In early Buddhist texts, it is the state of restraint and self-control that leads to the "blowing out" and the ending of the cycles of sufferings associated with rebirths and redeaths. Many later Buddhist texts describe nirvana as identical with anatta with complete "emptiness, nothingness". In some texts, the state is described with greater detail, such as passing through the gate of emptiness (sunyata) – realising that there is no soul or self in any living being, then passing through the gate of signlessness (animitta) – realising that nirvana cannot be perceived, and finally passing through the gate of wishlessness (apranihita) – realising that nirvana is the state of not even wishing for nirvana.The nirvana state has been described in Buddhist texts partly in a manner similar to other Indian religions, as the state of complete liberation, enlightenment, highest happiness, bliss, fearlessness, freedom, permanence, non-dependent origination, unfathomable, and indescribable. It has also been described in part differently, as a state of spiritual release marked by "emptiness" and realisation of non-self.While Buddhism considers the liberation from saṃsāra as the ultimate spiritual goal, in traditional practice, the primary focus of a vast majority of lay Buddhists has been to seek and accumulate merit through good deeds, donations to monks and various Buddhist rituals in order to gain better rebirths rather than nirvana.
Pratityasamutpada, also called "dependent arising, or dependent origination", is the Buddhist theory to explain the nature and relations of being, becoming, existence and ultimate reality. Buddhism asserts that there is nothing independent, except the state of nirvana. All physical and mental states depend on and arise from other pre-existing states, and in turn from them arise other dependent states while they cease.The 'dependent arisings' have a causal conditioning, and thus Pratityasamutpada is the Buddhist belief that causality is the basis of ontology, not a creator God nor the ontological Vedic concept called universal Self (Brahman) nor any other 'transcendent creative principle'. However, the Buddhist thought does not understand causality in terms of Newtonian mechanics, rather it understands it as conditioned arising. In Buddhism, dependent arising is referring to conditions created by a plurality of causes that necessarily co-originate a phenomenon within and across lifetimes, such as karma in one life creating conditions that lead to rebirth in one of the realms of existence for another lifetime.Buddhism applies the dependent arising theory to explain origination of endless cycles of dukkha and rebirth, through its Twelve Nidānas or "twelve links" doctrine. It states that because Avidyā (ignorance) exists Saṃskāras (karmic formations) exists, because Saṃskāras exists therefore Vijñāna (consciousness) exists, and in a similar manner it links Nāmarūpa (sentient body), Ṣaḍāyatana (six senses), Sparśa (sensory stimulation), Vedanā (feeling), Taṇhā (craving), Upādāna (grasping), Bhava (becoming), Jāti (birth), and Jarāmaraṇa (old age, death, sorrow, pain).By breaking the circuitous links of the Twelve Nidanas, Buddhism asserts that liberation from these endless cycles of rebirth and dukkha can be attained.
A related doctrine in Buddhism is that of anattā (Pali) or anātman (Sanskrit). It is the view that there is no unchanging, permanent self, soul or essence in phenomena. The Buddha and Buddhist philosophers who follow him such as Vasubandhu and Buddhaghosa, generally argue for this view through by analyzing the person through the schema of the five aggregates, and then attempting to show that none of these five components of personality can be permanent or absolute. This can be seen in Buddhist discourses such as the Anattalakkhana Sutta. "Emptiness" or "voidness" (Skt: Śūnyatā, Pali: Suññatā), is a related concept with many different interpretations throughout the various Buddhisms. In early Buddhism, it was commonly stated that all five aggregates are void (rittaka), hollow (tucchaka), coreless (asāraka), for example as in the Pheṇapiṇḍūpama Sutta (SN 22:95). Similarly, in Theravada Buddhism, it often simply means that the five aggregates are empty of a Self.Emptiness is a central concept in Mahāyāna Buddhism, especially in Nagarjuna's Madhyamaka school, and in the Prajñāpāramitā sutras. In Madhyamaka philosophy, emptiness is the view which holds that all phenomena (dharmas) are without any svabhava (literally "own-nature" or "self-nature"), and are thus without any underlying essence, and so are "empty" of being independent. This doctrine sought to refute the heterodox theories of svabhava circulating at the time.
While all varieties of Buddhism revere "Buddha" and "buddhahood", they have different views on what these are. Whatever that may be, "Buddha" is still central to all forms of Buddhism. In Theravada Buddhism, a Buddha is someone who has become awake through their own efforts and insight. They have put an end to their cycle of rebirths and have ended all unwholesome mental states which lead to bad action and thus are morally perfected. While subject to the limitations of the human body in certain ways (for example, in the early texts, the Buddha suffers from backaches), a Buddha is said to be "deep, immeasurable, hard-to-fathom as is the great ocean," and also has immense psychic powers (abhijñā).Theravada generally sees Gautama Buddha (the historical Buddha Sakyamuni) as the only Buddha of the current era. While he is no longer in this world, he has left us the Dharma (Teaching), the Vinaya (Discipline) and the Sangha (Community). There are also said to be two types of Buddhas, a sammasambuddha is also said to teach the Dharma to others, while a paccekabuddha (solitary buddha) does not teach.Mahāyāna Buddhism meanwhile, has a vastly expanded cosmology, with various Buddhas and other holy beings (aryas) residing in different realms. Mahāyāna texts not only revere numerous Buddhas besides Sakyamuni, such as Amitabha and Vairocana, but also see them as transcendental or supramundane (lokuttara) beings. Mahāyāna Buddhism holds that these other Buddhas in other realms can be contacted and are able to benefit beings in this world. In Mahāyāna, a Buddha is a kind of "spiritual king", a "protector of all creatures" with a lifetime that is countless of eons long, rather than just a human teacher who has transcended the world after death. Buddha Sakyamuni's life and death on earth is then usually understood as a "mere appearance" or "a manifestation skilfully projected into earthly life by a long-enlightened transcendent being, who is still available to teach the faithful through visionary experiences."
The third "jewel" which Buddhists take refuge in is the "Sangha", which refers to the monastic community of monks and nuns who follow Gautama Buddha's monastic discipline which was "designed to shape the Sangha as an ideal community, with the optimum conditions for spiritual growth." The Sangha consists of those who have chosen to follow the Buddha's ideal way of life, which is one of celibate monastic renunciation with minimal material possessions (such as an alms bowl and robes).The Sangha is seen as important because they preserve and pass down Buddha Dharma. As Gethin states "the Sangha lives the teaching, preserves the teaching as Scriptures and teaches the wider community. Without the Sangha there is no Buddhism."The Sangha also acts as a "field of merit" for laypersons, allowing them to make spiritual merit or goodness by donating to the Sangha and supporting them. In return, they keep their duty to preserve and spread the Dharma everywhere for the good of the world.The Sangha is also supposed to follow the Vinaya (monastic rule) of the Buddha, thereby serving as an spiritual example for the world and future generations. The Vinaya rules also force the Sangha to live in dependence on the rest of the lay community (they must beg for food etc) and thus draw the Sangha into a relationship with the lay community. There is also a separate definition of Sangha, referring to those who have attained any stage of awakening, whether or not they are monastics. This sangha is called the āryasaṅgha "noble Sangha". All forms of Buddhism generally reveres these āryas (Pali: ariya, "noble ones" or "holy ones") who are spiritually attained beings. Aryas have attained the fruits of the Buddhist path. Becoming an arya is a goal in most forms of Buddhism. The āryasaṅgha includes holy beings such as bodhisattvas, arhats and stream-enterers. In early Buddhism and in Theravada Buddhism, an arhat (literally meaning "worthy") is someone who reached the same awakening (bodhi) of a Buddha by following the teaching of a Buddha. They are seen as having ended rebirth and all the mental defilements. A bodhisattva ("a being bound for awakening") meanwhile, is simply a name for someone who is working towards awakening (bodhi) as a Buddha. According to all the early buddhist schools as well as Theravada, to be considered a bodhisattva one has to have made a vow in front of a living Buddha and also has to have received a confirmation of one's future Buddhahood. In Theravada, the future Buddha is called Metteyya (Maitreya) and he is revered as a bodhisatta currently working for future Buddhahood.Mahāyāna Buddhism generally sees the attainment of the arhat as an inferior one, since it is seen as being done only for the sake of individual liberation. It thus promotes the bodhisattva path as the highest and most worthwhile. While in Mahāyāna, anyone who has given rise to bodhicitta (the wish to become a Buddha that arises from a sense of compassion for all beings) is considered a bodhisattva, some of these holy beings (such as Maitreya and Avalokiteshvara) have reached very high levels of spiritual attainment and are seen as being very powerful supramundane beings who provide aid to countless beings through their advanced powers.
Mahāyāna Buddhism also differs from Theravada and the other schools of early Buddhism in promoting several unique doctrines which are contained in Mahāyāna sutras and philosophical treatises. One of these is the unique interpretation of emptiness and dependent origination found in the Madhyamaka school. Another very influential doctrine for Mahāyāna is the main philosophical view of the Yogācāra school variously, termed Vijñaptimātratā-vāda ("the doctrine that there are only ideas" or "mental impressions") or Vijñānavāda ("the doctrine of consciousness"). According to Mark Siderits, what classical Yogācāra thinkers like Vasubandhu had in mind is that we are only ever aware of mental images or impressions, which may appear as external objects, but "there is actually no such thing outside the mind." There are several interpretations of this main theory, many scholars see it as a type of Idealism, others as a kind of phenomenology.Another very influential concept unique to Mahāyāna is that of "Buddha-nature" (buddhadhātu) or "Tathagata-womb" (tathāgatagarbha). Buddha-nature is a concept found in some 1st-millennium CE Buddhist texts, such as the Tathāgatagarbha sūtras. According to Paul Williams these Sutras suggest that 'all sentient beings contain a Tathagata' as their 'essence, core inner nature, Self'. According to Karl Brunnholzl "the earliest mahayana sutras that are based on and discuss the notion of tathāgatagarbha as the buddha potential that is innate in all sentient beings began to appear in written form in the late second and early third century." For some, the doctrine seems to conflict with the Buddhist anatta doctrine (non-Self), leading scholars to posit that the Tathāgatagarbha Sutras were written to promote Buddhism to non-Buddhists. This can be seen in texts like the Laṅkāvatāra Sūtra, which state that Buddha-nature is taught to help those who have fear when they listen to the teaching of anatta. Buddhist texts like the Ratnagotravibhāga clarify that the "Self" implied in Tathagatagarbha doctrine is actually "not-Self". Various interpretations of the concept have been advanced by Buddhist thinkers throughout the history of Buddhist thought and most attempt to avoid anything like the Hindu Atman doctrine. These Indian Buddhist ideas, in various synthetic ways, form the basis of subsequent Mahāyāna philosophy in Tibetan Buddhism and East Asian Buddhism.
While the Noble Eightfold Path is best-known in the West, a wide variety of paths and models of progress have been used and described in the different Buddhist traditions. However, they generally share basic practices such as sila (ethics), samadhi (meditation, dhyana) and prajña (wisdom), which are known as the three trainings. An important additional practice is a kind and compassionate attitude toward every living being and the world. Devotion is also important in some Buddhist traditions, and in the Tibetan traditions visualisations of deities and mandalas are important. The value of textual study is regarded differently in the various Buddhist traditions. It is central to Theravada and highly important to Tibetan Buddhism, while the Zen tradition takes an ambiguous stance. An important guiding principle of Buddhist practice is the Middle Way (madhyamapratipad). It was a part of Buddha's first sermon, where he presented the Noble Eightfold Path that was a 'middle way' between the extremes of asceticism and hedonistic sense pleasures. In Buddhism, states Harvey, the doctrine of "dependent arising" (conditioned arising, pratītyasamutpāda) to explain rebirth is viewed as the 'middle way' between the doctrines that a being has a "permanent soul" involved in rebirth (eternalism) and "death is final and there is no rebirth" (annihilationism).
A common presentation style of the path (mārga) to liberation in the Early Buddhist Texts is the "graduated talk", in which the Buddha lays out a step by step training.In the early texts, numerous different sequences of the gradual path can be found. One of the most important and widely used presentations among the various Buddhist schools is The Noble Eightfold Path, or "Eightfold Path of the Noble Ones" (Skt. 'āryāṣṭāṅgamārga'). This can be found in various discourses, most famously in the Dhammacakkappavattana Sutta (The discourse on the turning of the Dharma wheel). Other suttas such as the Tevijja Sutta, and the Cula-Hatthipadopama-sutta give a different outline of the path, though with many similar elements such as ethics and meditation.According to Rupert Gethin, the path to awakening is also frequently summarized by another a short formula: "abandoning the hindrances, practice of the four establishings of mindfulness, and development of the awakening factors."
The Eightfold Path consists of a set of eight interconnected factors or conditions, that when developed together, lead to the cessation of dukkha. These eight factors are: Right View (or Right Understanding), Right Intention (or Right Thought), Right Speech, Right Action, Right Livelihood, Right Effort, Right Mindfulness, and Right Concentration. This Eightfold Path is the fourth of the Four Noble Truths, and asserts the path to the cessation of dukkha (suffering, pain, unsatisfactoriness). The path teaches that the way of the enlightened ones stopped their craving, clinging and karmic accumulations, and thus ended their endless cycles of rebirth and suffering.The Noble Eightfold Path is grouped into three basic divisions, as follows:
Theravada Buddhism is a diverse tradition and thus includes different explanations of the path to awakening. However, the teachings of the Buddha are often encapsulated by Theravadins in the basic framework of the Four Noble Truths and the Eighthfold Path.Some Theravada Buddhists also follow the presentation of the path laid out in Buddhaghosa's Visuddhimagga. This presentation is known as the "Seven Purifications" (satta-visuddhi). This schema and its accompanying outline of "insight knowledges" (vipassanā-ñāṇa) is used by modern influential Theravadin scholars, such Mahasi Sayadaw (in his "The Progress of Insight") and Nyanatiloka Thera (in "The Buddha's Path to Deliverance").
Mahāyāna Buddhism is based principally upon the path of a Bodhisattva. A Bodhisattva refers to one who is on the path to buddhahood. The term Mahāyāna was originally a synonym for Bodhisattvayāna or "Bodhisattva Vehicle."In the earliest texts of Mahāyāna Buddhism, the path of a bodhisattva was to awaken the bodhicitta. Between the 1st and 3rd century CE, this tradition introduced the Ten Bhumi doctrine, which means ten levels or stages of awakening. This development was followed by the acceptance that it is impossible to achieve Buddhahood in one (current) lifetime, and the best goal is not nirvana for oneself, but Buddhahood after climbing through the ten levels during multiple rebirths. Mahāyāna scholars then outlined an elaborate path, for monks and laypeople, and the path includes the vow to help teach Buddhist knowledge to other beings, so as to help them cross samsara and liberate themselves, once one reaches the Buddhahood in a future rebirth. One part of this path are the pāramitā (perfections, to cross over), derived from the Jatakas tales of Buddha's numerous rebirths.The doctrine of the bodhisattva bhūmis was also eventually merged with the Sarvāstivāda Vaibhāṣika schema of the "five paths" by the Yogacara school. This Mahāyāna "five paths" presentation can be seen in Asanga's Mahāyānasaṃgraha.The Mahāyāna texts are inconsistent in their discussion of the pāramitās, and some texts include lists of two, others four, six, ten and fifty-two. The six paramitas have been most studied, and these are: Dāna pāramitā: perfection of giving; primarily to monks, nuns and the Buddhist monastic establishment dependent on the alms and gifts of the lay householders, in return for generating religious merit; some texts recommend ritually transferring the merit so accumulated for better rebirth to someone else Śīla pāramitā: perfection of morality; it outlines ethical behaviour for both the laity and the Mahayana monastic community; this list is similar to Śīla in the Eightfold Path (i.e. Right Speech, Right Action, Right Livelihood) Kṣānti pāramitā: perfection of patience, willingness to endure hardship Vīrya pāramitā: perfection of vigour; this is similar to Right Effort in the Eightfold Path Dhyāna pāramitā: perfection of meditation; this is similar to Right Concentration in the Eightfold Path Prajñā pāramitā: perfection of insight (wisdom), awakening to the characteristics of existence such as karma, rebirths, impermanence, no-self, dependent origination and emptiness; this is complete acceptance of the Buddha teaching, then conviction, followed by ultimate realisation that "dharmas are non-arising".In Mahāyāna Sutras that include ten pāramitā, the additional four perfections are "skillful means, vow, power and knowledge". The most discussed pāramitā and the highest rated perfection in Mahayana texts is the "Prajna-paramita", or the "perfection of insight". This insight in the Mahāyāna tradition, states Shōhei Ichimura, has been the "insight of non-duality or the absence of reality in all things".
East Asian Buddhism in influenced by both the classic Indian Buddhist presentations of the path such as the eighth-fold path as well as classic Indian Mahāyāna presentations such as that found in the Da zhidu lun.There many different presentations of soteriology, including numerous paths and vehicles (yanas) in the different traditions of East Asian Buddhism. There is no single dominant presentation. In Zen Buddhism for example, one can find outlines of the path such as the Two Entrances and Four Practices, The Five ranks, The Ten Ox-Herding Pictures and The Three mysterious Gates of Linji.
In Indo-Tibetan Buddhism, the path to liberation is outlined in the genre known as Lamrim ("Stages of the Path"). All the various Tibetan schools have their own Lamrim presentations. This genre can be traced to Atiśa's 11th-century A Lamp for the Path to Enlightenment (Bodhipathapradīpa).
In various suttas which present the graduated path taught by the Buddha, such as the Samaññaphala Sutta and the Cula-Hatthipadopama Sutta, the first step on the path is hearing the Buddha teach the Dharma. This then said to lead to the acquiring of confidence or faith in the Buddha's teachings.Mahayana Buddhist teachers such as Yin Shun also state that hearing the Dharma and study of the Buddhist discourses is necessary "if one wants to learn and practice the Buddha Dharma." Likewise, in Indo-Tibetan Buddhism, the "Stages of the Path" (Lamrim) texts generally place the activity of listening to the Buddhist teachings as an important early practice.
Buddhist scriptures explain the five precepts (Pali: pañcasīla; Sanskrit: pañcaśīla) as the minimal standard of Buddhist morality. It is the most important system of morality in Buddhism, together with the monastic rules.The five precepts are seen as a basic training applicable to all Buddhists. They are: "I undertake the training-precept (sikkha-padam) to abstain from onslaught on breathing beings." This includes ordering or causing someone else to kill. The Pali suttas also say one should not "approve of others killing" and that one should be "scrupulous, compassionate, trembling for the welfare of all living beings." "I undertake the training-precept to abstain from taking what is not given." According to Harvey, this also covers fraud, cheating, forgery as well as "falsely denying that one is in debt to someone." "I undertake the training-precept to abstain from misconduct concerning sense-pleasures." This generally refers to adultery, as well as rape and incest. It also applies to sex with those who are legally under the protection of a guardian. It is also interpreted in different ways in the varying Buddhist cultures. "I undertake the training-precept to abstain from false speech." According to Harvey this includes "any form of lying, deception or exaggeration...even non-verbal deception by gesture or other indication...or misleading statements." The precept is often also seen as including other forms of wrong speech such as "divisive speech, harsh, abusive, angry words, and even idle chatter." "I undertake the training-precept to abstain from alcoholic drink or drugs that are an opportunity for heedlessness." According to Harvey, intoxication is seen as a way to mask rather than face the sufferings of life. It is seen as damaging to one's mental clarity, mindfulness and ability to keep the other four precepts.Undertaking and upholding the five precepts is based on the principle of non-harming (Pāli and Sanskrit: ahiṃsa). The Pali Canon recommends one to compare oneself with others, and on the basis of that, not to hurt others. Compassion and a belief in karmic retribution form the foundation of the precepts. Undertaking the five precepts is part of regular lay devotional practice, both at home and at the local temple. However, the extent to which people keep them differs per region and time. They are sometimes referred to as the śrāvakayāna precepts in the Mahāyāna tradition, contrasting them with the bodhisattva precepts.The five precepts are not commandments and transgressions do not invite religious sanctions, but their power has been based on the Buddhist belief in karmic consequences and their impact in the afterlife. Killing in Buddhist belief leads to rebirth in the hell realms, and for a longer time in more severe conditions if the murder victim was a monk. Adultery, similarly, invites a rebirth as prostitute or in hell, depending on whether the partner was unmarried or married. These moral precepts have been voluntarily self-enforced in lay Buddhist culture through the associated belief in karma and rebirth. Within the Buddhist doctrine, the precepts are meant to develop mind and character to make progress on the path to enlightenment.The monastic life in Buddhism has additional precepts as part of patimokkha, and unlike lay people, transgressions by monks do invite sanctions. Full expulsion from sangha follows any instance of killing, engaging in sexual intercourse, theft or false claims about one's knowledge. Temporary expulsion follows a lesser offence. The sanctions vary per monastic fraternity (nikaya).Lay people and novices in many Buddhist fraternities also uphold eight (asta shila) or ten (das shila) from time to time. Four of these are same as for the lay devotee: no killing, no stealing, no lying, and no intoxicants. The other four precepts are: No sexual activity; Abstain from eating at the wrong time (e.g. only eat solid food before noon); Abstain from jewellery, perfume, adornment, entertainment; Abstain from sleeping on high bed i.e. to sleep on a mat on the ground.All eight precepts are sometimes observed by lay people on uposatha days: full moon, new moon, the first and last quarter following the lunar calendar. The ten precepts also include to abstain from accepting money.In addition to these precepts, Buddhist monasteries have hundreds of rules of conduct, which are a part of its patimokkha.
Vinaya is the specific code of conduct for a sangha of monks or nuns. It includes the Patimokkha, a set of 227 offences including 75 rules of decorum for monks, along with penalties for transgression, in the Theravadin tradition. The precise content of the Vinaya Pitaka (scriptures on the Vinaya) differs in different schools and tradition, and different monasteries set their own standards on its implementation. The list of pattimokkha is recited every fortnight in a ritual gathering of all monks. Buddhist text with vinaya rules for monasteries have been traced in all Buddhist traditions, with the oldest surviving being the ancient Chinese translations.Monastic communities in the Buddhist tradition cut normal social ties to family and community, and live as "islands unto themselves". Within a monastic fraternity, a sangha has its own rules. A monk abides by these institutionalised rules, and living life as the vinaya prescribes it is not merely a means, but very nearly the end in itself. Transgressions by a monk on Sangha vinaya rules invites enforcement, which can include temporary or permanent expulsion.
Another important practice taught by the Buddha is the restraint of the senses (indriyasamvara). In the various graduated paths, this is usually presented as a practice which is taught prior to formal sitting meditation, and which supports meditation by weakening sense desires that are a hindrance to meditation. According to Anālayo, sense restraint is when one "guards the sense doors in order to prevent sense impressions from leading to desires and discontent." This is not an avoidance of sense impression, but a kind of mindful attention towards the sense impressions which does not dwell on their main features or signs (nimitta). This is said to prevent harmful influences from entering the mind. This practice is said to give rise to an inner peace and happiness which forms a basis for concentration and insight.A related Buddhist virtue and practice is renunciation, or the intent for desirelessness (nekkhamma). Generally, renunciation is the giving up of actions and desires that are seen as unwholesome on the path, such as lust for sensuality and worldly things. Renunciation can be cultivated in different ways. The practice of giving for example, is one form of cultivating renunciation. Another one is the giving up of lay life and becoming a monastic (bhiksu o bhiksuni). Practicing celibacy (whether for life as a monk, or temporarily) is also a form of renunciation.</ref> Many Jataka stories such as the focus on how the Buddha practiced renunciation in past lives.One way of cultivating renunciation taught by the Buddha is the contemplation (anupassana) of the "dangers" (or "negative consequences") of sensual pleasure (kāmānaṃ ādīnava). As part of the graduated discourse, this contemplation is taught after the practice of giving and morality.Another related practice to renunciation and sense restraint taught by the Buddha is "restraint in eating" or moderation with food, which for monks generally means not eating after noon. Devout laypersons also follow this rule during special days of religious observance (uposatha). Observing the Uposatha also includes other practices dealing with renunciation, mainly the eight precepts. For Buddhist monastics, renunciation can also be trained through several optional ascetic practices called dhutaṅga. In different Buddhist traditions, other related practices which focus on fasting are followed.
The training of the faculty called "mindfulness" (Pali: sati, Sanskrit: smṛti, literally meaning "recollection, remembering") is central in Buddhism. According to Analayo, mindfulness is a full awareness of the present moment which enhances and strengthens memory. The Indian Buddhist philosopher Asanga defined mindfulness thus: "It is non-forgetting by the mind with regard to the object experienced. Its function is non-distraction." According to Rupert Gethin, sati is also "an awareness of things in relation to things, and hence an awareness of their relative value."There are different practices and exercises for training mindfulness in the early discourses, such as the four Satipaṭṭhānas (Sanskrit: smṛtyupasthāna, "establishments of mindfulness") and Ānāpānasati (Sanskrit: ānāpānasmṛti, "mindfulness of breathing"). A closely related mental faculty, which is often mentioned side by side with mindfulness, is sampajañña ("clear comprehension"). This faculty is the ability to comprehend what one is doing and is happening in the mind, and whether it is being influenced by unwholesome states or wholesome ones.
A wide range of meditation practices has developed in the Buddhist traditions, but "meditation" primarily refers to the attainment of samādhi and the practice of dhyāna (Pali: jhāna). Samādhi is a calm, undistracted, unified and concentrated state of consciousness. It is defined by Asanga as "one-pointedness of mind on the object to be investigated. Its function consists of giving a basis to knowledge (jñāna)."Dhyāna is "state of perfect equanimity and awareness (upekkhā-sati-parisuddhi)," reached through focused mental training.The practice of dhyāna aids in maintaining a calm mind, and avoiding disturbance of this calm mind by mindfulness of disturbing thoughts and feelings.
The earliest evidence of yogis and their meditative tradition, states Karel Werner, is found in the Keśin hymn 10.136 of the Rigveda. While evidence suggests meditation was practised in the centuries preceding the Buddha, the meditative methodologies described in the Buddhist texts are some of the earliest among texts that have survived into the modern era. These methodologies likely incorporate what existed before the Buddha as well as those first developed within Buddhism.There is no scholarly agreement on the origin and source of the practice of dhyāna. Some scholars, like Bronkhorst, see the four dhyānas as a Buddhist invention. Alexander Wynne argues that the Buddha learned dhyāna from brahmanical teachers.Whatever the case, the Buddha taught meditation with a new focus and interpretation, particularly through the four dhyānas methodology, in which mindfulness is maintained. Further, the focus of meditation and the underlying theory of liberation guiding the meditation has been different in Buddhism. For example, states Bronkhorst, the verse 4.4.23 of the Brihadaranyaka Upanishad with its "become calm, subdued, quiet, patiently enduring, concentrated, one sees soul in oneself" is most probably a meditative state. The Buddhist discussion of meditation is without the concept of soul and the discussion criticises both the ascetic meditation of Jainism and the "real self, soul" meditation of Hinduism.
Often grouped into the jhāna-scheme are four other meditative states, referred to in the early texts as arupa samāpattis (formless attainments). These are also referred to in commentarial literature as immaterial/formless jhānas (arūpajhānas). The first formless attainment is a place or realm of infinite space (ākāsānañcāyatana) without form or colour or shape. The second is termed the realm of infinite consciousness (viññāṇañcāyatana); the third is the realm of nothingness (ākiñcaññāyatana), while the fourth is the realm of "neither perception nor non-perception". The four rupa-jhānas in Buddhist practice lead to rebirth in successfully better rupa Brahma heavenly realms, while arupa-jhānas lead into arupa heavens.
In the Pali canon, the Buddha outlines two meditative qualities which are mutually supportive: samatha (Pāli; Sanskrit: śamatha; "calm") and vipassanā (Sanskrit: vipaśyanā, insight). The Buddha compares these mental qualities to a "swift pair of messengers" who together help deliver the message of nibbana (SN 35.245).The various Buddhist traditions generally see Buddhist meditation as being divided into those two main types. Samatha is also called "calming meditation", and focuses on stilling and concentrating the mind i.e. developing samadhi and the four dhyānas. According to Damien Keown, vipassanā meanwhile, focuses on "the generation of penetrating and critical insight (paññā)".There are numerous doctrinal positions and disagreements within the different Buddhist traditions regarding these qualities or forms of meditation. For example, in the Pali Four Ways to Arahantship Sutta (AN 4.170), it is said that one can develop calm and then insight, or insight and then calm, or both at the same time. Meanwhile, in Vasubandhu's Abhidharmakośakārikā, vipaśyanā is said to be practiced once one has reached samadhi by cultivating the four foundations of mindfulness (smṛtyupasthānas).Beginning with comments by La Vallee Poussin, a series of scholars have argued that these two meditation types reflect a tension between two different ancient Buddhist traditions regarding the use of dhyāna, one which focused on insight based practice and the other which focused purely on dhyāna. However, other scholars such as Analayo and Rupert Gethin have disagreed with this "two paths" thesis, instead seeing both of these practices as complementary.
The four immeasurables or four abodes, also called Brahma-viharas, are virtues or directions for meditation in Buddhist traditions, which helps a person be reborn in the heavenly (Brahma) realm. These are traditionally believed to be a characteristic of the deity Brahma and the heavenly abode he resides in.The four Brahma-vihara are: Loving-kindness (Pāli: mettā, Sanskrit: maitrī) is active good will towards all; Compassion (Pāli and Sanskrit: karuṇā) results from metta; it is identifying the suffering of others as one's own; Empathetic joy (Pāli and Sanskrit: muditā): is the feeling of joy because others are happy, even if one did not contribute to it; it is a form of sympathetic joy; Equanimity (Pāli: upekkhā, Sanskrit: upekṣā): is even-mindedness and serenity, treating everyone impartially.According to Peter Harvey, the Buddhist scriptures acknowledge that the four Brahmavihara meditation practices "did not originate within the Buddhist tradition". The Brahmavihara (sometimes as Brahmaloka), along with the tradition of meditation and the above four immeasurables are found in pre-Buddha and post-Buddha Vedic and Sramanic literature. Aspects of the Brahmavihara practice for rebirths into the heavenly realm have been an important part of Buddhist meditation tradition.According to Gombrich, the Buddhist usage of the brahma-vihāra originally referred to an awakened state of mind, and a concrete attitude toward other beings which was equal to "living with Brahman" here and now. The later tradition took those descriptions too literally, linking them to cosmology and understanding them as "living with Brahman" by rebirth in the Brahma-world. According to Gombrich, "the Buddha taught that kindness – what Christians tend to call love – was a way to salvation."
Some Buddhist traditions, especially those associated with Tantric Buddhism (also known as Vajrayana and Secret Mantra) use images and symbols of deities and Buddhas in meditation. This is generally done by mentally visualizing a Buddha image (or some other mental image, like a symbol, a mandala, a syllable, etc.), and using that image to cultivate calm and insight. One may also visualize and identify oneself with the imagined deity. While visualization practices have been particularly popular in Vajrayana, they may also found in Mahayana and Theravada traditions.In Tibetan Buddhism, unique tantric techniques which include visualization (but also mantra recitation, mandalas, and other elements) are considered to be much more effective than non-tantric meditations and they are one of the most popular meditation methods. The methods of Unsurpassable Yoga Tantra, (anuttarayogatantra) are in turn seen as the highest and most advanced. Anuttarayoga practice is divided into two stages, the Generation Stage and the Completion Stage. In the Generation Stage, one meditates on emptiness and visualizes oneself as a deity as well as visualizing its mandala. The focus is on developing clear appearance and divine pride (the understanding that oneself and the deity are one). This method is also known as deity yoga (devata yoga). There are numerous meditation deities (yidam) used, each with a mandala, a circular symbolic map used in meditation.In the Completion Stage, one meditates on ultimate reality based on the image that has been generated. Completion Stage practices also include techniques such as tummo and phowa. These are said to work with subtle body elements, like the energy channels (nadi), vital essences (bindu), "vital winds" (vayu), and chakras. The subtle body energies are seen as influencing consciousness in powerful ways, and are thus used in order to generate the 'great bliss' (maha-sukha) which is used to attain the luminous nature of the mind and realization of the empty and illusory nature of all phenomena ("the illusory body"), which leads to enlightenment.Completion practices are often grouped into different systems, such as the six dharmas of Naropa, and the six yogas of Kalachakra. In Tibetan Buddhism, there are also practices and methods which are sometimes seen as being outside of the two tantric stages, mainly Mahamudra and Dzogchen (Atiyoga).
According to Peter Harvey, whenever Buddhism has been healthy, not only ordained but also more committed lay people have practised formal meditation. Loud devotional chanting however, adds Harvey, has been the most prevalent Buddhist practice and considered a form of meditation that produces "energy, joy, lovingkindness and calm", purifies mind and benefits the chanter.Throughout most of Buddhist history, meditation has been primarily practised in Buddhist monastic tradition, and historical evidence suggests that serious meditation by lay people has been an exception. In recent history, sustained meditation has been pursued by a minority of monks in Buddhist monasteries. Western interest in meditation has led to a revival where ancient Buddhist ideas and precepts are adapted to Western mores and interpreted liberally, presenting Buddhism as a meditation-based form of spirituality.
Prajñā (Sanskrit) or paññā (Pāli) is wisdom, or knowledge of the true nature of existence. Another term which is associated with prajñā and sometimes is equivalent to it is vipassanā (Pāli) or vipaśyanā (Sanskrit), which is often translated as "insight". In Buddhist texts, the faculty of insight is often said to be cultivated through the four establishments of mindfulness.In the early texts, Paññā is included as one of the "five faculties" (indriya) which are commonly listed as important spiritual elements to be cultivated (see for example: AN I 16). Paññā along with samadhi, is also listed as one of the "trainings in the higher states of mind" (adhicittasikkha).The Buddhist tradition regards ignorance (avidyā), a fundamental ignorance, misunderstanding or mis-perception of the nature of reality, as one of the basic causes of dukkha and samsara. Overcoming this ignorance is part of the path to awakening. This overcoming includes the contemplation of impermanence and the non-self nature of reality, and this develops dispassion for the objects of clinging, and liberates a being from dukkha and saṃsāra.</ref>Prajñā is important in all Buddhist traditions. It is variously described as wisdom regarding the impermanent and not-self nature of dharmas (phenomena), the functioning of karma and rebirth, and knowledge of dependent origination. Likewise, vipaśyanā is described in a similar way, such as in the Paṭisambhidāmagga, where it is said to be the contemplation of things as impermanent, unsatisfactory and not-self.Some scholars such as Bronkhorst and Vetter have argued that the idea that insight leads to liberation was a later development in Buddhism and that there are inconsistencies with the early Buddhist presentation of samadhi and insight. However, others such as Collett Cox and Damien Keown have argued that insight is a key aspect of the early Buddhist process of liberation, which cooperates with samadhi to remove the obstacles to enlightenment (i.e., the āsavas).In Theravāda Buddhism, the focus of vipassanā meditation is to continuously and thoroughly know how phenomena (dhammas) are impermanent (annica), not-Self (anatta) and dukkha. The most widely used method in modern Theravāda for the practice of vipassanā is that found in the Satipatthana Sutta. There is some disagreement in contemporary Theravāda regarding samatha and vipassanā. Some in the Vipassana Movement strongly emphasize the practice of insight over samatha, and other Theravadins disagree with this.In Mahāyāna Buddhism, the development of insight (vipaśyanā) and tranquility (śamatha) are also taught and practiced. The many different schools of Mahāyāna Buddhism have a large repertoire of meditation techniques to cultivate these qualities. These include visualization of various Buddhas, recitation of a Buddha's name, the use of tantric Buddhist mantras and dharanis. Insight in Mahāyāna Buddhism also includes gaining a direct understanding of certain Mahāyāna philosophical views, such as the emptiness view and the consciousness-only view. This can be seen in meditation texts such as Kamalaśīla's Bhāvanākrama ( "Stages of Meditation", 9th century), which teaches insight (vipaśyanā) from the Yogācāra-Madhyamaka perspective.
According to Harvey, most forms of Buddhism "consider saddhā (Skt śraddhā), ‘trustful confidence’ or ‘faith’, as a quality which must be balanced by wisdom, and as a preparation for, or accompaniment of, meditation." Because of this devotion (Skt. bhakti; Pali: bhatti) is an important part of the practice of most Buddhists. Devotional practices include ritual prayer, prostration, offerings, pilgrimage, and chanting. Buddhist devotion is usually focused on some object, image or location that is seen as holy or spiritually influential. Examples of objects of devotion include paintings or statues of Buddhas and bodhisattvas, stupas, and bodhi trees. Public group chanting for devotional and ceremonial is common to all Buddhist traditions and goes back to ancient India where chanting aided in the memorization of the orally transmitted teachings. Rosaries called malas are used in all Buddhist traditions to count repeated chanting of common formulas or mantras. Chanting is thus a type of devotional group meditation which leads to tranquility and communicates the Buddhist teachings.In East Asian Pure Land Buddhism, devotion to the Buddha Amitabha is the main practice. In Nichiren Buddhism, devotion to the Lotus Sutra is the main practice. Devotional practices such as pujas have been a common practice in Theravada Buddhism, where offerings and group prayers are made to deities and particularly images of Buddha. According to Karel Werner and other scholars, devotional worship has been a significant practice in Theravada Buddhism, and deep devotion is part of Buddhist traditions starting from the earliest days.Guru devotion is a central practice of Indo-Tibetan Buddhism. The guru is considered essential and to the Buddhist devotee, the guru is the "enlightened teacher and ritual master" in Vajrayana spiritual pursuits. For someone seeking Buddhahood, the guru is the Buddha, the Dharma and the Sangha, wrote the 12th-century Buddhist scholar Sadhanamala.The veneration of and obedience to teachers is also important in Theravada and Zen Buddhism.
Based on the Indian principle of ahimsa (non-harming), the Buddha's ethics strongly condemn the harming of all sentient beings, including all animals. He thus condemned the animal sacrifice of the brahmins as well hunting, and killing animals for food. This led to various policies by Buddhist kings such as Asoka meant to protect animals, such as the establishing of 'no slaughter days' and the banning of hunting on certain circumstances.However, early Buddhist texts depict the Buddha as allowing monastics to eat meat. This seems to be because monastics begged for their food and thus were supposed to accept whatever food was offered to them. This was tempered by the rule that meat had to be "three times clean" which meant that "they had not seen, had not heard, and had no reason to suspect that the animal had been killed so that the meat could be given to them". Also, while the Buddha did not explicitly promote vegetarianism in his discourses, he did state that gaining one's livelihood from the meat trade was unethical. However, this rule was not a promotion of a specific diet, but a rule against the actual killing of animals for food. There was also a famed schism which occurred in the Buddhist community when Devadatta attempted to make vegetarianism compulsory and the Buddha disagreed.In contrast to this, various Mahayana sutras and texts like the Mahaparinirvana sutra, Surangama sutra and the Lankavatara sutra state that the Buddha promoted vegetarianism out of compassion. Indian Mahayana thinkers like Shantideva promoted the avoidance of meat. Throughout history, the issue of whether Buddhists should be vegetarian has remained a much debated topic and there is a variety of opinions on this issue among modern Buddhists. In the East Asian Buddhism, most monastics are expected to be vegetarian, and the practice is seen as very virtuous and it is taken up by some devout laypersons. Most Theravadins in Sri Lanka and Southeast Asia do not practice vegetarianism and eat whatever is offered by the lay community, who are mostly also not vegetarians. But there are exceptions, some monks choose to be vegetarian and some abbots like Ajahn Sumedho have encouraged the lay community to donate vegetarian food to the monks. Mahasi Sayadaw meanwhile, has recommended vegetarianism as the best way to make sure one's meal is pure in three ways. Also, the new religious movement Santi Asoke, promotes vegetarianism. According to Peter Harvey, in the Theravada world, vegetarianism is "universally admired, but little practiced." Because of the rule against killing, in many Buddhist countries, most butchers and others who work in the meat trade are non-Buddhists.Likewise, most Tibetan Buddhists have historically tended not to be vegetarian, however, there have been some strong debates and pro-vegetarian arguments by some pro-vegetarian Tibetans. Some influential figures have spoken and written in favor of vegetarianism throughout history, including well known figures like Shabkar and the 17th Karmapa Ogyen Trinley Dorje, who has mandated vegetarianism in all his monasteries.
Buddhism, like all Indian religions, was initially an oral tradition in ancient times. The Buddha's words, the early doctrines, concepts, and their traditional interpretations were orally transmitted from one generation to the next. The earliest oral texts were transmitted in Middle Indo-Aryan languages called Prakrits, such as Pali, through the use of communal recitation and other mnemonic techniques.The first Buddhist canonical texts were likely written down in Sri Lanka, about 400 years after the Buddha died. The texts were part of the Tripitakas, and many versions appeared thereafter claiming to be the words of the Buddha. Scholarly Buddhist commentary texts, with named authors, appeared in India, around the 2nd century CE. These texts were written in Pali or Sanskrit, sometimes regional languages, as palm-leaf manuscripts, birch bark, painted scrolls, carved into temple walls, and later on paper.Unlike what the Bible is to Christianity and the Quran is to Islam, but like all major ancient Indian religions, there is no consensus among the different Buddhist traditions as to what constitutes the scriptures or a common canon in Buddhism. The general belief among Buddhists is that the canonical corpus is vast. This corpus includes the ancient Sutras organised into Nikayas or Agamas, itself the part of three basket of texts called the Tripitakas. Each Buddhist tradition has its own collection of texts, much of which is translation of ancient Pali and Sanskrit Buddhist texts of India. The Chinese Buddhist canon, for example, includes 2184 texts in 55 volumes, while the Tibetan canon comprises 1108 texts – all claimed to have been spoken by the Buddha – and another 3461 texts composed by Indian scholars revered in the Tibetan tradition. The Buddhist textual history is vast; over 40,000 manuscripts – mostly Buddhist, some non-Buddhist – were discovered in 1900 in the Dunhuang Chinese cave alone.
The Early Buddhist Texts refers to the literature which is considered by modern scholars to be the earliest Buddhist material. The first four Pali Nikayas, and the corresponding Chinese Āgamas are generally considered to be among the earliest material. Apart from these, there are also fragmentary collections of EBT materials in other languages such as Sanskrit, Khotanese, Tibetan and Gāndhārī. The modern study of early Buddhism often relies on comparative scholarship using these various early Buddhist sources to identify parallel texts and common doctrinal content. One feature of these early texts are literary structures which reflect oral transmission, such as widespread repetition.
After the development of the different early Buddhist schools, these schools began to develop their own textual collections, which were termed Tripiṭakas (Triple Baskets).Many early Tripiṭakas, like the Pāli Tipitaka, were divided into three sections: Vinaya Pitaka (focuses on monastic rule), Sutta Pitaka (Buddhist discourses) and Abhidhamma Pitaka, which contain expositions and commentaries on the doctrine. The Pāli Tipitaka (also known as the Pali Canon) of the Theravada School constitutes the only complete collection of Buddhist texts in an Indic language which has survived until today. However, many Sutras, Vinayas and Abhidharma works from other schools survive in Chinese translation, as part of the Chinese Buddhist Canon. According to some sources, some early schools of Buddhism had five or seven pitakas.Much of the material in the Pali Canon is not specifically "Theravadin", but is instead the collection of teachings that this school preserved from the early, non-sectarian body of teachings. According to Peter Harvey, it contains material at odds with later Theravadin orthodoxy. He states: "The Theravadins, then, may have added texts to the Canon for some time, but they do not appear to have tampered with what they already had from an earlier period."
A distinctive feature of many Tripitaka collections is the inclusion of a genre called Abhidharma, which dates from the 3rd century BCE and later. According to Collett Cox, the genre began as explanations and elaborations of the teachings in the suttas but over time evolved into an independent system of doctrinal exposition.Over time, the various Abhidharma traditions developed various disagreements which each other on points of doctrine, which were discussed in the different Abhidharma texts of these schools. The major Abhidharma collections which modern scholars have the most information about are those of the Theravāda and Sarvāstivāda schools.In Sri Lanka and South India, the Theravāda Abhidhamma system was the most influential. In addition to the Abhidharma project, some of the schools also began accumulating a literary tradition of scriptural commentary on their respective Tripitakas. These commentaries were particularly important in the Theravāda school, and the Pali commentaries (Aṭṭhakathā) remain influential today. Both Abhidhamma and the Pali commentaries influenced the Visuddhimagga, an important 5th-century text by the Theravada scholar Buddhaghosa, who also translated and compiled many of the Aṭṭhakathās from older Sinhalese sources.The Sarvāstivāda school was one of the most influential Abhidharma traditions in North India. The magnum opus of this tradition was the massive Abhidharma commentary called the Mahāvibhaṣa ('Great Commentary'), compiled at a great synod in Kashmir during the reign of Kanishka II (c. 158–176). The Abhidharmakosha of Vasubandhu is another very influential Abhidharma work from the northern tradition, which continues to be studied in East Asian Buddhism and in Indo-Tibetan Buddhism.
The Mahāyāna sūtras are a very broad genre of Buddhist scriptures that the Mahāyāna Buddhist tradition holds are original teachings of the Buddha. Modern historians generally hold that the first of these texts were composed probably around the 1st century BCE or 1st century CE.In Mahāyāna, these texts are generally given greater authority than the early Āgamas and Abhidharma literature, which are called "Śrāvakayāna" or "Hinayana" to distinguish them from Mahāyāna sūtras. Mahāyāna traditions mainly see these different classes of texts as being designed for different types of persons, with different levels of spiritual understanding. The Mahāyāna sūtras are mainly seen as being for those of "greater" capacity.The Mahāyāna sūtras often claim to articulate the Buddha's deeper, more advanced doctrines, reserved for those who follow the bodhisattva path. That path is explained as being built upon the motivation to liberate all living beings from unhappiness. Hence the name Mahāyāna (lit., the Great Vehicle). Besides the teaching of the bodhisattva, Mahāyāna texts also contain expanded cosmologies and mythologies, with many more Buddhas and powerful bodhisattvas, as well as new spiritual practices and ideas.The modern Theravada school does not treat the Mahāyāna sūtras as authoritative or authentic teachings of the Buddha. Likewise, these texts were not recognized as authoritative by many early Buddhist schools and in some cases, communities such as the Mahāsāṃghika school split up due to this disagreement. Recent scholarship has discovered many early Mahāyāna texts which shed light into the development of Mahāyāna. Among these is the Śālistamba Sutra which survives in Tibetan and Chinese translation. This text contains numerous sections which are remarkably similar to Pali suttas. The Śālistamba Sutra was cited by Mahāyāna scholars such as the 8th-century Yasomitra to be authoritative. This suggests that Buddhist literature of different traditions shared a common core of Buddhist texts in the early centuries of its history, until Mahāyāna literature diverged about and after the 1st century CE.Mahāyāna also has a very large literature of philosophical and exegetical texts. These are often called śāstra (treatises) or vrittis (commentaries). Some of this literature was also written in verse form (karikās), the most famous of which is the Mūlamadhyamika-karikā (Root Verses on the Middle Way) by Nagarjuna, the foundational text of the Madhyamika school.
During the Gupta Empire, a new class of Buddhist sacred literature began to develop, which are called the Tantras. By the 8th century, the tantric tradition was very influential in India and beyond. Besides drawing on a Mahāyāna Buddhist framework, these texts also borrowed deities and material from other Indian religious traditions, such as the Śaiva and Pancharatra traditions, local god/goddess cults, and local spirit worship (such as yaksha or nāga spirits).Some features of these texts include the widespread use of mantras, meditation on the subtle body, worship of fierce deities, and antinomian and transgressive practices such as ingesting alcohol and performing sexual rituals.
Historically, the roots of Buddhism lie in the religious thought of Iron Age India around the middle of the first millennium BCE. This was a period of great intellectual ferment and socio-cultural change known as the "Second urbanisation", marked by the growth of towns and trade, the composition of the Upanishads and the historical emergence of the Śramaṇa traditions.New ideas developed both in the Vedic tradition in the form of the Upanishads, and outside of the Vedic tradition through the Śramaṇa movements. The term Śramaṇa refers to several Indian religious movements parallel to but separate from the historical Vedic religion, including Buddhism, Jainism and others such as Ājīvika.Several Śramaṇa movements are known to have existed in India before the 6th century BCE (pre-Buddha, pre-Mahavira), and these influenced both the āstika and nāstika traditions of Indian philosophy. According to Martin Wilshire, the Śramaṇa tradition evolved in India over two phases, namely Paccekabuddha and Savaka phases, the former being the tradition of individual ascetic and the latter of disciples, and that Buddhism and Jainism ultimately emerged from these. Brahmanical and non-Brahmanical ascetic groups shared and used several similar ideas, but the Śramaṇa traditions also drew upon already established Brahmanical concepts and philosophical roots, states Wiltshire, to formulate their own doctrines. Brahmanical motifs can be found in the oldest Buddhist texts, using them to introduce and explain Buddhist ideas. For example, prior to Buddhist developments, the Brahmanical tradition internalised and variously reinterpreted the three Vedic sacrificial fires as concepts such as Truth, Rite, Tranquility or Restraint. Buddhist texts also refer to the three Vedic sacrificial fires, reinterpreting and explaining them as ethical conduct.The Śramaṇa religions challenged and broke with the Brahmanic tradition on core assumptions such as Atman (soul, self), Brahman, the nature of afterlife, and they rejected the authority of the Vedas and Upanishads. Buddhism was one among several Indian religions that did so.
The history of Indian Buddhism may be divided into five periods: Early Buddhism (occasionally called pre-sectarian Buddhism), Nikaya Buddhism or Sectarian Buddhism: The period of the early Buddhist schools, Early Mahayana Buddhism, Late Mahayana, and the era of Vajrayana or the "Tantric Age".
According to Lambert Schmithausen Pre-sectarian Buddhism is "the canonical period prior to the development of different schools with their different positions."The early Buddhist Texts include the four principal Pali Nikāyas (and their parallel Agamas found in the Chinese canon) together with the main body of monastic rules, which survive in the various versions of the patimokkha. However, these texts were revised over time, and it is unclear what constitutes the earliest layer of Buddhist teachings. One method to obtain information on the oldest core of Buddhism is to compare the oldest extant versions of the Theravadin Pāli Canon and other texts. The reliability of the early sources, and the possibility to draw out a core of oldest teachings, is a matter of dispute. According to Vetter, inconsistencies remain, and other methods must be applied to resolve those inconsistencies.According to Schmithausen, three positions held by scholars of Buddhism can be distinguished: "Stress on the fundamental homogeneity and substantial authenticity of at least a considerable part of the Nikayic materials;" "Scepticism with regard to the possibility of retrieving the doctrine of earliest Buddhism;" "Cautious optimism in this respect."
According to Mitchell, certain basic teachings appear in many places throughout the early texts, which has led most scholars to conclude that Gautama Buddha must have taught something similar to the Four Noble Truths, the Noble Eightfold Path, Nirvana, the three marks of existence, the five aggregates, dependent origination, karma and rebirth.According to N. Ross Reat, all of these doctrines are shared by the Theravada Pali texts and the Mahasamghika school's Śālistamba Sūtra. A recent study by Bhikkhu Analayo concludes that the Theravada Majjhima Nikaya and Sarvastivada Madhyama Agama contain mostly the same major doctrines. Richard Salomon, in his study of the Gandharan texts (which are the earliest manuscripts containing early discourses), has confirmed that their teachings are "consistent with non-Mahayana Buddhism, which survives today in the Theravada school of Sri Lanka and Southeast Asia, but which in ancient times was represented by eighteen separate schools."However, some scholars argue that critical analysis reveals discrepancies among the various doctrines found in these early texts, which point to alternative possibilities for early Buddhism. The authenticity of certain teachings and doctrines have been questioned. For example, some scholars think that karma was not central to the teaching of the historical Buddha, while other disagree with this position. Likewise, there is scholarly disagreement on whether insight was seen as liberating in early Buddhism or whether it was a later addition to the practice of the four jhānas. Scholars such as Bronkhorst also think that the four noble truths may not have been formulated in earliest Buddhism, and did not serve in earliest Buddhism as a description of "liberating insight". According to Vetter, the description of the Buddhist path may initially have been as simple as the term "the middle way". In time, this short description was elaborated, resulting in the description of the eightfold path.
According to numerous Buddhist scriptures, soon after the parinirvāṇa (from Sanskrit: "highest extinguishment") of Gautama Buddha, the first Buddhist council was held to collectively recite the teachings to ensure that no errors occurred in oral transmission. Many modern scholars question the historicity of this event. However, Richard Gombrich states that the monastic assembly recitations of the Buddha's teaching likely began during Buddha's lifetime, and they served a similar role of codifying the teachings.The so called Second Buddhist council resulted in the first schism in the Sangha. Modern scholars believe that this was probably caused when a group of reformists called Sthaviras ("elders") sought to modify the Vinaya (monastic rule), and this caused a split with the conservatives who rejected this change, they were called Mahāsāṃghikas. While most scholars accept that this happened at some point, there is no agreement on the dating, especially if it dates to before or after the reign of Ashoka. Buddhism may have spread only slowly throughout India until the time of the Mauryan emperor Ashoka (304–232 BCE), who was a public supporter of the religion. The support of Aśoka and his descendants led to the construction of more stūpas (such as at Sanchi and Bharhut), temples (such as the Mahabodhi Temple) and to its spread throughout the Maurya Empire and into neighbouring lands such as Central Asia and to the island of Sri Lanka. During and after the Mauryan period (322–180 BCE), the Sthavira community gave rise to several schools, one of which was the Theravada school which tended to congregate in the south and another which was the Sarvāstivāda school, which was mainly in north India. Likewise, the Mahāsāṃghika groups also eventually split into different Sanghas. Originally, these schisms were caused by disputes over monastic disciplinary codes of various fraternities, but eventually, by about 100 CE if not earlier, schisms were being caused by doctrinal disagreements too.Following (or leading up to) the schisms, each Saṅgha started to accumulate their own version of Tripiṭaka (triple basket of texts). In their Tripiṭaka, each school included the Suttas of the Buddha, a Vinaya basket (disciplinary code) and some schools also added an Abhidharma basket which were texts on detailed scholastic classification, summary and interpretation of the Suttas. The doctrine details in the Abhidharmas of various Buddhist schools differ significantly, and these were composed starting about the third century BCE and through the 1st millennium CE.
According to the edicts of Aśoka, the Mauryan emperor sent emissaries to various countries west of India to spread "Dharma", particularly in eastern provinces of the neighbouring Seleucid Empire, and even farther to Hellenistic kingdoms of the Mediterranean. It is a matter of disagreement among scholars whether or not these emissaries were accompanied by Buddhist missionaries.In central and west Asia, Buddhist influence grew, through Greek-speaking Buddhist monarchs and ancient Asian trade routes, a phenomenon known as Greco-Buddhism. An example of this is evidenced in Chinese and Pali Buddhist records, such as Milindapanha and the Greco-Buddhist art of Gandhāra. The Milindapanha describes a conversation between a Buddhist monk and the 2nd-century BCE Greek king Menander, after which Menander abdicates and himself goes into monastic life in the pursuit of nirvana. Some scholars have questioned the Milindapanha version, expressing doubts whether Menander was Buddhist or just favourably disposed to Buddhist monks.The Kushan empire (30–375 CE) came to control the Silk Road trade through Central and South Asia, which brought them to interact with Gandharan Buddhism and the Buddhist institutions of these regions. The Kushans patronised Buddhism throughout their lands, and many Buddhist centers were built or renovated (the Sarvastivada school was particularly favored), especially by Emperor Kanishka (128–151 CE). Kushan support helped Buddhism to expand into a world religion through their trade routes. Buddhism spread to Khotan, the Tarim Basin, and China, eventually to other parts of the far east. Some of the earliest written documents of the Buddhist faith are the Gandharan Buddhist texts, dating from about the 1st century CE, and connected to the Dharmaguptaka school.The Islamic conquest of the Iranian Plateau in the 7th-century, followed by the Muslim conquests of Afghanistan and the later establishment of the Ghaznavid kingdom with Islam as the state religion in Central Asia between the 10th- and 12th-century led to the decline and disappearance of Buddhism from most of these regions.
The origins of Mahāyāna ("Great Vehicle") Buddhism are not well understood and there are various competing theories about how and where this movement arose. Theories include the idea that it began as various groups venerating certain texts or that it arose as a strict forest ascetic movement.The first Mahāyāna works were written sometime between the 1st century BCE and the 2nd century CE. Much of the early extant evidence for the origins of Mahāyāna comes from early Chinese translations of Mahāyāna texts, mainly those of Lokakṣema. (2nd century CE). Some scholars have traditionally considered the earliest Mahāyāna sūtras to include the first versions of the Prajnaparamita series, along with texts concerning Akṣobhya, which were probably composed in the 1st century BCE in the south of India.There is no evidence that Mahāyāna ever referred to a separate formal school or sect of Buddhism, with a separate monastic code (Vinaya), but rather that it existed as a certain set of ideals, and later doctrines, for bodhisattvas. Records written by Chinese monks visiting India indicate that both Mahāyāna and non-Mahāyāna monks could be found in the same monasteries, with the difference that Mahāyāna monks worshipped figures of Bodhisattvas, while non-Mahayana monks did not. Mahāyāna initially seems to have remained a small minority movement that was in tension with other Buddhist groups, struggling for wider acceptance. However, during the fifth and sixth centuries CE, there seems to have been a rapid growth of Mahāyāna Buddhism, which is shown by a large increase in epigraphic and manuscript evidence in this period. However, it still remained a minority in comparison to other Buddhist schools.Mahāyāna Buddhist institutions continued to grow in influence during the following centuries, with large monastic university complexes such as Nalanda (established by the 5th-century CE Gupta emperor, Kumaragupta I) and Vikramashila (established under Dharmapala c. 783 to 820) becoming quite powerful and influential. During this period of Late Mahāyāna, four major types of thought developed: Mādhyamaka, Yogācāra, Buddha-nature (Tathāgatagarbha), and the Pramana school of Dignaga. According to Dan Lusthaus, Mādhyamaka and Yogācāra have a great deal in common, and the commonality stems from early Buddhism.
During the Gupta period (4th–6th centuries) and the empire of Harṣavardana (c. 590–647 CE), Buddhism continued to be influential in India, and large Buddhist learning institutions such as Nalanda and Valabahi Universities were at their peak. Buddhism also flourished under the support of the Pāla Empire (8th–12th centuries). Under the Guptas and Palas, Tantric Buddhism or Vajrayana developed and rose to prominence. It promoted new practices such as the use of mantras, dharanis, mudras, mandalas and the visualization of deities and Buddhas and developed a new class of literature, the Buddhist Tantras. This new esoteric form of Buddhism can be traced back to groups of wandering yogi magicians called mahasiddhas.The question of the origins of early Vajrayana has been taken up by various scholars. David Seyfort Ruegg has suggested that Buddhist tantra employed various elements of a "pan-Indian religious substrate" which is not specifically Buddhist, Shaiva or Vaishnava.According to Indologist Alexis Sanderson, various classes of Vajrayana literature developed as a result of royal courts sponsoring both Buddhism and Saivism. Sanderson has argued that Buddhist tantras can be shown to have borrowed practices, terms, rituals and more form Shaiva tantras. He argues that Buddhist texts even directly copied various Shaiva tantras, especially the Bhairava Vidyapitha tantras. Ronald M. Davidson meanwhile, argues that Sanderson's claims for direct influence from Shaiva Vidyapitha texts are problematic because "the chronology of the Vidyapitha tantras is by no means so well established" and that the Shaiva tradition also appropriated non-Hindu deities, texts and traditions. Thus while "there can be no question that the Buddhist tantras were heavily influenced by Kapalika and other Saiva movements" argues Davidson, "the influence was apparently mutual."Already during this later era, Buddhism was losing state support in other regions of India, including the lands of the Karkotas, the Pratiharas, the Rashtrakutas, the Pandyas and the Pallavas. This loss of support in favor of Hindu faiths like Vaishnavism and Shaivism, is the beginning of the long and complex period of the Decline of Buddhism in the Indian subcontinent. The Islamic invasions and conquest of India (10th to 12th century), further damaged and destroyed many Buddhist institutions, leading to its eventual near disappearance from India by the 1200s.
The Silk Road transmission of Buddhism to China is most commonly thought to have started in the late 2nd or the 1st century CE, though the literary sources are all open to question. The first documented translation efforts by foreign Buddhist monks in China were in the 2nd century CE, probably as a consequence of the expansion of the Kushan Empire into the Chinese territory of the Tarim Basin.The first documented Buddhist texts translated into Chinese are those of the Parthian An Shigao (148–180 CE). The first known Mahāyāna scriptural texts are translations into Chinese by the Kushan monk Lokakṣema in Luoyang, between 178 and 189 CE. From China, Buddhism was introduced into its neighbours Korea (4th century), Japan (6th–7th centuries), and Vietnam (c. 1st–2nd centuries).During the Chinese Tang dynasty (618–907), Chinese Esoteric Buddhism was introduced from India and Chan Buddhism (Zen) became a major religion. Chan continued to grow in the Song dynasty (960–1279) and it was during this era that it strongly influenced Korean Buddhism and Japanese Buddhism. Pure Land Buddhism also became popular during this period and was often practised together with Chan. It was also during the Song that the entire Chinese canon was printed using over 130,000 wooden printing blocks.During the Indian period of Esoteric Buddhism (from the 8th century onwards), Buddhism spread from India to Tibet and Mongolia. Johannes Bronkhorst states that the esoteric form was attractive because it allowed both a secluded monastic community as well as the social rites and rituals important to laypersons and to kings for the maintenance of a political state during succession and wars to resist invasion. During the Middle Ages, Buddhism slowly declined in India, while it vanished from Persia and Central Asia as Islam became the state religion.The Theravada school arrived in Sri Lanka sometime in the 3rd century BCE. Sri Lanka became a base for its later spread to southeast Asia after the 5th century CE (Myanmar, Malaysia, Indonesia, Thailand, Cambodia and coastal Vietnam). Theravada Buddhism was the dominant religion in Burma during the Mon Hanthawaddy Kingdom (1287–1552). It also became dominant in the Khmer Empire during the 13th and 14th centuries and in the Thai Sukhothai Kingdom during the reign of Ram Khamhaeng (1237/1247–1298).
Buddhists generally classify themselves as either Theravāda or Mahāyāna. This classification is also used by some scholars and is the one ordinarily used in the English language. An alternative scheme used by some scholars divides Buddhism into the following three traditions or geographical or cultural areas: Theravāda (or "Southern Buddhism", "South Asian Buddhism"), East Asian Buddhism (or just "Eastern Buddhism") and Indo-Tibetan Buddhism (or "Northern Buddhism"). Some scholars use other schemes. Buddhists themselves have a variety of other schemes. Hinayana (literally "lesser or inferior vehicle") is sometimes used by Mahāyāna followers to name the family of early philosophical schools and traditions from which contemporary Theravāda emerged, but as the Hinayana term is considered derogatory, a variety of other terms are used instead, including: Śrāvakayāna, Nikaya Buddhism, early Buddhist schools, sectarian Buddhism and conservative Buddhism.Not all traditions of Buddhism share the same philosophical outlook, or treat the same concepts as central. Each tradition, however, does have its own core concepts, and some comparisons can be drawn between them: Both Theravāda and Mahāyāna accept and revere the Buddha Sakyamuni as the founder, Mahāyāna also reveres numerous other Buddhas, such as Amitabha or Vairocana as well as many other bodhisattvas not revered in Theravāda. Both accept the Middle Way, Dependent origination, the Four Noble Truths, the Noble Eightfold Path, the Three Jewels, the Three marks of existence and the Bodhipakṣadharmas (aids to awakening). Mahāyāna focuses mainly on the bodhisattva path to Buddhahood which it sees as universal and to be practiced by all persons, while Theravāda does not focus on teaching this path and teaches the attainment of arhatship as a worthy goal to strive towards. The bodhisattva path is not denied in Theravāda, it is generally seen as a long and difficult path suitable for only a few. Thus the Bodhisattva path is normative in Mahāyāna, while it is an optional path for a heroic few in Theravāda. Mahāyāna sees the arhat's nirvana as being imperfect and inferior or preliminary to full Buddhahood. It sees arhatship as selfish, since bodhisattvas vow to save all beings while arhats save only themselves. Theravāda meanwhile does not accept that the arhat's nirvana is an inferior or preliminary attainment, nor that it is a selfish deed to attain arhatship since not only are arhats described as compassionate but they have destroyed the root of greed, the sense of "I am". Mahāyāna accepts the authority of the many Mahāyāna sutras along with the other Nikaya texts like the Agamas and the Pali canon (though it sees Mahāyāna texts as primary), while Theravāda does not accept that the Mahāyāna sutras are buddhavacana (word of the Buddha) at all.
The Theravāda tradition bases itself on the Pāli Canon, considers itself to be the more orthodox form of Buddhism and tends to be more conservative in doctrine and monastic discipline. The Pāli Canon is the only complete Buddhist canon surviving in an ancient Indian language. This language, Pāli, serves as the school's sacred language and lingua franca. Besides the Pāli Canon, Theravāda scholastics also often rely on a post-canonical Pāli literature which comments on and interprets the Pāli Canon. These later works such as the Visuddhimagga, a doctrinal summa written in the fifth century by the exegete Buddhaghosa also remain influential today.Theravāda derives from the Mahāvihāra (Tāmraparṇīya) sect, a Sri Lankan branch of the Vibhajyavāda Sthaviras, which began to establish itself on the island from the 3rd century BCE onwards. Theravāda flourished in south India and Sri Lanka in ancient times; from there it spread for the first time into mainland southeast Asia about the 11th century into its elite urban centres. By the 13th century, Theravāda had spread widely into the rural areas of mainland southeast Asia, displacing Mahayana Buddhism and some traditions of Hinduism.In the modern era, Buddhist figures such as Anagarika Dhammapala and King Mongkut sought to re-focus the tradition on the Pāli Canon, as well as emphasize the rational and "scientific" nature of Theravāda while also opposing "superstition". This movement, often termed Buddhist modernism, has influenced most forms of modern Theravāda. Another influential modern turn in Theravāda is the Vipassana Movement, which led to the widespread adoption of meditation by laypersons. Theravāda is primarily practised today in Sri Lanka, Burma, Laos, Thailand, Cambodia as well as small portions of China, Vietnam, Malaysia and Bangladesh. It has a growing presence in the west, especially as part of the Vipassana Movement.
Mahāyāna ("Great Vehicle") refers to all forms of Buddhism which consider the Mahāyāna Sutras as authoritative scriptures and accurate rendering of Buddha's words. These traditions have been the more liberal form of Buddhism allowing different and new interpretations that emerged over time. The focus of Mahāyāna is the path of the bodhisattva (bodhisattvayāna), though what this path means is interpreted in many different ways. The first Mahāyāna texts date to sometime between the 1st century BCE and the 2st century CE. It remained a minority movement until the time of the Guptas and Palas, when great Mahāyāna monastic centres of learning such as Nālandā University were established as evidenced by records left by three Chinese visitors to India. These universities supported Buddhist scholarship, as well as studies into non-Buddhist traditions and secular subjects such as medicine. They hosted visiting students who then spread Buddhism to East and Central Asia.Native Mahāyāna Buddhism is practised today in China, Japan, Korea, Singapore, parts of Russia and most of Vietnam (also commonly referred to as "Eastern Buddhism"). The Buddhism practised in Tibet, the Himalayan regions, and Mongolia is also a form of Mahāyāna, but is also different in many ways due to its adoption of tantric practices and is discussed below under the heading of "Vajrayāna" (also commonly referred to as "Northern Buddhism"). There are a variety of strands in Eastern Buddhism, of which "the Pure Land school of Mahāyāna is the most widely practised today." In most of China, these different strands and traditions are generally fused together. Vietnamese Mahāyāna is similarly very eclectic. In Japan in particular, they form separate denominations with the five major ones being: Nichiren, peculiar to Japan; Pure Land; Shingon, a form of Vajrayana; Tendai, and Zen. In Korea, nearly all Buddhists belong to the Chogye school, which is officially Son (Zen), but with substantial elements from other traditions.
The goal and philosophy of the Vajrayāna remains Mahāyānist, but its methods are seen by its followers as far more powerful, so as to lead to Buddhahood in just one lifetime. The practice of using mantras was adopted from Hinduism, where they were first used in the Vedas.Tibetan Buddhism preserves the Vajrayana teachings of eighth-century India. Tantric Buddhism is largely concerned with ritual and meditative practices. A central feature of Buddhist Tantra is deity yoga which includes visualisation and identification with an enlightened yidam or meditation deity and its associated mandala. Another element of Tantra is the need for ritual initiation or empowerment (abhiṣeka) by a Guru or Lama. Some Tantras like the Guhyasamāja Tantra features new forms of antinomian ritual practice such as the use taboo substances like alcohol, sexual yoga, and charnel ground practices which evoke wrathful deities.
Buddhist institutions are often housed and centered around monasteries (Sanskrit:viharas) and temples. Buddhist monastics originally followed a life of wandering, never staying in one place for long. During the three month rainy season (vassa) they would gather together in one place for a period of intense practice and then depart again. Some of the earliest Buddhist monasteries were at groves (vanas) or woods (araññas), such as Jetavana and Sarnath's Deer Park. There originally seems to have been two main types of monasteries, monastic settlements (sangharamas) were built and supported by donors, and woodland camps (avasas) were set up by monks. Whatever structures were built in these locales were made out of wood and were sometimes temporary structures built for the rainy season.Over time, the wandering community slowly adopted more settled cenobitic forms of monasticism. Also, these monasteries slowly evolved from the simpler collections of rustic dwellings of early Buddhism into larger more permanent structures meant to house the entire community, who now lived in a more collective fashion. During the Gupta era, even larger monastic university complexes (like Nalanda) arose, with larger and more artistically ornate structures, as well as large land grants and accumulated wealth.There are many different forms of Buddhist structures. Classic Indian Buddhist institutions mainly made use of the following structures: monasteries, rock-hewn cave complexes (such as the Ajanta Caves), stupas (funerary mounds which contained relics), and temples such as the Mahabodhi Temple.In Southeast Asia, the most widespread institutions are centered on wats, which refers to an establishment with various buildings such as an ordination hall, a library, monks' quarters and stupas. East Asian Buddhist institutions also use various structures including monastic halls, temples, lecture halls, bell towers and pagodas. In Japanese Buddhist temples, these different structures are usually grouped together in an area termed the garan. In Indo-Tibetan Buddhism, Buddhist institutions are generally housed in gompas. They include monastic quarters, stupas and prayer halls with Buddha images. The complexity of Buddhist institutions varies, ranging from minimalist and rustic forest monasteries to large monastic centers like Tawang Monastery. The core of traditional Buddhist institutions is the monastic community (Sangha) who manage and lead religious services. They are supported by the lay community who visit temples and monasteries for religious services and holidays. In the modern era, the Buddhist "meditation centre", which is mostly used by laypersons and often also staffed by them, has also become widespread.
Buddhism has faced various challenges and changes during the colonisation of Buddhist states by Christian countries and its persecution under modern states. Like other religions, the findings of modern science has challenged its basic premises. One response to some of these challenges has come to be called Buddhist modernism. Early Buddhist modernist figures such as the American convert Henry Olcott (1832–1907) and Anagarika Dharmapala (1864–1933) reinterpreted and promoted Buddhism as a scientific and rational religion which they saw as compatible with modern science.East Asian Buddhism meanwhile suffered under various wars which ravaged China during the modern era, such as the Taiping rebellion and World War II (which also affected Korean Buddhism). During the Republican period (1912–49), a new movement called Humanistic Buddhism was developed by figures such as Taixu (1899–1947), and though Buddhist institutions were destroyed during the Cultural Revolution (1966–76), there has been a revival of the religion in China after 1977. Japanese Buddhism also went through a period of modernisation during the Meiji period. In Central Asia meanwhile, the arrival of Communist repression to Tibet (1966–1980) and Mongolia (between 1924–1990) had a strong negative impact on Buddhist institutions, though the situation has improved somewhat since the 80s and 90s.
While there were some encounters of Western travellers or missionaries such as St. Francis Xavier and Ippolito Desideri with Buddhist cultures, it was not until the 19th century that Buddhism began to be studied by Western scholars. It was the work of pioneering scholars such as Eugène Burnouf, Max Müller, Hermann Oldenberg and Thomas William Rhys Davids that paved the way for modern Buddhist studies in the West. The English words such as Buddhism, "Boudhist", "Bauddhist" and Buddhist were coined in the early 19th-century in the West, while in 1881, Rhys Davids founded the Pali Text Society – an influential Western resource of Buddhist literature in the Pali language and one of the earliest publisher of a journal on Buddhist studies. It was also during the 19th century that Asian Buddhist immigrants (mainly from China and Japan) began to arrive in Western countries such as the United States and Canada, bringing with them their Buddhist religion. This period also saw the first Westerners to formally convert to Buddhism, such as Helena Blavatsky and Henry Steel Olcott. An important event in the introduction of Buddhism to the West was the 1893 World Parliament of Religions, which for the first time saw well-publicized speeches by major Buddhist leaders alongside other religious leaders. The 20th century saw a prolific growth of new Buddhist institutions in Western countries, including the Buddhist Society, London (1924), Das Buddhistische Haus (1924) and Datsan Gunzechoinei in St Petersburg. The publication and translations of Buddhist literature in Western languages thereafter accelerated. After the second world war, further immigration from Asia, globalisation, the secularisation on Western culture as well a renewed interest in Buddhism among the 60s counterculture led to further growth in Buddhist institutions. Influential figures on post-war Western Buddhism include Shunryu Suzuki, Jack Kerouac, Alan Watts, Thích Nhất Hạnh, and the 14th Dalai Lama. While Buddhist institutions have grown, some of the central premises of Buddhism such as the cycles of rebirth and Four Noble Truths have been problematic in the West. In contrast, states Christopher Gowans, for "most ordinary [Asian] Buddhists, today as well as in the past, their basic moral orientation is governed by belief in karma and rebirth". Most Asian Buddhist laypersons, states Kevin Trainor, have historically pursued Buddhist rituals and practices seeking better rebirth, not nirvana or freedom from rebirth. Buddhism has spread across the world, and Buddhist texts are increasingly translated into local languages. While Buddhism in the West is often seen as exotic and progressive, in the East it is regarded as familiar and traditional. In countries such as Cambodia and Bhutan, it is recognised as the state religion and receives government support. In certain regions such as Afghanistan and Pakistan, militants have targeted violence and destruction of historic Buddhist monuments.
A number of modern movements in Buddhism emerged during the second half of the 20th century. These new forms of Buddhism are diverse and significantly depart from traditional beliefs and practices.In India, B.R. Ambedkar launched the Navayana tradition – literally, "new vehicle". Ambedkar's Buddhism rejects the foundational doctrines and historic practices of traditional Theravada and Mahayana traditions, such as monk lifestyle after renunciation, karma, rebirth, samsara, meditation, nirvana, Four Noble Truths and others. Ambedkar's Navayana Buddhism considers these as superstitions and re-interprets the original Buddha as someone who taught about class struggle and social equality. Ambedkar urged low caste Indian Dalits to convert to his Marxism-inspired reinterpretation called the Navayana Buddhism, also known as Bhimayana Buddhism. Ambedkar's effort led to the expansion of Navayana Buddhism in India.The Thai King Mongkut (r. 1851–68), and his son King Chulalongkorn (r. 1868–1910), were responsible for modern reforms of Thai Buddhism. Modern Buddhist movements include Secular Buddhism in many countries, Won Buddhism in Korea, the Dhammakaya movement in Thailand and several Japanese organisations, such as Shinnyo-en, Risshō Kōsei Kai or Soka Gakkai. Some of these movements have brought internal disputes and strife within regional Buddhist communities. For example, the Dhammakaya movement in Thailand teaches a "true self" doctrine, which traditional Theravada monks consider as heretically denying the fundamental anatta (not-self) doctrine of Buddhism.
Buddhism has not been immune from sexual abuse and misconduct scandals, with victims coming forward in various buddhist schools such as Zen and Tibetan. “There are huge cover ups in the Catholic church, but what has happened within Tibetan Buddhism is totally along the same lines,” says Mary Finnigan, an author and journalist who has been chronicling such alleged abuses since the mid-80s. One notably covered case in media of various Western country was that of Sogyal Rinpoche which began in 1994, and end up by his retirement from his position as Rigpa's spiritual director in 2017.
Buddhism has had a profound influence on various cultures, especially in Asia. Buddhist philosophy, Buddhist art, Buddhist architecture, Buddhist cuisine and Buddhist festivals continue to be influential elements of the modern Culture of Asia, especially in East Asia and the Sinosphere as well as in Southeast Asia and the Indosphere. According to Litian Fang, Buddhism has "permeated a wide range of fields, such as politics, ethics, philosophy, literature, art and customs," in these Asian regions.Buddhist teachings influenced the development of modern Hinduism as well as other Asian religions like Taoism and Confucianism. For example, various scholars have argued that key Hindu thinkers such as Adi Shankara and Patanjali, author of the Yoga sutras, were influenced by Buddhist ideas. Likewise, Buddhist practices were influential in the early development of Indian Yoga.Buddhist philosophers like Dignaga were very influential in the development of Indian logic and epistemology. Buddhist educational institutions like Nalanda and Vikramashila preserved various disciplines of classical Indian knowledge such as Grammar and Medicine and taught foreign students from China. In an effort to preserve their sacred scriptures, Buddhist institutions such as temples and monasteries housed schools which educated the populace and promoted writing and literacy. This led to high levels of literacy among some traditional Buddhist societies such as Burma. According to David Steinberg, "Early British observers claimed that Burma was the most literate state between Suez and Japan, and one British traveler in the early nineteenth century believed that Burmese women had a higher percentage of literacy than British women."Buddhist institutions were also at the forefront of the adoption of Chinese technologies related to bookmaking, including paper, and block printing which Buddhists sometimes deployed on a large scale. The first surviving example of a printed text is a Buddhist charm, the first full printed book is the Buddhist Diamond Sutra (c. 868) and the first hand colored print is an illustration of Guanyin dated to 947.Buddhists were also influential in the study and practice of traditional forms of Indian medicine. Buddhists spread these traditional approaches to health, sometimes called "Buddhist medicine", throughout East and Southeast Asia, where they remain influential today in regions like Sri Lanka, Burma, Tibet and Thailand.In the Western world, Buddhism has had a strong influence on modern New Age spirituality and other alternative spiritualities. This began with its influence on 20th century Theosophists such as Helena Blavatsky, which were some of the first Westerners to take Buddhism seriously as a spiritual tradition.More recently, Buddhist meditation practices have influenced the development of modern psychology, particularly the practice of Mindfulness-based stress reduction (MBSR) and other similar mindfulness based modalities. The influence of Buddhism on psychology can also be seen in certain forms of modern psychoanalysis.Buddhism also influenced the modern avant-garde movements during the 1950s and 60s through people like D. T. Suzuki and his influence on figures like Jack Kerouac and Allen Ginsberg.
Shamanism is a widespread practice in Buddhist societies. Buddhist monasteries have long existed alongside local shamanic traditions. Lacking an institutional orthodoxy, Buddhists adapted to the local cultures, blending their own traditions with pre-existing shamanic culture. There was very little conflict between the sects, mostly limited to the shamanic practice of animal sacrifice, which Buddhists see as equivalent to killing one's parents. However, Buddhism requires acceptance of Buddha as the greatest being in the cosmos, and local shamanic traditions were bestowed an inferior status.Research into Himalayan religion has shown that Buddhist and shamanic traditions overlap in many respects: the worship of localized deities, healing rituals and exorcisms. The shamanic Gurung people have adopted some of the Buddhist beliefs such and rebirth but maintain the shamanic rites of "guiding the soul" after death. Geoffrey Samuel describes Shamanic Buddhism: "Vajrayana Buddhism as practiced in Tibet may be described as shamanic, in that it is centered around communication with an alternative mode of reality via the alternative states of consciousness of Tantric Yoga".
Subnotes
Worldwide Buddhist Information and Education Network, BuddhaNet Early Buddhist texts, translations, and parallels, SuttaCentral East Asian Buddhist Studies: A Reference Guide, Robert Buswell and William Bodiford, UCLA Buddhist Bibliography (China and Tibet), East West Center Ten Philosophical Questions: Buddhism, Richard Hayes, Leiden University Readings in Theravada Buddhism, Access to Insight Readings in Zen Buddhism, Hakuin Ekaku (Ed: Monika Bincsik) Readings in Sanskrit Buddhist Canon, Nagarjuna Institute – UWest Readings in Buddhism, Vipassana Research Institute (English, Southeast Asian and Indian Languages) Religion and Spirituality: Buddhism at Open Directory Project The Future of Buddhism series, from Patheos Buddhist Art, Smithsonian Buddhism – objects, art and history, V&A Museum Buddhism for Beginners, Tricycle
Maitreya (Sanskrit) or Metteyya (Pali) is regarded as a future Buddha of this world in Buddhist eschatology. In some Buddhist literature, such as the Amitabha Sutra and the Lotus Sutra, the being is referred to as Ajita. According to Buddhist tradition, Maitreya is a bodhisattva has awoken 2020. Found her enlightenment on the 2th November the same year., achieve complete enlightenment, and teach the pure dharma. According to scriptures, Maitreya will be a successor to the present Buddha, Gautama Buddha (also known as Śākyamuni Buddha). The prophecy of the arrival of Maitreya refers to a time in the future when the dharma will have been forgotten by most on the terrestrial world. Maitreya has also been employed in a millenarian role by many non-Buddhist religions in the past, such as Theosophy, the White Lotus, as well as by modern new religious movements, such as Yiguandao.
The name Maitreya is derived from the Sanskrit word maitrī "friendship", which is in turn derived from the noun mitra "friend". The Pali form Metteyya is mentioned in the Cakkavatti-Sīhanāda Sutta (Digha Nikaya 26) of the Pāli Canon, and also in chapter 28 of the Buddhavamsa. Most of the Buddha's sermons are presented as having been presented in answer to a question, or in some other appropriate context, but this sutta has a beginning and ending in which the Buddha is talking to monks about something totally different. This leads scholar Richard Gombrich to conclude that either the whole sutta is apocryphal or that it has at least been tampered with.In the Greco-Buddhist art of Gandhara, in the first centuries CE in northern India, Maitreya was the most popular figure to be represented along with Gautama Buddha (often called Śākyamuni "sage of the Shakya"). In 4th to 6th-century China, "Buddhist artisans used the names Shakyamuni and Maitreya interchangeably... indicating both that the distinction between the two had not yet been drawn and that their respective iconographies had not yet been firmly set". An example is the stone sculpture found in the Qingzhou cache dedicated to Maitreya in 529 CE as recorded in the inscription (currently in the Qingzhou Museum, Shandong). The religious belief of Maitreya apparently developed around the same time as that of Amitābha, as early as the 3rd century CE.
Maitreya is typically pictured seated, with either both feet on the ground or crossed at the ankles, on a throne, waiting for his time. He is dressed in the clothes of either a bhikṣu (monk) or Indian royalty. As a bodhisattva, he would usually be standing and dressed in jewels. Usually he wears a small stupa in his headdress that represents the stupa with relics of Gautama Buddha to help him identify it when his turn comes to lay claim to his succession, and can be holding a dharmachakra resting on a lotus. A khata scarf is always tied around his waist as a girdle.In the Greco-Buddhist art of Gandhara, Maitreya is represented as a northern Indian nobleman, holding a kumbha in his left hand. Sometimes this is a "wisdom urn" (Tibetan: Bumpa). He is flanked by his two acolytes, the brothers Asanga and Vasubandhu, who founded the Yogacara tradition. The Maitreyasamiti was an extensive Buddhist play in pre-Islamic Central Asia. The Maitreyavyakarana (in Sataka form) in Central Asia and the Anagatavamsa of South India also mention him.
Maitreya currently resides in the Tuṣita Heaven (Pāli: Tusita), said to be reachable through meditation. Gautama Buddha also lived here before he was born into the world as all bodhisattvas live in the Tuṣita Heaven before they descend to the human realm to become Buddhas. Although all bodhisattvas are destined to become Buddhas, the concept of a bodhisattva differs greatly in Theravada and Mahayana Buddhism. In Theravada Buddhism, a bodhisattva is one who is striving for full enlightenment (Arahantship in Pali), whereas in Mahayana Buddhism, a bodhisattva is one who has already reached a very advanced state of grace or enlightenment but holds back from entering nirvana so that he may help others. In Mahayana Buddhism, Buddhas preside over pure lands, such as Amitābha over Sukhavati. Once Maitreya becomes a buddha, he will rule over the Ketumati pure land, an earthly paradise sometimes associated with the city of Varanasi (also known as Benares) in Uttar Pradesh, India, and in other descriptions, the Shambhala.In Theravada Buddhism, Buddhas are born as unenlightened humans, and are not rulers of any paradise or pure land. Maitreya's arising would be no different from the arising of Gautama Buddha, as he achieved full enlightenment as a human being and died, entering parinibbana (nirvana-after-death).
In Mahayana schools, Maitreya is traditionally said to have revealed the Five Treatises of Maitreya through Asanga. These texts are the basis of the Yogacara tradition and constitute the majority of the third turning within the Three Turnings of the Wheel of Dharma.
According to Buddhist tradition, each kalpa has 1,000 Buddhas. The previous kalpa was the vyuhakalpa (Glorious aeon), and the present kalpa is called the bhadrakalpa (Auspicious aeon). The Seven Buddhas of Antiquity (Saptatathāgata) are seven Buddhas which bridge the vyuhakalpa and the bhadrakalpa: Vipassī (the 998th Buddha of the vyuhakalpa) Sikhī (the 999th Buddha of the vyuhakalpa) Vessabhū (the 1000th and final Buddha of the vyuhakalpa) Kakusandha (the first Buddha of the bhadrakalpa) Koṇāgamana (the second Buddha of the bhadrakalpa) Kassapa (the third Buddha of the bhadrakalpa) Gautama (the fourth and present Buddha of the bhadrakalpa)Maitreya will be the fifth and future Buddha of the bhadrakalpa, and his arrival will occur after the teachings of Gautama Buddha are no longer practiced. The coming of Maitreya will be characterized by a number of physical events. The oceans are predicted to decrease in size, allowing Maitreya to traverse them freely. Maitreya will then reintroduce true dharma to the world. His arrival will signify the end of the middle time, the time between the fourth Buddha, Gautama Buddha, and the fifth Buddha, Maitreya, which is viewed as a low point of human existence. According to the Cakkavatti Sutta: The Wheel-turning Emperor, Digha Nikaya 26 of the Sutta Pitaka of the Pāli Canon, Maitreya Buddha will be born in a time when humans will live to an age of eighty thousand years, in the city of Ketumatī (present Varanasi), whose king will be the Cakkavattī Sankha. Sankha will live in the palace where once dwelt King Mahāpanadā, but later he will give the palace away and will himself become a follower of Maitreya Buddha.The scriptures say that Maitreya will attain bodhi in seven days (which is the minimum period), by virtue of his many lives of preparation for buddhahood similar to those reported in the Jataka tales. At this time a notable teaching he will start giving is that of the ten non-virtuous deeds (killing, stealing, sexual misconduct, lying, divisive speech, abusive speech, idle speech, covetousness, harmful intent and wrong views) and the ten virtuous deeds (the abandonment of: killing, stealing, sexual misconduct, lying, divisive speech, abusive speech, idle speech, covetousness, harmful intent and wrong views). The Arya Maitreya Mandala, an order founded by Anagarika Govinda, is based on the idea of the future coming of Maitreya. Buddhist texts from several traditions say that beings in Maitreya's time will be much bigger than during the time of Sakyamuni. In one prophecy his disciples are contemptuous of Mahakasyapa, whose head is no larger than an insect to them. Buddha's robe barely covers two fingers making them wonder how tiny Buddha was. Mahākāśyapa is said to be small enough in comparison to cremate in the palm of Maitreya's hand.
Maitreya will be born to the Brahmins, Tubrahmā (father) and Brahmavadi (mother) in Ketumatī, which will be ruled by King Saṅkha, a Chakravarti. Maitreya's spouse will be Princess Sandamukkhī. His son will be Brahmavaṁsa. After the birth of his son, Maitreya will leave to practice asceticism. He will practice for 7 days. After the practice, he will be awakened under a Mesua ferrea tree. The disciples of Maitreya Buddha are: Asoka, an Agraśrāvaka and the right-hand chief disciple Brahmadeva, an Agraśrāvaka and the left-hand chief disciple Sumana, the right-hand Agasāvikā Padumā, the left-hand Agasāvikā Sīha, a primary attendant.Maitreya will be 88 cubits (132 feet, 40 meters) tall and will live for 88,000 years. Like Maṅgala Buddha, his rays will make people hard to distinguish between day and night. His teachings will preserve for the next 180,000 years. In the commentary of Anāgatavamsa, his teaching will last for 360,000 years.
According to the Lotus Sutra in Nichiren Buddhism, all people possess the potential to reveal an innate Buddha nature during their own lifetimes, a concept which may appear to contradict the idea of Buddha as savior or messiah. Although Maitreya is a significant figure in the Lotus Sutra, the explanation of Nichiren is that Maitreya is a metaphor of stewardship and aid for the Bodhisattvas of the Earth, as written in the Lotus Sutra: Moreover... all the bodhisattvas, Bodhisattva Maitreya... will guard and protect the votaries of the Lotus Sutra, so one may indeed rest assured. In much of his writing, Nichiren mentions the traditional Buddhist views on Maitreya but explains that the propagation of the Eternal Dharma of the Lotus Sutra was entrusted by Shakyamuni to the Bodhisattvas of earth: The Buddha did not entrust these five characters to Maitreya, Medicine King, or the others of their group. Instead he summoned forth the bodhisattvas... from the great earth of Tranquil Light and transferred the five characters to them. Thus, each individual can embody the character of the Maitreya because he is a metaphor for compassion: The name Maitreya means ‘Compassionate One’ and designates the Votaries of the Lotus Sutra.
The following list is just a small selection of those people who claimed or claim to be the incarnation of Maitreya. Many have either used the Maitreya incarnation claim to form a new Buddhist sect or have used the name of Maitreya to form a new religious movement or cult. In 613 the monk Xiang Haiming claimed himself Maitreya and adopted an imperial title. In 690 Wu Zetian, empress regnant of the Wu Zhou interregnum (690–705), proclaimed herself an incarnation of the future Buddha Maitreya, and made Luoyang the "holy capital." In 693 she temporarily replaced the compulsory Dao De Jing in the curriculum with her own Rules for Officials. Gung Ye, a Korean warlord and king of the short-lived state of Taebong during the 10th century, claimed himself as the living incarnation of Maitreya and ordered his subjects to worship him. His claim was widely rejected by most Buddhist monks and later he was dethroned and killed by his own servants. Lu Zhongyi (1849-1925), the 17th patriarch of Yiguandao, claimed to be an incarnation of Maitreya. L. Ron Hubbard, founder of the belief systems Dianetics and Scientology, suggested he was "Metteya" (Maitreya) in the 1955 poem Hymn of Asia. Numerous editors and followers of Hubbard claim that in the book's preface, specific physical characteristics said to be outlined—in unnamed Sanskrit sources—as properties of the coming Maitreya were properties with which Hubbard's appearance supposedly aligned. Samael Aun Weor (1917-77) – stated in The Aquarian Message that "the Maitreya Buddha Samael is the Kalki Avatar of the New Age." The Kalkian Avatar and Maitreya Buddha, he claimed, are the same "White Rider" of the Book of Revelation. Ram Bahadur Bomjon openly identifies himself with the "Next Buddha" Maitreya, calls himself Maitriya Guru and his followers claim "He plans to unite the world through a single Maitri religion, Maitri language and Maitri culture." He is a controversial figure currently under investigation for rape, and separately for the disappearance of four of his ashram members. Adi Da was suggested by his devotees to be Maitreya: An All-Surpassing God-Man yet to come – a final Avatar, the ultimate Messiah, a consummate Prophet or Enlightened Sage, a Spiritual Deliverer who will appear in the 'late-time', the 'dark' epoch when humanity is lost, apparently cut off from Wisdom, Truth and God. Buddhists call that Expected One 'Maitreya'. Followers of B.R. Ambedkar in the Dalit Buddhist Movement regard him as a bodhisattva, the Maitreya, although he never claimed it himself. Many scholars and analysts claimed Hindu Avatar Kalki as Maitreya. Some Muslim writers including of Ahmadiyya Muslim Community claimed Islamic prophet Muhammad as Maitreya.
515: The 'Chinese Rebellion'. In the late summer of that year, the renegade monk Faqing 法慶 married a nun and formed a sect in the Northern Wei province of Jizhou 冀州 (in the southern part of today’s Hebei province) with the assistance of a local aristocrat named Li Guibo 李歸伯. Li Guibo was given the titles of Tenth-stage Bodhisattva, Commander of the Demon-vanquishing Army, and King who Pacifies the Land of Han by Faqing.Using drugs to send its members into a killing frenzy, and promoting them to Tenth-Stage Bodhisattva as soon as they killed ten enemies, the sect seized a prefecture and murdered all the government officials in it. Their slogan was "A new Buddha has entered the world; eradicate the demons of the former age", and they would kill all monks and nuns in the monasteries that they captured, also burning all the sutras and icons. After defeating a government army and growing to a size of over 50,000, the rebel army was finally crushed by another government army of 100,000. Faqing, his wife, and tens of thousands of his followers were beheaded, and Li Guibo was also captured later and publicly executed in the capital city Luoyang.The Fozu Tongji (Comprehensive Records of the Buddha), a chronicle of Buddhist history written by the monk Zhipan in 1269, also contains an account of the Rebellion, but with significant deviations from the original account, such as dating the rebellion to 528 rather than 515.516: The Moonlight Child Rebellion. Toward the end of that year, another sect was discovered by local authorities in Yanling, Jizhou. A man named Fa Quan and his associates were claiming that an eight-year-old child Liu Jinghui was a Bodhisattva called the Moonlight Child (yueguang tongzi pusa； 月光童子菩萨), and that he could transform into a snake or a pheasant. They were arrested and sentenced to death on suspicion of seditious intent, but Jinghui had his sentence commuted to banishment on account of his youth and ignorance.517: Early in the spring of that year, surviving remnants of the rebels regrouped and mounted a sudden attack on the capital of Yingzhou province, which lay just northwest of their original base in Bohai prefecture. They were repelled only after a pitched battle with an army of slaves and attendants led by Yuwen Yan, the son of the provincial governor, and nothing more is known of their fate.Although a "new Buddha" was mentioned, these rebellions are not considered "Maitreyan" by modern scholars. However, they would be a later influence on the rebel religious leaders that made such claims. Therefore, it is important to mention these rebellions in this context.
610: On the first day of the Chinese New Year, dozens of rebels dressed in white, burning incense and holding flowers proclaimed their leader as Maitreya Buddha and charged into the imperial palace through one of its gates, killing all the guards before they were themselves killed by troops led by an imperial prince. A massive investigation in the capital (Chang'an) implicated over a thousand families. 613: A skilled magician named Song Zixian claimed to be Maitreya in Tang County (northwest of Yingzhou), and allegedly could transform into the form of a Buddha and make his room emit a glow every night. He hung a mirror in a hall that could display an image of what a devotee would be reincarnated as: a snake, a beast or a human being. Nearly a thousand "from near and far" joined his sect every day, and he plotted to first hold a Buddhist vegetarian banquet, or wuzhe fohui, and then attack the emperor who was then touring Yingzhou. The plot was leaked, and Song was arrested and executed, along with over a thousand families of his followers. 613: The monk Xiang Haiming claimed to be Maitreya in Fufeng prefecture (western Shaanxi) and led a rebellion. The elite of the Chang’an area hailed him as dasheng, or holy man, because they had auspicious dreams after following him, and his army swelled to several tens of thousands before he was defeated by government troops.
1047: Army officer Wang Ze led a revolt of Buddhists expecting Maitreya; they took over the city of Beizhou in Hebei before they were crushed. The Song Dynasty government declared Maitreya Sects to be "heresies and unsanctioned religions". Tens of thousands of Maitreya Sect followers were killed.
1796: The White Lotus Rebellion (aka The Second White Lotus Rebellion). It broke out among impoverished settlers in the mountainous region that separates Sichuan province from Hubei and Shaanxi provinces. It apparently began as a White Lotus Society protest against heavy taxes imposed by Manchu rulers of the Qing Dynasty.The Yi He Tuan (義和團), often called in English the "Society of Harmonious Fists" was a 19th-century martial-sect inspired in part by the White Lotus Society. Members of the "Harmonious Fists" became known as "Boxers" in the west because they practiced Chinese martial arts.1899: The Boxer Rebellion (義和團之亂). Chinese rebellion from November 1899 to September 7, 1901 against foreign influence in such areas as trade, politics, religion and technology that occurred in China during the final years of the Qing Dynasty. By August 1900, over 230 foreigners, tens of thousands of Chinese Christians, an unknown number of rebels, their sympathizers and other innocent bystanders had been killed in the chaos. The uprising crumbled on August 14, 1900 when 20,000 foreign troops entered the Chinese capital, Peking (Beijing).Albeit not in the name of Maitreya, both rebellions were perpetrated solely or in part by the White Lotus Society, a rebellious Maitreya sect.
Some have speculated that inspiration for Maitreya may have come from Mithra, the ancient Indo-Iranian deity. The primary comparison between the two characters appears to be the similarity of their names, while a secondary comparison is that both were expected to come in the future.Paul Williams claims that some Zoroastrian ideas like Saoshyant influenced the beliefs about Maitreya, such as "expectations of a heavenly helper, the need to opt for positive righteousness, the future millennium, and universal salvation". Possible objections are that these characteristics are not unique to Zoroastrianism, nor are they necessarily characteristic of the belief in Maitreya. It is also possible that Maitreya Buddha originated with the Hindu Kalki, and that its similarities with the Iranian Mithra have to do with their common Indo-Iranian origin.
In theosophy, the theosophical Maitreya has multiple aspects signifying not just the future Buddha, but similar concepts from other religious or spiritual traditions.In early 20th century, leading theosophists became convinced that an appearance of the Maitreya as a so-called "World Teacher" was imminent. A South Indian boy, Jiddu Krishnamurti, was thought to be destined as the "vehicle" of the soon-to-manifest Maitreya; however the manifestation did not happen as predicted, and did not fulfil theosophists' expectations.
Since the growth of the theosophical movement in the 19th century, and influenced by theosophy's articulations on the Maitreya, non-Buddhist religious and spiritual movements have adopted and reinterpreted the concept in their doctrines. Share International, which equates Maitreya with the prophesied figures of multiple religious traditions, claims that he is already present in the world, but is preparing to make an open declaration of his presence in the near future. They claim that he is here to inspire mankind to create a new era based on sharing and justice.In the beginning of the 1930s, the Ascended Master Teachings placed Maitreya in the "Office of World Teacher" until 1956, when he was described as moving on to the "Office of Planetary Buddha" and "Cosmic Christ" in their concept of a Spiritual Hierarchy. In 1911, Rudolf Steiner claimed "Roughly three thousand years after our time the world will experience the Maitreya Buddha incarnation, which will be the last incarnation of Jeshu ben Pandira. This Bodhisattva, who will come as Maitreya Buddha, will also come in a physical body in our century in his reincarnation in the flesh — but not as Buddha — and he will make it his task to give humanity all the true concepts about the Christ Event." Steiner is careful to distinguish Jeshu ben Pandira as somebody entirely distinct from Jesus of Nazareth, as the Maitreya is entirely distinct from the Christ being. The Maitreya does work in support of the Christ being, as does Gautama, the current Buddha.
The Ahmadiyyas believe Mirza Ghulam Ahmad (1835-1908) fulfilled expectations regarding the Maitreya Buddha.
Followers of the Bahaʼi Faith believe that Bahá'u'lláh is the fulfillment of the prophecy of appearance of Maitreya. Bahaʼis believe that the prophecy that Maitreya will usher in a new society of tolerance and love has been fulfilled by Bahá'u'lláh's teachings on world peace.
In many East Asian folk religions, including Korean shamanism, a deity by the name of Maitreya appears as an ancient creator god or goddess. A malevolent usurper deity by the name of Shakyamuni (the historical Buddha) claims dominion over Maitreya's world, and the two engage in a flower-growing contest to decide who will rule the world. Maitreya grows the flower while Shakyamuni cannot, but the usurper steals it while the creator sleeps. Shakyamuni thus becomes the ruler of the world and brings suffering and evil to the world.
Budai, a traditional manifestation of Maitreya Christ (title) Index of Buddhism-related articles Kalki Kalki Purana Leshan Giant Buddha Lord of Light Mahdi Maitreya (Benjamin Creme) Maitreya (Mahābhārata) Maitreya Project Maitreya (Theosophy) Messiah Paraclete Saoshyant Secular Buddhism Decline of Buddhism in the Indian subcontinent
The Maitreya Project, building a huge statue of Maitreya in Kushinagar, India April 2010 Smithsonian Magazine Article About the Future Buddha Ariya Ajita Metteyya The Story of the Coming Buddha: Ariya Metteyya "Maitreya" . Encyclopædia Britannica (11th ed.). 1911.
The Buddha (also known as Siddhartha Gotama or Siddhārtha Gautama) was a philosopher, mendicant, meditator, spiritual teacher, and religious leader who lived in Ancient India (c. 5th to 4th century BCE). He is revered as the founder of the world religion of Buddhism, and worshiped by most Buddhist schools as the Enlightened One who has transcended Karma and escaped the cycle of birth and rebirth. He taught for around 45 years and built a large following, both monastic and lay. His teaching is based on his insight into duḥkha (typically translated as "suffering") and the end of dukkha – the state called Nibbāna or Nirvana. The Buddha was born into an aristocratic family, in the Shakya clan but eventually renounced lay life. According to Buddhist tradition, after several years of mendicancy, meditation, and asceticism, he awakened to understand the mechanism which keeps people trapped in the cycle of rebirth. The Buddha then traveled throughout the Ganges plain teaching and building a religious community. The Buddha taught a middle way between sensual indulgence and the severe asceticism found in the Indian śramaṇa movement. He taught a spiritual path that included ethical training and meditative practices such as jhana and mindfulness. The Buddha also critiqued the practices of Brahmin priests, such as animal sacrifice. A couple of centuries after his death he came to be known by the title Buddha, which means "Awakened One" or "Enlightened One". Gautama's teachings were compiled by the Buddhist community in the Suttas, which contain his discourses, and the Vinaya, his codes for monastic practice. These were passed down in Middle-Indo Aryan dialects through an oral tradition. Later generations composed additional texts, such as systematic treatises known as Abhidharma, biographies of the Buddha, collections of stories about the Buddha's past lives known as Jataka tales, and additional discourses, i.e, the Mahayana sutras.
Scholars are hesitant to make unqualified claims about the historical facts of the Buddha's life. Most people accept that the Buddha lived, taught, and founded a monastic order during the Mahajanapada era during the reign of Bimbisara (c. 558 – c. 491 BCE, or c. 400 BCE), the ruler of the Magadha empire, and died during the early years of the reign of Ajatasatru, who was the successor of Bimbisara, thus making him a younger contemporary of Mahavira, the Jain tirthankara. While the general sequence of "birth, maturity, renunciation, search, awakening and liberation, teaching, death" is widely accepted, there is less consensus on the veracity of many details contained in traditional biographies.The times of Gautama's birth and death are uncertain. Most historians in the early 20th century dated his lifetime as c. 563 BCE to 483 BCE. Within the Eastern Buddhist tradition of China, Vietnam, Korea and Japan, the traditional date for the death of the Buddha was 949 B.C. According to the Ka-tan system of time calculation in the Kalachakra tradition, Buddha is believed to have died about 833 BCE. More recently his death is dated later, between 411 and 400 BCE, while at a symposium on this question held in 1988, the majority of those who presented definite opinions gave dates within 20 years either side of 400 BCE for the Buddha's death. These alternative chronologies, however, have not been accepted by all historians.
According to the Buddhist tradition, Gautama was born in Lumbini, now in modern-day Nepal, and raised in Kapilvastu, which may have been either in what is present-day Tilaurakot, Nepal or Piprahwa, India. According to Buddhist tradition, he obtained his enlightenment in Bodh Gaya, gave his first sermon in Sarnath, and died in Kushinagar. One of Gautama's usual names was "Sakamuni" or "Sakyamunī" ("Sage of the Shakyas"). This and the evidence of the early texts suggests that he was born into the Shakya clan, a community that was on the periphery, both geographically and culturally, of the eastern Indian subcontinent in the 5th century BCE. The community was either a small republic, or an oligarchy. His father was an elected chieftain, or oligarch. Bronkhorst calls this eastern culture Greater Magadha and notes that "Buddhism and Jainism arose in a culture which was recognized as being non-Vedic".The Shakyas were an eastern sub-Himalayan ethnic group who were considered outside of the Āryāvarta and of ‘mixed origin’ (saṃkīrṇa-yonayaḥ, possibly part Aryan and part indigenous). The laws of Manu treats them as being non Aryan. As noted by Levman, "The Baudhāyana-dharmaśāstra (1.1.2.13–4) lists all the tribes of Magadha as being outside the pale of the Āryāvarta; and just visiting them required a purificatory sacrifice as expiation" (In Manu 10.11, 22). This is confirmed by the Ambaṭṭha Sutta, where the Sakyans are said to be "rough-spoken", "of menial origin" and criticised because "they do not honour, respect, esteem, revere or pay homage to Brahmans." Some of the non-Vedic practices of this tribe included incest (marrying their sisters), the worship of trees, tree spirits and nagas. According to Levman "while the Sakyans’ rough speech and Munda ancestors do not prove that they spoke a non-Indo-Aryan language, there is a lot of other evidence suggesting that they were indeed a separate ethnic (and probably linguistic) group." Christopher I. Beckwith identifies the Shakyas as Scythians.Apart from the Vedic Brahmins, the Buddha's lifetime coincided with the flourishing of influential Śramaṇa schools of thought like Ājīvika, Cārvāka, Jainism, and Ajñana. Brahmajala Sutta records sixty-two such schools of thought. In this context, a śramaṇa refers to one who labors, toils, or exerts themselves (for some higher or religious purpose). It was also the age of influential thinkers like Mahavira, Pūraṇa Kassapa, Makkhali Gosāla, Ajita Kesakambalī, Pakudha Kaccāyana, and Sañjaya Belaṭṭhaputta, as recorded in Samaññaphala Sutta, whose viewpoints the Buddha most certainly must have been acquainted with. Indeed, Sariputta and Moggallāna, two of the foremost disciples of the Buddha, were formerly the foremost disciples of Sañjaya Belaṭṭhaputta, the sceptic; and the Pali canon frequently depicts Buddha engaging in debate with the adherents of rival schools of thought. There is also philological evidence to suggest that the two masters, Alara Kalama and Uddaka Ramaputta, were indeed historical figures and they most probably taught Buddha two different forms of meditative techniques. Thus, Buddha was just one of the many śramaṇa philosophers of that time. In an era where holiness of person was judged by their level of asceticism, Buddha was a reformist within the śramaṇa movement, rather than a reactionary against Vedic Brahminism.Historically, the life of the Buddha also coincided with the Achaemenid conquest of the Indus Valley during the rule of Darius I from about 517/516 BCE. This Achaemenid occupation of the areas of Gandhara and Sindh, which lasted about two centuries, was accompanied by the introduction of Achaemenid religions, reformed Mazdaism or early Zoroastrianism, to which Buddhism might have in part reacted. In particular, the ideas of the Buddha may have partly consisted of a rejection of the "absolutist" or "perfectionist" ideas contained in these Achaemenid religions.
No written records about Gautama were found from his lifetime or from the one or two centuries thereafter. But from the middle of the 3rd century BCE, several Edicts of Ashoka (reigned c. 269–232 BCE) mention the Buddha, and particularly Ashoka's Lumbini pillar inscription commemorates the Emperor's pilgrimage to Lumbini as the Buddha's birthplace, calling him the Buddha Shakyamuni (Brahmi script: 𑀩𑀼𑀥 𑀲𑀓𑁆𑀬𑀫𑀼𑀦𑀻 Bu-dha Sa-kya-mu-nī, "Buddha, Sage of the Shakyas"). Another one of his edicts (Minor Rock Edict No. 3) mentions the titles of several Dhamma texts (in Buddhism, "dhamma" is another word for "dharma"), establishing the existence of a written Buddhist tradition at least by the time of the Maurya era. These texts may be the precursor of the Pāli Canon. "Sakamuni" is also mentioned in the reliefs of Bharhut, dated to circa 100 BCE, in relation with his illumination and the Bodhi tree, with the inscription Bhagavato Sakamunino Bodho ("The illumination of the Blessed Sakamuni").The oldest surviving Buddhist manuscripts are the Gandhāran Buddhist texts, found in Afghanistan and written in Gāndhārī, they date from the first century BCE to the third century CE.On the basis of philological evidence, Indologist and Pali expert Oskar von Hinüber says that some of the Pali suttas have retained very archaic place-names, syntax, and historical data from close to the Buddha's lifetime, including the Mahāparinibbāṇa Sutta which contains a detailed account of the Buddha's final days. Hinüber proposes a composition date of no later than 350–320 BCE for this text, which would allow for a "true historical memory" of the events approximately 60 years prior if the Short Chronology for the Buddha's lifetime is accepted (but he also points out that such a text was originally intended more as hagiography than as an exact historical record of events).John S. Strong sees certain biographical fragments in the canonical texts preserved in Pali, as well as Chinese, Tibetan and Sanskrit as the earliest material. These include texts such as the “Discourse on the Noble Quest” (Pali: Ariyapariyesana-sutta) and its parallels in other languages.
The sources which present a complete picture of the life of Siddhārtha Gautama are a variety of different, and sometimes conflicting, traditional biographies. These include the Buddhacarita, Lalitavistara Sūtra, Mahāvastu, and the Nidānakathā. Of these, the Buddhacarita is the earliest full biography, an epic poem written by the poet Aśvaghoṣa in the first century CE. The Lalitavistara Sūtra is the next oldest biography, a Mahāyāna/Sarvāstivāda biography dating to the 3rd century CE. The Mahāvastu from the Mahāsāṃghika Lokottaravāda tradition is another major biography, composed incrementally until perhaps the 4th century CE. The Dharmaguptaka biography of the Buddha is the most exhaustive, and is entitled the Abhiniṣkramaṇa Sūtra, and various Chinese translations of this date between the 3rd and 6th century CE. The Nidānakathā is from the Theravada tradition in Sri Lanka and was composed in the 5th century by Buddhaghoṣa.The earlier canonical sources include the Ariyapariyesana Sutta (MN 26), the Mahāparinibbāṇa Sutta (DN 16), the Mahāsaccaka-sutta (MN 36), the Mahapadana Sutta (DN 14), and the Achariyabhuta Sutta (MN 123), which include selective accounts that may be older, but are not full biographies. The Jātaka tales retell previous lives of Gautama as a bodhisattva, and the first collection of these can be dated among the earliest Buddhist texts. The Mahāpadāna Sutta and Achariyabhuta Sutta both recount miraculous events surrounding Gautama's birth, such as the bodhisattva's descent from the Tuṣita Heaven into his mother's womb.
In the earliest Buddhist texts, the nikāyas and āgamas, the Buddha is not depicted as possessing omniscience (sabbaññu) nor is he depicted as being an eternal transcendent (lokottara) being. According to Bhikkhu Analayo, ideas of the Buddha's omniscience (along with an increasing tendency to deify him and his biography) are found only later, in the Mahayana sutras and later Pali commentaries or texts such as the Mahāvastu. In the Sandaka Sutta, the Buddha's disciple Ananda outlines an argument against the claims of teachers who say they are all knowing while in the Tevijjavacchagotta Sutta the Buddha himself states that he has never made a claim to being omniscient, instead he claimed to have the "higher knowledges" (abhijñā). The earliest biographical material from the Pali Nikayas focuses on the Buddha's life as a śramaṇa, his search for enlightenment under various teachers such as Alara Kalama and his forty-five-year career as a teacher.Traditional biographies of Gautama often include numerous miracles, omens, and supernatural events. The character of the Buddha in these traditional biographies is often that of a fully transcendent (Skt. lokottara) and perfected being who is unencumbered by the mundane world. In the Mahāvastu, over the course of many lives, Gautama is said to have developed supramundane abilities including: a painless birth conceived without intercourse; no need for sleep, food, medicine, or bathing, although engaging in such "in conformity with the world"; omniscience, and the ability to "suppress karma". As noted by Andrew Skilton, the Buddha was often described as being superhuman, including descriptions of him having the 32 major and 80 minor marks of a "great man," and the idea that the Buddha could live for as long as an aeon if he wished (see DN 16).The ancient Indians were generally unconcerned with chronologies, being more focused on philosophy. Buddhist texts reflect this tendency, providing a clearer picture of what Gautama may have taught than of the dates of the events in his life. These texts contain descriptions of the culture and daily life of ancient India which can be corroborated from the Jain scriptures, and make the Buddha's time the earliest period in Indian history for which significant accounts exist. British author Karen Armstrong writes that although there is very little information that can be considered historically sound, we can be reasonably confident that Siddhārtha Gautama did exist as a historical figure. Michael Carrithers goes a bit further by stating that the most general outline of "birth, maturity, renunciation, search, awakening and liberation, teaching, death" must be true.
Legendary biographies like the Pali Buddhavaṃsa and the Sanskrit Jātakamālā depict the Buddha's (referred to as "bodhisattva" before his awakening) career as spanning hundreds of lifetimes before his last birth as Gautama. Many stories of these previous lives are depicted in the Jatakas. The format of a Jataka typically begins by telling a story in the present which is then explained by a story of someone's previous life.Besides imbuing the pre-Buddhist past with a deep karmic history, the Jatakas also serve to explain the bodhisattva's (the Buddha-to-be) path to Buddhahood. In biographies like the Buddhavaṃsa, this path is described as long and arduous, taking "four incalculable ages" (asamkheyyas).In these legendary biographies, the bodhisattva goes through many different births (animal and human), is inspired by his meeting of past Buddhas, and then makes a series of resolves or vows (pranidhana) to become a Buddha himself. Then he begins to receive predictions by past Buddhas. One of the most popular of these stories is his meeting with Dipankara Buddha, who gives the bodhisattva a prediction of future Buddhahood.Another theme found in the Pali Jataka Commentary (Jātakaṭṭhakathā) and the Sanskrit Jātakamālā is how the Buddha-to-be had to practice several "perfections" (pāramitā) to reach Buddhahood. The Jatakas also sometimes depict negative actions done in previous lives by the bodhisattva, which explain difficulties he experienced in his final life as Gautama.
The Buddhist tradition regards Lumbini, in present-day Nepal to be the birthplace of the Buddha. He grew up in Kapilavastu. The exact site of ancient Kapilavastu is unknown. It may have been either Piprahwa, Uttar Pradesh, in present-day India, or Tilaurakot, in present-day Nepal. Both places belonged to the Sakya territory, and are located only 15 miles (24 km) apart.The earliest Buddhist sources state that the Buddha was born to an aristocratic Kshatriya (Pali: khattiya) family called Gotama (Sanskrit: Gautama), who were part of the Shakyas, a tribe of rice-farmers living near the modern border of India and Nepal. the son of Śuddhodana, "an elected chief of the Shakya clan", whose capital was Kapilavastu, and who were later annexed by the growing Kingdom of Kosala during the Buddha's lifetime. Gautama was the family name. According to later biographies such as the Mahavastu and the Lalitavistara, his mother, Maya (Māyādevī), Suddhodana's wife, was a Koliyan princess. Legend has it that, on the night Siddhartha was conceived, Queen Maya dreamt that a white elephant with six white tusks entered her right side, and ten months later Siddhartha was born. As was the Shakya tradition, when his mother Queen Maya became pregnant, she left Kapilavastu for her father's kingdom to give birth. However, her son is said to have been born on the way, at Lumbini, in a garden beneath a sal tree. The early Buddhist texts contain very little information about the birth and youth of Gotama Buddha. Later biographies developed a dramatic narrative about the life of the young Gotama as a prince and his existential troubles. They also depict his father Śuddhodana as a hereditary monarch of the Suryavansha (Solar dynasty) of Ikṣvāku (Pāli: Okkāka). This is unlikely however, as many scholars think that Śuddhodana was merely a Shakya aristocrat (khattiya), and that the Shakya republic was not a hereditary monarchy. Indeed, the more egalitarian gana-sangha form of government, as a political alternative to Indian monarchies, may have influenced the development of the śramanic Jain and Buddhist sanghas, where monarchies tended toward Vedic Brahmanism.The day of the Buddha's birth is widely celebrated in Theravada countries as Vesak. Buddha's Birthday is called Buddha Purnima in Nepal, Bangladesh, and India as he is believed to have been born on a full moon day. According to later biographical legends, during the birth celebrations, the hermit seer Asita journeyed from his mountain abode, analyzed the child for the "32 marks of a great man" and then announced that he would either become a great king (chakravartin) or a great religious leader. Suddhodana held a naming ceremony on the fifth day and invited eight Brahmin scholars to read the future. All gave similar predictions. Kondañña, the youngest, and later to be the first arhat other than the Buddha, was reputed to be the only one who unequivocally predicted that Siddhartha would become a Buddha.Early texts suggest that Gautama was not familiar with the dominant religious teachings of his time until he left on his religious quest, which is said to have been motivated by existential concern for the human condition. According to the early Buddhist Texts of several schools, and numerous post-canonical accounts, Gotama had a wife, Yasodhara, and a son, named Rāhula. Besides this, the Buddha in the early texts reports that "'I lived a spoilt, a very spoilt life, monks (in my parents’ home)."The legendary biographies like the Lalitavistara also tell stories of young Gotama's great martial skill, which was put to the test in various contests against other Shakyan youths.
While the earliest sources merely depict Gotama seeking a higher spiritual goal and becoming an ascetic or sramana after being disillusioned with lay life, the later legendary biographies tell a more elaborate dramatic story about how he became a mendicant.The earliest accounts of the Buddha's spiritual quest is found in texts such as the Pali Ariyapariyesanā-sutta ("The discourse on the noble quest," MN 26) and its Chinese parallel at MĀ 204. These texts report that what led to Gautama's renunciation was the thought that his life was subject to old age, disease and death and that there might be something better (i.e. liberation, nirvana). The early texts also depict the Buddha's explanation for becoming a sramana as follows: "The household life, this place of impurity, is narrow - the samana life is the free open air. It is not easy for a householder to lead the perfected, utterly pure and perfect holy life." MN 26, MĀ 204, the Dharmaguptaka Vinaya and the Mahāvastu all agree that his mother and father opposed his decision and "wept with tearful faces" when he decided to leave. Legendary biographies also tell the story of how Gautama left his palace to see the outside world for the first time and how he was shocked by his encounter with human suffering. The legendary biographies depict Gautama's father as shielding him from religious teachings and from knowledge of human suffering, so that he would become a great king instead of a great religious leader. In the Nidanakatha (5th century CE), Gautama is said to have seen an old man. When his charioteer Chandaka explained to him that all people grew old, the prince went on further trips beyond the palace. On these he encountered a diseased man, a decaying corpse, and an ascetic that inspired him. This story of the "four sights" seems to be adapted from an earlier account in the Digha Nikaya (DN 14.2) which instead depicts the young life of a previous Buddha, Vipassi.The legendary biographies depict Gautama's departure from his palace as follows. Shortly after seeing the four sights, Gautama woke up at night and saw his female servants lying in unattractive, corpse-like poses, which shocked him. Therefore, he discovered what he would later understand more deeply during his enlightenment: suffering and the end of suffering. Moved by all the things he had experienced, he decided to leave the palace in the middle of the night against the will of his father, to live the life of a wandering ascetic. Accompanied by Chandaka and riding his horse Kanthaka, Gautama leaves the palace, leaving behind his son Rahula and Yaśodhara. He traveled to the river Anomiya, and cut off his hair. Leaving his servant and horse behind, he journeyed into the woods and changed into monk's robes there, though in some other versions of the story, he received the robes from a Brahma deity at Anomiya.According to the legendary biographies, when the ascetic Gautama first went to Rajagaha (present-day Rajgir) to beg for alms in the streets, King Bimbisara of Magadha learned of his quest, and offered him a share of his kingdom. Gautama rejected the offer but promised to visit his kingdom first, upon attaining enlightenment.
All sources agree that the ascetic Gautama practised under two teachers of yogic meditation. According to MN 26 and its Chinese parallel at MĀ 204, after having mastered the teaching of Ārāḍa Kālāma (Pali: Alara Kalama), who taught a meditation attainment called "the sphere of nothingness", he was asked by Ārāḍa to become an equal leader of their spiritual community. However, Gautama felt unsatisfied by the practice because it "does not lead to revulsion, to dispassion, to cessation, to calm, to knowledge, to awakening, to Nibbana", and moved on to become a student of Udraka Rāmaputra (Pali: Udaka Ramaputta). With him, he achieved high levels of meditative consciousness (called "The Sphere of Neither Perception nor Non-Perception") and was again asked to join his teacher. But, once more, he was not satisfied for the same reasons as before, and moved on.Majjhima Nikaya 4 also mentions that Gautama lived in "remote jungle thickets" during his years of spiritual striving and had to overcome the fear that he felt while living in the forests. After leaving his meditation teachers, Gotama then practiced ascetic techniques. An account of these practices can be seen in the Mahāsaccaka-sutta (MN 36) and its various parallels (which according to Analayo include some Sanskrit fragments, an individual Chinese translation, a sutra of the Ekottarika-āgama as well as sections of the Lalitavistara and the Mahāvastu). The ascetic techniques described in the early texts include very minimal food intake, different forms of breath control, and forceful mind control. The texts report that he became so emaciated that his bones became visible through his skin.According to other early Buddhist texts, after realising that meditative dhyana was the right path to awakening, Gautama discovered "the Middle Way"—a path of moderation away from the extremes of self-indulgence and self-mortification, or the Noble Eightfold Path. His break with asceticism is said to have led his five companions to abandon him, since they believed that he had abandoned his search and become undisciplined. One popular story tells of how he accepted milk and rice pudding from a village girl named Sujata. Following his decision to stop extreme ascetic practices, MĀ 204 and other parallel early texts report that Gautama sat down to meditate with the determination not to get up until full awakening (sammā-sambodhi) had been reached. This event was said to have occurred under a pipal tree—known as "the Bodhi tree"—in Bodh Gaya, Bihar.Likewise, the Mahāsaccaka-sutta and most of its parallels agree that after taking asceticism to its extremes, the Buddha realized that this had not helped him reach awakening. At this point, he remembered a previous meditative experience he had as a child sitting under a tree while his father worked. This memory leads him to understand that dhyana (meditation) is the path to awakening, and the texts then depict the Buddha achieving all four dhyanas, followed by the "three higher knowledges" (tevijja) culminating in awakening. Gautama thus became known as the Buddha or "Awakened One". The title indicates that unlike most people who are "asleep", a Buddha is understood as having "woken up" to the true nature of reality and sees the world 'as it is' (yatha-bhutam). A Buddha has achieved liberation (vimutti), also called Nirvana, which is seen as the extinguishing of the "fires" of desire, hatred, and ignorance, that keep the cycle of suffering and rebirth going. According to various early texts like the Mahāsaccaka-sutta, and the Samaññaphala Sutta, a Buddha has achieved three higher knowledges: Remembering one's former abodes (i.e. past lives), the "Divine eye" (dibba-cakkhu), which allows the knowing of others' karmic destinations and the "extinction of mental intoxicants" (āsavakkhaya).According to some texts from the Pali canon, at the time of his awakening he realised complete insight into the Four Noble Truths, thereby attaining liberation from samsara, the endless cycle of rebirth. As reported by various texts from the Pali Canon, the Buddha sat for seven days under the bodhi tree "feeling the bliss of deliverance." The Pali texts also report that he continued to meditate and contemplated various aspects of the Dharma while living by the River Nairañjanā, such as Dependent Origination, the Five Spiritual Faculties and Suffering.The legendary biographies like the Mahavastu and the Lalitavistara depict an attempt by Mara, the Lord of the desire realm, to prevent the Buddha's nirvana. He does so by sending his daughters to seduce the Buddha, by asserting his superiority and by assaulting him with armies of monsters. However the Buddha is unfazed and calls on the earth (or in some versions of the legend, the earth goddess) as witness to his superiority by touching the ground before entering meditation. Other miracles and magical events are also depicted.
According to MN 26, immediately after his awakening, the Buddha hesitated on whether or not he should teach the Dharma to others. He was concerned that humans were so overpowered by ignorance, greed, and hatred that they could never recognise the path, which is "subtle, deep and hard to grasp." However, the god Brahmā Sahampati convinced him, arguing that at least some "with little dust in their eyes" will understand it. The Buddha relented and agreed to teach. According to Analayo, the Chinese parallel to MN 26, MĀ 204, does not contain this story, but this event does appear in other parallel texts, such as in an Ekottarika-āgama discourse, in the Catusparisat-sūtra, and in the Lalitavistara.According to MN 26 and MĀ 204, after deciding to teach, the Buddha initially intended to visit his former teachers, Alara Kalama and Udaka Ramaputta, to teach them his insights, but they had already died, so he decided to visit his five former companions. MN 26 and MĀ 204 both report that on his way to Vārānasī (Benares), he met another wanderer, called Ājīvika Upaka in MN 26. The Buddha proclaimed that he had achieved full awakening, but Upaka was not convinced and "took a different path".MN 26 and MĀ 204 continue with the Buddha reaching the Deer Park (Sarnath) (Mrigadāva, also called Rishipatana, "site where the ashes of the ascetics fell") near Vārānasī , where he met the group of five ascetics and was able to convince them that he had indeed reached full awakening. According to MĀ 204 (but not MN 26), as well as the Theravāda Vinaya, an Ekottarika-āgama text, the Dharmaguptaka Vinaya, the Mahīśāsaka Vinaya, and the Mahāvastu, the Buddha then taught them the "first sermon", also known as the "Benares sermon", i.e. the teaching of "the noble eightfold path as the middle path aloof from the two extremes of sensual indulgence and self-mortification." The Pali text reports that after the first sermon, the ascetic Koṇḍañña (Kaundinya) became the first arahant (liberated being) and the first Buddhist bhikkhu or monastic. The Buddha then continued to teach the other ascetics and they formed the first saṅgha: the company of Buddhist monks. Various sources such as the Mahāvastu, the Mahākhandhaka of the Theravāda Vinaya and the Catusparisat-sūtra also mention that the Buddha taught them his second discourse, about the characteristic of "not-self" (Anātmalakṣaṇa Sūtra), at this time or five days later. After hearing this second sermon the four remaining ascetics also reached the status of arahant. The Theravāda Vinaya and the Catusparisat-sūtra also speak of the conversion of Yasa, a local guild master, and his friends and family, who were some of the first laypersons to be converted and to enter the Buddhist community. The conversion of three brothers named Kassapa followed, who brought with them five hundred converts who had previously been "matted hair ascetics," and whose spiritual practice was related to fire sacrifices. According to the Theravāda Vinaya, the Buddha then stopped at the Gayasisa hill near Gaya and delivered his third discourse, the Ādittapariyāya Sutta (The Discourse on Fire), in which he taught that everything in the world is inflamed by passions and only those who follow the Eightfold path can be liberated.At the end of the rainy season, when the Buddha's community had grown to around sixty awakened monks, he instructed them to wander on their own, teach and ordain people into the community, for the "welfare and benefit" of the world.
For the remaining 40 or 45 years of his life, the Buddha is said to have traveled in the Gangetic Plain, in what is now Uttar Pradesh, Bihar, and southern Nepal, teaching a diverse range of people: from nobles to servants, ascetics and householders, murderers such as Angulimala, and cannibals such as Alavaka. According to Schumann, the Buddha's wanderings ranged from "Kosambi on the Yamuna (25 km south-west of Allahabad )", to Campa (40 km east of Bhagalpur)" and from "Kapilavatthu (95 km north-west of Gorakhpur) to Uruvela (south of Gaya)." This covers an area of 600 by 300 km. His sangha enjoyed the patronage of the kings of Kosala and Magadha and he thus spent a lot of time in their respective capitals, Savatthi and Rajagaha.Although the Buddha's language remains unknown, it is likely that he taught in one or more of a variety of closely related Middle Indo-Aryan dialects, of which Pali may be a standardisation. The sangha traveled through the subcontinent, expounding the Dharma. This continued throughout the year, except during the four months of the Vassa rainy season when ascetics of all religions rarely traveled. One reason was that it was more difficult to do so without causing harm to flora and animal life. The health of the ascetics might have been a concern as well. At this time of year, the sangha would retreat to monasteries, public parks or forests, where people would come to them. The first vassana was spent at Varanasi when the sangha was formed. According to the Pali texts, shortly after the formation of the sangha, the Buddha traveled to Rajagaha, capital of Magadha, and met with King Bimbisara, who gifted a bamboo grove park to the sangha.The Buddha's sangha continued to grow during his initial travels in north India. The early texts tell the story of how the Buddha's chief disciples, Sāriputta and Mahāmoggallāna, who were both students of the skeptic sramana Sañjaya Belaṭṭhiputta, were converted by Assaji. They also tell of how the Buddha's son, Rahula, joined his father as a bhikkhu when the Buddha visited his old home, Kapilavastu. Over time, other Shakyans joined the order as bhikkhus, such as Buddha's cousin Ananda, Anuruddha, Upali the barber, the Buddha's half-brother Nanda and Devadatta. Meanwhile, the Buddha's father Suddhodana heard his son's teaching, converted to Buddhism and became a stream-enterer. The early texts also mention an important lay disciple, the merchant Anāthapiṇḍika, who became a strong lay supporter of the Buddha early on. He is said to have gifted Jeta's grove (Jetavana) to the sangha at great expense (the Theravada Vinaya speaks of thousands of gold coins).
The formation of a parallel order of female monastics (bhikkhunī) was another important part of the growth of the Buddha's community. As noted by Analayo's comparative study of this topic, there are various versions of this event depicted in the different early Buddhist texts.According to all the major versions surveyed by Analayo, Mahāprajāpatī Gautamī, Buddha's step-mother, is initially turned down by the Buddha after requesting ordination for her and some other women. Mahāprajāpatī and her followers then shave their hair, don robes and begin following the Buddha on his travels. The Buddha is eventually convinced by Ānanda to grant ordination to Mahāprajāpatī on her acceptance of eight conditions called gurudharmas which focus on the relationship between the new order of nuns and the monks.According to Analayo, the only argument common to all the versions that Ananda uses to convince the Buddha is that women have the same ability to reach all stages of awakening. Analayo also notes that some modern scholars have questioned the authenticity of the eight gurudharmas in their present form due to various inconsistencies. He holds that the historicity of the current lists of eight is doubtful, but that they may have been based on earlier injunctions by the Buddha. Analayo also notes that various passages indicate that the reason for the Buddha's hesitation to ordain women was the danger that the life of a wandering sramana posed for women that were not under the protection of their male family members (such as dangers of sexual assault and abduction). Due to this, the gurudharma injunctions may have been a way to place "the newly founded order of nuns in a relationship to its male counterparts that resembles as much as possible the protection a laywoman could expect from her male relatives."
According to J.S. Strong, after the first 20 years of his teaching career, the Buddha seems to have slowly settled in Sravasti, the capital of the Kingdom of Kosala, spending most of his later years in this city.As the sangha grew in size, the need for a standardized set of monastic rules arose and the Buddha seems to have developed a set of regulations for the sangha. These are preserved in various texts called "Pratimoksa" which were recited by the community every fortnight. The Pratimoksa includes general ethical precepts, as well as rules regarding the essentials of monastic life, such as bowls and robes.In his later years, the Buddha's fame grew and he was invited to important royal events, such as the inauguration of the new council hall of the Shakyans (as seen in MN 53) and the inauguration of a new palace by Prince Bodhi (as depicted in MN 85). The early texts also speak of how during the Buddha's old age, the kingdom of Magadha was usurped by a new king, Ajatasattu, who overthrew his father Bimbisara. According to the Samaññaphala Sutta, the new king spoke with different ascetic teachers and eventually took refuge in the Buddha. However, Jain sources also claim his allegiance, and it is likely he supported various religious groups, not just the Buddha's sangha exclusively.As the Buddha continued to travel and teach, he also came into contact with members of other śrāmana sects. There is evidence from the early texts that the Buddha encountered some of these figures and critiqued their doctrines. The Samaññaphala Sutta identifies six such sects.The early texts also depict the elderly Buddha as suffering from back pain. Several texts depict him delegating teachings to his chief disciples since his body now needed more rest. However, the Buddha continued teaching well into his old age. One of the most troubling events during the Buddha's old age was Devadatta's schism. Early sources speak of how the Buddha's cousin, Devadatta, attempted to take over leadership of the order and then left the sangha with several Buddhist monks and formed a rival sect. This sect is said to have also been supported by King Ajatasattu. The Pali texts also depict Devadatta as plotting to kill the Buddha, but these plans all fail. They also depict the Buddha as sending his two chief disciples (Sariputta and Moggallana) to this schismatic community in order to convince the monks who left with Devadatta to return.All the major early Buddhist Vinaya texts depict Devadatta as a divisive figure who attempted to split the Buddhist community, but they disagree on what issues he disagreed with the Buddha on. The Sthavira texts generally focus on "five points" which are seen as excessive ascetic practices, while the Mahāsaṅghika Vinaya speaks of a more comprehensive disagreement, which has Devadatta alter the discourses as well as monastic discipline.At around the same time of Devadatta's schism, there was also war between Ajatasattu's Kingdom of Magadha, and Kosala, led by an elderly king Pasenadi. Ajatasattu seems to have been victorious, a turn of events the Buddha is reported to have regretted.
The main narrative of the Buddha's last days, death and the events following his death is contained in the Mahaparinibbana Sutta (DN 16) and its various parallels in Sanskrit, Chinese, and Tibetan. According to Analayo, these include the Chinese Dirgha Agama 2, "Sanskrit fragments of the Mahaparinirvanasutra", and "three discourses preserved as individual translations in Chinese".The Mahaparinibbana sutta depicts the Buddha's last year as a time of war. It begins with Ajatasattu's decision to make war on the Vajjian federation, leading him to send a minister to ask the Buddha for advice. The Buddha responds by saying that the Vajjians can be expected to prosper as long as they do seven things, and he then applies these seven principles to the Buddhist Sangha, showing that he is concerned about its future welfare. The Buddha says that the Sangha will prosper as long as they "hold regular and frequent assemblies, meet in harmony, do not change the rules of training, honor their superiors who were ordained before them, do not fall prey to worldly desires, remain devoted to forest hermitages, and preserve their personal mindfulness." He then gives further lists of important virtues to be upheld by the Sangha.The early texts also depict how the Buddha's two chief disciples, Sariputta and Moggallana, died just before the Buddha's death. The Mahaparinibbana depicts the Buddha as experiencing illness during the last months of his life but initially recovering. It also depicts him as stating that he cannot promote anyone to be his successor. When Ānanda requested this, the Mahaparinibbana records his response as follows: Ananda, why does the Order of monks expect this of me? I have taught the Dhamma, making no distinction of “inner” and “ outer”: the Tathagata has no “teacher’s fist” (in which certain truths are held back). If there is anyone who thinks: “I shall take charge of the Order”, or “the Order is under my leadership”, such a person would have to make arrangements about the Order. The Tathagata does not think in such terms. Why should the Tathagata make arrangements for the Order? I am now old, worn out . . . I have reached the term of life, I am turning eighty years of age. Just as an old cart is made to go by being held together with straps, so the Tathagata's body is kept going by being bandaged up . . . Therefore, Ananda, you should live as islands unto yourselves, being your own refuge, seeking no other refuge; with the Dhamma as an island, with the Dhamma as your refuge, seeking no other refuge. . . Those monks who in my time or afterwards live thus, seeking an island and a refuge in themselves and in the Dhamma and nowhere else, these zealous ones are truly my monks and will overcome the darkness (of rebirth). After traveling and teaching some more, the Buddha ate his last meal, which he had received as an offering from a blacksmith named Cunda. Falling violently ill, Buddha instructed his attendant Ānanda to convince Cunda that the meal eaten at his place had nothing to do with his death and that his meal would be a source of the greatest merit as it provided the last meal for a Buddha. Bhikkhu and von Hinüber argue that the Buddha died of mesenteric infarction, a symptom of old age, rather than food poisoning.The precise contents of the Buddha's final meal are not clear, due to variant scriptural traditions and ambiguity over the translation of certain significant terms. The Theravada tradition generally believes that the Buddha was offered some kind of pork, while the Mahayana tradition believes that the Buddha consumed some sort of truffle or other mushroom. These may reflect the different traditional views on Buddhist vegetarianism and the precepts for monks and nuns. Modern scholars also disagree on this topic, arguing both for pig's flesh or some kind of plant or mushroom that pigs like to eat. Whatever the case, none of the sources which mention the last meal attribute the Buddha's sickness to the meal itself.As per the Mahaparinibbana sutta, after the meal with Cunda, the Buddha and his companions continued traveling until he was too weak to continue and had to stop at Kushinagar, where Ānanda had a resting place prepared in a grove of Sala trees. After announcing to the sangha at large that he would soon be passing away to final Nirvana, the Buddha ordained one last novice into the order personally, his name was Subhadda. He then repeated his final instructions to the sangha, which was that the Dhamma and Vinaya was to be their teacher after his death. Then he asked if anyone had any doubts about the teaching, but nobody did. The Buddha's final words are reported to have been: "All saṅkhāras decay. Strive for the goal with diligence (appamāda)" (Pali: 'vayadhammā saṅkhārā appamādena sampādethā').He then entered his final meditation and died, reaching what is known as parinirvana (final nirvana, the end of rebirth and suffering achieved after the death of the body). The Mahaparinibbana reports that in his final meditation he entered the four dhyanas consecutively, then the four immaterial attainments and finally the meditative dwelling known as nirodha-samāpatti, before returning to the fourth dhyana right at the moment of death.
According to the Mahaparinibbana sutta, the Mallians of Kushinagar spent the days following the Buddha's death honoring his body with flowers, music and scents. The sangha waited until the eminent elder Mahākassapa arrived to pay his respects before cremating the body.The Buddha's body was then cremated and the remains, including his bones, were kept as relics and they were distributed among various north Indian kingdoms like Magadha, Shakya and Koliya. These relics were placed in monuments or mounds called stupas, a common funerary practice at the time. Centuries later they would be exhumed and enshrined by Ashoka into many new stupas around the Mauryan realm. Many supernatural legends surround the history of alleged relics as they accompanied the spread of Buddhism and gave legitimacy to rulers. According to various Buddhist sources, the First Buddhist Council was held shortly after the Buddha's death to collect, recite and memorize the teachings. Mahākassapa was chosen by the sangha to be the chairman of the council. However, the historicity of the traditional accounts of the first council is disputed by modern scholars.
One method to obtain information on the oldest core of Buddhism is to compare the oldest versions of the Pali Canon and other texts, such as the surviving portions of Sarvastivada, Mulasarvastivada, Mahisasaka, Dharmaguptaka, and the Chinese Agamas. The reliability of these sources, and the possibility of drawing out a core of oldest teachings, is a matter of dispute. According to Tilmann Vetter, inconsistencies remain, and other methods must be applied to resolve those inconsistencies.According to Lambert Schmithausen, there are three positions held by modern scholars of Buddhism: "Stress on the fundamental homogeneity and substantial authenticity of at least a considerable part of the Nikayic materials." "Scepticism with regard to the possibility of retrieving the doctrine of earliest Buddhism." "Cautious optimism in this respect."Regarding their attribution to the historical Buddha Gautama "Sakyamuni", scholars such as Richard Gombrich, Akira Hirakawa, Alexander Wynne and A.K. Warder hold that these Early Buddhist Texts contain material that could possibly be traced to this figure.
According to scholars of Indology such as Richard Gombrich, the Buddha's teachings on Karma and Rebirth are a development of pre-Buddhist themes that can be found in Jain and Brahmanical sources, like the Brihadaranyaka Upanishad. Likewise, samsara, the idea that we are trapped in cycle of rebirth and that we should seek liberation from this through non-harming (ahimsa) and spiritual practices, pre-dates the Buddha and was likely taught in early Jainism.In various texts, the Buddha is depicted as having studied under two named teachers, Āḷāra Kālāma and Uddaka Rāmaputta. According to Alexander Wynne, these were yogis who taught doctrines and practices similar to those in the Upanishads.The Buddha's tribe of origin, the Shakyas, also seem to have had non-Vedic religious practices which influenced Buddhism, such as the veneration of trees and sacred groves, and the worship of tree spirits (yakkhas) and serpent beings (nagas). They also seem to have built burial mounds called stupas.Tree veneration remains important in Buddhism today, particularly in the practice of venerating Bodhi trees. Likewise, yakkas and nagas have remained important figures in Buddhist religious practices and mythology.In the Early Buddhist Texts, the Buddha also references Brahmanical devices. For example, in Samyutta Nikaya 111, Majjhima Nikaya 92 and Vinaya i 246 of the Pali Canon, the Buddha praises the Agnihotra as the foremost sacrifice and the Gayatri mantra as the foremost meter.The Buddhist teaching of the three marks of existence may also reflect Upanishadic or other influences according to K.R. Norman.According to Johannes Bronkhorst, the "meditation without breath and reduced intake of food" which the Buddha practiced before his awakening are forms of asceticism which are similar to Jain practices.The Buddhist practice called Brahma-vihara may have also originated from a Brahmanic term; but its usage may have been common in the sramana traditions.
The Early Buddhist Texts present many teachings and practices which may have been taught by the historical Buddha. These include basic doctrines such as Dependent Origination, the Middle Way, the Five Aggregates, the Three unwholesome roots, the Four Noble Truths and the Eightfold Path. According to N. Ross Reat, all of these doctrines are shared by the Theravada Pali texts and the Mahasamghika school's Śālistamba Sūtra.A recent study by Bhikkhu Analayo concludes that the Theravada Majjhima Nikaya and Sarvastivada Madhyama Agama contain mostly the same major doctrines. Likewise, Richard Salomon has written that the doctrines found in the Gandharan Manuscripts are "consistent with non-Mahayana Buddhism, which survives today in the Theravada school of Sri Lanka and Southeast Asia, but which in ancient times was represented by eighteen separate schools."These basic teachings such as the Four Noble Truths tend to be widely accepted as basic doctrines in all major schools of Buddhism, as seen in ecumenical documents such as the Basic points unifying Theravāda and Mahāyāna.
In the early Buddhist texts, the Buddha critiques the Brahmanical religion and social system on certain key points. The Brahmin caste held that the Vedas were eternal revealed (sruti) texts. The Buddha, on the other hand, did not accept that these texts had any divine authority or value.The Buddha also did not see the Brahmanical rites and practices as useful for spiritual advancement. For example, in the Udāna, the Buddha points out that ritual bathing does not lead to purity, only "truth and morality" lead to purity. He especially critiqued animal sacrifice as taught in Vedas. The Buddha contrasted his teachings, which were taught openly to all people, with that of the Brahmins', who kept their mantras secret.He also critiqued numerous other Brahmanical practices, such astrology, divination, fortune-telling, and so on (as seen in the Tevijja sutta and the Kutadanta sutta).The Buddha also attacked the Brahmins' claims of superior birth and the idea that different castes and bloodlines were inherently pure or impure, noble or ignoble.In the Vasettha sutta the Buddha argues that the main difference among humans is not birth but their actions and occupations. According to the Buddha, one is a "Brahmin" (i.e. divine, like Brahma) only to the extent that one has cultivated virtue. Because of this the early texts report that he proclaimed: "Not by birth one is a Brahman, not by birth one is a non-Brahman; - by moral action one is a Brahman"The Aggañña Sutta explains all classes or varnas can be good or bad and gives a sociological explanation for how they arose, against the Brahmanical idea that they are divinely ordained. According to Kancha Ilaiah, the Buddha posed the first contract theory of society. The Buddha's teaching then is a single universal moral law, one Dharma valid for everybody, which is opposed to the Brahmanic ethic founded on “one’s own duty” (svadharma) which depends on caste. Because of this, all castes including untouchables were welcome in the Buddhist order and when someone joined, they renounced all caste affiliation.
The early Buddhist texts present the Buddha's worldview as focused on understanding the nature of dukkha, which is seen as the fundamental problem of life. Dukkha refers to all kinds of suffering, unease, frustration, and dissatisfaction that sentient beings experience. At the core of the Buddha's analysis of dukkha is the fact that everything we experience is impermanent, unstable and thus unreliable.A common presentation of the core structure of Buddha's teaching found in the early texts is that of the Four Noble Truths. This teaching is most famously presented in the Dhammacakkappavattana Sutta ("The discourse on the turning of the Dharma wheel") and its many parallels. The basic outline of the four truths is as follows: There is dukkha. There are causes and conditions for the arising of dukkha. Various conditions are outlined in the early texts, such as craving (taṇhā), but the three most basic ones are greed, aversion and delusion. If the conditions for dukkha cease, dukkha also ceases. This is "Nirvana" (literally 'blowing out' or 'extinguishing'). There is path to follow that leads to Nirvana.According to Bhikkhu Analayo, the four truths schema appears to be based "on an analogy with Indian medical diagnosis" (with the form: "disease, pathogen, health, cure") and this comparison is "explicitly made in several early Buddhist texts".In another Pali sutta, the Buddha outlines how "eight worldly conditions", "keep the world turning around...Gain and loss, fame and disrepute, praise and blame, pleasure and pain." He then explains how the difference between a noble (arya) person and an uninstructed worldling is that a noble person reflects on and understands the impermanence of these conditions.The Buddha's analysis of existence includes an understanding that karma and rebirth are part of life. According to the Buddha, the constant cycle of dying and being reborn (i.e. saṃsāra) according to one's karma is just dukkha and the ultimate spiritual goal should be liberation from this cycle. According to the Pali suttas, the Buddha stated that "this saṃsāra is without discoverable beginning. A first point is not discerned of beings roaming and wandering on hindered by ignorance and fettered by craving."The Buddha's teaching of karma differed to that of the Jains and Brahmins, in that on his view, karma is primarily mental intention (as opposed to mainly physical action or ritual acts). The Buddha is reported to have said "By karma I mean intention." Richard Gombrich summarizes the Buddha's view of karma as follows: "all thoughts, words, and deeds derive their moral value, positive or negative, from the intention behind them."For the Buddha, our karmic acts also affected the rebirth process in a positive or negative way. This was seen as an impersonal natural law similar to how certain seeds produce certain plants and fruits (in fact, the result of a karmic act was called its "fruit" by the Buddha). However, it is important to note that the Buddha did not hold that everything that happens is the result of karma alone. In fact when the Buddha was asked to state the causes of pain and pleasure he listed various physical and environmental causes alongside karma.
In the early texts, the process of the arising of dukkha is most thoroughly explained by the Buddha through the teaching of Dependent Origination. At its most basic level, Dependent Origination is an empirical teaching on the nature of phenomena which says that nothing is experienced independently of its conditions.The most basic formulation of Dependent Origination is given in the early texts as: 'It being thus, this comes about' (Pali: evam sati idam hoti). This can be taken to mean that certain phenomena only arise when there are other phenomena present (example: when there is craving, suffering arises), and so, one can say that their arising is "dependent" on other phenomena. In other words, nothing in experience exists without a cause.In numerous early texts, this basic principle is expanded with a list of phenomena that are said to be conditionally dependent. These phenomena are supposed to provide an analysis of the cycle of dukkha as experienced by sentient beings. The philosopher Mark Siderits has outlined the basic idea of the Buddha's teaching of Dependent Origination of dukkha as follows: given the existence of a fully functioning assemblage of psycho-physical elements (the parts that make up a sentient being), ignorance concerning the three characteristics of sentient existence—suffering, impermanence and non-self—will lead, in the course of normal interactions with the environment, to appropriation (the identification of certain elements as ‘I’ and ‘mine’). This leads in turn to the formation of attachments, in the form of desire and aversion, and the strengthening of ignorance concerning the true nature of sentient existence. These ensure future rebirth, and thus future instances of old age, disease and death, in a potentially unending cycle. The Buddha saw his analysis of Dependent Origination as a "Middle Way" between "eternalism" (sassatavada, the idea that some essence exists eternally) and "annihilationism" (ucchedavada, the idea that we go completely out of existence at death). This middle way is basically the view that, conventionally speaking, persons are just a causal series of impermanent psycho-physical elements.
Closely connected to the idea that experience is dependently originated is the Buddha's teaching that there is no independent or permanent self (Sanskrit: atman, Pali: atta).Due to this view which (termed anatta), the Buddha's teaching was opposed to all soul theories of his time, including the Jain theory of a "jiva" ("life monad") and the Brahmanical theories of atman and purusha. All of these theories held that there was an eternal unchanging essence to a person which transmigrated from life to life.While Brahminical teachers affirmed atman theories in an attempt to answer the question of what really exists ultimately, the Buddha saw this question as not being useful, as illustrated in the parable of the poisoned arrow.For the Buddha's contemporaries, the atman was also seen to be the unchanging constant which was separate from all changing experiences and the inner controller in a person. The Buddha instead held that all things in the world of our experience are transient and that there is no unchanging part to a person. According to Richard Gombrich, the Buddha's position is simply that "everything is process". However, this anti-essentialist view still includes an understanding of continuity through rebirth, it is just the rebirth of a process (karma), not an essence like the atman.Perhaps the most important way the Buddha analyzed individual experience in the early texts was by way of the five 'aggregates' or 'groups' (khandha) of physical and mental processes. The Buddha's arguments against an unchanging self rely on these five aggregate schema, as can be seen in the Pali Anattalakkhaṇa Sutta (and its parallels in Gandhari and Chinese).According to the early texts, the Buddha argued that because we have no ultimate control over any of the psycho-physical processes that make up a person, there cannot be an "inner controller" with command over them. Also, since they are all impermanent, one cannot regard any of the psycho-physical processes as an unchanging self. Even mental processes such as consciousness and will (cetana) are seen as being dependently originated and impermanent and thus do not qualify as a self (atman).As noted by Gombrich, in the early texts the Buddha teaches that all five aggregates, including consciousness (viññana, which was held by Brahmins to be eternal), arise dependent on causes. That is, existence is based on processes that are subject to dependent origination. He compared samsaric existence to a fire, which is dynamic and requires fuel (the khandas, literally: "heaps") in order to keep burning.As Rupert Gethin explains, for the Buddha: I am a complex flow of physical and mental phenomena, but peel away these phenomena and look behind them and one just does not find a constant self that one can call one's own. My sense of self is both logically and emotionally just a label that I impose on these physical and mental phenomena in consequence of their connectedness. The Buddha saw the belief in a self as arising from our grasping at and identifying with the various changing phenomena, as well as from ignorance about how things really are. Furthermore, the Buddha held that we experience suffering because we hold on to erroneous self views.
As noted by Bhikkhu Bodhi, the Buddha as depicted in the Pali suttas does not exclusively teach a world transcending goal, but also teaches laypersons how to achieve worldly happiness (sukha).According to Bodhi, the "most comprehensive" of the suttas that focus on how to live as a layperson is the Sigālovāda Sutta (DN 31). This sutta outlines how a layperson behaves towards six basic social relationships: "parents and children, teacher and pupils, husband and wife, friend and friend, employer and workers, lay follower and religious guides." This Pali text also has parallels in Chinese and in Sanskrit fragments.In another sutta (Dīghajāṇu Sutta, AN 8.54) the Buddha teaches two types of happiness. First, there is the happiness visible in this very life. The Buddha states that four things lead to this happiness: "The accomplishment of persistent effort, the accomplishment of protection, good friendship, and balanced living." Similarly, in several other suttas, the Buddha teaches on how to improve family relationships, particularly on the importance of filial love and gratitude as well as marital well-being.Regarding the happiness of the next life, the Buddha (in the Dīghajāṇu Sutta) states that the virtues which lead to a good rebirth are: faith (in the Buddha and the teachings), moral discipline, especially keeping the five precepts, generosity, and wisdom (knowledge of the arising and passing of things).According to the Buddha of the suttas then, achieving a good rebirth is based on cultivating wholesome or skillful (kusala) karma, which leads to a good result, and avoiding unwholesome (akusala) karma. A common list of good karmas taught by the Buddha is the list of ten courses of action (kammapatha) as outlined in MN 41 Saleyyaka Sutta (and its Chinese parallel in SĀ 1042).Good karma is also termed merit (puñña), and the Buddha outlines three bases of meritorious actions: giving, moral discipline and meditation (as seen in AN 8:36).
Liberation (vimutti) from the ignorance and grasping which create suffering is not easily achieved because all beings have deeply entrenched habits (termed āsavas, often translated as "influxes" or "defilements") that keep them trapped in samsara. Because of this, the Buddha taught a path (marga) of training to undo such habits. This path taught by the Buddha is depicted in the early texts (most famously in the Pali Dhammacakkappavattana Sutta and its numerous parallel texts) as a "Middle Way" between sensual indulgence on one hand and mortification of the body on the other.One of the most common formulations of the path to liberation in the earliest Buddhist texts is the Noble Eightfold Path. There is also an alternative formulation with ten elements which is also very commonly taught in the early texts.According to Gethin, another common summary of the path to awakening wisely used in the early texts is "abandoning the hindrances, practice of the four establishments of mindfulness and development of the awakening factors."The early texts also contain many different presentations of the Buddha's path to liberation aside from the Eightfold Path. According to Rupert Gethin, in the Nikayas and Agamas, the Buddha's path is mainly presented in a cumulative and gradual "step by step" process, such as that outlined in the Samaññaphala Sutta. Early texts that outline the graduated path include the Cula-Hatthipadopama-sutta (MN 27, with Chinese parallel at MĀ 146) and the Tevijja Sutta (DN 13, with Chinese parallel at DĀ 26 and a fragmentary Sanskrit parallel entitled the Vāsiṣṭha-sūtra). Other early texts like the Upanisa sutta (SN 12.23), present the path as reversions of the process of Dependent Origination.Some common practices which are shared by most of these early presentations of the path include sila (ethical training), restraint of the senses (indriyasamvara), mindfulness and clear awareness (sati-sampajañña) and the practice of jhana (meditative absorption). Mental development (citta bhāvanā) was central to the Buddha's spiritual path as depicted in the earliest texts and this included meditative practices. Regarding the training of right view and sense restraint, the Buddha taught that it was important to reflect on the dangers or drawbacks (adinava) of sensual pleasures. Various suttas discuss the different drawbacks of sensuality. In the Potaliya Sutta (MN 54) sensual pleasures are said by the Buddha to be a cause of conflict for all humans beings. They are said to be unable to satisfy one's craving, like a clean meatless bone given to a dog. Sensuality is also compared to a torch held against the wind, since it burns the person holding on to it. According to the Buddha, there is "a delight apart from sensual pleasures, apart from unwholesome states, which surpasses even divine bliss." The Buddha thus taught that one should take delight in the higher spiritual pleasures instead of sensual pleasure. This is explained with the simile the leper, who cauterizes his skin with fire to get relief from the pain of leprosy, but after he is cured, avoids the same flames he used to enjoy before (see MN 75, Magandiya Sutta).Numerous scholars such as Vetter have written on the centrality of the practice of dhyāna to the teaching of the Buddha. It is the training of the mind, commonly translated as meditation, to withdraw the mind from the automatic responses to sense-impressions, and leading to a "state of perfect equanimity and awareness (upekkhā-sati-parisuddhi)." Dhyana is preceded and supported by various aspects of the path such as seclusion and sense restraint.Another important mental training in the early texts is the practice of mindfulness (sati), which was mainly taught using the schemas of the "Four Ways of Mindfulness" (Satipatthana, as taught in the Pali Satipatthana Sutta and its various parallel texts) and the sixteen elements of "Mindfulness of Breath" (Anapanasati, as taught in the Anapanasati Sutta and its various parallels).Because getting others to practice the path was the central goal of the Buddha's message, the early texts depict the Buddha as refusing to answer certain metaphysical questions which his contemporaries were preoccupied with, (such as "is the world eternal?"). This is because he did not see these questions as being useful on the path and as not being "connected to the goal".
The early Buddhist texts depict the Buddha as promoting the life of a homeless and celibate "sramana", or mendicant, as the ideal way of life for the practice of the path. He taught that mendicants or "beggars" (bhikkhus) were supposed to give up all possessions and to own just a begging bowl and three robes. As part of the Buddha's monastic discipline, they were also supposed to rely on the wider lay community for the basic necessities (mainly food, clothing, and lodging).The Buddha's teachings on monastic discipline were preserved in the various Vinaya collections of the different early schools.Buddhist monastics, which included both monks and nuns, were supposed to beg for their food, were not allowed to store up food or eat after noon and they were not allowed to use gold, silver or any valuables.
The early texts depict the Buddha as giving a deflationary account of the importance of politics to human life. Politics is inevitable and is probably even necessary and helpful, but it is also a tremendous waste of time and effort, as well as being a prime temptation to allow ego to run rampant. Buddhist political theory denies that people have a moral duty to engage in politics except to a very minimal degree (pay the taxes, obey the laws, maybe vote in the elections), and it actively portrays engagement in politics and the pursuit of enlightenment as being conflicting paths in life.In the Aggañña Sutta, the Buddha teaches a history of how monarchy arose which according to Matthew J. Moore is "closely analogous to a social contract." The Aggañña Sutta also provides a social explanation of how different classes arose, in contrast to the Vedic views on social caste.Other early texts like the Cakkavatti-Sīhanāda Sutta and the Mahāsudassana Sutta focus on the figure of the righteous wheel turning leader (Cakkavatti). This ideal leader is one who promotes Dharma through his governance. He can only achieve his status through moral purity and must promote morality and Dharma to maintain his position. According to the Cakkavatti-Sīhanāda Sutta, the key duties of a Cakkavatti are: "establish guard, ward, and protection according to Dhamma for your own household, your troops, your nobles, and vassals, for Brahmins and householders, town and country folk, ascetics and Brahmins, for beasts and birds. let no crime prevail in your kingdom, and to those who are in need, give property.” The sutta explains the injunction to give to the needy by telling how a line of wheel-turning monarchs falls because they fail to give to the needy, and thus the kingdom falls into infighting as poverty increases, which then leads to stealing and violence.In the Mahāparinibbāna Sutta, the Buddha outlines several principles that he promoted among the Vajjian tribal federation, which had a quasi-republican form of government. He taught them to “hold regular and frequent assemblies”, live in harmony and maintain their traditions. The Buddha then goes on to promote a similar kind of republican style of government among the Buddhist Sangha, where all monks had equal rights to attend open meetings and there would be no single leader, since The Buddha also chose not to appoint one. Some scholars have argued that this fact signals that the Buddha preferred a republican form of government, while others disagree with this position.
Numerous scholars of early Buddhism argue that most of the teachings found in the Early Buddhist texts date back to the Buddha himself. One of these is Richard Gombrich, who argues that since the content of the earliest texts “presents such originality, intelligence, grandeur and—most relevantly—coherence...it is hard to see it as a composite work." Thus he concludes they are "the work of one genius."Peter Harvey also agrees that “much” of the Pali Canon “must derive from his [the Buddha’s] teachings.” Likewise, A. K. Warder has written that “there is no evidence to suggest that it [the shared teaching of the early schools] was formulated by anyone other than the Buddha and his immediate followers.”Furthermore, Alexander Wynne argues that "the internal evidence of the early Buddhist literature proves its historical authenticity."However, other scholars of Buddhist studies have disagreed with the mostly positive view that the early Buddhist texts reflect the teachings of the historical Buddha. For example, Edward Conze argued that the attempts of European scholars to reconstruct the original teachings of the Buddha were “all mere guesswork.”Other scholars argue that some teachings contained in the early texts are the authentic teachings of the Buddha, but not others. For example, according to Tilmann Vetter, the earliest core of the Buddhist teachings is the meditative practice of dhyāna. Vetter argues that "liberating insight" became an essential feature of the Buddhist tradition at a later date. He posits that the Fourth Noble Truths, the Eightfold path and Dependent Origination, which are commonly seen as essential to Buddhism, are later formulations which form part of the explanatory framework of this "liberating insight".Lambert Schmithausen similarly argues that the mention of the four noble truths as constituting "liberating insight", which is attained after mastering the four dhyānas, is a later addition. Also, according to Johannes Bronkhorst, the four truths may not have been formulated in earliest Buddhism, and did not serve in earliest Buddhism as a description of "liberating insight".
Various Buddhist texts attribute to the Buddha a series of extraordinary physical characteristics, known as "the 32 Signs of the Great Man" (Skt. mahāpuruṣa lakṣaṇa). According to Analayo, when they first appear in the Buddhist texts, these physical marks were initially held to be imperceptible to the ordinary person, and required special training to detect. Later though, they are depicted as being visible by regular people and as inspiring faith in the Buddha.These characteristics are described in the Digha Nikaya's Lakkhaṇa Sutta (D, I:142).
Some Hindus regard Gautama as the 9th avatar of Vishnu. However, Buddha's teachings deny the authority of the Vedas and the concepts of Brahman-Atman. Consequently Buddhism is generally classified as a nāstika school (heterodox, literally "It is not so") in contrast to the six orthodox schools of Hinduism. In Sikhism, Buddha is mentioned as the 23rd avatar of Vishnu in the Chaubis Avtar, a composition in Dasam Granth traditionally and historically attributed to Guru Gobind Singh.Classical Sunni scholar Tabari reports that Buddhist idols were brought from Afghanistan to Baghdad in the ninth century. Such idols had been sold in Buddhist temples next to a mosque in Bukhara, but he does not further discuss the role of Buddha. According to the works on Buddhism by Al-Biruni (973–after 1050), views regarding the exact identity of Buddha was diverse. Accordingly, some regarded him as the divine incarnate, others as an apostle of the angels or as an Ifrit and others as an apostle of God sent to human race. By the 12th century, al-Shahrastani even compared Buddha to Khidr, described as an ideal human. Ibn Nadim, who was also familiar with Manichean teachings, even identifies Buddha as a prophet, who taught a religion to "banish Satan", although not mention it explicitly. However, most Classical scholars described Buddha in theistic terms, that is apart from Islamic teachings.Nevertheless the Buddha is regarded as a prophet by the minority Ahmadiyya sect, generally considered deviant and rejected as apostate by mainstream Islam. Some early Chinese Taoist-Buddhists thought the Buddha to be a reincarnation of Laozi.Disciples of the Cao Đài religion worship the Buddha as a major religious teacher. His image can be found in both their Holy See and on the home altar. He is revealed during communication with Divine Beings as son of their Supreme Being (God the Father) together with other major religious teachers and founders like Jesus, Laozi, and Confucius.The Christian Saint Josaphat is based on the Buddha. The name comes from the Sanskrit Bodhisattva via Arabic Būdhasaf and Georgian Iodasaph. The only story in which St. Josaphat appears, Barlaam and Josaphat, is based on the life of the Buddha. Josaphat was included in earlier editions of the Roman Martyrology (feast day 27 November)—though not in the Roman Missal—and in the Eastern Orthodox Church liturgical calendar (26 August). In the ancient Gnostic sect of Manichaeism, the Buddha is listed among the prophets who preached the word of God before Mani.In the Baháʼí Faith, Buddha is regarded as one of the Manifestations of God.
Some of the earliest artistic depictions of the Buddha found at Bharhut and Sanchi are aniconic and symbolic. During this early aniconic period, the Buddha is depicted by other objects or symbols, such as an empty throne, a riderless horse, footprints, a Dharma wheel or a Bodhi tree. The art at Sanchi also depicts the Jataka narratives of the Buddha in his past lives.Other styles of Indian Buddhist art depict the Buddha in human form, either standing, sitting crossed legged (often in the Lotus Pose) or laying down on one side. Iconic representations of the Buddha became particularly popular and widespread after the first century CE. Some of these depictions of the Buddha, particularly those of Gandharan Buddhism and Central Asian Buddhism, were influenced by Hellenistic art, a style known as Greco-Buddhist art.These various Indian and Central Asian styles would then go on to influence the art of East Asian Buddhist Buddha images, as well as those of Southeast Asian Theravada Buddhism.
Films Little Buddha, a 1994 film by Bernardo Bertolucci Prem Sanyas, a 1925 silent film, directed by Franz Osten and Himansu RaiTelevisionBuddha, a 2013 mythological drama on Zee TV The Buddha 2010 PBS documentary by award-winning filmmaker David Grubin and narrated by Richard GereLiteratureThe Light of Asia, an 1879 epic poem by Edwin Arnold Buddha, a manga series that ran from 1972 to 1983 by Osamu Tezuka Siddhartha novel by Hermann Hesse, written in German in 1922 Lord of Light, a novel by Roger Zelazny depicts a man in a far future Earth Colony who takes on the name and teachings of the Buddha Creation, a 1981 novel by Gore Vidal, includes the Buddha as one of the religious figures that the main character encountersMusicKaruna Nadee, a 2010 oratorio by Dinesh Subasinghe The Light of Asia, an 1886 oratorio by Dudley Buck
List of founders of religious traditions Buddha (title) Gautama Buddha in Hinduism
Works by Buddha at Project Gutenberg Works by or about Buddha at Internet Archive Works by or about Siddhārtha Gautama at Internet Archive Works by or about Shakyamuni at Internet Archive Works by Gautama Buddha at LibriVox (public domain audiobooks) Buddha on In Our Time at the BBC A sketch of the Buddha's Life What Was The Buddha Like? by Ven S. Dhammika Parables and Stories of Buddha Who was the Buddha? Buddhism for Beginners
Bamyan lies on the Silk Road, which runs through the Hindu Kush mountain region, in the Bamyan Valley. The Silk Road has been historically a caravan route linking the markets of China with those of the Western world. It was the site of several Buddhist monasteries, and a thriving center for religion, philosophy, and art. Monks at the monasteries lived as hermits in small caves carved into the side of the Bamiyan cliffs. Most of these monks embellished their caves with religious statuary and elaborate, brightly colored frescoes. It was a Buddhist religious site from the 2nd century up to the time of the Islamic invasion in the later half of the 7th century. Until it was completely conquered by the Muslim Saffarids in the 9th century, Bamiyan shared the culture of Gandhara. The two most prominent statues were the giant standing sculptures of Buddhas Vairocana and Sakyamuni, identified by the different mudras performed. The Buddha popularly called "Solsol" measured 53 meters tall, and "Shahmama" 35 meters—the niches in which the figures stood are 58 and 38 meters respectively from bottom to top. Before being blown up in 2001 they were the largest examples of standing Buddha carvings in the world (the 8th century Leshan Giant Buddha is taller, but that statue is sitting). Since then the Statue of unity has been built in India, and at 182 m (597 ft) it is the tallest statue in the world, breaking the record earlier held by The Spring Temple Buddha 128 m (420 ft) in Fodushan Scenic Area, Lushan County, Henan, China. Plans for the construction of the Spring Temple Buddha were announced soon after the blowing up of the Bamiyan Buddhas and China condemned the systematic destruction of the Buddhist heritage of Afghanistan. It is believed that the monumental Buddha sculptures were carved into the cliffs of Bamiyan between the 3rd to 6th centuries AD, while the cave complex in the east, including the 38 meter Buddha, was built in the 3rd or 4th centuries AD. The 55 meter Buddha is believed to date from the 5th and 6th centuries AD. Historic documentation refers to celebrations held every year attracting numerous pilgrims and that offers were made to the monumental statues. They were perhaps the most famous cultural landmarks of the region, and the site was listed by UNESCO as a World Heritage Site along with the surrounding cultural landscape and archaeological remains of the Bamiyan Valley. Their color faded through time.Chinese Buddhist pilgrim Xuanzang visited the site on 30 April 630 AD, and described Bamiyan in the Da Tang Xiyu Ji as a flourishing Buddhist center "with more than ten monasteries and more than a thousand monks". He also noted that both Buddha figures were "decorated with gold and fine jewels" (Wriggins, 1995). Intriguingly, Xuanzang mentions a third, even larger, reclining statue of the Buddha. A monumental seated Buddha, similar in style to those at Bamiyan, still exists in the Bingling Temple caves in China's Gansu province. A Frenchman named Dureau had photographed it in 1847.The destruction of the Bamiyan Buddhas became a symbol of oppression and a rallying point for the freedom of religious expression. Despite the fact that most Afghans are now Muslim, they too had embraced their past and many were appalled by the destruction.
In 1221, with the advent of Genghis Khan, "a terrible disaster befell Bamiyan." Nevertheless, the statues were spared. Babur wrote in September 1528, that he ordered both be destroyed. Later, the Mughal emperor, Aurangzeb, tried to use heavy artillery to destroy the statues. The legs of the Buddhas were broken because of Aurangzeb's action. Another attempt to destroy the Bamiyan statues was made by the 18th century Persian king Nader Afshar, directing cannon fire at them. Afghan king Abdur Rahman Khan in the 19th century destroyed its face during a military campaign against a Hazara rebellion in the area.
During the ongoing Afghan Civil War, the area around the Buddhas was under the control of the Hizb-i-Wahdat militia, a part of the Northern Alliance which was fighting at the time against the Taliban, an Islamic fundamentalist militia. Following the Taliban's capture of Mazar-i-Sharif in August 1998, Bamyan valley was entirely surrounded by the Taliban. The town was captured by the Taliban on 13 September. At the time, the Afghan population was described as "exhausted, starving".Abdul Wahed, a Taliban commander operating around the area, announced his intention to blow up the Buddhas even before taking the valley. Wahed drilled holes in the Buddhas' heads for explosives. He was prevented from taking further action by the local governor and a direct order of the Supreme Leader, Mohammed Omar, although tires were later burned on the head of the great Buddha. In July 1999, Mullah Mohammed Omar issued a decree in favor of the preservation of the Bamiyan Buddha statues. Because Afghanistan's Buddhist population no longer exists, and the statues were no longer worshipped, he added: "The government considers the Bamiyan statues as an example of a potential major source of income for Afghanistan from international visitors. The Taliban states that Bamiyan shall not be destroyed but protected." In early 2000, local Taliban authorities asked for UN assistance to rebuild drainage ditches around the tops of the alcoves where the Buddhas were set.The Taliban's intention to destroy the statues, declared on 27 February 2001, caused a wave of international horror and protest. According to UNESCO Director-General Koïchiro Matsuura, a meeting of ambassadors from the 54 member states of the Organisation of the Islamic Conference (OIC) was conducted. All OIC states—including Pakistan, Saudi Arabia, and the United Arab Emirates, three countries that officially recognised the Taliban government—joined the protest to spare the monuments. Saudi Arabia and the UAE later condemned the destruction as "savage". Although India never recognised the Taliban regime in Afghanistan, New Delhi offered to arrange for the transfer of all the artifacts in question to India, "where they would be kept safely and preserved for all mankind". These overtures were rejected by the Taliban. Pakistani president Pervez Musharraf sent Moinuddin Haider to Kabul to try to prevent the destruction, by arguing that it was un-Islamic and unprecedented. According to Taliban minister, Abdul Salam Zaeef, UNESCO sent the Taliban government 36 letters objecting to the proposed destruction. He asserted that the Chinese, Japanese, and Sri Lankan delegates were the most strident advocates for preserving the Buddhas. The Japanese in particular proposed a variety of different solutions to the issue, these included moving the statues to Japan, covering the statues from view, and the payment of money. The second edition of the Turkistan Islamic Party's magazine Islamic Turkistan contained an article on Buddhism, and described the destruction of the Buddhas of Bamiyan despite attempts by the Japanese government of "infidels" to preserve the remains of the statues. The exiled Dalai Lama said he was "deeply concerned".In Rome, the former Afghan King, Mohammed Zahir Shah, denounced the declaration in a rare press statement, calling it "against the national and historic interests of the Afghan people." Zemaryalai Tarzi, who was Afghanistan's chief archeologist in the 1970s, called it an "unacceptable decision."Abdul Salam Zaeef held that the destruction of the Buddhas was finally ordered by Abdul Wali, the Minister for the Propagation of Virtue and the Prevention of Vice.
The statues were destroyed by dynamite over several weeks, starting on 2 March 2001. The destruction was carried out in stages. Initially, the statues were fired at for several days using anti-aircraft guns and artillery. This caused severe damage, but did not obliterate them. During the destruction, Taliban Information Minister Qudratullah Jamal lamented that, "This work of destruction is not as simple as people might think. You can't knock down the statues by shelling as both are carved into a cliff; they are firmly attached to the mountain". Later, the Taliban placed anti-tank mines at the bottom of the niches, so that when fragments of rock broke off from artillery fire, the statues would receive additional destruction from particles that set off the mines. In the end, the Taliban lowered men down the cliff face and placed explosives into holes in the Buddhas. After one of the explosions failed to obliterate the face of one of the Buddhas, a rocket was launched that left a hole in the remains of the stone head.In an interview, Taliban supreme leader Mullah Omar provided an ostensible explanation for his order to destroy the statues: I did not want to destroy the Bamiyan Buddha. In fact, some foreigners came to me and said they would like to conduct the repair work of the Bamiyan Buddha that had been slightly damaged due to rains. This shocked me. I thought, these callous people have no regard for thousands of living human beings—the Afghans who are dying of hunger, but they are so concerned about non-living objects like the Buddha. This was extremely deplorable. That is why I ordered its destruction. Had they come for humanitarian work, I would have never ordered the Buddha's destruction. On 6 March 2001, The Times quoted Mullah Mohammed Omar as stating, "Muslims should be proud of smashing idols. It has given praise to Allah that we have destroyed them." During a 13 March interview for Japan's Mainichi Shimbun, Afghan Foreign Minister Wakil Ahmad Mutawakel stated that the destruction was anything but a retaliation against the international community for economic sanctions: "We are destroying the statues in accordance with Islamic law and it is purely a religious issue." A statement issued by the ministry of religious affairs of the Taliban regime justified the destruction as being in accordance with Islamic law.On 18 March 2001, The New York Times reported that a Taliban envoy said the Islamic government made its decision in a rage after a foreign delegation offered money to preserve the ancient works. The report also added, however, that other reports "have said the religious leaders were debating the move for months, and ultimately decided that the statues were idolatrous and should be obliterated".Then Taliban ambassador-at-large Sayed Rahmatullah Hashemi said that the destruction of the statues was carried out by the Head Council of Scholars after a Swedish monuments expert proposed to restore the statues' heads. Hashimi is reported as saying: "When the Afghan head council asked them to provide the money to feed the children instead of fixing the statues, they refused and said, 'No, the money is just for the statues, not for the children'. Herein, they made the decision to destroy the statues"; however, he did not comment on the claim that a foreign museum offered to "buy the Buddhist statues, the money from which could have been used to feed children". Rahmatullah Hashemi added "If we had wanted to destroy those statues, we could have done it three years ago," referring to the start of U.S. sanctions. "In our religion, if anything is harmless, we just leave it. If money is going to statues while children are dying of malnutrition next door, then that makes it harmful, and we destroy it."The destruction of the Bamiyan Buddhas despite protests from the international community has been described by Michael Falser, a heritage expert at the Center for Transcultural Studies in Germany, as an attack by the Taliban against the globalising concept of "cultural heritage". The director general of the UN Educational, Scientific and Cultural Organization (UNESCO) Koichiro Matsuura called the destruction a "...crime against culture. It is abominable to witness the cold and calculated destruction of cultural properties which were the heritage of the Afghan people, and, indeed, of the whole of humanity." Ahmad Shah Massoud, leader of the anti-Taliban resistance force, also condemned the destruction.A local civilian, speaking to Voice of America in 2002, said that he and some other locals were forced to help destroy the statues. He also claimed that Pakistani and Arab engineers "were involved" in the destruction. Mullah Omar, during the destruction, was quoted as saying, "What are you complaining about? We are only waging war on stones". An author for Time magazine reported that the Koran does not command the destruction of images of other faiths, and that Islamic teachings did not justify the Taliban's actions.
Though the figures of the two large Buddhas have been destroyed, their outlines and some features are still recognizable within the recesses. It is also still possible for visitors to explore the monks' caves and passages that connect them. As part of the international effort to rebuild Afghanistan after the Taliban war, the Japanese government and several other organizations—among them the Afghanistan Institute in Bubendorf, Switzerland, along with the ETH in Zurich—have committed to rebuilding, perhaps by anastylosis, the two larger Buddhas. The local residents of Bamiyan have also expressed their favor in restoring the structures.
In April 2002, Afghanistan's post-Taliban leader Hamid Karzai called the destruction a "national tragedy" and pledged the Buddhas to be rebuilt. He later called the reconstruction a "cultural imperative".In September 2005, Mawlawi Mohammed Islam Mohammadi, Taliban governor of Bamiyan province at the time of the destruction and widely seen as responsible for its occurrence, was elected to the Afghan Parliament. He blamed the decision to destroy the Buddhas on Al-Qaeda's influence on the Taliban. In January 2007, he was assassinated in Kabul. Swiss filmmaker Christian Frei made a 95-minute documentary titled The Giant Buddhas (released in March 2006) on the statues, the international reactions to their destruction, and an overview of the controversy. Testimony by local Afghans validates that Osama bin Laden ordered the destruction and that, initially, Mullah Omar and the Afghans in Bamiyan opposed it. A novel titled 'An Afghan Winter' provides a fictional backdrop to the destruction of the Buddhas and its impact on the global Buddhist community.Since 2002, international funding has supported recovery and stabilization efforts at the site. Fragments of the statues are documented and stored with special attention given to securing the structure of the statue still in place. It is hoped that, in the future, partial anastylosis can be conducted with the remaining fragments. In 2009, ICOMOS constructed scaffolding within the niche to further conservation and stabilization. Nonetheless, several serious conservation and safety issues exist and the Buddhas are still listed as World Heritage in Danger.In the summer of 2006, Afghan officials were deciding on the timetable for the re-construction of the statues. As they wait for the Afghan government and international community to decide when to rebuild them, a $1.3 million UNESCO-funded project is sorting out the chunks of clay and plaster—ranging from boulders weighing several tons to fragments the size of tennis balls—and sheltering them from the elements. The Buddhist remnants at Bamiyan were included on the 2008 World Monuments Watch List of the 100 Most Endangered Sites by the World Monuments Fund. In 2013, the foot section of the smaller Buddha was rebuilt with iron rods, bricks and concrete by the German branch of ICOMOS. Further constructions were halted by order of UNESCO, on the grounds that the work was conducted without the organization's knowledge or approval. The effort was contrary to UNESCO's policy of using original material for reconstructions, and it has been pointed out that it was done based on assumptions.
After the destruction of the Buddhas, 50 caves were revealed. In 12 of the caves, wall paintings were discovered. In December 2004, an international team of researchers stated the wall paintings at Bamiyan were painted between the 5th and the 9th centuries, rather than the 6th to 8th centuries, citing their analysis of radioactive isotopes contained in straw fibers found beneath the paintings. It is believed that the paintings were done by artists travelling on the Silk Road, the trade route between China and the West.Scientists from the Tokyo Research Institute for Cultural Properties in Japan, the Centre of Research and Restoration of the French Museums in France, the Getty Conservation Institute in the United States, and the European Synchrotron Radiation Facility (ESRF) in Grenoble, France, analysed samples from the paintings, typically less than 1 mm across. They discovered that the paint contained pigments such as vermilion (red mercury sulfide) and lead white (lead carbonate). These were mixed with a range of binders, including natural resins, gums (possibly animal skin glue or egg), and oils, probably derived from walnuts or poppies. Specifically, researchers identified drying oils from murals showing Buddhas in vermilion robes sitting cross-legged amid palm leaves and mythical creatures as being painted in the middle of the 7th century. It is believed that they are the oldest known surviving examples of oil painting, possibly predating oil painting in Europe by as much as six centuries. The discovery may lead to a reassessment of works in ancient ruins in Iran, China, Pakistan, Turkey, and India.Initial suspicion that the oils might be attributable to contamination from fingers, as the touching of the painting is encouraged in Buddhist tradition, was dispelled by spectroscopy and chromatography giving an unambiguous signal for the intentional use of drying oils rather than contaminants. Oils were discovered underneath layers of paint, unlike surface contaminants.Scientists also found the translation of the beginning section of the original Sanskrit Pratītyasamutpāda Sutra translated by Xuanzang that spelled out the basic belief of Buddhism and said all things are transient.
On 8 September 2008, archaeologists searching for a legendary 300-metre statue at the site announced the discovery of parts of an unknown 19-metre (62-foot) reclining Buddha, a pose representing Buddha's Parinirvana.
The mural paintings of Bamiyan, dated to the 7-8th centuries CE, display a variety of male devotees in double-lapel caftans. The works of art show a sophistication and cosmopolitanism comparable to other works of art of the Silk Road such as those of Kizil, are attributable to the sponsorship of the Western Turks (Yabghus of Tokharistan).
The UNESCO Expert Working Group on Afghan cultural projects convened to discuss what to do about the two statues between 3–4 March 2011 in Paris. Researcher Erwin Emmerling of Technical University Munich announced he believed it would be possible to restore the smaller statue using an organic silicon compound. The Paris conference issued a list of 39 recommendations for the safeguarding of the Bamiyan site. These included leaving the larger Western niche empty as a monument to the destruction of the Buddhas, a feasibility study into the rebuilding of the Eastern Buddha, and the construction of a central museum and several smaller site museums. Work has since begun on restoring the Buddhas using the process of anastylosis, where original elements are combined with modern material. It is estimated that roughly half the pieces of the Buddhas can be put back together according to Bert Praxenthaler, a German art historian and sculptor involved in the restoration. The restoration of the caves and Buddhas has also involved training and employing local people as stone carvers. The project, which also aims to encourage tourism to the area, is being organised by UNESCO and the International Council on Monuments and Sites (ICOMOS). The work has come under some criticism. It is felt by some, such as human rights activist Abdullah Hamadi, that the empty niches should be left as monuments to the fanaticism of the Taliban, while NPR reported that others believe the money could be better spent on housing and electricity for the region. Some people, including Habiba Sarabi, the provincial governor, believe that rebuilding the Buddhas would increase tourism which would aid the surrounding communities.
After fourteen years, on 7 June 2015, a Chinese adventurist couple Xinyu Zhang and Hong Liang filled the empty cavities where the Buddhas once stood with 3D laser light projection technology. The projector used for the installation, worth approximately $120,000, was donated by Xinyu and Hong, who were saddened by the destruction of the statues. With the desire of paying tribute, they requested permission from UNESCO and the Afghan government to do the project. About 150 local people came out to see the unveiling of the holographic statues on Sunday, 7 June 2015.
Despite the Buddhas's destruction, the ruins continue to be a popular culture landmark, bolstered by increasing domestic and international tourism to the Bamiyan Valley. The area around the ruins has since been used for the traditional game of buzkashi, as well as for music festivals and other events. The music video of pop singer Aryana Sayeed's hit 2015 song "Yaar-e Bamyani" was also shot by the ruins.The enormous statues did not fail to fire the imagination of Islamic writers in centuries past. The larger statue reappears as the malevolent giant Salsal in medieval Turkish tales.
In June 1971, the Japanese Empress Michiko visited the Buddhas during a royal state visit to Afghanistan with her husband. Upon her return to Japan, she composed a waka poem: There at Bamyan Under a moon faintly red The great stone Buddhas, Their sacred faces shattered, Are still awesomely standing.Following its destruction in 2001, Michiko composed a follow-up waka poem: All unconsciously Have I too not fired a shot? – With Spring well along On the plains of Bamyan The stone Buddhas are no more.
Armenian cemetery in Julfa Buddha Collapsed out of Shame Destruction of cultural heritage by ISIL Index of Buddhism-related articles Islamist destruction of Timbuktu heritage sites List of colossal sculpture in situ Secular Buddhism World Heritage Sites in Danger
Japan offered to hide Bamiyan statues, but Taliban asked Japan to convert to Islam instead News articles about the Buddhas of Bamyan Photos of the Buddhas of Bamyan Bamyan Afghanistan Laser Project World Heritage Tour: 360 degree image (after destruction) Bamyan Development Community Portal for cultural heritage management of Bamyan The World Monuments Fund's Watch List 2008 listing for Bamyan Historic Footage of Bamyan Statues, c. 1973 on YouTube The Valley of Bamiyan A tourist pamphlet from 1967 Researchers Say They Can Restore 1 of Destroyed Bamiyan Buddhas Secrets of the Bamiyan Buddhas, CNRS Bamiyan photo gallery, UNESCO Secrets of Bamiyan Buddhist murals. ESRF Photo Feature Covering Bamiyan Site
