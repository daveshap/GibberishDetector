In 1893, at the age of 33, Bates, an English professor at Wellesley College, had taken a train trip to Colorado Springs, Colorado, to teach a short summer school session at Colorado College. Several of the sights on her trip inspired her, and they found their way into her poem, including the World's Columbian Exposition in Chicago, the "White City" with its promise of the future contained within its gleaming white buildings; the wheat fields of America's heartland Kansas, through which her train was riding on July 16; and the majestic view of the Great Plains from high atop Pikes Peak.On the pinnacle of that mountain, the words of the poem started to come to her, and she wrote them down upon returning to her hotel room at the original Antlers Hotel. The poem was initially published two years later in The Congregationalist to commemorate the Fourth of July. It quickly caught the public's fancy. An amended version was published in 1904. The first known melody written for the song was sent in by Silas Pratt when the poem was published in The Congregationalist. By 1900, at least 75 different melodies had been written. A hymn tune composed in 1882 by Samuel A. Ward, the organist and choir director at Grace Church, Newark, was generally considered the best music as early as 1910 and is still the popular tune today. Just as Bates had been inspired to write her poem, Ward, too, was inspired. The tune came to him while he was on a ferryboat trip from Coney Island back to his home in New York City after a leisurely summer day and he immediately wrote it down. He composed the tune for the old hymn "O Mother Dear, Jerusalem", retitling the work "Materna". Ward's music combined with Bates's poem were first published together in 1910 and titled "America the Beautiful".Ward died in 1903, not knowing the national stature his music would attain. Bates was more fortunate, since the song's popularity was well established by the time of her death in 1929. It is included in songbooks in many religious congregations in the United States.At various times in the more than one hundred years that have elapsed since the song was written, particularly during the John F. Kennedy administration, there have been efforts to give "America the Beautiful" legal status either as a national hymn or as a national anthem equal to, or in place of, "The Star-Spangled Banner", but so far this has not succeeded. Proponents prefer "America the Beautiful" for various reasons, saying it is easier to sing, more melodic, and more adaptable to new orchestrations while still remaining as easily recognizable as "The Star-Spangled Banner". Some prefer "America the Beautiful" over "The Star-Spangled Banner" due to the latter's war-oriented imagery; others prefer "The Star-Spangled Banner" for the same reason. While that national dichotomy has stymied any effort at changing the tradition of the national anthem, "America the Beautiful" continues to be held in high esteem by a large number of Americans, and was even being considered before 1931 as a candidate to become the national anthem of the United States.
Bing Crosby included the song in a medley on his album 101 Gang Songs (1961). Frank Sinatra recorded the song with Nelson Riddle during the sessions for The Concert Sinatra in February 1963, for a projected 45 single release. The 45 was not commercially issued however, but the song was later added as a bonus track to the enhanced 2012 CD release of The Concert Sinatra. In 1976, while the United States celebrated its bicentennial, a soulful version popularized by Ray Charles peaked at number 98 on the US R&B chart.Three different renditions of the song have entered the Hot Country Songs charts. The first was by Charlie Rich, which went to number 22 in 1976. A second, by Mickey Newbury, peaked at number 82 in 1980. An all-star version of "America the Beautiful" performed by country singers Trace Adkins, Sherrié Austin, Billy Dean, Vince Gill, Carolyn Dawn Johnson, Toby Keith, Brenda Lee, Lonestar, Lyle Lovett, Lila McCann, Lorrie Morgan, Jamie O'Neal, The Oak Ridge Boys, Collin Raye, Kenny Rogers, Keith Urban and Phil Vassar reached number 58 in July 2001. The song re-entered the chart following the September 11 attacks.Popularity of the song increased greatly following the September 11 attacks; at some sporting events it was sung in addition to the traditional singing of the national anthem. During the first taping of the Late Show with David Letterman following the attacks, CBS newsman Dan Rather cried briefly as he quoted the fourth verse.For Super Bowl XLVIII, The Coca-Cola Company aired a multilingual version of the song, sung in several different languages. The commercial received some criticism on social media sites, such as Twitter and Facebook, and from some conservatives, such as Glenn Beck. Despite the controversies, Coca-Cola later reused the Super Bowl ad during Super Bowl LI, the opening ceremonies of the 2014 Winter Olympics and 2016 Summer Olympics and for patriotic holidays.
Lynn Sherr's 2001 book America the Beautiful discusses the origins of the song and the backgrounds of its authors in depth. The book points out that the poem has the same meter as that of "Auld Lang Syne"; the songs can be sung interchangeably. Additionally, Sherr discusses the evolution of the lyrics, for instance, changes to the original third verse written by Bates.Melinda M. Ponder, in her 2017 biography Katharine Lee Bates: From Sea to Shining Sea, draws heavily on Bates's diaries and letters to trace the history of the poem and its place in American culture.
MP3 and RealAudio recordings available at the United States Library of Congress Free sheet music of America the Beautiful from Cantorion.org Words, sheet music & MIDI file at the Cyber Hymnal America the Beautiful Park in Colorado Springs named for Katharine Lee Bates' words. Archival collection of America the Beautiful lantern slides from the 1930s. Another free sheet music
The Americas (also collectively called America) is a landmass comprising the totality of North and South America. The Americas make up most of the land in Earth's Western Hemisphere and comprise the New World.Along with their associated islands, the Americas cover 8% of Earth's total surface area and 28.4% of its land area. The topography is dominated by the American Cordillera, a long chain of mountains that runs the length of the west coast. The flatter eastern side of the Americas is dominated by large river basins, such as the Amazon, St. Lawrence River–Great Lakes basin, Mississippi, and La Plata. Since the Americas extend 14,000 km (8,700 mi) from north to south, the climate and ecology vary widely, from the arctic tundra of Northern Canada, Greenland, and Alaska, to the tropical rain forests in Central America and South America. Humans first settled the Americas from Asia between 42,000 and 17,000 years ago. A second migration of Na-Dene speakers followed later from Asia. The subsequent migration of the Inuit into the neoarctic around 3500 BCE completed what is generally regarded as the settlement by the indigenous peoples of the Americas. The first known European settlement in the Americas was by the Norse explorer Leif Erikson. However, the colonization never became permanent and was later abandoned. The Spanish voyages of Christopher Columbus from 1492 to 1504 resulted in permanent contact with European (and subsequently, other Old World) powers, which eventually led to the Columbian exchange and inaugurated a period of exploration, conquest, and colonization whose effects and consequences persist to the present. The Spanish presence involved the enslavement of large numbers of the indigenous population of America.Diseases introduced from Europe and West Africa devastated the indigenous peoples, and the European powers colonized the Americas. Mass emigration from Europe, including large numbers of indentured servants, and importation of African slaves largely replaced the indigenous peoples. Decolonization of the Americas began with the American Revolution in the 1770s and largely ended with the Spanish–American War in the late 1890s. Currently, almost all of the population of the Americas resides in independent countries; however, the legacy of the colonization and settlement by Europeans is that the Americas share many common cultural traits, most notably Christianity and the use of Indo-European languages: primarily Spanish, English, Portuguese, French, and, to a lesser extent, Dutch. The Americas are home to over a billion inhabitants, two-thirds of whom reside in the United States, Brazil, and Mexico. It is home to eight megacities (metropolitan areas with ten million inhabitants or more): New York City (23.9 million), Mexico City (21.2 million), São Paulo (21.2 million), Los Angeles (18.8 million), Buenos Aires (15.6 million), Rio de Janeiro (13.0 million), Bogotá (10.4 million), and Lima (10.1 million).
The name America was first recorded in 1507. A two-dimensional globe created by Martin Waldseemüller was the earliest recorded use of the term. The name was also used (together with the related term Amerigen) in the Cosmographiae Introductio, apparently written by Matthias Ringmann, in reference to South America. It was applied to both North and South America by Gerardus Mercator in 1538. America derives from Americus, the Latin version of Italian explorer Amerigo Vespucci's first name. The feminine form America accorded with the feminine names of Asia, Africa, and Europa.In modern English, North and South America are generally considered separate continents, and taken together are called the Americas, or more rarely America. When conceived as a unitary continent, the form is generally the continent of America in the singular. However, without a clarifying context, singular America in English commonly refers to the United States of America.Historically, in the English-speaking world, the term America usually referred to a single continent until the 1950s (as in Van Loon's Geography of 1937): According to historians Kären Wigen and Martin W. Lewis, While it might seem surprising to find North and South America still joined into a single continent in a book published in the United States in 1937, such a notion remained fairly common until World War II. It cannot be coincidental that this idea served American geopolitical designs at the time, which sought both Western Hemispheric domination and disengagement from the "Old World" continents of Europe, Asia, and Africa. By the 1950s, however, virtually all American geographers had come to insist that the visually distinct landmasses of North and South America deserved separate designations.
The first inhabitants migrated into the Americas from Asia. Habitation sites are known in Alaska and the Yukon from at least 20,000 years ago, with suggested ages of up to 40,000 years. Beyond that, the specifics of the Paleo-Indian migration to and throughout the Americas, including the dates and routes traveled, are subject to ongoing research and discussion. Widespread habitation of the Americas occurred during the late glacial maximum, from 16,000 to 13,000 years ago. The traditional theory has been that these early migrants moved into the Beringia land bridge between eastern Siberia and present-day Alaska around 40,000–17,000 years ago, when sea levels were significantly lowered during the Quaternary glaciation. These people are believed to have followed herds of now-extinct pleistocene megafauna along ice-free corridors that stretched between the Laurentide and Cordilleran ice sheets. Another route proposed is that, either on foot or using primitive boats, they migrated down the Pacific coast to South America. Evidence of the latter would since have been covered by a sea level rise of hundreds of meters following the last ice age. Both routes may have been taken, although the genetic evidences suggests a single founding population. The micro-satellite diversity and distributions specific to South American Indigenous people indicates that certain populations have been isolated since the initial colonization of the region.A second migration occurred after the initial peopling of the Americas; Na Dene speakers found predominantly in North American groups at varying genetic rates with the highest frequency found among the Athabaskans at 42% derive from this second wave. Linguists and biologists have reached a similar conclusion based on analysis of Amerindian language groups and ABO blood group system distributions. Then the people of the Arctic small tool tradition, a broad cultural entity that developed along the Alaska Peninsula, around Bristol Bay, and on the eastern shores of the Bering Strait c. 2,500 BCE moved into North America. The Arctic small tool tradition, a Paleo-Eskimo culture branched off into two cultural variants, including the Pre-Dorset, and the Independence traditions of Greenland. The descendants of the Pre-Dorset cultural group, the Dorset culture was displaced by the final migrants from the Bering sea coast line the ancestors of modern Inuit, the Thule people by 1000 Common Era (CE). Around the same time as the Inuit migrated into Greenland, Viking settlers began arriving in Greenland in 982 and Vinland shortly thereafter, establishing a settlement at L'Anse aux Meadows, near the northernmost tip of Newfoundland. The Viking settlers quickly abandoned Vinland, and disappeared from Greenland by 1500.
The pre-Columbian era incorporates all period subdivisions in the history and prehistory of the Americas before the appearance of significant European influences on the American continents, spanning the time of the original settlement in the Upper Paleolithic to European colonization during the Early Modern period. The term Pre-Columbian is used especially often in the context of the great indigenous civilizations of the Americas, such as those of Mesoamerica (the Olmec, the Toltec, the Teotihuacano, the Zapotec, the Mixtec, the Aztec, and the Maya) and the Andes (Inca, Moche, Muisca, Cañaris). Many pre-Columbian civilizations established characteristics and hallmarks which included permanent or urban settlements, agriculture, civic and monumental architecture, and complex societal hierarchies. Some of these civilizations had long faded by the time of the first permanent European arrivals (c. late 15th–early 16th centuries), and are known only through archeological investigations. Others were contemporary with this period, and are also known from historical accounts of the time. A few, such as the Maya, had their own written records. However, most Europeans of the time viewed such texts as pagan, and much was destroyed in Christian pyres. Only a few hidden documents remain today, leaving modern historians with glimpses of ancient culture and knowledge.
Although there had been previous trans-oceanic contact, large-scale European colonization of the Americas began with the first voyage of Christopher Columbus in 1492. The first Spanish settlement in the Americas was La Isabela in northern Hispaniola. This town was abandoned shortly after in favor of Santo Domingo de Guzmán, founded in 1496, the oldest American city of European foundation. This was the base from which the Spanish monarchy administered its new colonies and their expansion. Santo Domingo was subject to frequent raids by English and French pirates. On the continent, Panama City on the Pacific coast of Central America, founded on August 15, 1519, played an important role, being the base for the Spanish conquest of South America. Conquistador Lucas Vázquez de Ayllón established San Miguel de Guadalupe, the first European settlement in what is now the United States, on the Pee Dee River in South Carolina. During the first half of the 16th century, Spanish colonists conducted raids throughout the Caribbean Basin, bringing captives from Central America, northern South America, and Florida back to Hispaniola and other Spanish settlements.France, led by Jacques Cartier and Giovanni da Verrazano, focused primarily on North America. English explorations of the Americas were led by Giovanni Caboto and Sir Walter Raleigh. The Dutch in New Netherland confined their operations to Manhattan Island, Long Island, the Hudson River Valley, and what later became New Jersey. The spread of new diseases brought by Europeans and African slaves killed many of the inhabitants of North America and South America, with a general population crash of Native Americans occurring in the mid-16th century, often well ahead of European contact. One of the most devastating diseases was smallpox.European immigrants were often part of state-sponsored attempts to found colonies in the Americas. Migration continued as people moved to the Americas fleeing religious persecution or seeking economic opportunities. Millions of individuals were forcibly transported to the Americas as slaves, prisoners or indentured servants. Decolonization of the Americas began with the American Revolution and the Haitian Revolution in the late 1700s. This was followed by numerous Latin American wars of independence in the early 1800s. Between 1811 and 1825, Paraguay, Argentina, Chile, Gran Colombia, the United Provinces of Central America, Mexico, Brazil, Peru, and Bolivia gained independence from Spain and Portugal in armed revolutions. After the Dominican Republic won independence from Haiti, it was re-annexed by Spain in 1861, but reclaimed its independence in 1865 at the conclusion of the Dominican Restoration War. The last violent episode of decolonization was the Cuban War of Independence which became the Spanish–American War, which resulted in the independence of Cuba in 1898, and the transfer of sovereignty over Puerto Rico from Spain to the United States. Peaceful decolonization began with the purchase by the United States of Louisiana from France in 1803, Florida from Spain in 1819, of Alaska from Russia in 1867, and the Danish West Indies from Denmark in 1916. Canada became independent of the United Kingdom, starting with the Balfour Declaration of 1926, Statute of Westminster 1931, and ending with the patriation of the Canadian Constitution in 1982. The Dominion of Newfoundland similarly achieved partial independence under the Balfour Declaration and Statute of Westminster, but was re-absorbed into the United Kingdom in 1934. It was subsequently confederated with Canada in 1949. The remaining European colonies in the Caribbean began to achieve peaceful independence well after World War II. Jamaica and Trinidad and Tobago became independent in 1962, and Guyana and Barbados both achieved independence in 1966. In the 1970s, the Bahamas, Grenada, Dominica, St. Lucia, and St. Vincent and the Grenadines all became independent of the United Kingdom, and Suriname became independent of the Netherlands. Belize, Antigua and Barbuda, and Saint Kitts and Nevis achieved independence from the United Kingdom in the 1980s.
The Americas make up most of the land in Earth's western hemisphere. The northernmost point of the Americas is Kaffeklubben Island, which is the most northerly point of land on Earth. The southernmost point is the islands of Southern Thule, although they are sometimes considered part of Antarctica. The mainland of the Americas is the world's longest north-to-south landmass. The distance between its two polar extremities, the Boothia Peninsula in northern Canada and Cape Froward in Chilean Patagonia, is roughly 14,000 km (8,700 mi). The mainland's most westerly point is the end of the Seward Peninsula in Alaska; Attu Island, further off the Alaskan coast to the west, is considered the westernmost point of the Americas. Ponta do Seixas in northeastern Brazil forms the easternmost extremity of the mainland, while Nordostrundingen, in Greenland, is the most easterly point of the continental shelf.
South America broke off from the west of the supercontinent Gondwana around 135 million years ago, forming its own continent. Around 15 million years ago, the collision of the Caribbean Plate and the Pacific Plate resulted in the emergence of a series of volcanoes along the border that created a number of islands. The gaps in the archipelago of Central America filled in with material eroded off North America and South America, plus new land created by continued volcanism. By three million years ago, the continents of North America and South America were linked by the Isthmus of Panama, thereby forming the single landmass of the Americas. The Great American Interchange resulted in many species being spread across the Americas, such as the cougar, porcupine, opossums, armadillos and hummingbirds.
The geography of the western Americas is dominated by the American cordillera, with the Andes running along the west coast of South America and the Rocky Mountains and other North American Cordillera ranges running along the western side of North America. The 2,300-kilometer-long (1,400 mi) Appalachian Mountains run along the east coast of North America from Alabama to Newfoundland. North of the Appalachians, the Arctic Cordillera runs along the eastern coast of Canada.The largest mountain ranges are the Andes and Rocky Mountains. The Sierra Nevada and the Cascade Range reach similar altitudes as the Rocky Mountains, but are significantly smaller. In North America, the greatest number of fourteeners are in the United States, and more specifically in the U.S. state of Colorado. The highest peaks of the Americas are located in the Andes, with Aconcagua of Argentina being the highest; in North America Denali (Mount McKinley) in the U.S. state of Alaska is the tallest. Between its coastal mountain ranges, North America has vast flat areas. The Interior Plains spread over much of the continent, with low relief. The Canadian Shield covers almost 5 million km2 of North America and is generally quite flat. Similarly, the north-east of South America is covered by the flat Amazon Basin. The Brazilian Highlands on the east coast are fairly smooth but show some variations in landform, while farther south the Gran Chaco and Pampas are broad lowlands.
The climate of the Americas varies significantly from region to region. Tropical rainforest climate occurs in the latitudes of the Amazon, American cloud forests, southeastern Florida and Darien Gap. In the Rocky Mountains and Andes, dry and continental climates are observed. Often the higher altitudes of these mountains are snow-capped. Southeastern North America is well known for its occurrence of tornadoes and hurricanes, of which the vast majority of tornadoes occur in the United States' Tornado Alley, as well as in the southerly Dixie Alley in the North American late-winter and early spring seasons. Often parts of the Caribbean are exposed to the violent effects of hurricanes. These weather systems are formed by the collision of dry, cool air from Canada and wet, warm air from the Atlantic.
With coastal mountains and interior plains, the Americas have several large river basins that drain the continents. The largest river basin in North America is that of the Mississippi, covering the second largest watershed on the planet. The Mississippi-Missouri river system drains most of 31 states of the U.S., most of the Great Plains, and large areas between the Rocky and Appalachian mountains. This river is the fourth longest in the world and tenth most powerful in the world. In North America, to the east of the Appalachian Mountains, there are no major rivers but rather a series of rivers and streams that flow east with their terminus in the Atlantic Ocean, such as the Hudson River, Saint John River, and Savannah River. A similar instance arises with central Canadian rivers that drain into Hudson Bay; the largest being the Churchill River. On the west coast of North America, the main rivers are the Colorado River, Columbia River, Yukon River, Fraser River, and Sacramento River. The Colorado River drains much of the Southern Rockies and parts of the Great Basin and Range Province. The river flows approximately 1,450 miles (2,330 km) into the Gulf of California, during which over time it has carved out natural phenomena such as the Grand Canyon and created phenomena such as the Salton Sea. The Columbia is a large river, 1,243 miles (2,000 km) long, in central western North America and is the most powerful river on the West Coast of the Americas. In the far northwest of North America, the Yukon drains much of the Alaskan peninsula and flows 1,980 miles (3,190 km) from parts of Yukon and the Northwest Territory to the Pacific. Draining to the Arctic Ocean of Canada, the Mackenzie River drains waters from the Arctic Great Lakes of Arctic Canada, as opposed to the Saint-Lawrence River that drains the Great Lakes of Southern Canada into the Atlantic Ocean. The Mackenzie River is the largest in Canada and drains 1,805,200 square kilometers (697,000 sq mi).The largest river basin in South America is that of the Amazon, which has the highest volume flow of any river on Earth. The second largest watershed of South America is that of the Paraná River, which covers about 2.5 million km2.
North America and South America began to develop a shared population of flora and fauna around 2.5 million years ago, when continental drift brought the two continents into contact via the Isthmus of Panama. Initially, the exchange of biota was roughly equal, with North American genera migrating into South America in about the same proportions as South American genera migrated into North America. This exchange is known as the Great American Interchange. The exchange became lopsided after roughly a million years, with the total spread of South American genera into North America far more limited in scope than the spread on North American genera into South America.
There are 35 sovereign states in the Americas, as well as an autonomous country of Denmark, three overseas departments of France, three overseas collectivities of France, and one uninhabited territory of France, eight overseas territories of the United Kingdom, three constituent countries of the Netherlands, three public bodies of the Netherlands, two unincorporated territories of the United States, and one uninhabited territory of the United States.
In 2015 the total population of the Americas was about 985 million people, divided as follows: North America: 569 million (includes Central America and the Caribbean) South America: 416 million
There are three urban centers that each hold titles for being the largest population area based on the three main demographic concepts: City properA city proper is the locality with legally fixed boundaries and an administratively recognized urban status that is usually characterized by some form of local government.Urban areaAn urban area is characterized by higher population density and vast human features in comparison to areas surrounding it. Urban areas may be cities, towns or conurbations, but the term is not commonly extended to rural settlements such as villages and hamlets. Urban areas are created and further developed by the process of urbanization and do not include large swaths of rural land, as do metropolitan areas.Metropolitan areaUnlike an urban area, a metropolitan area includes not only the urban area, but also satellite cities plus intervening rural land that is socio-economically connected to the urban core city, typically by employment ties through commuting, with the urban core city being the primary labor market.In accordance with these definitions, the three largest population centers in the Americas are: Mexico City, anchor to the largest metropolitan area in the Americas; New York City, anchor to the largest urban area in the Americas; and São Paulo, the largest city proper in the Americas. All three cities maintain Alpha classification and large scale influence. Urban centers within the Americas
The population of the Americas is made up of the descendants of four large ethnic groups and their combinations. The Indigenous peoples of the Americas, being Amerindians, Inuit, and Aleuts. Those of European ancestry, mainly Spanish, British and Irish, Portuguese, German, Italian, French and Dutch. Those of African ancestry, mainly of West African descent. Asians, that is, those of Eastern, South, and Southeast Asian ancestry. Mestizos (Metis people in Canada), those of mixed European and Amerindian ancestry. Mulattoes, people of mixed African and European ancestry. Zambos (Spanish) or Cafuzos (Portuguese), those of mixed African and Indigenous ancestry.The majority of the population live in Latin America, named for its predominant cultures, rooted in Latin Europe (including the two dominant languages, Spanish and Portuguese, both Romance languages), more specifically in the Iberian nations of Portugal and Spain (hence the use of the term Ibero-America as a synonym). Latin America is typically contrasted with Anglo-America, where English, a Germanic language, is prevalent, and which comprises Canada (with the exception of francophone Canada rooted in Latin Europe [France]—see Québec and Acadia) and the United States. Both countries are located in North America, with cultures deriving predominantly from Anglo-Saxon and other Germanic roots.
The most prevalent faiths in the Americas are as follows: Christianity (86 percent)Roman Catholicism: Practiced by 69 percent of the Latin American population, 81 percent in Mexico and 61 percent in Brazil whose Roman Catholic population of 134 million is the greatest of any nation's; approximately 24 percent of the United States' population and about 39 percent of Canada's. Protestantism: Practiced mostly in the United States, where half of the population are Protestant, Canada, with slightly more than a quarter of the population, and Greenland; there is a growing contingent of Evangelical and Pentecostal movements in predominantly Catholic Latin America. Eastern Orthodoxy: Found mostly in the United States (1 percent) and Canada; this Christian group is growing faster than many other Christian groups in Canada and now represents roughly 3 percent of the Canadian population. Non-denominational Christians and other Christians (some 1,000 different Christian denominations and sects practiced in the Americas). Irreligion: About 12 percent, including atheists and agnostics, as well as those who profess some form of spirituality but do not identify themselves as members of any organized religion. Islam: Together, Muslims constitute about 1 percent of the North American population and 0.3 percent of all Latin Americans. It is practiced by 3 percent of Canadians and 0.6 percent of the U.S. population. Argentina has the largest Muslim population in Latin America with up to 600,000 persons, or 1.9 percent of the population. Judaism (practiced by 2 percent of North Americans—approximately 2.5 percent of the U.S. population and 1.2 percent of Canadians—and 0.23 percent of Latin Americans—Argentina has the largest Jewish population in Latin America with 200,000 members)Other faiths include Buddhism; Hinduism; Sikhism; Baháʼí Faith; a wide variety of indigenous religions, many of which can be categorized as animistic; new age religions and many African and African-derived religions. Syncretic faiths can also be found throughout the Americas.
Various languages are spoken in the Americas. Some are of European origin, others are spoken by indigenous peoples or are the mixture of various languages like the different creoles.The most widely spoken language in the Americas is Spanish. The dominant language of Latin America is Spanish, though the most populous nation in Latin America, Brazil, speaks Portuguese. Small enclaves of French-, Dutch- and English-speaking regions also exist in Latin America, notably in French Guiana, Suriname, and Belize and Guyana respectively. Haitian Creole is dominant in the nation of Haiti, where French is also spoken. Native languages are more prominent in Latin America than in Anglo-America, with Nahuatl, Quechua, Aymara and Guaraní as the most common. Various other native languages are spoken with less frequency across both Anglo-America and Latin America. Creole languages other than Haitian Creole are also spoken in parts of Latin America. The dominant language of Anglo-America is English. French is also official in Canada, where it is the predominant language in Quebec and an official language in New Brunswick along with English. It is also an important language in Louisiana, and in parts of New Hampshire, Maine, and Vermont. Spanish has kept an ongoing presence in the Southwestern United States, which formed part of the Viceroyalty of New Spain, especially in California and New Mexico, where a distinct variety of Spanish spoken since the 17th century has survived. It has more recently become widely spoken in other parts of the United States because of heavy immigration from Latin America. High levels of immigration in general have brought great linguistic diversity to Anglo-America, with over 300 languages known to be spoken in the United States alone, but most languages are spoken only in small enclaves and by relatively small immigrant groups. The nations of Guyana, Suriname, and Belize are generally considered not to fall into either Anglo-America or Latin America because of their language differences from Latin America, geographic differences from Anglo-America, and cultural and historical differences from both regions; English is the primary language of Guyana and Belize, and Dutch is the primary language of Suriname. Most of the non-native languages have, to different degrees, evolved differently from the mother country, but are usually still mutually intelligible. Some have combined, however, which has even resulted in completely new languages, such as Papiamento, which is a combination of Portuguese, Spanish, Dutch (representing the respective colonizers), native Arawak, various African languages, and, more recently English. The lingua franca Portuñol, a mixture of Portuguese and Spanish, is spoken in the border regions of Brazil and neighboring Spanish-speaking countries. More specifically, Riverense Portuñol is spoken by around 100,000 people in the border regions of Brazil and Uruguay. Because of immigration, there are many communities where other languages are spoken from all parts of the world, especially in the United States, Brazil, Argentina, Canada, Chile, Costa Rica and Uruguay—very important destinations for immigrants.
Speakers of English generally refer to the landmasses of North America and South America as the Americas, the Western Hemisphere, or the New World. The adjective American may be used to indicate something pertains to the Americas, but this term is primarily used in English to indicate something pertaining to the United States. Some non-ambiguous alternatives exist, such as the adjective Pan-American, or New Worlder as a demonym for a resident of the closely related New World. Use of America in the hemispherical sense is sometimes retained, or can occur when translated from other languages. For example, the Association of National Olympic Committees (ANOC) in Paris maintains a single continental association for "America", represented by one of the five Olympic rings.American essayist H.L. Mencken said, "The Latin-Americans use Norteamericano in formal writing, but, save in Panama, prefer nicknames in colloquial speech." To avoid "American" one can use constructed terms in their languages derived from "United States" or even "North America". In Canada, its southern neighbor is often referred to as "the United States", "the U.S.A.", or (informally) "the States", while U.S. citizens are generally referred to as "Americans". Most Canadians resent being referred to as "Americans".
In Spanish, América is a single continent composed of the subcontinents of América del Sur and América del Norte, the land bridge of América Central, and the islands of the Antillas. Americano or americana in Spanish refers to a person from América in a similar way that europeo or europea refers to a person from Europa. The terms sudamericano/a, centroamericano/a, antillano/a and norteamericano/a can be used to more specifically refer to the location where a person may live. Citizens of the United States of America are normally referred to by the term estadounidense (rough literal translation: "United Statesian") instead of americano or americana which is discouraged, and the country's name itself is officially translated as Estados Unidos de América (United States of America), commonly abbreviated as Estados Unidos (EEUU). Also, the term norteamericano (North American) may refer to a citizen of the United States. This term is primarily used to refer to citizens of the United States, and less commonly to those of other North American countries.
In French the word américain may be used for things relating to the Americas; however, similar to English, it is most often used for things relating to the United States, with the term états-unien sometimes used for clarity. Panaméricain may be used as an adjective to refer to the Americas without ambiguity. French speakers may use the noun Amérique to refer to the whole landmass as one continent, or two continents, Amérique du Nord and Amérique du Sud. In French, Amérique is seldom used to refer to the United States, leading to some ambiguity when it is. Similar to English usage, les Amériques or des Amériques is used to refer unambiguously to the Americas.
In Dutch, the word Amerika mostly refers to the United States. Although the United States is equally often referred to as de Verenigde Staten ("the United States") or de VS ("the US"), Amerika relatively rarely refers to the Americas, but it is the only commonly used Dutch word for the Americas. This often leads to ambiguity; and to stress that something concerns the Americas as a whole, Dutch uses a combination, namely Noord- en Zuid-Amerika (North and South America). Latin America is generally referred to as Latijns Amerika or Midden-Amerika for Central America. The adjective Amerikaans is most often used for things or people relating to the United States. There are no alternative words to distinguish between things relating to the United States or to the Americas. Dutch uses the local alternative for things relating to elsewhere in the Americas, such as Argentijns for Argentine, etc.
The following is a list of multinational organizations in the Americas.
Dominica, Panama and the Dominican Republic have the fastest-growing economy in the Americas according to the International Monetary Fund (IMF),In 2016, five to seven countries in the southern part of the Americas had weakening economies in decline, compared to only three countries in the northern part of the Americas. Haiti has the lowest GDP per capita in the Americas, although its economy was growing slightly as of 2016.
United Nations population data by latest available Census: 2008–2009 Organization of American States Council on Hemispheric Affairs Gannett, Henry; Ingersoll, Ernest; Winship, George Parker (1905). "America and others" . New International Encyclopedia.
The Arctic Ocean is the smallest and shallowest of the world's five major oceans. It is also known as the coldest of all the oceans. The International Hydrographic Organization (IHO) recognizes it as an ocean, although some oceanographers call it the Arctic Mediterranean Sea. It is sometimes classified as an estuary of the Atlantic Ocean, and it is also seen as the northernmost part of the all-encompassing World Ocean. The Arctic Ocean includes the North Pole region in the middle of the Northern Hemisphere, and extends south to about 60°N. The Arctic Ocean is surrounded by Eurasia and North America, and the borders follow topographic features; the Bering Strait on the Pacific side, and the Greenland Scotland Ridge on the Atlantic side. It is mostly covered by sea ice throughout the year and almost completely in winter. The Arctic Ocean's surface temperature and salinity vary seasonally as the ice cover melts and freezes; its salinity is the lowest on average of the five major oceans, due to low evaporation, heavy fresh water inflow from rivers and streams, and limited connection and outflow to surrounding oceanic waters with higher salinities. The summer shrinking of the ice has been quoted at 50%. The US National Snow and Ice Data Center (NSIDC) uses satellite data to provide a daily record of Arctic sea ice cover and the rate of melting compared to an average period and specific past years, showing a continuous decline in sea ice extent. In September 2012, the Arctic ice extent reached a new record minimum. Compared to the average extent (1979-2000), the sea ice had diminished by 49%.
Human habitation in the North American polar region goes back at least 50,000–17,000 years ago, during the Wisconsin glaciation. At this time, falling sea levels allowed people to move across the Bering land bridge that joined Siberia to northwestern North America (Alaska), leading to the Settlement of the Americas. Paleo-Eskimo groups included the Pre-Dorset (c. 3200–850 BC); the Saqqaq culture of Greenland (2500–800 BC); the Independence I and Independence II cultures of northeastern Canada and Greenland (c. 2400–1800 BC and c. 800–1 BC); the Groswater of Labrador and Nunavik, and the Dorset culture (500 BC to AD 1500), which spread across Arctic North America. The Dorset were the last major Paleo-Eskimo culture in the Arctic before the migration east from present-day Alaska of the Thule, the ancestors of the modern Inuit.The Thule Tradition lasted from about 200 BC to AD 1600 around the Bering Strait, the Thule people being the prehistoric ancestors of the Inuit who now live in Northern Labrador.For much of European history, the north polar regions remained largely unexplored and their geography conjectural. Pytheas of Massilia recorded an account of a journey northward in 325 BC, to a land he called "Eschate Thule", where the Sun only set for three hours each day and the water was replaced by a congealed substance "on which one can neither walk nor sail". He was probably describing loose sea ice known today as "growlers" or "bergy bits"; his "Thule" was probably Norway, though the Faroe Islands or Shetland have also been suggested. Early cartographers were unsure whether to draw the region around the North Pole as land (as in Johannes Ruysch's map of 1507, or Gerardus Mercator's map of 1595) or water (as with Martin Waldseemüller's world map of 1507). The fervent desire of European merchants for a northern passage, the Northern Sea Route or the Northwest Passage, to "Cathay" (China) caused water to win out, and by 1723 mapmakers such as Johann Homann featured an extensive "Oceanus Septentrionalis" at the northern edge of their charts. The few expeditions to penetrate much beyond the Arctic Circle in this era added only small islands, such as Novaya Zemlya (11th century) and Spitzbergen (1596), though since these were often surrounded by pack-ice, their northern limits were not so clear. The makers of navigational charts, more conservative than some of the more fanciful cartographers, tended to leave the region blank, with only fragments of known coastline sketched in. This lack of knowledge of what lay north of the shifting barrier of ice gave rise to a number of conjectures. In England and other European nations, the myth of an "Open Polar Sea" was persistent. John Barrow, longtime Second Secretary of the British Admiralty, promoted exploration of the region from 1818 to 1845 in search of this. In the United States in the 1850s and 1860s, the explorers Elisha Kane and Isaac Israel Hayes both claimed to have seen part of this elusive body of water. Even quite late in the century, the eminent authority Matthew Fontaine Maury included a description of the Open Polar Sea in his textbook The Physical Geography of the Sea (1883). Nevertheless, as all the explorers who travelled closer and closer to the pole reported, the polar ice cap is quite thick, and persists year-round. Fridtjof Nansen was the first to make a nautical crossing of the Arctic Ocean, in 1896. The first surface crossing of the ocean was led by Wally Herbert in 1969, in a dog sled expedition from Alaska to Svalbard, with air support. The first nautical transit of the north pole was made in 1958 by the submarine USS Nautilus, and the first surface nautical transit occurred in 1977 by the icebreaker NS Arktika. Since 1937, Soviet and Russian manned drifting ice stations have extensively monitored the Arctic Ocean. Scientific settlements were established on the drift ice and carried thousands of kilometers by ice floes.In World War II, the European region of the Arctic Ocean was heavily contested: the Allied commitment to resupply the Soviet Union via its northern ports was opposed by German naval and air forces. Since 1954 commercial airlines have flown over the Arctic Ocean (see Polar route).
There are several ports and harbors around the Arctic Ocean
The ocean's Arctic shelf comprises a number of continental shelves, including the Canadian Arctic shelf, underlying the Canadian Arctic Archipelago, and the Russian continental shelf, which is sometimes simply called the "Arctic Shelf" because it is greater in extent. The Russian continental shelf consists of three separate, smaller shelves, the Barents Shelf, Chukchi Sea Shelf and Siberian Shelf. Of these three, the Siberian Shelf is the largest such shelf in the world. The Siberian Shelf holds large oil and gas reserves, and the Chukchi shelf forms the border between Russian and the United States as stated in the USSR–USA Maritime Boundary Agreement. The whole area is subject to international territorial claims.
The crystalline basement rocks of mountains around the Arctic Ocean were recrystallized or formed during the Ellesmerian orogeny, the regional phase of the larger Caledonian orogeny in the Paleozoic. Regional subsidence in the Jurassic and Triassic led to significant sediment deposition, creating many of the reservoir for current day oil and gas deposits. During the Cretaceous the Canadian Basin opened and tectonic activity due to the assembly of Alaska caused hydrocarbons to migrate toward what is now Prudhoe Bay. At the same time, sediments shed off the rising Canadian Rockies building out the large Mackenzie Delta. The rifting apart of the supercontinent Pangea, beginning in the Triassic opened the early Atlantic Ocean. Rifting then extended northward, opening the Arctic Ocean as mafic oceanic crust material erupted out of a branch of Mid-Atlantic Ridge. The Amerasia Basin may have opened first, with the Chulkchi Borderland moved along to the northeast by transform faults. Additional spreading helped to create the "triple-junction" of the Alpha-Mendeleev Ridge in the Late Cretaceous. Throughout the Cenozoic, the subduction of the Pacific plate, the collision of India with Eurasia and the continued opening of the North Atlantic created new hydrocarbon traps. The seafloor began spreading from the Gakkel Ridge in the Paleocene and Eocene, causing the Lomonosov Ridge to move farther from land and subside. Because of sea ice and remote conditions, the geology of the Arctic Ocean is still poorly explored. ACEX drilling shed some light on the Lomonosov Ridge, which appears to be continental crust separated from the Barents-Kara Shelf in the Paleocene and then starved of sediment. It may contain up to 10 billion barrels of oil. The Gakkel Ridge rift is also poorly understand and may extend into the Laptev Sea.
In large parts of the Arctic Ocean, the top layer (about 50 m [160 ft]) is of lower salinity and lower temperature than the rest. It remains relatively stable, because the salinity effect on density is bigger than the temperature effect. It is fed by the freshwater input of the big Siberian and Canadian streams (Ob, Yenisei, Lena, Mackenzie), the water of which quasi floats on the saltier, denser, deeper ocean water. Between this lower salinity layer and the bulk of the ocean lies the so-called halocline, in which both salinity and temperature rise with increasing depth. Because of its relative isolation from other oceans, the Arctic Ocean has a uniquely complex system of water flow. It resembles some hydrological features of the Mediterranean Sea, referring to its deep waters having only limited communication through the Fram Strait with the Atlantic Basin, "where the circulation is dominated by thermohaline forcing”. The Arctic Ocean has a total volume of 18.07×106 km3, equal to about 1.3% of the World Ocean. Mean surface circulation is predominately cyclonic on the Eurasian side and anticyclonic in the Canadian Basin.Water enters from both the Pacific and Atlantic Oceans and can be divided into three unique water masses. The deepest water mass is called Arctic Bottom Water and begins around 900 metres (3,000 feet) depth. It is composed of the densest water in the World Ocean and has two main sources: Arctic shelf water and Greenland Sea Deep Water. Water in the shelf region that begins as inflow from the Pacific passes through the narrow Bering Strait at an average rate of 0.8 Sverdrups and reaches the Chukchi Sea. During the winter, cold Alaskan winds blow over the Chukchi Sea, freezing the surface water and pushing this newly formed ice out to the Pacific. The speed of the ice drift is roughly 1–4 cm/s. This process leaves dense, salty waters in the sea that sink over the continental shelf into the western Arctic Ocean and create a halocline. This water is met by Greenland Sea Deep Water, which forms during the passage of winter storms. As temperatures cool dramatically in the winter, ice forms and intense vertical convection allows the water to become dense enough to sink below the warm saline water below. Arctic Bottom Water is critically important because of its outflow, which contributes to the formation of Atlantic Deep Water. The overturning of this water plays a key role in global circulation and the moderation of climate. In the depth range of 150–900 metres (490–2,950 feet) is a water mass referred to as Atlantic Water. Inflow from the North Atlantic Current enters through the Fram Strait, cooling and sinking to form the deepest layer of the halocline, where it circles the Arctic Basin counter-clockwise. This is the highest volumetric inflow to the Arctic Ocean, equalling about 10 times that of the Pacific inflow, and it creates the Arctic Ocean Boundary Current. It flows slowly, at about 0.02 m/s. Atlantic Water has the same salinity as Arctic Bottom Water but is much warmer (up to 3 °C [37 °F]). In fact, this water mass is actually warmer than the surface water, and remains submerged only due to the role of salinity in density. When water reaches the basin it is pushed by strong winds into a large circular current called the Beaufort Gyre. Water in the Beaufort Gyre is far less saline than that of the Chukchi Sea due to inflow from large Canadian and Siberian rivers.The final defined water mass in the Arctic Ocean is called Arctic Surface Water and is found from 150–200 metres (490–660 feet). The most important feature of this water mass is a section referred to as the sub-surface layer. It is a product of Atlantic water that enters through canyons and is subjected to intense mixing on the Siberian Shelf. As it is entrained, it cools and acts a heat shield for the surface layer. This insulation keeps the warm Atlantic Water from melting the surface ice. Additionally, this water forms the swiftest currents of the Arctic, with speed of around 0.3–0.6 m/s. Complementing the water from the canyons, some Pacific water that does not sink to the shelf region after passing through the Bering Strait also contributes to this water mass. Waters originating in the Pacific and Atlantic both exit through the Fram Strait between Greenland and Svalbard Island, which is about 2,700 metres (8,900 feet) deep and 350 kilometres (220 miles) wide. This outflow is about 9 Sv. The width of the Fram Strait is what allows for both inflow and outflow on the Atlantic side of the Arctic Ocean. Because of this, it is influenced by the Coriolis force, which concentrates outflow to the East Greenland Current on the western side and inflow to the Norwegian Current on the eastern side. Pacific water also exits along the west coast of Greenland and the Hudson Strait (1–2 Sv), providing nutrients to the Canadian Archipelago.As noted, the process of ice formation and movement is a key driver in Arctic Ocean circulation and the formation of water masses. With this dependence, the Arctic Ocean experiences variations due to seasonal changes in sea ice cover. Sea ice movement is the result of wind forcing, which is related to a number of meteorological conditions that the Arctic experiences throughout the year. For example, the Beaufort High—an extension of the Siberian High system—is a pressure system that drives the anticyclonic motion of the Beaufort Gyre. During the summer, this area of high pressure is pushed out closer to its Siberian and Canadian sides. In addition, there is a sea level pressure (SLP) ridge over Greenland that drives strong northerly winds through the Fram Strait, facilitating ice export. In the summer, the SLP contrast is smaller, producing weaker winds. A final example of seasonal pressure system movement is the low pressure system that exists over the Nordic and Barents Seas. It is an extension of the Icelandic Low, which creates cyclonic ocean circulation in this area. The low shifts to center over the North Pole in the summer. These variations in the Arctic all contribute to ice drift reaching its weakest point during the summer months. There is also evidence that the drift is associated with the phase of the Arctic Oscillation and Atlantic Multidecadal Oscillation.
Much of the Arctic Ocean is covered by sea ice that varies in extent and thickness seasonally. The mean extent of the Arctic sea ice has been continuously decreasing in the last decades, declining at a rate of currently 12.85% per decade since 1980 from the average winter value of 15,600,000 km2 (6,023,200 sq mi). The seasonal variations are about 7,000,000 km2 (2,702,700 sq mi) with the maximum in April and minimum in September. The sea ice is affected by wind and ocean currents, which can move and rotate very large areas of ice. Zones of compression also arise, where the ice piles up to form pack ice.Icebergs occasionally break away from northern Ellesmere Island, and icebergs are formed from glaciers in western Greenland and extreme northeastern Canada. Icebergs are not sea ice but may become embedded in the pack ice. Icebergs pose a hazard to ships, of which the Titanic is one of the most famous. The ocean is virtually icelocked from October to June, and the superstructure of ships are subject to icing from October to May. Before the advent of modern icebreakers, ships sailing the Arctic Ocean risked being trapped or crushed by sea ice (although the Baychimo drifted through the Arctic Ocean untended for decades despite these hazards).
Due to the pronounced seasonality of 2–6 months of midnight sun and polar night in the Arctic Ocean, the primary production of photosynthesizing organisms such as ice algae and phytoplankton is limited to the spring and summer months (March/April to September). Important consumers of primary producers in the central Arctic Ocean and the adjacent shelf seas include zooplankton, especially copepods (Calanus finmarchicus, Calanus glacialis, and Calanus hyperboreus) and euphausiids, as well as ice-associated fauna (e.g., amphipods). These primary consumers form an important link between the primary producers and higher trophic levels. The composition of higher trophic levels in the Arctic Ocean varies with region (Atlantic side vs. Pacific side), and with the sea-ice cover. Secondary consumers in the Barents Sea, an Atlantic-influenced Arctic shelf sea, are mainly sub-Arctic species including herring, young cod, and capelin. In ice-covered regions of the central Arctic Ocean, polar cod is a central predator of primary consumers. The apex predators in the Arctic Ocean - Marine mammals such as seals, whales, and polar bears, prey upon fish. Endangered marine species in the Arctic Ocean include walruses and whales. The area has a fragile ecosystem, and it is especially exposed to climate change, because it warms faster than the rest of the world. Lion's mane jellyfish are abundant in the waters of the Arctic, and the banded gunnel is the only species of gunnel that lives in the ocean.
Petroleum and natural gas fields, placer deposits, polymetallic nodules, sand and gravel aggregates, fish, seals and whales can all be found in abundance in the region.The political dead zone near the center of the sea is also the focus of a mounting dispute between the United States, Russia, Canada, Norway, and Denmark. It is significant for the global energy market because it may hold 25% or more of the world's undiscovered oil and gas resources.
The Arctic ice pack is thinning, and a seasonal hole in the ozone layer frequently occurs. Reduction of the area of Arctic sea ice reduces the planet's average albedo, possibly resulting in global warming in a positive feedback mechanism. Research shows that the Arctic may become ice-free in the summer for the first time in human history by 2040. Estimates vary for when the last time the Arctic was ice-free: 65 million years ago when fossils indicate that plants existed there to as recently as 5,500 years ago; ice and ocean cores going back 8,000 years to the last warm period or 125,000 during the last intraglacial period.Warming temperatures in the Arctic may cause large amounts of fresh meltwater to enter the north Atlantic, possibly disrupting global ocean current patterns. Potentially severe changes in the Earth's climate might then ensue.As the extent of sea ice diminishes and sea level rises, the effect of storms such as the Great Arctic Cyclone of 2012 on open water increases, as does possible salt-water damage to vegetation on shore at locations such as the Mackenzie's river delta as stronger storm surges become more likely.Global warming has increased encounters between polar bears and humans. Reduced sea ice due to melting is causing polar bears to search for new sources of food. Beginning in December 2018 and coming to an apex in February 2019, a mass invasion of polar bears into the archipelago of Novaya Zemlya caused local authorities to declare a state of emergency. Dozens of polar bears were seen entering homes and public buildings and inhabited areas.
Sea ice, and the cold conditions it sustains, serves to stabilize methane deposits on and near the shoreline, preventing the clathrate breaking down and outgassing methane into the atmosphere, causing further warming. Melting of this ice may release large quantities of methane, a powerful greenhouse gas into the atmosphere, causing further warming in a strong positive feedback cycle and marine genera and species to become extinct.
Other environmental concerns relate to the radioactive contamination of the Arctic Ocean from, for example, Russian radioactive waste dump sites in the Kara Sea Cold War nuclear test sites such as Novaya Zemlya, Camp Century's contaminants in Greenland, or radioactive contamination from Fukushima.On 16 July 2015, five nations (United States, Russia, Canada, Norway, Denmark/Greenland) signed a declaration committing to keep their fishing vessels out of a 1.1 million square mile zone in the central Arctic Ocean near the North Pole. The agreement calls for those nations to refrain from fishing there until there is better scientific knowledge about the marine resources and until a regulatory system is in place to protect those resources.
The Hidden Ocean Arctic 2005 Daily logs, photos and video from exploration mission. Oceanography Image of the Day, from the Woods Hole Oceanographic Institution Arctic Council The Northern Forum Arctic Environmental Atlas Interactive map NOAA Arctic Theme Page Arctic Great Rivers Observatory (ArcticGRO) "Arctic Ocean". The World Factbook. Central Intelligence Agency. Daily Arctic Ocean Rawinsonde Data from Soviet Drifting Ice Stations (1954–1990) at NSIDC NOAA North Pole Web Cam Images from Web Cams deployed in spring on an ice floe NOAA Near-realtime North Pole Weather Data Data from instruments deployed on an ice floe Search for Arctic Life Heats Up by Stephen Leahy International Polar Foundation "Daily report of Arctic ice cover based on satellite data". nsidc.org. National Snow and Ice Data Center. Marine Biodiversity Wiki
Astronomy (from Greek: ἀστρονομία, literally meaning the science that studies the laws of the stars) is a natural science that studies celestial objects and phenomena. It uses mathematics, physics, and chemistry in order to explain their origin and evolution. Objects of interest include planets, moons, stars, nebulae, galaxies, and comets. Relevant phenomena include supernova explosions, gamma ray bursts, quasars, blazars, pulsars, and cosmic microwave background radiation. More generally, astronomy studies everything that originates outside Earth's atmosphere. Cosmology is a branch of astronomy. It studies the Universe as a whole.Astronomy is one of the oldest natural sciences. The early civilizations in recorded history made methodical observations of the night sky. These include the Babylonians, Greeks, Indians, Egyptians, Chinese, Maya, and many ancient indigenous peoples of the Americas. In the past, astronomy included disciplines as diverse as astrometry, celestial navigation, observational astronomy, and the making of calendars. Nowadays, professional astronomy is often said to be the same as astrophysics.Professional astronomy is split into observational and theoretical branches. Observational astronomy is focused on acquiring data from observations of astronomical objects. This data is then analyzed using basic principles of physics. Theoretical astronomy is oriented toward the development of computer or analytical models to describe astronomical objects and phenomena. These two fields complement each other. Theoretical astronomy seeks to explain observational results and observations are used to confirm theoretical results. Astronomy is one of the few sciences in which amateurs play an active role. This is especially true for the discovery and observation of transient events. Amateur astronomers have helped with many important discoveries, such as finding new comets.
"Astronomy" and "astrophysics" are synonyms. Based on strict dictionary definitions, "astronomy" refers to "the study of objects and matter outside the Earth's atmosphere and of their physical and chemical properties," while "astrophysics" refers to the branch of astronomy dealing with "the behavior, physical properties, and dynamic processes of celestial objects and phenomena". In some cases, as in the introduction of the introductory textbook The Physical Universe by Frank Shu, "astronomy" may be used to describe the qualitative study of the subject, whereas "astrophysics" is used to describe the physics-oriented version of the subject. However, since most modern astronomical research deals with subjects related to physics, modern astronomy could actually be called astrophysics. Some fields, such as astrometry, are purely astronomy rather than also astrophysics. Various departments in which scientists carry out research on this subject may use "astronomy" and "astrophysics", partly depending on whether the department is historically affiliated with a physics department, and many professional astronomers have physics rather than astronomy degrees. Some titles of the leading scientific journals in this field include The Astronomical Journal, The Astrophysical Journal, and Astronomy & Astrophysics.
In early historic times, astronomy only consisted of the observation and predictions of the motions of objects visible to the naked eye. In some locations, early cultures assembled massive artifacts that possibly had some astronomical purpose. In addition to their ceremonial uses, these observatories could be employed to determine the seasons, an important factor in knowing when to plant crops and in understanding the length of the year.Before tools such as the telescope were invented, early study of the stars was conducted using the naked eye. As civilizations developed, most notably in Mesopotamia, Greece, Persia, India, China, Egypt, and Central America, astronomical observatories were assembled and ideas on the nature of the Universe began to develop. Most early astronomy consisted of mapping the positions of the stars and planets, a science now referred to as astrometry. From these observations, early ideas about the motions of the planets were formed, and the nature of the Sun, Moon and the Earth in the Universe were explored philosophically. The Earth was believed to be the center of the Universe with the Sun, the Moon and the stars rotating around it. This is known as the geocentric model of the Universe, or the Ptolemaic system, named after Ptolemy. A particularly important early development was the beginning of mathematical and scientific astronomy, which began among the Babylonians, who laid the foundations for the later astronomical traditions that developed in many other civilizations. The Babylonians discovered that lunar eclipses recurred in a repeating cycle known as a saros. Following the Babylonians, significant advances in astronomy were made in ancient Greece and the Hellenistic world. Greek astronomy is characterized from the start by seeking a rational, physical explanation for celestial phenomena. In the 3rd century BC, Aristarchus of Samos estimated the size and distance of the Moon and Sun, and he proposed a model of the Solar System where the Earth and planets rotated around the Sun, now called the heliocentric model. In the 2nd century BC, Hipparchus discovered precession, calculated the size and distance of the Moon and invented the earliest known astronomical devices such as the astrolabe. Hipparchus also created a comprehensive catalog of 1020 stars, and most of the constellations of the northern hemisphere derive from Greek astronomy. The Antikythera mechanism (c. 150–80 BC) was an early analog computer designed to calculate the location of the Sun, Moon, and planets for a given date. Technological artifacts of similar complexity did not reappear until the 14th century, when mechanical astronomical clocks appeared in Europe.
Medieval Europe housed a number of important astronomers. Richard of Wallingford (1292–1336) made major contributions to astronomy and horology, including the invention of the first astronomical clock, the Rectangulus which allowed for the measurement of angles between planets and other astronomical bodies, as well as an equatorium called the Albion which could be used for astronomical calculations such as lunar, solar and planetary longitudes and could predict eclipses. Nicole Oresme (1320–1382) and Jean Buridan (1300–1361) first discussed evidence for the rotation of the Earth, furthermore, Buridan also developed the theory of impetus (predecessor of the modern scientific theory of inertia) which was able to show planets were capable of motion without the intervention of angels. Georg von Peuerbach (1423–1461) and Regiomontanus (1436–1476) helped make astronomical progress instrumental to Copernicus's development of the heliocentric model decades later. Astronomy flourished in the Islamic world and other parts of the world. This led to the emergence of the first astronomical observatories in the Muslim world by the early 9th century. In 964, the Andromeda Galaxy, the largest galaxy in the Local Group, was described by the Persian Muslim astronomer Abd al-Rahman al-Sufi in his Book of Fixed Stars. The SN 1006 supernova, the brightest apparent magnitude stellar event in recorded history, was observed by the Egyptian Arabic astronomer Ali ibn Ridwan and Chinese astronomers in 1006. Some of the prominent Islamic (mostly Persian and Arab) astronomers who made significant contributions to the science include Al-Battani, Thebit, Abd al-Rahman al-Sufi, Biruni, Abū Ishāq Ibrāhīm al-Zarqālī, Al-Birjandi, and the astronomers of the Maragheh and Samarkand observatories. Astronomers during that time introduced many Arabic names now used for individual stars.It is also believed that the ruins at Great Zimbabwe and Timbuktu may have housed astronomical observatories. In Post-classical West Africa, Astronomers studied the movement of stars and relation to seasons, crafting charts of the heavens as well as precise diagrams of orbits of the other planets based on complex mathematical calculations. Songhai historian Mahmud Kati documented a meteor shower in August 1583. Europeans had previously believed that there had been no astronomical observation in sub-Saharan Africa during the pre-colonial Middle Ages, but modern discoveries show otherwise.For over six centuries (from the recovery of ancient learning during the late Middle Ages into the Enlightenment), the Roman Catholic Church gave more financial and social support to the study of astronomy than probably all other institutions. Among the Church's motives was finding the date for Easter.
During the Renaissance, Nicolaus Copernicus proposed a heliocentric model of the solar system. His work was defended by Galileo Galilei and expanded upon by Johannes Kepler. Kepler was the first to devise a system that correctly described the details of the motion of the planets around the Sun. However, Kepler did not succeed in formulating a theory behind the laws he wrote down. It was Isaac Newton, with his invention of celestial dynamics and his law of gravitation, who finally explained the motions of the planets. Newton also developed the reflecting telescope.Improvements in the size and quality of the telescope led to further discoveries. The English astronomer John Flamsteed catalogued over 3000 stars, More extensive star catalogues were produced by Nicolas Louis de Lacaille. The astronomer William Herschel made a detailed catalog of nebulosity and clusters, and in 1781 discovered the planet Uranus, the first new planet found.During the 18–19th centuries, the study of the three-body problem by Leonhard Euler, Alexis Claude Clairaut, and Jean le Rond d'Alembert led to more accurate predictions about the motions of the Moon and planets. This work was further refined by Joseph-Louis Lagrange and Pierre Simon Laplace, allowing the masses of the planets and moons to be estimated from their perturbations.Significant advances in astronomy came about with the introduction of new technology, including the spectroscope and photography. Joseph von Fraunhofer discovered about 600 bands in the spectrum of the Sun in 1814–15, which, in 1859, Gustav Kirchhoff ascribed to the presence of different elements. Stars were proven to be similar to the Earth's own Sun, but with a wide range of temperatures, masses, and sizes.The existence of the Earth's galaxy, the Milky Way, as its own group of stars was only proved in the 20th century, along with the existence of "external" galaxies. The observed recession of those galaxies led to the discovery of the expansion of the Universe. Theoretical astronomy led to speculations on the existence of objects such as black holes and neutron stars, which have been used to explain such observed phenomena as quasars, pulsars, blazars, and radio galaxies. Physical cosmology made huge advances during the 20th century. In the early 1900s the model of the Big Bang theory was formulated, heavily evidenced by cosmic microwave background radiation, Hubble's law, and the cosmological abundances of elements. Space telescopes have enabled measurements in parts of the electromagnetic spectrum normally blocked or blurred by the atmosphere. In February 2016, it was revealed that the LIGO project had detected evidence of gravitational waves in the previous September.
The main source of information about celestial bodies and other objects is visible light, or more generally electromagnetic radiation. Observational astronomy may be categorized according to the corresponding region of the electromagnetic spectrum on which the observations are made. Some parts of the spectrum can be observed from the Earth's surface, while other parts are only observable from either high altitudes or outside the Earth's atmosphere. Specific information on these subfields is given below.
Radio astronomy uses radiation with wavelengths greater than approximately one millimeter, outside the visible range. Radio astronomy is different from most other forms of observational astronomy in that the observed radio waves can be treated as waves rather than as discrete photons. Hence, it is relatively easier to measure both the amplitude and phase of radio waves, whereas this is not as easily done at shorter wavelengths.Although some radio waves are emitted directly by astronomical objects, a product of thermal emission, most of the radio emission that is observed is the result of synchrotron radiation, which is produced when electrons orbit magnetic fields. Additionally, a number of spectral lines produced by interstellar gas, notably the hydrogen spectral line at 21 cm, are observable at radio wavelengths.A wide variety of other objects are observable at radio wavelengths, including supernovae, interstellar gas, pulsars, and active galactic nuclei.
Infrared astronomy is founded on the detection and analysis of infrared radiation, wavelengths longer than red light and outside the range of our vision. The infrared spectrum is useful for studying objects that are too cold to radiate visible light, such as planets, circumstellar disks or nebulae whose light is blocked by dust. The longer wavelengths of infrared can penetrate clouds of dust that block visible light, allowing the observation of young stars embedded in molecular clouds and the cores of galaxies. Observations from the Wide-field Infrared Survey Explorer (WISE) have been particularly effective at unveiling numerous Galactic protostars and their host star clusters. With the exception of infrared wavelengths close to visible light, such radiation is heavily absorbed by the atmosphere, or masked, as the atmosphere itself produces significant infrared emission. Consequently, infrared observatories have to be located in high, dry places on Earth or in space. Some molecules radiate strongly in the infrared. This allows the study of the chemistry of space; more specifically it can detect water in comets.
Historically, optical astronomy, also called visible light astronomy, is the oldest form of astronomy. Images of observations were originally drawn by hand. In the late 19th century and most of the 20th century, images were made using photographic equipment. Modern images are made using digital detectors, particularly using charge-coupled devices (CCDs) and recorded on modern medium. Although visible light itself extends from approximately 4000 Å to 7000 Å (400 nm to 700 nm), that same equipment can be used to observe some near-ultraviolet and near-infrared radiation.
Ultraviolet astronomy employs ultraviolet wavelengths between approximately 100 and 3200 Å (10 to 320 nm). Light at those wavelengths is absorbed by the Earth's atmosphere, requiring observations at these wavelengths to be performed from the upper atmosphere or from space. Ultraviolet astronomy is best suited to the study of thermal radiation and spectral emission lines from hot blue stars (OB stars) that are very bright in this wave band. This includes the blue stars in other galaxies, which have been the targets of several ultraviolet surveys. Other objects commonly observed in ultraviolet light include planetary nebulae, supernova remnants, and active galactic nuclei. However, as ultraviolet light is easily absorbed by interstellar dust, an adjustment of ultraviolet measurements is necessary.
X-ray astronomy uses X-ray wavelengths. Typically, X-ray radiation is produced by synchrotron emission (the result of electrons orbiting magnetic field lines), thermal emission from thin gases above 107 (10 million) kelvins, and thermal emission from thick gases above 107 Kelvin. Since X-rays are absorbed by the Earth's atmosphere, all X-ray observations must be performed from high-altitude balloons, rockets, or X-ray astronomy satellites. Notable X-ray sources include X-ray binaries, pulsars, supernova remnants, elliptical galaxies, clusters of galaxies, and active galactic nuclei.
Gamma ray astronomy observes astronomical objects at the shortest wavelengths of the electromagnetic spectrum. Gamma rays may be observed directly by satellites such as the Compton Gamma Ray Observatory or by specialized telescopes called atmospheric Cherenkov telescopes. The Cherenkov telescopes do not detect the gamma rays directly but instead detect the flashes of visible light produced when gamma rays are absorbed by the Earth's atmosphere.Most gamma-ray emitting sources are actually gamma-ray bursts, objects which only produce gamma radiation for a few milliseconds to thousands of seconds before fading away. Only 10% of gamma-ray sources are non-transient sources. These steady gamma-ray emitters include pulsars, neutron stars, and black hole candidates such as active galactic nuclei.
In addition to electromagnetic radiation, a few other events originating from great distances may be observed from the Earth. In neutrino astronomy, astronomers use heavily shielded underground facilities such as SAGE, GALLEX, and Kamioka II/III for the detection of neutrinos. The vast majority of the neutrinos streaming through the Earth originate from the Sun, but 24 neutrinos were also detected from supernova 1987A. Cosmic rays, which consist of very high energy particles (atomic nuclei) that can decay or be absorbed when they enter the Earth's atmosphere, result in a cascade of secondary particles which can be detected by current observatories. Some future neutrino detectors may also be sensitive to the particles produced when cosmic rays hit the Earth's atmosphere.Gravitational-wave astronomy is an emerging field of astronomy that employs gravitational-wave detectors to collect observational data about distant massive objects. A few observatories have been constructed, such as the Laser Interferometer Gravitational Observatory LIGO. LIGO made its first detection on 14 September 2015, observing gravitational waves from a binary black hole. A second gravitational wave was detected on 26 December 2015 and additional observations should continue but gravitational waves require extremely sensitive instruments.The combination of observations made using electromagnetic radiation, neutrinos or gravitational waves and other complementary information, is known as multi-messenger astronomy.
One of the oldest fields in astronomy, and in all of science, is the measurement of the positions of celestial objects. Historically, accurate knowledge of the positions of the Sun, Moon, planets and stars has been essential in celestial navigation (the use of celestial objects to guide navigation) and in the making of calendars. Careful measurement of the positions of the planets has led to a solid understanding of gravitational perturbations, and an ability to determine past and future positions of the planets with great accuracy, a field known as celestial mechanics. More recently the tracking of near-Earth objects will allow for predictions of close encounters or potential collisions of the Earth with those objects.The measurement of stellar parallax of nearby stars provides a fundamental baseline in the cosmic distance ladder that is used to measure the scale of the Universe. Parallax measurements of nearby stars provide an absolute baseline for the properties of more distant stars, as their properties can be compared. Measurements of the radial velocity and proper motion of stars allows astronomers to plot the movement of these systems through the Milky Way galaxy. Astrometric results are the basis used to calculate the distribution of speculated dark matter in the galaxy.During the 1990s, the measurement of the stellar wobble of nearby stars was used to detect large extrasolar planets orbiting those stars.
Theoretical astronomers use several tools including analytical models and computational numerical simulations; each has its particular advantages. Analytical models of a process are better for giving broader insight into the heart of what is going on. Numerical models reveal the existence of phenomena and effects otherwise unobserved.Theorists in astronomy endeavor to create theoretical models and from the results predict observational consequences of those models. The observation of a phenomenon predicted by a model allows astronomers to select between several alternate or conflicting models as the one best able to describe the phenomena. Theorists also try to generate or modify models to take into account new data. In the case of an inconsistency between the data and model's results, the general tendency is to try to make minimal modifications to the model so that it produces results that fit the data. In some cases, a large amount of inconsistent data over time may lead to total abandonment of a model. Phenomena modeled by theoretical astronomers include: stellar dynamics and evolution; galaxy formation; large-scale distribution of matter in the Universe; origin of cosmic rays; general relativity and physical cosmology, including string cosmology and astroparticle physics. Astrophysical relativity serves as a tool to gauge the properties of large scale structures for which gravitation plays a significant role in physical phenomena investigated and as the basis for black hole (astro)physics and the study of gravitational waves. Some widely accepted and studied theories and models in astronomy, now included in the Lambda-CDM model are the Big Bang, dark matter and fundamental theories of physics. A few examples of this process: Along with Cosmic inflation, dark matter and dark energy are the current leading topics in astronomy, as their discovery and controversy originated during the study of the galaxies.
Astrophysics is the branch of astronomy that employs the principles of physics and chemistry "to ascertain the nature of the astronomical objects, rather than their positions or motions in space". Among the objects studied are the Sun, other stars, galaxies, extrasolar planets, the interstellar medium and the cosmic microwave background. Their emissions are examined across all parts of the electromagnetic spectrum, and the properties examined include luminosity, density, temperature, and chemical composition. Because astrophysics is a very broad subject, astrophysicists typically apply many disciplines of physics, including mechanics, electromagnetism, statistical mechanics, thermodynamics, quantum mechanics, relativity, nuclear and particle physics, and atomic and molecular physics. In practice, modern astronomical research often involves a substantial amount of work in the realms of theoretical and observational physics. Some areas of study for astrophysicists include their attempts to determine the properties of dark matter, dark energy, and black holes; whether or not time travel is possible, wormholes can form, or the multiverse exists; and the origin and ultimate fate of the universe. Topics also studied by theoretical astrophysicists include Solar System formation and evolution; stellar dynamics and evolution; galaxy formation and evolution; magnetohydrodynamics; large-scale structure of matter in the universe; origin of cosmic rays; general relativity and physical cosmology, including string cosmology and astroparticle physics.
Astrochemistry is the study of the abundance and reactions of molecules in the Universe, and their interaction with radiation. The discipline is an overlap of astronomy and chemistry. The word "astrochemistry" may be applied to both the Solar System and the interstellar medium. The study of the abundance of elements and isotope ratios in Solar System objects, such as meteorites, is also called cosmochemistry, while the study of interstellar atoms and molecules and their interaction with radiation is sometimes called molecular astrophysics. The formation, atomic and chemical composition, evolution and fate of molecular gas clouds is of special interest, because it is from these clouds that solar systems form. Studies in this field contribute to the understanding of the formation of the Solar System, Earth's origin and geology, abiogenesis, and the origin of climate and oceans.
Astrobiology is an interdisciplinary scientific field concerned with the origins, early evolution, distribution, and future of life in the universe. Astrobiology considers the question of whether extraterrestrial life exists, and how humans can detect it if it does. The term exobiology is similar.Astrobiology makes use of molecular biology, biophysics, biochemistry, chemistry, astronomy, physical cosmology, exoplanetology and geology to investigate the possibility of life on other worlds and help recognize biospheres that might be different from that on Earth. The origin and early evolution of life is an inseparable part of the discipline of astrobiology. Astrobiology concerns itself with interpretation of existing scientific data, and although speculation is entertained to give context, astrobiology concerns itself primarily with hypotheses that fit firmly into existing scientific theories. This interdisciplinary field encompasses research on the origin of planetary systems, origins of organic compounds in space, rock-water-carbon interactions, abiogenesis on Earth, planetary habitability, research on biosignatures for life detection, and studies on the potential for life to adapt to challenges on Earth and in outer space.
Cosmology (from the Greek κόσμος (kosmos) "world, universe" and λόγος (logos) "word, study" or literally "logic") could be considered the study of the Universe as a whole. Observations of the large-scale structure of the Universe, a branch known as physical cosmology, have provided a deep understanding of the formation and evolution of the cosmos. Fundamental to modern cosmology is the well-accepted theory of the Big Bang, wherein our Universe began at a single point in time, and thereafter expanded over the course of 13.8 billion years to its present condition. The concept of the Big Bang can be traced back to the discovery of the microwave background radiation in 1965.In the course of this expansion, the Universe underwent several evolutionary stages. In the very early moments, it is theorized that the Universe experienced a very rapid cosmic inflation, which homogenized the starting conditions. Thereafter, nucleosynthesis produced the elemental abundance of the early Universe. (See also nucleocosmochronology.) When the first neutral atoms formed from a sea of primordial ions, space became transparent to radiation, releasing the energy viewed today as the microwave background radiation. The expanding Universe then underwent a Dark Age due to the lack of stellar energy sources.A hierarchical structure of matter began to form from minute variations in the mass density of space. Matter accumulated in the densest regions, forming clouds of gas and the earliest stars, the Population III stars. These massive stars triggered the reionization process and are believed to have created many of the heavy elements in the early Universe, which, through nuclear decay, create lighter elements, allowing the cycle of nucleosynthesis to continue longer.Gravitational aggregations clustered into filaments, leaving voids in the gaps. Gradually, organizations of gas and dust merged to form the first primitive galaxies. Over time, these pulled in more matter, and were often organized into groups and clusters of galaxies, then into larger-scale superclusters.Various fields of physics are crucial to studying the universe. Interdisciplinary studies involve the fields of quantum mechanics, particle physics, plasma physics, condensed matter physics, statistical mechanics, optics, and nuclear physics. Fundamental to the structure of the Universe is the existence of dark matter and dark energy. These are now thought to be its dominant components, forming 96% of the mass of the Universe. For this reason, much effort is expended in trying to understand the physics of these components.
The study of objects outside our galaxy is a branch of astronomy concerned with the formation and evolution of Galaxies, their morphology (description) and classification, the observation of active galaxies, and at a larger scale, the groups and clusters of galaxies. Finally, the latter is important for the understanding of the large-scale structure of the cosmos. Most galaxies are organized into distinct shapes that allow for classification schemes. They are commonly divided into spiral, elliptical and Irregular galaxies.As the name suggests, an elliptical galaxy has the cross-sectional shape of an ellipse. The stars move along random orbits with no preferred direction. These galaxies contain little or no interstellar dust, few star-forming regions, and older stars. Elliptical galaxies are more commonly found at the core of galactic clusters, and may have been formed through mergers of large galaxies. A spiral galaxy is organized into a flat, rotating disk, usually with a prominent bulge or bar at the center, and trailing bright arms that spiral outward. The arms are dusty regions of star formation within which massive young stars produce a blue tint. Spiral galaxies are typically surrounded by a halo of older stars. Both the Milky Way and one of our nearest galaxy neighbors, the Andromeda Galaxy, are spiral galaxies. Irregular galaxies are chaotic in appearance, and are neither spiral nor elliptical. About a quarter of all galaxies are irregular, and the peculiar shapes of such galaxies may be the result of gravitational interaction. An active galaxy is a formation that emits a significant amount of its energy from a source other than its stars, dust and gas. It is powered by a compact region at the core, thought to be a super-massive black hole that is emitting radiation from in-falling material. A radio galaxy is an active galaxy that is very luminous in the radio portion of the spectrum, and is emitting immense plumes or lobes of gas. Active galaxies that emit shorter frequency, high-energy radiation include Seyfert galaxies, Quasars, and Blazars. Quasars are believed to be the most consistently luminous objects in the known universe.The large-scale structure of the cosmos is represented by groups and clusters of galaxies. This structure is organized into a hierarchy of groupings, with the largest being the superclusters. The collective matter is formed into filaments and walls, leaving large voids between.
The Solar System orbits within the Milky Way, a barred spiral galaxy that is a prominent member of the Local Group of galaxies. It is a rotating mass of gas, dust, stars and other objects, held together by mutual gravitational attraction. As the Earth is located within the dusty outer arms, there are large portions of the Milky Way that are obscured from view. In the center of the Milky Way is the core, a bar-shaped bulge with what is believed to be a supermassive black hole at its center. This is surrounded by four primary arms that spiral from the core. This is a region of active star formation that contains many younger, population I stars. The disk is surrounded by a spheroid halo of older, population II stars, as well as relatively dense concentrations of stars known as globular clusters.Between the stars lies the interstellar medium, a region of sparse matter. In the densest regions, molecular clouds of molecular hydrogen and other elements create star-forming regions. These begin as a compact pre-stellar core or dark nebulae, which concentrate and collapse (in volumes determined by the Jeans length) to form compact protostars.As the more massive stars appear, they transform the cloud into an H II region (ionized atomic hydrogen) of glowing gas and plasma. The stellar wind and supernova explosions from these stars eventually cause the cloud to disperse, often leaving behind one or more young open clusters of stars. These clusters gradually disperse, and the stars join the population of the Milky Way.Kinematic studies of matter in the Milky Way and other galaxies have demonstrated that there is more mass than can be accounted for by visible matter. A dark matter halo appears to dominate the mass, although the nature of this dark matter remains undetermined.
The study of stars and stellar evolution is fundamental to our understanding of the Universe. The astrophysics of stars has been determined through observation and theoretical understanding; and from computer simulations of the interior. Star formation occurs in dense regions of dust and gas, known as giant molecular clouds. When destabilized, cloud fragments can collapse under the influence of gravity, to form a protostar. A sufficiently dense, and hot, core region will trigger nuclear fusion, thus creating a main-sequence star.Almost all elements heavier than hydrogen and helium were created inside the cores of stars.The characteristics of the resulting star depend primarily upon its starting mass. The more massive the star, the greater its luminosity, and the more rapidly it fuses its hydrogen fuel into helium in its core. Over time, this hydrogen fuel is completely converted into helium, and the star begins to evolve. The fusion of helium requires a higher core temperature. A star with a high enough core temperature will push its outer layers outward while increasing its core density. The resulting red giant formed by the expanding outer layers enjoys a brief life span, before the helium fuel in the core is in turn consumed. Very massive stars can also undergo a series of evolutionary phases, as they fuse increasingly heavier elements.The final fate of the star depends on its mass, with stars of mass greater than about eight times the Sun becoming core collapse supernovae; while smaller stars blow off their outer layers and leave behind the inert core in the form of a white dwarf. The ejection of the outer layers forms a planetary nebula. The remnant of a supernova is a dense neutron star, or, if the stellar mass was at least three times that of the Sun, a black hole. Closely orbiting binary stars can follow more complex evolutionary paths, such as mass transfer onto a white dwarf companion that can potentially cause a supernova. Planetary nebulae and supernovae distribute the "metals" produced in the star by fusion to the interstellar medium; without them, all new stars (and their planetary systems) would be formed from hydrogen and helium alone.
At a distance of about eight light-minutes, the most frequently studied star is the Sun, a typical main-sequence dwarf star of stellar class G2 V, and about 4.6 billion years (Gyr) old. The Sun is not considered a variable star, but it does undergo periodic changes in activity known as the sunspot cycle. This is an 11-year oscillation in sunspot number. Sunspots are regions of lower-than- average temperatures that are associated with intense magnetic activity.The Sun has steadily increased in luminosity by 40% since it first became a main-sequence star. The Sun has also undergone periodic changes in luminosity that can have a significant impact on the Earth. The Maunder minimum, for example, is believed to have caused the Little Ice Age phenomenon during the Middle Ages.The visible outer surface of the Sun is called the photosphere. Above this layer is a thin region known as the chromosphere. This is surrounded by a transition region of rapidly increasing temperatures, and finally by the super-heated corona. At the center of the Sun is the core region, a volume of sufficient temperature and pressure for nuclear fusion to occur. Above the core is the radiation zone, where the plasma conveys the energy flux by means of radiation. Above that is the convection zone where the gas material transports energy primarily through physical displacement of the gas known as convection. It is believed that the movement of mass within the convection zone creates the magnetic activity that generates sunspots.A solar wind of plasma particles constantly streams outward from the Sun until, at the outermost limit of the Solar System, it reaches the heliopause. As the solar wind passes the Earth, it interacts with the Earth's magnetic field (magnetosphere) and deflects the solar wind, but traps some creating the Van Allen radiation belts that envelop the Earth. The aurora are created when solar wind particles are guided by the magnetic flux lines into the Earth's polar regions where the lines then descend into the atmosphere.
Planetary science is the study of the assemblage of planets, moons, dwarf planets, comets, asteroids, and other bodies orbiting the Sun, as well as extrasolar planets. The Solar System has been relatively well-studied, initially through telescopes and then later by spacecraft. This has provided a good overall understanding of the formation and evolution of the Sun's planetary system, although many new discoveries are still being made.The Solar System is subdivided into the inner planets, the asteroid belt, and the outer planets. The inner terrestrial planets consist of Mercury, Venus, Earth, and Mars. The outer gas giant planets are Jupiter, Saturn, Uranus, and Neptune. Beyond Neptune lies the Kuiper belt, and finally the Oort Cloud, which may extend as far as a light-year. The planets were formed 4.6 billion years ago in the protoplanetary disk that surrounded the early Sun. Through a process that included gravitational attraction, collision, and accretion, the disk formed clumps of matter that, with time, became protoplanets. The radiation pressure of the solar wind then expelled most of the unaccreted matter, and only those planets with sufficient mass retained their gaseous atmosphere. The planets continued to sweep up, or eject, the remaining matter during a period of intense bombardment, evidenced by the many impact craters on the Moon. During this period, some of the protoplanets may have collided and one such collision may have formed the Moon.Once a planet reaches sufficient mass, the materials of different densities segregate within, during planetary differentiation. This process can form a stony or metallic core, surrounded by a mantle and an outer crust. The core may include solid and liquid regions, and some planetary cores generate their own magnetic field, which can protect their atmospheres from solar wind stripping.A planet or moon's interior heat is produced from the collisions that created the body, by the decay of radioactive materials (e.g. uranium, thorium, and 26Al), or tidal heating caused by interactions with other bodies. Some planets and moons accumulate enough heat to drive geologic processes such as volcanism and tectonics. Those that accumulate or retain an atmosphere can also undergo surface erosion from wind or water. Smaller bodies, without tidal heating, cool more quickly; and their geological activity ceases with the exception of impact cratering.
Astronomy and astrophysics have developed significant interdisciplinary links with other major scientific fields. Archaeoastronomy is the study of ancient or traditional astronomies in their cultural context, utilizing archaeological and anthropological evidence. Astrobiology is the study of the advent and evolution of biological systems in the Universe, with particular emphasis on the possibility of non-terrestrial life. Astrostatistics is the application of statistics to astrophysics to the analysis of vast amount of observational astrophysical data. The study of chemicals found in space, including their formation, interaction and destruction, is called astrochemistry. These substances are usually found in molecular clouds, although they may also appear in low temperature stars, brown dwarfs and planets. Cosmochemistry is the study of the chemicals found within the Solar System, including the origins of the elements and variations in the isotope ratios. Both of these fields represent an overlap of the disciplines of astronomy and chemistry. As "forensic astronomy", finally, methods from astronomy have been used to solve problems of law and history.
Astronomy is one of the sciences to which amateurs can contribute the most.Collectively, amateur astronomers observe a variety of celestial objects and phenomena sometimes with equipment that they build themselves. Common targets of amateur astronomers include the Sun, the Moon, planets, stars, comets, meteor showers, and a variety of deep-sky objects such as star clusters, galaxies, and nebulae. Astronomy clubs are located throughout the world and many have programs to help their members set up and complete observational programs including those to observe all the objects in the Messier (110 objects) or Herschel 400 catalogues of points of interest in the night sky. One branch of amateur astronomy, amateur astrophotography, involves the taking of photos of the night sky. Many amateurs like to specialize in the observation of particular objects, types of objects, or types of events which interest them.Most amateurs work at visible wavelengths, but a small minority experiment with wavelengths outside the visible spectrum. This includes the use of infrared filters on conventional telescopes, and also the use of radio telescopes. The pioneer of amateur radio astronomy was Karl Jansky, who started observing the sky at radio wavelengths in the 1930s. A number of amateur astronomers use either homemade telescopes or use radio telescopes which were originally built for astronomy research but which are now available to amateurs (e.g. the One-Mile Telescope).Amateur astronomers continue to make scientific contributions to the field of astronomy and it is one of the few scientific disciplines where amateurs can still make significant contributions. Amateurs can make occultation measurements that are used to refine the orbits of minor planets. They can also discover comets, and perform regular observations of variable stars. Improvements in digital technology have allowed amateurs to make impressive advances in the field of astrophotography.
Although the scientific discipline of astronomy has made tremendous strides in understanding the nature of the Universe and its contents, there remain some important unanswered questions. Answers to these may require the construction of new ground- and space-based instruments, and possibly new developments in theoretical and experimental physics. What is the origin of the stellar mass spectrum? That is, why do astronomers observe the same distribution of stellar masses—the initial mass function—apparently regardless of the initial conditions? A deeper understanding of the formation of stars and planets is needed. Is there other life in the Universe? Especially, is there other intelligent life? If so, what is the explanation for the Fermi paradox? The existence of life elsewhere has important scientific and philosophical implications. Is the Solar System normal or atypical? What is the nature of dark matter and dark energy? These dominate the evolution and fate of the cosmos, yet their true nature remains unknown. What will be the ultimate fate of the universe? How did the first galaxies form? How did supermassive black holes form? What is creating the ultra-high-energy cosmic rays? Why is the abundance of lithium in the cosmos four times lower than predicted by the standard Big Bang model? What really happens beyond the event horizon?
NASA/IPAC Extragalactic Database (NED) (NED-Distances) International Year of Astronomy 2009 IYA2009 Main website Cosmic Journey: A History of Scientific Cosmology from the American Institute of Physics Southern Hemisphere Astronomy Celestia Motherlode Educational site for Astronomical journeys through space Kroto, Harry, Astrophysical Chemistry Lecture Series. Core books and Core journals in Astronomy, from the Smithsonian/NASA Astrophysics Data System A Journey with Fred Hoyle by Wickramasinghe, Chandra. Astronomy books from the History of Science Collection at Linda Hall Library
The Atlantic Ocean is the second-largest of the world's oceans, with an area of about 106,460,000 km2 (41,100,000 sq mi). It covers approximately 20 percent of Earth's surface and about 29 percent of its water surface area. It separates the "Old World" from the "New World". The Atlantic Ocean occupies an elongated, S-shaped basin extending longitudinally between Europe and Africa to the east, and the Americas to the west. As one component of the interconnected World Ocean, it is connected in the north to the Arctic Ocean, to the Pacific Ocean in the southwest, the Indian Ocean in the southeast, and the Southern Ocean in the south (other definitions describe the Atlantic as extending southward to Antarctica). The Equatorial Counter Current subdivides it into the North(ern) Atlantic Ocean and the South(ern) Atlantic Ocean at about 8°N.Scientific explorations of the Atlantic include the Challenger expedition, the German Meteor expedition, Columbia University's Lamont-Doherty Earth Observatory and the United States Navy Hydrographic Office.
The oldest known mentions of an "Atlantic" sea come from Stesichorus around mid-sixth century BC (Sch. A. R. 1. 211): Atlantikôi pelágei (Greek: Ἀτλαντικῷ πελάγει; English: 'the Atlantic sea'; etym. 'Sea of Atlantis') and in The Histories of Herodotus around 450 BC (Hdt. 1.202.4): Atlantis thalassa (Greek: Ἀτλαντὶς θάλασσα; English: 'Sea of Atlantis' or 'the Atlantis sea') where the name refers to "the sea beyond the pillars of Heracles" which is said to be part of the sea that surrounds all land. In these uses, the name refers to Atlas, the Titan in Greek mythology, who supported the heavens and who later appeared as a frontispiece in Medieval maps and also lent his name to modern atlases. On the other hand, to early Greek sailors and in Ancient Greek mythological literature such as the Iliad and the Odyssey, this all-encompassing ocean was instead known as Oceanus, the gigantic river that encircled the world; in contrast to the enclosed seas well known to the Greeks: the Mediterranean and the Black Sea. In contrast, the term "Atlantic" originally referred specifically to the Atlas Mountains in Morocco and the sea off the Strait of Gibraltar and the North African coast. The Greek word thalassa has been reused by scientists for the huge Panthalassa ocean that surrounded the supercontinent Pangaea hundreds of millions of years ago. The term "Aethiopian Ocean", derived from Ancient Ethiopia, was applied to the Southern Atlantic as late as the mid-19th century. During the Age of Discovery, the Atlantic was also known to English cartographers as the Great Western Ocean.The term The Pond is often used by British and American speakers in context to the Atlantic Ocean, as a form of meiosis, or sarcastic understatement. The term dates to as early as 1640, first appearing in print in pamphlet released during the reign of Charles I, and reproduced in 1869 in Nehemiah Wallington's Historical Notices of Events Occurring Chiefly in The Reign of Charles I, where "great Pond" is used in reference to the Atlantic Ocean by Francis Windebank, Charles I's Secretary of State.
The MAR divides the Atlantic longitudinally into two-halves, in each of which a series of basins are delimited by secondary, transverse ridges. The MAR reaches above 2,000 m (6,600 ft) along most of its length, but is interrupted by larger transform faults at two places: the Romanche Trench near the Equator and the Gibbs Fracture Zone at 53°N. The MAR is a barrier for bottom water, but at these two transform faults deep water currents can pass from one side to the other.The MAR rises 2–3 km (1.2–1.9 mi) above the surrounding ocean floor and its rift valley is the divergent boundary between the North American and Eurasian plates in the North Atlantic and the South American and African plates in the South Atlantic. The MAR produces basaltic volcanoes in Eyjafjallajökull, Iceland, and pillow lava on the ocean floor. The depth of water at the apex of the ridge is less than 2,700 m (1,500 fathoms; 8,900 ft) in most places, while the bottom of the ridge is three times as deep.The MAR is intersected by two perpendicular ridges: the Azores–Gibraltar Transform Fault, the boundary between the Nubian and Eurasian plates, intersects the MAR at the Azores Triple Junction, on either side of the Azores microplate, near the 40°N. A much vaguer, nameless boundary, between the North American and South American plates, intersects the MAR near or just north of the Fifteen-Twenty Fracture Zone, approximately at 16°N.In the 1870s, the Challenger expedition discovered parts of what is now known as the Mid-Atlantic Ridge, or: An elevated ridge rising to an average height of about 1,900 fathoms [3,500 m; 11,400 ft] below the surface traverses the basins of the North and South Atlantic in a meridianal direction from Cape Farewell, probably its far south at least as Gough Island, following roughly the outlines of the coasts of the Old and the New Worlds. The remainder of the ridge was discovered in the 1920s by the German Meteor expedition using echo-sounding equipment. The exploration of the MAR in the 1950s led to the general acceptance of seafloor spreading and plate tectonics.Most of the MAR runs under water but where it reaches the surfaces it has produced volcanic islands. While nine of these have collectively been nominated a World Heritage Site for their geological value, four of them are considered of "Outstanding Universal Value" based on their cultural and natural criteria: Þingvellir, Iceland; Landscape of the Pico Island Vineyard Culture, Portugal; Gough and Inaccessible Islands, United Kingdom; and Brazilian Atlantic Islands: Fernando de Noronha and Atol das Rocas Reserves, Brazil.
Continental shelves in the Atlantic are wide off Newfoundland, southernmost South America, and north-eastern Europe. In the western Atlantic carbonate platforms dominate large areas, for example, the Blake Plateau and Bermuda Rise. The Atlantic is surrounded by passive margins except at a few locations where active margins form deep trenches: the Puerto Rico Trench (8,376 m or 27,480 ft maximum depth) in the western Atlantic and South Sandwich Trench (8,264 m or 27,113 ft) in the South Atlantic. There are numerous submarine canyons off north-eastern North America, western Europe, and north-western Africa. Some of these canyons extend along the continental rises and farther into the abyssal plains as deep-sea channels.In 1922 a historic moment in cartography and oceanography occurred. The USS Stewart used a Navy Sonic Depth Finder to draw a continuous map across the bed of the Atlantic. This involved little guesswork because the idea of sonar is straight forward with pulses being sent from the vessel, which bounce off the ocean floor, then return to the vessel. The deep ocean floor is thought to be fairly flat with occasional deeps, abyssal plains, trenches, seamounts, basins, plateaus, canyons, and some guyots. Various shelves along the margins of the continents constitute about 11% of the bottom topography with few deep channels cut across the continental rise. The mean depth between 60°N and 60°S is 3,730 m (12,240 ft), or close to the average for the global ocean, with a modal depth between 4,000 and 5,000 m (13,000 and 16,000 ft).In the South Atlantic the Walvis Ridge and Rio Grande Rise form barriers to ocean currents. The Laurentian Abyss is found off the eastern coast of Canada.
On average, the Atlantic is the saltiest major ocean; surface water salinity in the open ocean ranges from 33 to 37 parts per thousand (3.3–3.7%) by mass and varies with latitude and season. Evaporation, precipitation, river inflow and sea ice melting influence surface salinity values. Although the lowest salinity values are just north of the equator (because of heavy tropical rainfall), in general, the lowest values are in the high latitudes and along coasts where large rivers enter. Maximum salinity values occur at about 25° north and south, in subtropical regions with low rainfall and high evaporation.The high surface salinity in the Atlantic, on which the Atlantic thermohaline circulation is dependent, is maintained by two processes: the Agulhas Leakage/Rings, which brings salty Indian Ocean waters into the South Atlantic, and the "Atmospheric Bridge", which evaporates subtropical Atlantic waters and exports it to the Pacific.
The Atlantic Ocean consists of four major, upper water masses with distinct temperature and salinity. The Atlantic Subarctic Upper Water in the northernmost North Atlantic is the source for Subarctic Intermediate Water and North Atlantic Intermediate Water. North Atlantic Central Water can be divided into the Eastern and Western North Atlantic central Water since the western part is strongly affected by the Gulf Stream and therefore the upper layer is closer to underlying fresher subpolar intermediate water. The eastern water is saltier because of its proximity to Mediterranean Water. North Atlantic Central Water flows into South Atlantic Central Water at 15°N.There are five intermediate waters: four low-salinity waters formed at subpolar latitudes and one high-salinity formed through evaporation. Arctic Intermediate Water, flows from north to become the source for North Atlantic Deep Water south of the Greenland-Scotland sill. These two intermediate waters have different salinity in the western and eastern basins. The wide range of salinities in the North Atlantic is caused by the asymmetry of the northern subtropical gyre and the large number of contributions from a wide range of sources: Labrador Sea, Norwegian-Greenland Sea, Mediterranean, and South Atlantic Intermediate Water.The North Atlantic Deep Water (NADW) is a complex of four water masses, two that form by deep convection in the open ocean — Classical and Upper Labrador Sea Water — and two that form from the inflow of dense water across the Greenland-Iceland-Scotland sill — Denmark Strait and Iceland-Scotland Overflow Water. Along its path across Earth the composition of the NADW is affected by other water masses, especially Antarctic Bottom Water and Mediterranean Overflow Water. The NADW is fed by a flow of warm shallow water into the northern North Atlantic which is responsible for the anomalous warm climate in Europe. Changes in the formation of NADW have been linked to global climate changes in the past. Since man-made substances were introduced into the environment, the path of the NADW can be traced throughout its course by measuring tritium and radiocarbon from nuclear weapon tests in the 1960s and CFCs.
The clockwise warm-water North Atlantic Gyre occupies the northern Atlantic, and the counter-clockwise warm-water South Atlantic Gyre appears in the southern Atlantic.In the North Atlantic, surface circulation is dominated by three inter-connected currents: the Gulf Stream which flows north-east from the North American coast at Cape Hatteras; the North Atlantic Current, a branch of the Gulf Stream which flows northward from the Grand Banks; and the Subpolar Front, an extension of the North Atlantic Current, a wide, vaguely defined region separating the subtropical gyre from the subpolar gyre. This system of currents transport warm water into the North Atlantic, without which temperatures in the North Atlantic and Europe would plunge dramatically. North of the North Atlantic Gyre, the cyclonic North Atlantic Subpolar Gyre plays a key role in climate variability. It is governed by ocean currents from marginal seas and regional topography, rather than being steered by wind, both in the deep ocean and at sea level. The subpolar gyre forms an important part of the global thermohaline circulation. Its eastern portion includes eddying branches of the North Atlantic Current which transport warm, saline waters from the subtropics to the north-eastern Atlantic. There this water is cooled during winter and forms return currents that merge along the eastern continental slope of Greenland where they form an intense (40–50 Sv) current which flows around the continental margins of the Labrador Sea. A third of this water becomes part of the deep portion of the North Atlantic Deep Water (NADW). The NADW, in its turn, feeds the meridional overturning circulation (MOC), the northward heat transport of which is threatened by anthropogenic climate change. Large variations in the subpolar gyre on a decade-century scale, associated with the North Atlantic oscillation, are especially pronounced in Labrador Sea Water, the upper layers of the MOC.The South Atlantic is dominated by the anti-cyclonic southern subtropical gyre. The South Atlantic Central Water originates in this gyre, while Antarctic Intermediate Water originates in the upper layers of the circumpolar region, near the Drake Passage and the Falkland Islands. Both these currents receive some contribution from the Indian Ocean. On the African east coast the small cyclonic Angola Gyre lies embedded in the large subtropical gyre. The southern subtropical gyre is partly masked by a wind-induced Ekman layer. The residence time of the gyre is 4.4–8.5 years. North Atlantic Deep Water flows southward below the thermocline of the subtropical gyre.
The Sargasso Sea in the western North Atlantic can be defined as the area where two species of Sargassum (S. fluitans and natans) float, an area 4,000 km (2,500 mi) wide and encircled by the Gulf Stream, North Atlantic Drift, and North Equatorial Current. This population of seaweed probably originated from Tertiary ancestors on the European shores of the former Tethys Ocean and has, if so, maintained itself by vegetative growth, floating in the ocean for millions of years. Other species endemic to the Sargasso Sea include the sargassum fish, a predator with algae-like appendages which hovers motionless among the Sargassum. Fossils of similar fishes have been found in fossil bays of the former Tethys Ocean, in what is now the Carpathian region, that were similar to the Sargasso Sea. It is possible that the population in the Sargasso Sea migrated to the Atlantic as the Tethys closed at the end of the Miocene around 17 Ma. The origin of the Sargasso fauna and flora remained enigmatic for centuries. The fossils found in the Carpathians in the mid-20th century, often called the "quasi-Sargasso assemblage", finally showed that this assemblage originated in the Carpathian Basin from where it migrated over Sicily to the Central Atlantic where it evolved into modern species of the Sargasso Sea.The location of the spawning ground for European eels remained unknown for decades. In the early 19th century it was discovered that the southern Sargasso Sea is the spawning ground for both the European and American eel and that the former migrate more than 5,000 km (3,100 mi) and the latter 2,000 km (1,200 mi). Ocean currents such as the Gulf Stream transport eel larvae from the Sargasso Sea to foraging areas in North America, Europe, and Northern Africa. Recent but disputed research suggests that eels possibly use Earth's magnetic field to navigate through the ocean both as larvae and as adults.
Climate is influenced by the temperatures of the surface waters and water currents as well as winds. Because of the ocean's great capacity to store and release heat, maritime climates are more moderate and have less extreme seasonal variations than inland climates. Precipitation can be approximated from coastal weather data and air temperature from water temperatures.The oceans are the major source of the atmospheric moisture that is obtained through evaporation. Climatic zones vary with latitude; the warmest zones stretch across the Atlantic north of the equator. The coldest zones are in high latitudes, with the coldest regions corresponding to the areas covered by sea ice. Ocean currents influence the climate by transporting warm and cold waters to other regions. The winds that are cooled or warmed when blowing over these currents influence adjacent land areas.The Gulf Stream and its northern extension towards Europe, the North Atlantic Drift is thought to have at least some influence on climate. For example, the Gulf Stream helps moderate winter temperatures along the coastline of southeastern North America, keeping it warmer in winter along the coast than inland areas. The Gulf Stream also keeps extreme temperatures from occurring on the Florida Peninsula. In the higher latitudes, the North Atlantic Drift, warms the atmosphere over the oceans, keeping the British Isles and north-western Europe mild and cloudy, and not severely cold in winter like other locations at the same high latitude. The cold water currents contribute to heavy fog off the coast of eastern Canada (the Grand Banks of Newfoundland area) and Africa's north-western coast. In general, winds transport moisture and air over land areas.
Icebergs are common from early February to the end of July across the shipping lanes near the Grand Banks of Newfoundland. The ice season is longer in the polar regions, but there is little shipping in those areas.Hurricanes are a hazard in the western parts of the North Atlantic during the summer and autumn. Due to a consistently strong wind shear and a weak Intertropical Convergence Zone, South Atlantic tropical cyclones are rare.
The Atlantic Ocean is underlain mostly by dense mafic oceanic crust made up of basalt and gabbro and overlain by fine clay, silt and siliceous ooze on the abyssal plain. The continental margins and continental shelf mark lower density, but greater thickness felsic continental rock that often much older than that of the seafloor. The oldest oceanic crust in the Atlantic is up to 145 million years and situated off the west coast of Africa and east coast of North America, or on either side of the South Atlantic.In many places, the continental shelf and continental slope are covered in thick sedimentary layers. For instance, on the North American side of the ocean, large carbonate deposits formed in warm shallow waters such as Florida and the Bahamas, while coarse river outwash sands and silt are common in shallow shelf areas like the Georges Bank. Coarse sand, boulders, and rocks were transported into some areas, such as off the coast of Nova Scotia or the Gulf of Maine during the Pleistocene ice ages.
The break-up of Pangaea began in the Central Atlantic, between North America and Northwest Africa, where rift basins opened during the Late Triassic and Early Jurassic. This period also saw the first stages of the uplift of the Atlas Mountains. The exact timing is controversial with estimates ranging from 200 to 170 Ma.The opening of the Atlantic Ocean coincided with the initial break-up of the supercontinent Pangaea, both of which were initiated by the eruption of the Central Atlantic Magmatic Province (CAMP), one of the most extensive and voluminous large igneous provinces in Earth's history associated with the Triassic–Jurassic extinction event, one of Earth's major extinction events. Theoliitic dikes, sills, and lava flows from the CAMP eruption at 200 Ma have been found in West Africa, eastern North America, and northern South America. The extent of the volcanism has been estimated to 4.5×106 km2 (1.7×106 sq mi) of which 2.5×106 km2 (9.7×105 sq mi) covered what is now northern and central Brazil.The formation of the Central American Isthmus closed the Central American Seaway at the end of the Pliocene 2.8 Ma ago. The formation of the isthmus resulted in the migration and extinction of many land-living animals, known as the Great American Interchange, but the closure of the seaway resulted in a "Great American Schism" as it affected ocean currents, salinity, and temperatures in both the Atlantic and Pacific. Marine organisms on both sides of the isthmus became isolated and either diverged or went extinct.
Geologically, the Northern Atlantic is the area delimited to the south by two conjugate margins, Newfoundland and Iberia, and to the north by the Arctic Eurasian Basin. The opening of the Northern Atlantic closely followed the margins of its predecessor, the Iapetus Ocean, and spread from the Central Atlantic in six stages: Iberia–Newfoundland, Porcupine–North America, Eurasia–Greenland, Eurasia–North America. Active and inactive spreading systems in this area are marked by the interaction with the Iceland hotspot.Seafloor spreading led to the extension of the crust and formations of troughs and sedimentary basins. The Rockall Trough opened between 105 and 84 million years ago although along the rift failed along with one leading into the Bay of Biscay. Spreading began opening the Labrador Sea around 61 million years ago, continuing until 36 million years ago. Geologists distinguish two magmatic phases. One from 62 to 58 million years ago predates the separation of Greenland from northern Europe while the second from 56 to 52 million years ago happened as the separation occurred. Iceland began to form 62 million years ago due to a particularly concentrated mantle plume. Large quantities of basalt erupted at this time period are found on Baffin Island, Greenland, the Faroe Islands, and Scotland, with ash falls in Western Europe acting as a stratigraphic marker. The opening of the North Atlantic caused significant uplift of continental crust along the coast. For instance, in spite of 7 km thick basalt, Gunnbjorn Field in East Greenland is the highest point on the island, elevated enough that it exposes older Mesozoic sedimentary rocks at its base, similar to old lava fields above sedimentary rocks in the uplifted Hebrides of western Scotland.
West Gondwana (South America and Africa) broke up in the Early Cretaceous to form the South Atlantic. The apparent fit between the coastlines of the two continents was noted on the first maps that included the South Atlantic and it was also the subject of the first computer-assisted plate tectonic reconstructions in 1965. This magnificent fit, however, has since then proven problematic and later reconstructions have introduced various deformation zones along the shorelines to accommodate the northward-propagating break-up. Intra-continental rifts and deformations have also been introduced to subdivide both continental plates into sub-plates.Geologically the South Atlantic can be divided into four segments: Equatorial segment, from 10°N to the Romanche Fracture Zone (RFZ);; Central segment, from RFZ to Florianopolis Fracture Zone (FFZ, north of Walvis Ridge and Rio Grande Rise); Southern segment, from FFZ to the Agulhas-Falkland Fracture Zone (AFFZ); and Falkland segment, south of AFFZ.In the southern segment the Early Cretaceous (133–130 Ma) intensive magmatism of the Paraná–Etendeka Large Igneous Province produced by the Tristan hotspot resulted in an estimated volume of 1.5×106 to 2.0×106 km3 (3.6×105 to 4.8×105 cu mi). It covered an area of 1.2×106 to 1.6×106 km2 (4.6×105 to 6.2×105 sq mi) in Brazil, Paraguay, and Uruguay and 0.8×105 km2 (3.1×104 sq mi) in Africa. Dyke swarms in Brazil, Angola, eastern Paraguay, and Namibia, however, suggest the LIP originally covered a much larger area and also indicate failed rifts in all these areas. Associated offshore basaltic flows reach as far south as the Falkland Islands and South Africa. Traces of magmatism in both offshore and onshore basins in the central and southern segments have been dated to 147–49 Ma with two peaks between 143 and 121 Ma and 90–60 Ma.In the Falkland segment rifting began with dextral movements between the Patagonia and Colorado sub-plates between the Early Jurassic (190 Ma) and the Early Cretaceous (126.7 Ma). Around 150 Ma sea-floor spreading propagated northward into the southern segment. No later than 130 Ma rifting had reached the Walvis Ridge–Rio Grande Rise.In the central segment rifting started to break Africa in two by opening the Benue Trough around 118 Ma. Rifting in the central segment, however, coincided with the Cretaceous Normal Superchron (also known as the Cretaceous quiet period), a 40 Ma period without magnetic reversals, which makes it difficult to date sea-floor spreading in this segment.The equatorial segment is the last phase of the break-up, but, because it is located on the Equator, magnetic anomalies cannot be used for dating. Various estimates date the propagation of sea-floor spreading in this segment to the period 120–96 Ma. This final stage, nevertheless, coincided with or resulted in the end of continental extension in Africa.About 50 Ma the opening of the Drake Passage resulted from a change in the motions and separation rate of the South American and Antarctic plates. First small ocean basins opened and a shallow gateway appeared during the Middle Eocene. 34–30 Ma a deeper seaway developed, followed by an Eocene–Oligocene climatic deterioration and the growth of the Antarctic ice sheet.
An embryonic subduction margin is potentially developing west of Gibraltar. The Gibraltar Arc in the western Mediterranean is migrating westward into the Central Atlantic where it joins the converging African and Eurasian plates. Together these three tectonic forces are slowly developing into a new subduction system in the eastern Atlantic Basin. Meanwhile, the Scotia Arc and Caribbean Plate in the western Atlantic Basin are eastward-propagating subduction systems that might, together with the Gibraltar system, represent the beginning of the closure of the Atlantic Ocean and the final stage of the Atlantic Wilson cycle.
Humans evolved in Africa; first by diverging from other apes around 7 mya; then developing stone tools around 2.6 mya; to finally evolve as modern humans around 100 kya. The earliest evidence for the complex behavior associated with this behavioral modernity has been found in the Greater Cape Floristic Region (GCFR) along the coast of South Africa. During the latest glacial stages, the now-submerged plains of the Agulhas Bank were exposed above sea level, extending the South African coastline farther south by hundreds of kilometers. A small population of modern humans — probably fewer than a thousand reproducing individuals — survived glacial maxima by exploring the high diversity offered by these Palaeo-Agulhas plains. The GCFR is delimited to the north by the Cape Fold Belt and the limited space south of it resulted in the development of social networks out of which complex Stone Age technologies emerged. Human history thus begins on the coasts of South Africa where the Atlantic Benguela Upwelling and Indian Ocean Agulhas Current meet to produce an intertidal zone on which shellfish, fur seal, fish and sea birds provided the necessary protein sources. The African origin of this modern behaviour is evidenced by 70,000 years-old engravings from Blombos Cave, South Africa.
Mitochondrial DNA (mtDNA) studies indicate that 80–60,000 years ago a major demographic expansion within Africa, derived from a single, small population, coincided with the emergence of behavioral complexity and the rapid MIS 5–4 environmental changes. This group of people not only expanded over the whole of Africa, but also started to disperse out of Africa into Asia, Europe, and Australasia around 65,000 years ago and quickly replaced the archaic humans in these regions. During the Last Glacial Maximum (LGM) 20,000 years ago humans had to abandon their initial settlements along the European North Atlantic coast and retreat to the Mediterranean. Following rapid climate changes at the end of the LGM this region was repopulated by Magdalenian culture. Other hunter-gatherers followed in waves interrupted by large-scale hazards such as the Laacher See volcanic eruption, the inundation of Doggerland (now the North Sea), and the formation of the Baltic Sea. The European coasts of the North Atlantic were permanently populated about 9–8.5 thousand years ago.This human dispersal left abundant traces along the coasts of the Atlantic Ocean. 50 kya-old, deeply stratified shell middens found in Ysterfontein on the western coast of South Africa are associated with the Middle Stone Age (MSA). The MSA population was small and dispersed and the rate of their reproduction and exploitation was less intense than those of later generations. While their middens resemble 12–11 kya-old Late Stone Age (LSA) middens found on every inhabited continent, the 50–45 kya-old Enkapune Ya Muto in Kenya probably represents the oldest traces of the first modern humans to disperse out of Africa. The same development can be seen in Europe. In La Riera Cave (23–13 kya) in Asturias, Spain, only some 26,600 molluscs were deposited over 10 kya. In contrast, 8–7 kya-old shell middens in Portugal, Denmark, and Brazil generated thousands of tons of debris and artefacts. The Ertebølle middens in Denmark, for example, accumulated 2,000 m3 (71,000 cu ft) of shell deposits representing some 50 million molluscs over only a thousand years. This intensification in the exploitation of marine resources has been described as accompanied by new technologies — such as boats, harpoons, and fish-hooks — because many caves found in the Mediterranean and on the European Atlantic coast have increased quantities of marine shells in their upper levels and reduced quantities in their lower. The earliest exploitation, however, took place on the now submerged shelves, and most settlements now excavated were then located several kilometers from these shelves. The reduced quantities of shells in the lower levels can represent the few shells that were exported inland.
During the LGM the Laurentide Ice Sheet covered most of northern North America while Beringia connected Siberia to Alaska. In 1973 late American geoscientist Paul S. Martin proposed a "blitzkrieg" colonization of the Americas by which Clovis hunters migrated into North America around 13,000 years ago in a single wave through an ice-free corridor in the ice sheet and "spread southward explosively, briefly attaining a density sufficiently large to overkill much of their prey." Others later proposed a "three-wave" migration over the Bering Land Bridge. These hypotheses remained the long-held view regarding the settlement of the Americas, a view challenged by more recent archaeological discoveries: the oldest archaeological sites in the Americas have been found in South America; sites in north-east Siberia report virtually no human presence there during the LGM; and most Clovis artefacts have been found in eastern North America along the Atlantic coast. Furthermore, colonisation models based on mtDNA, yDNA, and atDNA data respectively support neither the "blitzkrieg" nor the "three-wave" hypotheses but they also deliver mutually ambiguous results. Contradictory data from archaeology and genetics will most likely deliver future hypotheses that will, eventually, confirm each other. A proposed route across the Pacific to South America could explain early South American finds and another hypothesis proposes a northern path, through the Canadian Arctic and down the North American Atlantic coast. Early settlements across the Atlantic have been suggested by alternative theories, ranging from purely hypothetical to mostly disputed, including the Solutrean hypothesis and some of the Pre-Columbian trans-oceanic contact theories. The Norse settlement of the Faroe Islands and Iceland began during the 9th and 10th centuries. A settlement on Greenland was established before 1000 CE, but contact with it was lost in 1409 and it was finally abandoned during the early Little Ice Age. This setback was caused by a range of factors: an unsustainable economy resulted in erosion and denudation, while conflicts with the local Inuit resulted in the failure to adapt their Arctic technologies; a colder climate resulted in starvation, and the colony got economically marginalized as the Great Plague and Barbary pirates harvested its victims on Iceland in the 15th century. Iceland was initially settled 865–930 CE following a warm period when winter temperatures hovered around 2 °C (36 °F) which made farming favorable at high latitudes. This did not last, however, and temperatures quickly dropped; at 1080 CE summer temperatures had reached a maximum of 5 °C (41 °F). The Landnámabók (Book of Settlement) records disastrous famines during the first century of settlement — "men ate foxes and ravens" and "the old and helpless were killed and thrown over cliffs" — and by the early 1200s hay had to be abandoned for short-season crops such as barley.
Christopher Columbus reached the Americas in 1492 under Spanish flag. Six years later Vasco da Gama reached India under the Portuguese flag, by navigating south around the Cape of Good Hope, thus proving that the Atlantic and Indian Oceans are connected. In 1500, in his voyage to India following Vasco da Gama, Pedro Alvares Cabral reached Brazil, taken by the currents of the South Atlantic Gyre. Following these explorations, Spain and Portugal quickly conquered and colonized large territories in the New World and forced the Amerindian population into slavery in order to explore the vast quantities of silver and gold they found. Spain and Portugal monopolized this trade in order to keep other European nations out, but conflicting interests nevertheless led to a series of Spanish-Portuguese wars. A peace treaty mediated by the Pope divided the conquered territories into Spanish and Portuguese sectors while keeping other colonial powers away. England, France, and the Dutch Republic enviously watched the Spanish and Portuguese wealth grow and allied themselves with pirates such as Henry Mainwaring and Alexandre Exquemelin. They could explore the convoys leaving the Americas because prevailing winds and currents made the transport of heavy metals slow and predictable. In the colonies of the Americas, depredation, smallpox and others diseases, and slavery quickly reduced the indigenous population of the Americas to the extent that the Atlantic slave trade had to be introduced to replace them — a trade that became the norm and an integral part of the colonization. Between the 15th century and 1888, when Brazil became the last part of the Americas to end the slave trade, an estimated ten million Africans were exported as slaves, most of them destined for agricultural labour. The slave trade was officially abolished in the British Empire and the United States in 1808, and slavery itself was abolished in the British Empire in 1838 and in the United States in 1865 after the Civil War.From Columbus to the Industrial Revolution Trans-Atlantic trade, including colonialism and slavery, became crucial for Western Europe. For European countries with direct access to the Atlantic (including Britain, France, the Netherlands, Portugal, and Spain) 1500–1800 was a period of sustained growth during which these countries grew richer than those in Eastern Europe and Asia. Colonialism evolved as part of the Trans-Atlantic trade, but this trade also strengthened the position of merchant groups at the expense of monarchs. Growth was more rapid in non-absolutist countries, such as Britain and the Netherlands, and more limited in absolutist monarchies, such as Portugal, Spain, and France, where profit mostly or exclusively benefited the monarchy and its allies.Trans-Atlantic trade also resulted in increasing urbanization: in European countries facing the Atlantic, urbanization grew from 8% in 1300, 10.1% in 1500, to 24.5% in 1850; in other European countries from 10% in 1300, 11.4% in 1500, to 17% in 1850. Likewise, GDP doubled in Atlantic countries but rose by only 30% in the rest of Europe. By end of the 17th century, the volume of the Trans-Atlantic trade had surpassed that of the Mediterranean trade.
The Atlantic has contributed significantly to the development and economy of surrounding countries. Besides major transatlantic transportation and communication routes, the Atlantic offers abundant petroleum deposits in the sedimentary rocks of the continental shelves. The Atlantic harbors petroleum and gas fields, fish, marine mammals (seals and whales), sand and gravel aggregates, placer deposits, polymetallic nodules, and precious stones. Gold deposits are a mile or two under water on the ocean floor, however the deposits are also encased in rock that must be mined through. Currently, there is no cost-effective way to mine or extract gold from the ocean to make a profit.Various international treaties attempt to reduce pollution caused by environmental threats such as oil spills, marine debris, and the incineration of toxic wastes at sea.
The shelves of the Atlantic hosts one of the world's richest fishing resources. The most productive areas include the Grand Banks of Newfoundland, the Scotian Shelf, Georges Bank off Cape Cod, the Bahama Banks, the waters around Iceland, the Irish Sea, the Bay of Fundy, the Dogger Bank of the North Sea, and the Falkland Banks. Fisheries have, however, undergone significant changes since the 1950s and global catches can now be divided into three groups of which only two are observed in the Atlantic: fisheries in the Eastern Central and South-West Atlantic oscillate around a globally stable value, the rest of the Atlantic is in overall decline following historical peaks. The third group, "continuously increasing trend since 1950", is only found in the Indian Ocean and Western Pacific. In the North-East Atlantic total catches decreased between the mid-1970s and the 1990s and reached 8.7 million tons in 2013. Blue whiting reached a 2.4 million tons peak in 2004 but was down to 628,000 tons in 2013. Recovery plans for cod, sole, and plaice have reduced mortality in these species. Arctic cod reached its lowest levels in the 1960s–1980s but is now recovered. Arctic saithe and haddock are considered fully fished; Sand eel is overfished as was capelin which has now recovered to fully fished. Limited data makes the state of redfishes and deep-water species difficult to assess but most likely they remain vulnerable to overfishing. Stocks of northern shrimp and Norwegian lobster are in good condition. In the North-East Atlantic 21% of stocks are considered overfished. In the North-West Atlantic landings have decreased from 4.2 million tons in the early 1970s to 1.9 million tons in 2013. During the 21st century some species have shown weak signs of recovery, including Greenland halibut, yellowtail flounder, Atlantic halibut, haddock, spiny dogfish, while other stocks shown no such signs, including cod, witch flounder, and redfish. Stocks of invertebrates, in contrast, remain at record levels of abundance. 31% of stocks are overfished in the North-west Atlantic. In 1497 John Cabot became the first Western European since the Vikings to explore mainland North America and one of his major discoveries was the abundant resources of Atlantic cod off Newfoundland. Referred to as "Newfoundland Currency" this discovery yielded some 200 million tons of fish over five centuries. In the late 19th and early 20th centuries new fisheries started to exploit haddock, mackerel, and lobster. From the 1950s to the 1970s the introduction of European and Asian distant-water fleets in the area dramatically increased the fishing capacity and the number of exploited species. It also expanded the exploited areas from near-shore to the open sea and to great depths to include deep-water species such as redfish, Greenland halibut, witch flounder, and grenadiers. Overfishing in the area was recognised as early as the 1960s but, because this was occurring on international waters, it took until the late 1970s before any attempts to regulate was made. In the early 1990s, this finally resulted in the collapse of the Atlantic northwest cod fishery. The population of a number of deep-sea fishes also collapsed in the process, including American plaice, redfish, and Greenland halibut, together with flounder and grenadier.In the Eastern Central Atlantic small pelagic fishes constitute about 50% of landings with sardine reaching 0.6–1.0 million tons per year. Pelagic fish stocks are considered fully fished or overfished, with sardines south of Cape Bojador the notable exception. Almost half of the stocks are fished at biologically unsustainable levels. Total catches have been fluctuating since the 1970s; reaching 3.9 million tons in 2013 or slightly less than the peak production in 2010. In the Western Central Atlantic, catches have been decreasing since 2000 and reached 1.3 million tons in 2013. The most important species in the area, Gulf menhaden, reached a million tons in the mid-1980s but only half a million tons in 2013 and is now considered fully fished. Round sardinella was an important species in the 1990s but is now considered overfished. Groupers and snappers are overfished and northern brown shrimp and American cupped oyster are considered fully fished approaching overfished. 44% of stocks are being fished at unsustainable levels. In the South-East Atlantic catches have decreased from 3.3 million tons in the early 1970s to 1.3 million tons in 2013. Horse mackerel and hake are the most important species, together representing almost half of the landings. Off South Africa and Namibia deep-water hake and shallow-water Cape hake have recovered to sustainable levels since regulations were introduced in 2006 and the states of Southern African pilchard and anchovy have improved to fully fished in 2013.In the South-West Atlantic, a peak was reached in the mid-1980s and catches now fluctuate between 1.7 and 2.6 million tons. The most important species, the Argentine shortfin squid, which reached half a million tons in 2013 or half the peak value, is considered fully fished to overfished. Another important species was the Brazilian sardinella, with a production of 100,000 tons in 2013 it is now considered overfished. Half the stocks in this area are being fished at unsustainable levels: Whitehead's round herring has not yet reached fully fished but Cunene horse mackerel is overfished. The sea snail perlemoen abalone is targeted by illegal fishing and remain overfished.
Endangered marine species include the manatee, seals, sea lions, turtles, and whales. Drift net fishing can kill dolphins, albatrosses and other seabirds (petrels, auks), hastening the fish stock decline and contributing to international disputes. Municipal pollution comes from the eastern United States, southern Brazil, and eastern Argentina; oil pollution in the Caribbean Sea, Gulf of Mexico, Lake Maracaibo, Mediterranean Sea, and North Sea; and industrial waste and municipal sewage pollution in the Baltic Sea, North Sea, and Mediterranean Sea. North Atlantic hurricane activity has increased over past decades because of increased sea surface temperature (SST) at tropical latitudes, changes that can be attributed to either the natural Atlantic Multidecadal Oscillation (AMO) or to anthropogenic climate change. A 2005 report indicated that the Atlantic meridional overturning circulation (AMOC) slowed down by 30% between 1957 and 2004. If the AMO were responsible for SST variability, the AMOC would have increased in strength, which is apparently not the case. Furthermore, it is clear from statistical analyses of annual tropical cyclones that these changes do not display multidecadal cyclicity. Therefore, these changes in SST must be caused by human activities.The ocean mixed layer plays an important role in heat storage over seasonal and decadal time-scales, whereas deeper layers are affected over millennia and have a heat capacity about 50 times that of the mixed layer. This heat uptake provides a time-lag for climate change but it also results in thermal expansion of the oceans which contributes to sea-level rise. 21st-century global warming will probably result in an equilibrium sea-level rise five times greater than today, whilst melting of glaciers, including that of the Greenland ice-sheet, expected to have virtually no effect during the 21st century, will probably result in a sea-level rise of 3–6 m over a millennium.A USAF C-124 aircraft from Dover Air Force Base, Delaware was carrying three nuclear bombs over the Atlantic Ocean when it experienced a loss of power. For their own safety, the crew jettisoned two nuclear bombs, which were never recovered.On 7 June 2006, Florida's wildlife commission voted to take the manatee off the state's endangered species list. Some environmentalists worry that this could erode safeguards for the popular sea creature. Marine pollution is a generic term for the entry into the ocean of potentially hazardous chemicals or particles. The biggest culprits are rivers and with them many agriculture fertilizer chemicals as well as livestock and human waste. The excess of oxygen-depleting chemicals leads to hypoxia and the creation of a dead zone.Marine debris, which is also known as marine litter, describes human-created waste floating in a body of water. Oceanic debris tends to accumulate at the center of gyres and coastlines, frequently washing aground where it is known as beach litter. The North Atlantic garbage patch is estimated to be hundreds of kilometers across in size.
List of countries and territories bordering the Atlantic Ocean Seven Seas Gulf Stream shutdown Shipwrecks in the Atlantic Ocean Atlantic hurricanes Transatlantic crossing
Atlantic Ocean. Cartage.org.lb. "Map of Atlantic Coast of North America from the Chesapeake Bay to Florida" from 1639 via the World Digital Library
List of smoked foods
Media related to Back bacon at Wikimedia Commons "A Guide to Bacon Styles, and How to Make Proper British Rashers" at thepauperedchef.com weblog
A bacon sandwich (also known in parts of the United Kingdom and New Zealand as a bacon butty, bacon bap or bacon sarnie, in Ireland as a rasher sandwich and as a bacon sanger in Australia) is a sandwich of cooked bacon between bread that is optionally spread with butter, and may be seasoned with ketchup or brown sauce. It is generally served hot. In some establishments the sandwich will be made from bread toasted on only one side, while other establishments serve it on the same roll as is used for hamburgers. Bacon sandwiches are an all-day favourite throughout the United Kingdom. They are often served in British cafes, and are anecdotally recommended as a hangover cure.
The New York Times-London Journal reported on a study conducted at Leeds University which consisted of testing 700 variants of the sandwich. They experimented with different cooking styles, types of bacon, breads, oils, and special additions. Each variant was then ranked by 50 tasters. In conclusion, the best bacon sandwiches are made with "crispy, fried, and not-too-fat bacon between thick slices of white bread." Another study by the Direct Line for Business listed the top additions to the traditional bacon butty in England. Although the original was still the preferred sandwich, the next top contender was the "breggy" which adds an egg. The next popular accessory was mushrooms, followed by cheese. For sauces, brown sauce was slightly favoured over ketchup. The BLT is a popular variant of the bacon sandwich with the additional ingredients of lettuce and tomato, but served cold.In Canada, Peameal bacon § Sandwiches are a common variation, and are even considered the unofficial dish of Toronto.
Numerous studies have showed a connection between processed meats and an increased risk of serious health conditions such as type 2 diabetes, various cancers, and cardiovascular disease.A study in 2007 conducted by the World Cancer Research Fund found that there was "convincing evidence" of a link between processed meats and an increased chance of cancer. Although no numerical value was provided for the risk, they did state that "people should not eat more than 500g of red meat a week." The World Health Organization released a warning concerning the sodium content in bacon. For 100g of bacon, there are approximately 1,500 mg of sodium. Currently, the FDA reports that the average American adult should consume less than 2,300 mg per day. Too much sodium in the diet can lead to a higher probability of high blood pressure, which is a major cause of heart disease and stroke.
"Scientists 'perfect' bacon butty: Scientists have created a mathematical formula of how to make the perfect bacon butty". BBC. British Broadcasting Corporation. 9 April 2007.
A beacon is an intentionally conspicuous device designed to attract attention to a specific location. A common example is the lighthouse, which provides a fixed location that can be used to navigate around obstacles or into port. More modern examples include a variety of radio beacons that can be read on radio direction finders in all weather, and radar transponders that appear on radar displays. Beacons can also be combined with semaphoric or other indicators to provide important information, such as the status of an airport, by the colour and rotational pattern of its airport beacon, or of pending weather as indicated on a weather beacon mounted at the top of a tall building or similar site. When used in such fashion, beacons can be considered a form of optical telegraphy.
Beacons help guide navigators to their destinations. Types of navigational beacons include radar reflectors, radio beacons, sonic and visual signals. Visual beacons range from small, single-pile structures to large lighthouses or light stations and can be located on land or on water. Lighted beacons are called lights; unlighted beacons are called daybeacons. Aerodrome beacons are used to indicate locations of airports and helipads. Handheld beacons are also employed in aircraft marshalling, and are used by the marshal to deliver instructions to the crew of aircraft as they move around an active airport, heliport or aircraft carrier.
Classically, beacons were fires lit at well-known locations on hills or high places, used either as lighthouses for navigation at sea, or for signalling over land that enemy troops were approaching, in order to alert defenses. As signals, beacons are an ancient form of optical telegraph and were part of a relay league. Systems of this kind have existed for centuries over much of the world. The ancient Greeks called them phryctoriae, while beacons figure on several occasions on the column of Trajan. In ancient China, sentinels on and near the Great Wall of China used a sophisticated system of daytime smoke and nighttime flame to send signals along long chains of beacon towers.Legend has it that Zhōu Yōu Wáng, king of the Western Zhou dynasty, played a trick multiple times in order to amuse his often melancholy concubine, ordering beacon towers lit to fool his Marquess and soldiers. But when enemies, led by Marquess of Shen really arrived at the wall, although the towers were lit, no defenders came, leading to Yōu's death and the collapse of the Western Zhou dynasty.In the 10th century, during the Arab–Byzantine wars, the Byzantine Empire used a beacon system to transmit messages from the border with the Abbasid Caliphate, across Anatolia to the imperial palace in the Byzantine capital, Constantinople. It was devised by Leo the Mathematician for Emperor Theophilos, but either abolished or radically curtailed by Theophilos' son and successor, Michael III. Beacons were later used in Greece as well, while the surviving parts of the beacon system in Anatolia seem to have been reactivated in the 12th century by Emperor Manuel I Komnenos.In Scandinavia many hill forts were part of beacon networks to warn against invading pillagers. In Finland, these beacons were called vainovalkeat, "persecution fires", or vartiotulet, "guard fires", and were used to warn Finn settlements of imminent raids by the Vikings. In Wales, the Brecon Beacons were named for beacons used to warn of approaching English raiders. In England, the most famous examples are the beacons used in Elizabethan England to warn of the approaching Spanish Armada. Many hills in England were named Beacon Hill after such beacons. In England the authority to erect beacons originally lay with the King and later was deligated to the Lord High Admiral. The money due for the maintenance of beacons was called Beaconagium and was levied by the sheriff of each county. In the Scottish borders country, a system of beacon fires was at one time established to warn of incursions by the English. Hume and Eggerstone castles and Soltra Edge were part of this network.In Spain, the border of Granada in the territory of the Crown of Castile had a complex beacon network to warn against Moorish raiders and military campaigns.
Vehicular beacons are rotating or flashing lights affixed to the top of a vehicle to attract the attention of surrounding vehicles and pedestrians. Emergency vehicles such as fire engines, ambulances, police cars, tow trucks, construction vehicles, and snow-removal vehicles carry beacon lights. The color of the lamps varies by jurisdiction; typical colors are blue and/or red for police, fire, and medical-emergency vehicles; amber for hazards (slow-moving vehicles, wide loads, tow trucks, security personnel, construction vehicles, etc.); green for volunteer firefighters or for medical personnel, and violet for funerary vehicles. Beacons may be constructed with halogen bulbs similar to those used in vehicle headlamps, xenon flashtubes, or LEDs. Incandescent and xenon light sources require the vehicle's engine to continue running to ensure that the battery is not depleted when the lights are used for a prolonged period. The low power consumption of LEDs allows the vehicle's engine to remain turned off while the lights operate nodes.
Beacons and bonfires are also used to mark occasions and celebrate events. Beacons have also allegedly been abused by shipwreckers. An illicit fire at a wrong position would be used to direct a ship against shoals or beaches, so that its cargo could be looted after the ship sank or ran aground. There are, however, no historically substantiated occurrences of such intentional shipwrecking. In wireless networks, a beacon is a type of frame which is sent by the access point (or WiFi router) to indicate that it is on. Bluetooth based beacons periodically send out a data packet and this could be used by software to identify the beacon location. This is typically used by indoor navigation and positioning applications.Beaconing is the process that allows a network to self-repair network problems. The stations on the network notify the other stations on the ring when they are not receiving the transmissions. Beaconing is used in Token ring and FDDI networks.
In Aeschylus' tragedy Agamemnon, a chain of eight beacons manned by so-called lampadóphoroi inform Clytemnestra in Argos, within a single night's time, that Troy has just fallen under her husband king Agamemnon's control, after a famous ten years siege. In J. R. R. Tolkien's high fantasy novel, The Lord of the Rings, a series of beacons alerts the entire realm of Gondor when the kingdom is under attack. These beacon posts were manned by messengers who would carry word of their lighting to either Rohan or Belfalas. In Peter Jackson's film adaptation of the novel, the beacons serve as a connection between the two realms of Rohan and Gondor, alerting one another directly when they require military aid, as opposed to relying on messengers as in the novel.
Beacons are sometimes used in retail to send digital coupons or invites to customers passing by.
An infrared beacon (IR beacon) transmits a modulated light beam in the infrared spectrum, which can be identified easily and positively. A line of sight clear of obstacles between the transmitter and the receiver is essential. IR beacons have a number of applications in robotics and in Combat Identification (CID). Infrared beacons are the key infrastructure for the Universal Traffic Management System (UTMS) in Japan. They perform two-way communication with travelling vehicles based on highly directional infrared communication technology and have a vehicle detecting capability to provide more accurate traffic information.A contemporary military use of an Infrared beacon is reported in Operation Acid Gambit.
A sonar beacon is an underwater device which transmits sonic or ultrasonic signals for the purpose of providing bearing information. The most common type is that of a rugged watertight sonar transmitter attached to a submarine and capable of operating independently of the electrical system of the boat. It can be used in cases of emergencies to guide salvage vessels to the location of a disabled submarine.
Aerodrome beacon Beacon mode service Beacon School Belisha beacon Emergency locator beacon Emergency position-indicating radiobeacon station (ELTs, PLBs & EPIRBs) iBeacon Lantern Leading lights Lighthouse of Alexandria Milestone/Kilometric point Polaris Strobe beacon Time ball Trail blazing Warning light (disambiguation) Weather beacon Web beacon
Behavioral economics studies the effects of psychological, cognitive, emotional, cultural and social factors on the decisions of individuals and institutions and how those decisions vary from those implied by classical economic theory. Behavioral economics is primarily concerned with the bounds of rationality of economic agents. Behavioral models typically integrate insights from psychology, neuroscience and microeconomic theory. The study of behavioral economics includes how market decisions are made and the mechanisms that drive public choice.
During the classical period of economics, microeconomics was closely linked to psychology. For example, Adam Smith wrote The Theory of Moral Sentiments, which proposed psychological explanations of individual behavior, including concerns about fairness and justice. Jeremy Bentham wrote extensively on the psychological underpinnings of utility. Then, during the development of neo-classical economics, economists sought to reshape the discipline as a natural science, deducing behavior from assumptions about the nature of economic agents. They developed the concept of homo economicus, whose behavior was fundamentally rational. Neo-classical economists did incorporate psychological explanations: this was true of Francis Edgeworth, Vilfredo Pareto and Irving Fisher. Economic psychology emerged in the 20th century in the works of Gabriel Tarde, George Katona, and Laszlo Garai. Expected utility and discounted utility models began to gain acceptance, generating testable hypotheses about decision-making given uncertainty and intertemporal consumption, respectively. Observed and repeatable anomalies eventually challenged those hypotheses, and further steps were taken by Maurice Allais, for example, in setting out the Allais paradox, a decision problem he first presented in 1953 that contradicts the expected utility hypothesis. In the 1960s cognitive psychology began to shed more light on the brain as an information processing device (in contrast to behaviorist models). Psychologists in this field, such as Ward Edwards, Amos Tversky and Daniel Kahneman began to compare their cognitive models of decision-making under risk and uncertainty to economic models of rational behavior. Mathematical psychology reflects a longstanding interest in preference transitivity and the measurement of utility.
In 2002, psychologist Daniel Kahneman was awarded the Nobel Memorial Prize in Economic Sciences "for having integrated insights from psychological research into economic science, especially concerning human judgment and decision-making under uncertainty." In 2013, economist Robert J. Shiller received the Nobel Memorial Prize in Economic Sciences "for his empirical analysis of asset prices" (within the field of behavioral finance). In 2017, economist Richard Thaler was awarded the Nobel Memorial Prize in Economic Sciences for "his contributions to behavioral economics and his pioneering work in establishing that people are predictably irrational in ways that defy economic theory." Kahneman and Tversky's in the late 1960s, published about 200 works, most of which relate to psychological concepts with implications for behavioral finance. In 2002, Kahneman received the Nobel Memorial Prize in Economic Sciences for his contributions to the study of rationality in economics, a total of six Nobel prizes have been awarded for behavioral research.
Bounded rationality is the idea that when individuals make decisions, their rationality is limited by the tractability of the decision problem, their cognitive limitations and the time available. Decision-makers in this view act as satisficers, seeking a satisfactory solution rather than an optimal one. Herbert A. Simon proposed bounded rationality as an alternative basis for the mathematical modeling of decision-making. It complements "rationality as optimization", which views decision-making as a fully rational process of finding an optimal choice given the information available. Simon used the analogy of a pair of scissors, where one blade represents human cognitive limitations and the other the "structures of the environment", illustrating how minds compensate for limited resources by exploiting known structural regularity in the environment.Bounded rationality implicates the idea that humans take shortcuts that may lead to suboptimal decision-making. Behavioral economists engage in mapping the decision shortcuts that agents use in order to help increase the effectiveness of human decision-making. One treatment of this idea comes from Cass Sunstein and Richard Thaler's Nudge. Sunstein and Thaler recommend that choice architectures are modified in light of human agents' bounded rationality. A widely cited proposal from Sunstein and Thaler urges that healthier food be placed at sight level in order to increase the likelihood that a person will opt for that choice instead of less healthy option. Some critics of Nudge have lodged attacks that modifying choice architectures will lead to people becoming worse decision-makers.Bounded rationality was shown to be essential to predict human sociability properties in a particular model by Vernon L. Smith and Michael J. Campbell. There, an agent-based model correctly predicts that agents are averse to resentment and punishment, and that there is an asymmetry between gratitude/reward and resentment/punishment. The purely rational Nash equilibrium is shown to have no predictive power for that model, and the boundedly rational Gibbs equilibrium must be used to predict phenomena outlined in Humanomics.
In 1979, Kahneman and Tversky published Prospect Theory: An Analysis of Decision Under Risk, that used cognitive psychology to explain various divergences of economic decision making from neo-classical theory. Prospect theory has two stages: an editing stage and an evaluation stage. In the editing stage, risky situations are simplified using various heuristics. In the evaluation phase, risky alternatives are evaluated using various psychological principles that include: Reference dependence: When evaluating outcomes, the decision maker considers a "reference level." Outcomes are then compared to the reference point and classified as "gains" if greater than the reference point and "losses" if less than the reference point. Loss aversion: Losses are avoided more than equivalent gains are sought. In their 1992 paper, Kahneman and Tversky found the median coefficient of loss aversion to be about 2.25, i.e., losses hurt about 2.25 times more than equivalent gains reward. Non-linear probability weighting: Decision makers overweigh small probabilities and underweigh large probabilities—this gives rise to the inverse-S shaped "probability weighting function." Diminishing sensitivity to gains and losses: As the size of the gains and losses relative to the reference point increase in absolute value, the marginal effect on the decision maker's utility or satisfaction falls.Prospect theory is able to explain everything that the two main existing decision theories—expected utility theory and rank dependent utility theory—can explain. Further, prospect theory has been used to explain phenomena that existing decision theories have great difficulty in explaining. These include backward bending labor supply curves, asymmetric price elasticities, tax evasion and co-movement of stock prices and consumption. In 1992, in the Journal of Risk and Uncertainty, Kahneman and Tversky gave a revised account of prospect theory that they called cumulative prospect theory. The new theory eliminated the editing phase in prospect theory and focused just on the evaluation phase. Its main feature was that it allowed for non-linear probability weighting in a cumulative manner, which was originally suggested in John Quiggin's rank-dependent utility theory. Psychological traits such as overconfidence, projection bias, and the effects of limited attention are now part of the theory. Other developments include a conference at the University of Chicago, a special behavioral economics edition of the Quarterly Journal of Economics ("In Memory of Amos Tversky"), and Kahneman's 2002 Nobel Prize for having "integrated insights from psychological research into economic science, especially concerning human judgment and decision-making under uncertainty."
Nudge is a concept in behavioral science, political theory and economics which proposes positive reinforcement and indirect suggestions as ways to influence the behavior and decision making of groups or individuals. Nudging contrasts with other ways to achieve compliance, such as education, legislation or enforcement. The concept has influenced British and American politicians. Several nudge units exist around the world at the national level (UK, Germany, Japan and others) as well as at the international level (OECD, World Bank, UN). The first formulation of the term and associated principles was developed in cybernetics by James Wilk before 1995 and described by Brunel University academic D. J. Stewart as "the art of the nudge" (sometimes referred to as micronudges). It also drew on methodological influences from clinical psychotherapy tracing back to Gregory Bateson, including contributions from Milton Erickson, Watzlawick, Weakland and Fisch, and Bill O'Hanlon. In this variant, the nudge is a microtargetted design geared towards a specific group of people, irrespective of the scale of intended intervention. In 2008, Richard Thaler and Cass Sunstein's book Nudge: Improving Decisions About Health, Wealth, and Happiness brought nudge theory to prominence. It also gained a following among US and UK politicians, in the private sector and in public health. The authors refer to influencing behavior without coercion as libertarian paternalism and the influencers as choice architects. Thaler and Sunstein defined their concept as: A nudge, as we will use the term, is any aspect of the choice architecture that alters people's behavior in a predictable way without forbidding any options or significantly changing their economic incentives. To count as a mere nudge, the intervention must be easy and cheap to avoid. Nudges are not mandates. Putting fruit at eye level counts as a nudge. Banning junk food does not. In this form, drawing on behavioral economics, the nudge is more generally applied to influence behavior. One of the most frequently cited examples of a nudge is the etching of the image of a housefly into the men's room urinals at Amsterdam's Schiphol Airport, which is intended to "improve the aim."Nudging techniques aim to use judgmental heuristics to our advantage. In other words, a nudge alters the environment so that when heuristic, or System 1, decision-making is used, the resulting choice will be the most positive or desired outcome. An example of such a nudge is switching the placement of junk food in a store, so that fruit and other healthy options are located next to the cash register, while junk food is relocated to another part of the store.In 2008, the United States appointed Sunstein, who helped develop the theory, as administrator of the Office of Information and Regulatory Affairs.Notable applications of nudge theory include the formation of the British Behavioural Insights Team in 2010. It is often called the "Nudge Unit", at the British Cabinet Office, headed by David Halpern.Both Prime Minister David Cameron and President Barack Obama sought to employ nudge theory to advance domestic policy goals during their terms.In Australia, the government of New South Wales established a Behavioural Insights community of practice.Nudge theory has also been applied to business management and corporate culture, such as in relation to health, safety and environment (HSE) and human resources. Regarding its application to HSE, one of the primary goals of nudge is to achieve a "zero accident culture."Leading Silicon Valley companies are forerunners in applying nudge theory in a corporate setting. These companies are using nudges in various forms to increase the productivity and happiness of employees. Recently, further companies are gaining interest in using what is called "nudge management" to improve the productivity of their white-collar workers.Behavioral insights and nudges are currently used in many countries around the world.
Nudging has also been criticised. Tammy Boyce, from public health foundation The King's Fund, has said: "We need to move away from short-term, politically motivated initiatives such as the 'nudging people' idea, which is not based on any good evidence and doesn't help people make long-term behaviour changes."Cass Sunstein has responded to critiques at length in his The Ethics of Influence making the case in favor of nudging against charges that nudges diminish autonomy, threaten dignity, violate liberties, or reduce welfare. Ethicists have debated this rigorously. These charges have been made by various participants in the debate from Bovens to Goodwin. Wilkinson for example charges nudges for being manipulative, while others such as Yeung question their scientific credibility.Some, such as Hausman & Welch have inquired whether nudging should be permissible on grounds of (distributive) justice; Lepenies & Malecka have questioned whether nudges are compatible with the rule of law. Similarly, legal scholars have discussed the role of nudges and the law.Behavioral economists such as Bob Sugden have pointed out that the underlying normative benchmark of nudging is still homo economicus, despite the proponents' claim to the contrary.It has been remarked that nudging is also a euphemism for psychological manipulation as practiced in social engineering.There exists an anticipation and, simultaneously, implicit criticism of the nudge theory in works of Hungarian social psychologists who emphasize the active participation in the nudge of its target (Ferenc Merei and Laszlo Garai).
Behavioral Finance is the study of the influence of psychology on the behavior of investors or financial analyst. It assumes that investors are not always rational, have limits to their self-control and are influenced by their own biases. For example, behavioral law and economics scholars studying the growth of financial firms’ technological capabilities have attributed decision science to irrational consumer decisions. It also includes the subsequent effects on the markets. Behavioral Finance attempts to explain the reasoning patterns of investors and measures the influential power of these patterns on the investor's decision making. The central issue in behavioral finance is explaining why market participants make irrational systematic errors contrary to assumption of rational market participants. Such errors affect prices and returns, creating market inefficiencies.
The foundations of behavioral finance can be traced back over 150 years. Several original books written in the 1800s and early 1900s marked the beginning of the behavioral finance school. Originally published in 1841, MacKay's 'Extraordinary Popular Delusions' and 'The Madness of Crowds' presents a chronological timeline of the various panics and schemes throughout history. This work shows how group behavior applies to the financial markets of today. Le Bon's important work, The Crowd: A Study of the Popular Mind, discusses the role of "crowds" (also known as crowd psychology) and group behavior as they apply to the fields of behavioral finance, social psychology, sociology, and history. Selden's 1912 book Psychology of The Stock Market was one of the first to apply the field of psychology directly to the stock market. This classic discusses the emotional and psychological forces at work on investors and traders in the financial markets. These three works along with several others form the foundation of applying psychology and sociology to the field of finance. The foundation of behavioral finance is an area based on an interdisciplinary approach including scholars from the social sciences and business schools. From the liberal arts perspective, this includes the fields of psychology, sociology, anthropology, economics and behavioral economics. On the business administration side, this covers areas such as management, marketing, finance, technology and accounting.
The accepted theories of finance are referred to as traditional finance. The foundation of traditional finance is associated with the modern portfolio theory (MPT) and the efficient-market hypothesis (EMH). Modern portfolio theory is a stock or portfolio's expected return, standard deviation, and its correlation with the other stocks or mutual funds held within the portfolio. With these three concepts, an efficient portfolio can be created for any group of stocks or bonds. An efficient portfolio is a group of stocks that has the maximum (highest) expected return given the amount of risk assumed, contains the lowest possible risk for a given expected return. The efficient-market hypothesis states that all information has already been reflected in a security's price or market value, and that the current price of the stock or bond always trades at its fair value. The proponents of the traditional theories believe that 'investors should just own the entire market rather than attempting to outperform the market'. Behavioral finance has emerged as an alternative to these theories of traditional finance and the behavioral aspects of psychology and sociology are integral catalysts within this field of study.
The prevalent themes in behavioral finance are:
Heuristics are mental shortcuts or rules of thumb. that simplify the complex methods to make a judgment. Investor as a decision-maker confronts a set of choices within certainty and limited ability to quantify results. Heuristics help to make decision. Some of heuristics are representativeness, anchoring and adjustments, familiarity, overconfidence, regret aversion, conservatism, mental accounting, availability, ambiguity aversion and effect.
Framing: The collection of anecdotes and stereotypes that make up the mental filters individuals rely on to understand and respond to events. The perceptions of choices that people have are strongly influenced by how these choices are framed. It means choices depend on how question is framed, even though the objective facts remain constant. Psychologists refer this behavior as a 'frame dependence'.
Emotions and associated human unconscious needs, fantasies, and fears drive much decision of human beings. Behavioral finance recognizes the role Keynes's "animal spirit" plays in explaining investor choices, and thus shaping financial markets. Decision making based on extreme emotions or emotional strains such as anxiety, anger, fear, or excitement are a key reason why people do not make rational choices.
These include mis-pricing and non-rational decision making. Behavioral finance affirms the fact that market prices did not appear to be fair. Traditional finance argues that investors' mistakes would not affect market prices because when prices deviate from fundamental value, rational investor would exploit the mispricing for their own profit. Even institutional investor exhibits the inefficiency. Arbitragers are those who make good profit when there is mispricing between the two markets. Market with people of different beliefs and motives prevents rational investor from correcting price deviations from fundamental value. This leaves open the possibility that correlated cognitive errors of investor could affect market prices.
The concept of anchoring is based on one's tendency to attach or "anchor" thoughts to a reference point. Investor when making a decision evaluates a fact or a past event as a reference point which may or may not have a logical bearing to the decision in question. This becomes quite common when individuals evaluate decisions that are new or unfamiliar to them.
Investors tend to overreact on the news, which ends up creating a disproportionate effect on share prices. Market overreacting signals a buying opportunity for investors. Availability bias is a part of overreaction, when investors decision-making process is strongly influenced by events closest and most available to them.
Mental accounting refers to the propensity to allocate money for specific purposes. Mental accounting is a behavioral bias that causes one to separate money into different categories known as mental accounts either based on the source of money or the intention of the money.
Investor is inclined to confirm to that piece of information or pay more attention to the things that support their own preconceived ideas and opinions, this is known as confirmation bias. Anything that is contrary to the confirmed view is ignored and rationalized. Hindsight bias is a tendency to see the past as being predictable and explainable. The truth is information now and information then is completely different.
Herd behavior states that people tend to mimic the financial behaviors of the majority of the herd. Herd behavior is like the collective wisdom of the crowd. Everyone thinks the same way, shares the same optimism and feel safe and secure with the crowd.
Investors become overconfident of their abilities when they are on a winning streak. The ability to hand-pick selective stocks, beat the market and make a huge ton of money are all characteristics of Overconfidence. The skills are overestimated and the risks involved is underestimated.
Disposition bias refers to when investors sell their winners and hang onto their losers. Investors' thinking is that they want to realize gains quickly. When an investment is losing money, they'll hold onto it because they want to get back to even or their initial price. Investors tend to admit they are correct about an investment quickly (when there's a gain). However, investors are reluctant to admit when they made an investment mistake (when there's a loss).
An experiential bias occurs when investors' memory of recent events makes them biased or leads them to believe that the event is far more likely to occur again. When investors have a bias toward accepting information that confirms their past experience in an investment. If information surfaces, investors accept it readily to confirm that they're correct about their investment decision and continue hold this bias unless proved wrong.
The familiarity bias is when investors tend to invest in what they know, such as domestic companies or locally owned investments. As a result, investors are not diversified across multiple sectors and types of investments, which can reduce risk. Investors tend to go with investments that they have a history with or have familiarity.
Loss aversion occurs when investors place a greater weighting on the concern for losses than the pleasure from market gains. In other words, they're far more likely to try to assign a higher priority on avoiding losses than making investment gains. As a result, some investors might want a higher payout to compensate for losses. If the high payout isn't likely, they might try to avoid losses altogether even if the investment's risk is acceptable from a rational standpoint.
Critics such as Eugene Fama typically support the efficient-market hypothesis. They contend that behavioral finance is more a collection of anomalies than a true branch of finance and that these anomalies are either quickly priced out of the market or explained by appealing to market microstructure arguments. However, individual cognitive biases are distinct from social biases; the former can be averaged out by the market, while the other can create positive feedback loops that drive the market further and further from a "fair price" equilibrium. Fama argued that many of the findings in behavioral finance itself appear to be collection of anomalies that can be explained by market. It is observed that, the problem with the general area of behavioral finance is that it only serves as a complement to general economics. Similarly, for an anomaly to violate market efficiency, an investor must be able to trade against it and earn abnormal profits; this is not the case for many anomalies.A specific example of this criticism appears in some explanations of the equity premium puzzle. It is argued that the cause is entry barriers (both practical and psychological) and that the equity premium should reduce as electronic resources open up the stock market to more traders. In response, others contend that most personal investment funds are managed through superannuation funds, minimizing the effect of these putative entry barriers. In addition, professional investors and fund managers seem to hold more bonds than one would expect given return differentials.
Quantitative behavioral finance uses mathematical and statistical methodology to understand behavioral biases. In marketing research, a study shows little evidence that escalating biases impact marketing decisions. Leading contributors include Gunduz Caginalp (Editor of the Journal of Behavioral Finance from 2001 to 2004), and collaborators include 2002 Nobel Laureate Vernon Smith, David Porter, Don Balenovich, Vladimira Ilieva and Ahmet Duran, and Ray Sturm.
Some financial models used in money management and asset valuation incorporate behavioral finance parameters. Examples: Thaler's model of price reactions to information, with three phases (underreaction, adjustment, and overreaction), creating a price trend.One characteristic of overreaction is that average returns following announcements of good news is lower than following bad news. In other words, overreaction occurs if the market reacts too strongly or for too long to news, thus requiring an adjustment in the opposite direction. As a result, outperforming assets in one period is likely to underperform in the following period. This also applies to customers' irrational purchasing habits. The stock image coefficient.
A handful of comparative psychologists have attempted to demonstrate quasi-economic reasoning in non-human animals. Early attempts along these lines focus on the behavior of rats and pigeons. These studies draw on the tenets of comparative psychology, where the main goal is to discover analogs to human behavior in experimentally-tractable non-human animals. They are also methodologically similar to the work of Ferster and Skinner. Methodological similarities aside, early researchers in non-human economics deviate from behaviorism in their terminology. Although such studies are set up primarily in an operant conditioning chamber using food rewards for pecking/bar-pressing behavior, the researchers describe pecking and bar-pressing not in terms of reinforcement and stimulus-response relationships but instead in terms of work, demand, budget, and labor. Recent studies have adopted a slightly different approach, taking a more evolutionary perspective, comparing economic behavior of humans to a species of non-human primate, the capuchin monkey.
Many early studies of non-human economic reasoning were performed on rats and pigeons in an operant conditioning chamber. These studies looked at things like peck rate (in the case of the pigeon) and bar-pressing rate (in the case of the rat) given certain conditions of reward. Early researchers claim, for example, that response pattern (pecking/bar-pressing rate) is an appropriate analogy to human labor supply. Researchers in this field advocate for the appropriateness of using animal economic behavior to understand the elementary components of human economic behavior. In a paper by Battalio, Green, and Kagel, they write, Space considerations do not permit a detailed discussion of the reasons why economists should take seriously the investigation of economic theories using nonhuman subjects....[Studies of economic behavior in non-human animals] provide a laboratory for identifying, testing, and better understanding general laws of economic behavior. Use of this laboratory is predicated on the fact that behavior, as well as structure, vary continuously across species, and that principles of economic behavior would be unique among behavioral principles if they did not apply, with some variation, of course, to the behavior of nonhumans.
The typical laboratory environment to study labor supply in pigeons is set up as follows. Pigeons are first deprived of food. Since the animals become hungry, food becomes highly desired. The pigeons are then placed in an operant conditioning chamber and through orienting and exploring the environment of the chamber they discover that by pecking a small disk located on one side of the chamber, food is delivered to them. In effect, pecking behavior becomes reinforced, as it is associated with food. Before long, the pigeon pecks at the disk (or stimulus) regularly. In this circumstance, the pigeon is said to "work" for the food by pecking. The food, then, is thought of as the currency. The value of the currency can be adjusted in several ways, including the amount of food delivered, the rate of food delivery and the type of food delivered (some foods are more desirable than others). Economic behavior similar to that observed in humans is discovered when the hungry pigeons stop working/work less when the reward is reduced. Researchers argue that this is similar to labor supply behavior in humans. That is, like humans (who, even in need, will only work so much for a given wage), the pigeons demonstrate decreases in pecking (work) when the reward (value) is reduced.
In human economics, a typical demand curve has negative slope. This means that as the price of a certain good increase, the amount that consumers are willing and able to purchase decreases. Researchers studying the demand curves of non-human animals, such as rats, also find downward slopes. Researchers have studied demand in rats in a manner distinct from studying labor supply in pigeons. Specifically, in an operant conditioning chamber containing rats as experimental subjects, we require them to press a bar, instead of pecking a small disk, to receive a reward. The reward can be food (reward pellets), water, or a commodity drink such as cherry cola. Unlike in previous pigeon studies, where the work analog was pecking and the monetary analog was a reward, the work analog in this experiment is bar-pressing. Under these circumstances, the researchers claim that changing the number of bar presses required to obtain a commodity item is analogous to changing the price of a commodity item in human economics.In effect, results of demand studies in non-human animals show that, as the bar-pressing requirement (cost) increase, the number of times an animal presses the bar equal to or greater than the bar-pressing requirement (payment) decreases.
Behavioral economics has been applied to intertemporal choice, which is defined as making a decision and having the effects of such decision happening in a different time. Intertemporal choice behavior is largely inconsistent, as exemplified by George Ainslie's hyperbolic discounting—one of the prominently studied observations—and further developed by David Laibson, Ted O'Donoghue and Matthew Rabin. Hyperbolic discounting describes the tendency to discount outcomes in the near future more than outcomes in the far future. This pattern of discounting is dynamically inconsistent (or time-inconsistent), and therefore inconsistent with basic models of rational choice, since the rate of discount between time t and t+1 will be low at time t-1 when t is the near future, but high at time t when t is the present and time t+1 is the near future. This pattern can also be explained through models of sub-additive discounting that distinguish the delay and interval of discounting: people are less patient (per-time-unit) over shorter intervals regardless of when they occur.
Behavioral game theory, invented by Colin Camerer, analyzes interactive strategic decisions and behavior using the methods of game theory, experimental economics, and experimental psychology. Experiments include testing deviations from typical simplifications of economic theory such as the independence axiom and neglect of altruism, fairness, and framing effects. On the positive side, the method has been applied to interactive learning and social preferences. As a research program, the subject is a development of the last three decades.
Much of the decisions are more and more made either by human beings with the assistance of artificial intelligent machines or wholly made by these machines. Tshilidzi Marwala and Evan Hurwitz in their book, studied the utility of behavioral economics in such situations and concluded that these intelligent machines reduce the impact of bounded rational decision making. In particular, they observed that these intelligent machines reduce the degree of information asymmetry in the market, improve decision making and thus making markets more rational. The use of AI machines in the market in applications such as online trading and decision making has changed major economic theories. Other theories where AI has had impact include in rational choice, rational expectations, game theory, Lewis turning point, portfolio optimization and counterfactual thinking.
Other branches of behavioral economics enrich the model of the utility function without implying inconsistency in preferences. Ernst Fehr, Armin Falk, and Rabin studied fairness, inequity aversion and reciprocal altruism, weakening the neoclassical assumption of perfect selfishness. This work is particularly applicable to wage setting. The work on "intrinsic motivation by Gneezy and Rustichini and "identity" by Akerlof and Kranton assumes that agents derive utility from adopting personal and social norms in addition to conditional expected utility. According to Aggarwal, in addition to behavioral deviations from rational equilibrium, markets are also likely to suffer from lagged responses, search costs, externalities of the commons, and other frictions making it difficult to disentangle behavioral effects in market behavior."Conditional expected utility" is a form of reasoning where the individual has an illusion of control, and calculates the probabilities of external events and hence their utility as a function of their own action, even when they have no causal ability to affect those external events.Behavioral economics caught on among the general public with the success of books such as Dan Ariely's Predictably Irrational. Practitioners of the discipline have studied quasi-public policy topics such as broadband mapping.Applications for behavioral economics include the modeling of the consumer decision-making process for applications in artificial intelligence and machine learning. The Silicon Valley-based start-up Singularities is using the AGM postulates proposed by Alchourrón, Gärdenfors, and Makinson—the formalization of the concepts of beliefs and change for rational entities—in a symbolic logic to create a "machine learning and deduction engine that uses the latest data science and big data algorithms in order to generate the content and conditional rules (counterfactuals) that capture customer's behaviors and beliefs."Applications of behavioral economics also exist in other disciplines, for example in the area of supply chain management.
From a biological point of view, human behaviors are essentially the same during crises accompanied by stock market crashes and during bubble growth when share prices exceed historic highs. During those periods, most market participants see something new for themselves, and this inevitably induces a stress response in them with accompanying changes in their endocrine profiles and motivations. The result is quantitative and qualitative changes in behavior. This is one example where behavior affecting economics and finance can be observed and variably-contrasted using behavioral economics. Behavioral economics' usefulness applies beyond environments similar to stock exchanges. Selfish-reasoning, 'adult behaviors', and similar, can be identified within criminal-concealment(s), and legal-deficiencies and neglect of different types can be observed and discovered. Awareness of indirect consequence (or lack of), at least in potential with different experimental models and methods, can be used as well—behavioral economics' potential uses are broad, but its reliability needs scrutiny. Underestimation of the role of novelty as a stressor is the primary shortcoming of current approaches for market research. It is necessary to account for the biologically determined diphasisms of human behavior in everyday low-stress conditions and in response to stressors. Limitations of experimental methods (e.g. randomized control trials) and their use in economics were famously analyzed by Angus Deaton.
Critics of behavioral economics typically stress the rationality of economic agents. A fundamental critique is provided by Maialeh (2019) who argues that no behavioral research can establish an economic theory. Examples provided on this account include pillars of behavioral economics such as satisficing behavior or prospect theory, which are confronted from the neoclassical perspective of utility maximization and expected utility theory respectively. The author shows that behavioral findings are hardly generalizable and that they do not disprove typical mainstream axioms related to rational behavior.Others note that cognitive theories, such as prospect theory, are models of decision-making, not generalized economic behavior, and are only applicable to the sort of once-off decision problems presented to experiment participants or survey respondents. Others argue that decision-making models, such as the endowment effect theory, that have been widely accepted by behavioral economists may be erroneously established as a consequence of poor experimental design practices that do not adequately control subject misconceptions.A notable concern is that despite a great deal of rhetoric, no unified behavioral theory has yet been espoused: behavioral economists have proposed no unified theory. David Gal has argued that many of these issues stem from behavioral economics being too concerned with understanding how behavior deviates from standard economic models rather than with understanding why people behave the way they do. Understanding why behavior occurs is necessary for the creation of generalizable knowledge, the goal of science. He has referred to behavioral economics as a "triumph of marketing" and particularly cited the example of loss aversion.Traditional economists are skeptical of the experimental and survey-based techniques that behavioral economics uses extensively. Economists typically stress revealed preferences over stated preferences (from surveys) in the determination of economic value. Experiments and surveys are at risk of systemic biases, strategic behavior and lack of incentive compatibility. Some researchers point out that participants of experiments conducted by behavioral economists are not representative enough and drawing broad conclusions on the basis of such experiments is not possible. An acronym WEIRD has been coined in order to describe the studies participants - as those, who come from Western, Educated, Industrialized, Rich, and Democratic societies.
Matthew Rabin dismisses these criticisms, countering that consistent results typically are obtained in multiple situations and geographies and can produce good theoretical insight. Behavioral economists, however, responded to these criticisms by focusing on field studies rather than lab experiments. Some economists see a fundamental schism between experimental economics and behavioral economics, but prominent behavioral and experimental economists tend to share techniques and approaches in answering common questions. For example, behavioral economists are investigating neuroeconomics, which is entirely experimental and has not been verified in the field.The epistemological, ontological, and methodological components of behavioral economics are increasingly debated, in particular by historians of economics and economic methodologists.According to some researchers, when studying the mechanisms that form the basis of decision-making, especially financial decision-making, it is necessary to recognize that most decisions are made under stress because, "Stress is the nonspecific body response to any demands presented to it."
Experimental economics is the application of experimental methods, including statistical, econometric, and computational, to study economic questions. Data collected in experiments are used to estimate effect size, test the validity of economic theories, and illuminate market mechanisms. Economic experiments usually use cash to motivate subjects, in order to mimic real-world incentives. Experiments are used to help understand how and why markets and other exchange systems function as they do. Experimental economics have also expanded to understand institutions and the law (experimental law and economics).A fundamental aspect of the subject is design of experiments. Experiments may be conducted in the field or in laboratory settings, whether of individual or group behavior.Variants of the subject outside such formal confines include natural and quasi-natural experiments.
Neuroeconomics is an interdisciplinary field that seeks to explain human decision making, the ability to process multiple alternatives and to follow a course of action. It studies how economic behavior can shape our understanding of the brain, and how neuroscientific discoveries can constrain and guide models of economics.It combines research methods from neuroscience, experimental and behavioral economics, and cognitive and social psychology. As research into decision-making behavior becomes increasingly computational, it has also incorporated new approaches from theoretical biology, computer science, and mathematics. Neuroeconomics studies decision making by using a combination of tools from these fields so as to avoid the shortcomings that arise from a single-perspective approach. In mainstream economics, expected utility (EU) and the concept of rational agents are still being used. Many economic behaviors are not fully explained by these models, such as heuristics and framing.Behavioral economics emerged to account for these anomalies by integrating social, cognitive, and emotional factors in understanding economic decisions. Neuroeconomics adds another layer by using neuroscientific methods in understanding the interplay between economic behavior and neural mechanisms. By using tools from various fields, some scholars claim that neuroeconomics offers a more integrative way of understanding decision making.
An evolutionary psychology perspective states that many of the perceived limitations in rational choice can be explained as being rational in the context of maximizing biological fitness in the ancestral environment, but not necessarily in the current one. Thus, when living at subsistence level where a reduction of resources may result in death, it may have been rational to place a greater value on preventing losses than on obtaining gains. It may also explain behavioral differences between groups, such as males being less risk-averse than females since males have more variable reproductive success than females. While unsuccessful risk-seeking may limit reproductive success for both sexes, males may potentially increase their reproductive success from successful risk-seeking much more than females can.
Also known as cognitive psychology.
In psychological trait theory, the Big Five personality traits, also known as the five-factor model (FFM) and the OCEAN model, is a suggested taxonomy, or grouping, for personality traits, developed from the 1980s onwards. When factor analysis (a statistical technique) is applied to personality survey data, it reveals semantic associations: some words used to describe aspects of personality are often applied to the same person. For example, someone described as conscientious is more likely to be described as "always prepared" rather than "messy". These associations suggest five broad dimensions used in common language to describe the human personality and psyche.The theory identifies five factors: openness to experience (inventive/curious vs. consistent/cautious) conscientiousness (efficient/organized vs. extravagant/careless) extraversion (outgoing/energetic vs. solitary/reserved) agreeableness (friendly/compassionate vs. challenging/callous) neuroticism (sensitive/nervous vs. resilient/confident)The five factors are abbreviated in the acronyms OCEAN or CANOE. Beneath each proposed global factor, there are a number of correlated and more specific primary factors. For example, extraversion is typically associated with qualities such as gregariousness, assertiveness, excitement-seeking, warmth, activity, and positive emotions.Family life and upbringing will affect these traits. Twin studies and other research have shown that about half of the variation between individuals results from their genetic inheritance and half from their environment. Researchers have found conscientiousness, extraversion, openness to experience, and neuroticism to be relatively stable from childhood through adulthood.
The Big Five personality traits was the model to comprehend the relationship between personality and academic behaviors. This model was defined by several independent sets of researchers who used factor analysis of verbal descriptors of human behavior. These researchers began by studying relationships between a large number of verbal descriptors related to personality traits. They reduced the lists of these descriptors by 5–10 fold and then used factor analysis to group the remaining traits (using data mostly based upon people's estimations, in self-report questionnaire and peer ratings) in order to find the underlying factors of personality.The initial model was advanced by Ernest Tupes and Raymond Christal in 1961, but failed to reach an academic audience until the 1980s. In 1990, J.M. Digman advanced his five-factor model of personality, which Lewis Goldberg extended to the highest level of organization. These five overarching domains have been found to contain and subsume most known personality traits and are assumed to represent the basic structure behind all personality traits.At least four sets of researchers have worked independently within lexical hypothesis in personality theory for decades on this problem and have identified generally the same five factors: Tupes and Christal were first, followed by Goldberg at the Oregon Research Institute, Cattell at the University of Illinois, and Costa and McCrae. These four sets of researchers used somewhat different methods in finding the five traits, and thus each set of five factors has somewhat different names and definitions. However, all have been found to be highly inter-correlated and factor-analytically aligned. Studies indicate that the Big Five traits are not nearly as powerful in predicting and explaining actual behavior as are the more numerous facets or primary traits.Each of the Big Five personality traits contains two separate, but correlated, aspects reflecting a level of personality below the broad domains but above the many facet scales that are also part of the Big Five. The aspects are labeled as follows: Volatility and Withdrawal for Neuroticism; Enthusiasm and Assertiveness for Extraversion; Intellect and Openness for Openness to Experience; Industriousness and Orderliness for Conscientiousness; and Compassion and Politeness for Agreeableness. People who do not exhibit a clear predisposition to a single factor in each dimension above are considered adaptable, moderate and reasonable, yet they can also be perceived as unprincipled, inscrutable and calculating.
Openness to experience is a general appreciation for art, emotion, adventure, unusual ideas, imagination, curiosity, and variety of experience. People who are open to experience are intellectually curious, open to emotion, sensitive to beauty and willing to try new things. They tend to be, when compared to closed people, more creative and more aware of their feelings. They are also more likely to hold unconventional beliefs. High openness can be perceived as unpredictability or lack of focus, and more likely to engage in risky behavior or drug-taking. Moreover, individuals with high openness are said to pursue self-actualization specifically by seeking out intense, euphoric experiences. Conversely, those with low openness seek to gain fulfillment through perseverance and are characterized as pragmatic and data-driven—sometimes even perceived to be dogmatic and closed-minded. Some disagreement remains about how to interpret and contextualize the openness factor.
Conscientiousness is a tendency to display self-discipline, act dutifully, and strive for achievement against measures or outside expectations. It is related to the way in which people control, regulate, and direct their impulses. High conscientiousness is often perceived as being stubborn and focused. Low conscientiousness is associated with flexibility and spontaneity, but can also appear as sloppiness and lack of reliability. High scores on conscientiousness indicate a preference for planned rather than spontaneous behavior. The average level of conscientiousness rises among young adults and then declines among older adults.
Extraversion is characterized by breadth of activities (as opposed to depth), surgency from external activity/situations, and energy creation from external means. The trait is marked by pronounced engagement with the external world. Extraverts enjoy interacting with people, and are often perceived as full of energy. They tend to be enthusiastic, action-oriented individuals. They possess high group visibility, like to talk, and assert themselves. Extraverted people may appear more dominant in social settings, as opposed to introverted people in this setting.Introverts have lower social engagement and energy levels than extraverts. They tend to seem quiet, low-key, deliberate, and less involved in the social world. Their lack of social involvement should not be interpreted as shyness or depression; instead they are more independent of their social world than extraverts. Introverts need less stimulation, and more time alone than extraverts. This does not mean that they are unfriendly or antisocial; rather, they are reserved in social situations.Generally, people are a combination of extraversion and introversion, with personality psychologist Hans Eysenck suggesting a model by which individual neurological differences produce these traits.
The agreeableness trait reflects individual differences in general concern for social harmony. Agreeable individuals value getting along with others. They are generally considerate, kind, generous, trusting and trustworthy, helpful, and willing to compromise their interests with others. Agreeable people also have an optimistic view of human nature. Disagreeable individuals place self-interest above getting along with others. They are generally unconcerned with others' well-being, and are less likely to extend themselves for other people. Sometimes their skepticism about others' motives causes them to be suspicious, unfriendly, and uncooperative. Low agreeableness personalities are often competitive or challenging people, which can be seen as argumentative or untrustworthy.Because agreeableness is a social trait, research has shown that one's agreeableness positively correlates with the quality of relationships with one's team members. Agreeableness also positively predicts transformational leadership skills. In a study conducted among 169 participants in leadership positions in a variety of professions, individuals were asked to take a personality test and have two evaluations completed by directly supervised subordinates. Leaders with high levels of agreeableness were more likely to be considered transformational rather than transactional. Although the relationship was not strong (r=0.32, β=0.28, p<0.01), it was the strongest of the Big Five traits. However, the same study showed no predictive power of leadership effectiveness as evaluated by the leader's direct supervisor.Conversely, agreeableness has been found to be negatively related to transactional leadership in the military. A study of Asian military units showed leaders with a high level of agreeableness to be more likely to receive a low rating for transformational leadership skills. Therefore, with further research, organizations may be able to determine an individual's potential for performance based on their personality traits. For instance, in their journal article "Which Personality Attributes Are Most Important in the Workplace?" Paul Sackett and Philip Walmsley claim that conscientiousness and agreeableness are “important to success across many different jobs."
Neuroticism is the tendency to experience negative emotions, such as anger, anxiety, or depression. It is sometimes called emotional instability, or is reversed and referred to as emotional stability. According to Hans Eysenck's (1967) theory of personality, neuroticism is interlinked with low tolerance for stress or aversive stimuli. Neuroticism is a classic temperament trait that has been studied in temperament research for decades, before it was adapted by the FFM. Those who score high in neuroticism are emotionally reactive and vulnerable to stress. They are more likely to interpret ordinary situations as threatening. They can perceive minor frustrations as hopelessly difficult. They also tend to be flippant in the way they express emotions. Their negative emotional reactions tend to persist for unusually long periods of time, which means they are often in a bad mood. For instance, neuroticism is connected to a pessimistic approach toward work, to certainty that work impedes personal relationships, and to higher levels of anxiety from the pressures at work. Furthermore, those who score high on neuroticism may display more skin-conductance reactivity than those who score low on neuroticism. These problems in emotional regulation can diminish the ability of a person scoring high on neuroticism to think clearly, make decisions, and cope effectively with stress. Lacking contentment in one's life achievements can correlate with high neuroticism scores and increase one's likelihood of falling into clinical depression. Moreover, individuals high in neuroticism tend to experience more negative life events, but neuroticism also changes in response to positive and negative life experiences. Also, individuals with higher levels of neuroticism tend to have worse psychological well being.At the other end of the scale, individuals who score low in neuroticism are less easily upset and are less emotionally reactive. They tend to be calm, emotionally stable, and free from persistent negative feelings. Freedom from negative feelings does not mean that low-scorers experience a lot of positive feelings.Neuroticism is similar but not identical to being neurotic in the Freudian sense (i.e., neurosis.) Some psychologists prefer to call neuroticism by the term emotional instability to differentiate it from the term neurotic in a career test.
Historically preceding The Big Five personality traits (B5) or the Five Factors Model (FFM), was Hippocrates's four types of temperament— sanguine, phlegmatic, choleric, and melancholic. The sanguine type is most closely related to emotional stability and extraversion, the phlegmatic type is also stable but introverted, the choleric type is unstable and extraverted, and the melancholic type is unstable and introverted.In 1884, Sir Francis Galton was the first person who is known to have investigated the hypothesis that it is possible to derive a comprehensive taxonomy of human personality traits by sampling language: the lexical hypothesis.In 1936, Gordon Allport and S. Odbert put Sir Francis Galton's hypothesis into practice by extracting 4,504 adjectives which they believed were descriptive of observable and relatively permanent traits from the dictionaries at that time. In 1940, Raymond Cattell retained the adjectives, and eliminated synonyms to reduce the total to 171. He constructed a self-report instrument for the clusters of personality traits he found from the adjectives, which he called the Sixteen Personality Factor Questionnaire. In 1949, the first systematic multivariate research of personality was conducted by Joy P. Guilford. Guilford analyzed ten factors of personality, which he measured by the Guilford-Zimmerman Temperament Survey. These scales included general activity (energy vs inactivity); restraint (seriousness vs impulsiveness); ascendance (social boldness vs submissiveness); sociability (social interest vs shyness); emotional stability (evenness vs fluctuation of mood); objectivity (thick-skinned vs hypersensitive); friendliness (agreeableness vs belligerence); thoughtfulness (reflective vs disconnected), personal relations (tolerance vs hypercritical); masculinity (hard-boiled vs sympathetic). These overlapping scales were later further analyzed by Guilford et al., and condense into three dimensions: social activity (general activity, ascendence, sociability), introversion-extraversion (restraint, thoughtfulness), and emotional health (emotional stability, objectivity, friendliness, personal relations).Based on a subset of only 20 of the 36 dimensions that Cattell had originally discovered, Ernest Tupes and Raymond Christal (1961) claimed to have found just five broad factors which they labeled: "surgency", "agreeableness", "dependability", "emotional stability", and "culture". Warren Norman subsequently relabeled "dependability" as "conscientiousness".
During the late 1960s to 1970s, the changing zeitgeist made publication of personality research difficult. In his 1968 book Personality and Assessment, Walter Mischel asserted that personality instruments could not predict behavior with a correlation of more than 0.3. Social psychologists like Mischel argued that attitudes and behavior were not stable, but varied with the situation. Predicting behavior from personality instruments was claimed to be impossible.
The paradigm shift back to acceptance of the five-factor model came in the early 1980s. In a 1980 symposium in Honolulu, four prominent researchers, Lewis Goldberg, Naomi Takemoto-Chock, Andrew Comrey, and John M. Digman, reviewed the available personality instruments of the day. This event was followed by widespread acceptance of the five-factor model among personality researchers during the 1980s.By 1983, experiments had demonstrated that the predictions of personality models correlated better with real-life behavior under stressful emotional conditions, as opposed to typical survey administration under neutral emotional conditions.Peter Saville and his team included the five-factor "Pentagon" model with the original OPQ in 1984. Pentagon was closely followed by the NEO five-factor personality inventory, published by Costa and McCrae in 1985. However, the methodology employed in constructing the NEO instrument has been subject to critical scrutiny (see section below).Emerging methodologies increasing confirmed personality theories during the 1980s. Though generally failing to predict single instances of behavior, researchers found that they could predict patterns of behavior by aggregating large numbers of observations. As a result, correlations between personality and behavior increased substantially, and it was clear that "personality" did in fact exist.Personality and social psychologists now generally agree that both personal and situational variables are needed to account for human behavior. Trait theories amassed favorable evidence, and there was a resurgence of interest in this area. In the 1980s, Lewis Goldberg started his own lexical project, again emphasizing five broad factors which he later labeled the "Big Five". Colin G. DeYoung et al. (2016) tested how these 25 facets could be integrated with the 10-factor structure of traits within the Big Five. The developers mainly researched the Big Five model and how the five broad factors are compatible with the 25 scales of the Personality Inventory (PID-5) for the DSM-5. DeYoung et al. considers the PID-5 to measure facet-level traits. Because the Big Five factors are broader than the 25 scales of the PID-5, there is disagreement in personality psychology relating to the number of factors within the Big Five. According to DeYoung et al. (2016), "the number of valid facets might be limited only by the number of traits that can be shown to have discriminant validity."The FFM-associated test was used by Cambridge Analytica, and was part of the "psychographic profiling" controversy during the 2016 US presidential election.
There of course are factors that influence a personality and these are called the determinants of personality. These factors determine the traits which a person develops in the course of development from a child.
There are debates between researchers of temperament and researchers of personality as to whether or not biologically-based differences define a concept of temperament or a part of personality. The presence of such differences in pre-cultural individuals (such as animals or young infants) suggests that they belong to temperament since personality is a socio-cultural concept. For this reason developmental psychologists generally interpret individual differences in children as an expression of temperament rather than personality. Some researchers argue that temperaments and personality traits are age-specific manifestations of virtually the same latent qualities. Some believe that early childhood temperaments may become adolescent and adult personality traits as individuals' basic genetic characteristics actively, reactively, and passively interact with their changing environments.Researchers of adult temperament point out that, similarly to sex, age and mental illness, temperament is based on biochemical systems whereas personality is a product of socialization of an individual possessing these four types of features. Temperament interacts with social-cultural factors, but still cannot be controlled or easily changed by these factors. Therefore, it is suggested that temperament should be kept as an independent concept for further studies and not be conflated with personality. Moreover, temperament refers to dynamical features of behavior (energetic, tempo, sensitivity and emotionality-related), whereas personality is to be considered a psycho-social construct comprising the content characteristics of human behavior (such as values, attitudes, habits, preferences, personal history, self-image). Temperament researchers point out that the lack of attention to extant temperament research by the developers of the Big Five model lead to an overlap between its dimensions and dimensions described in multiple temperament models much earlier. For example, neuroticism reflects the traditional temperament dimension of emotionality, extraversion the temperament dimension of "energy" or "activity", and openness to experience the temperament dimension of sensation-seeking.
Genetically informative research, including twin studies, suggest that heritability and environmental factors both influence all five factors to the same degree. Among four recent twin studies, the mean percentage for heritability was calculated for each personality and it was concluded that heritability influenced the five factors broadly. The self-report measures were as follows: openness to experience was estimated to have a 57% genetic influence, extraversion 54%, conscientiousness 49%, neuroticism 48%, and agreeableness 42%.
The Big Five personality traits have been assessed in some non-human species but methodology is debatable. In one series of studies, human ratings of chimpanzees using the Hominoid Personality Questionnaire, revealed factors of extraversion, conscientiousness and agreeableness – as well as an additional factor of dominance – across hundreds of chimpanzees in zoological parks, a large naturalistic sanctuary, and a research laboratory. Neuroticism and openness factors were found in an original zoo sample, but were not replicated in a new zoo sample or in other settings (perhaps reflecting the design of the CPQ). A study review found that markers for the three dimensions extraversion, neuroticism, and agreeableness were found most consistently across different species, followed by openness; only chimpanzees showed markers for conscientious behavior.
Research on the Big Five, and personality in general, has focused primarily on individual differences in adulthood, rather than in childhood and adolescence, and often include temperament traits. Recently, there has been growing recognition of the need to study child and adolescent personality trait development in order to understand how traits develop and change throughout the lifespan.Recent studies have begun to explore the developmental origins and trajectories of the Big Five among children and adolescents, especially those that relate to temperament. Many researchers have sought to distinguish between personality and temperament. Temperament often refers to early behavioral and affective characteristics that are thought to be driven primarily by genes. Models of temperament often include four trait dimensions: surgency/ sociability, negative emotionality, persistence/effortful control, and activity level. Some of these differences in temperament are evident at, if not before, birth. For example, both parents and researchers recognize that some newborn infants are peaceful and easily soothed while others are comparatively fussy and hard to calm. Unlike temperament, however, many researchers view the development of personality as gradually occurring throughout childhood. Contrary to some researchers who question whether children have stable personality traits, Big Five or otherwise, most researchers contend that there are significant psychological differences between children that are associated with relatively stable, distinct, and salient behavior patterns.The structure, manifestations, and development of the Big Five in childhood and adolescence have been studied using a variety of methods, including parent- and teacher-ratings, preadolescent and adolescent self- and peer-ratings, and observations of parent-child interactions. Results from these studies support the relative stability of personality traits across the human lifespan, at least from preschool age through adulthood. More specifically, research suggests that four of the Big Five –namely Extraversion, Neuroticism, Conscientiousness, and Agreeableness- reliably describe personality differences in childhood, adolescence, and adulthood. However, some evidence suggests that Openness may not be a fundamental, stable part of childhood personality. Although some researchers have found that Openness in children and adolescents relates to attributes such as creativity, curiosity, imagination, and intellect, many researchers have failed to find distinct individual differences in Openness in childhood and early adolescence. Potentially, Openness may (a) manifest in unique, currently unknown ways in childhood or (b) may only manifest as children develop socially and cognitively. Other studies have found evidence for all of the Big Five traits in childhood and adolescence as well as two other child-specific traits: Irritability and Activity. Despite these specific differences, the majority of findings suggest that personality traits –particularly Extraversion, Neuroticism, Conscientiousness, and Agreeableness- are evident in childhood and adolescence and are associated with distinct social-emotional patterns of behavior that are largely consistent with adult manifestations of those same personality traits. Some researchers have proposed the youth personality trait is best described by six trait dimensions: neuroticism, extraversion, openness to experience, agreeableness, conscientiousness, and activity. Despite some preliminary evidence for this “Little Six” model, research in this area has been delayed by a lack of available measures. Previous research has found evidence that most adults become more agreeable, conscientious, and less neurotic as they age. This has been referred to as the maturation effect. Many researchers have sought to investigate how trends in adult personality development compare to trends in youth personality development. Two main population-level indices have been important in this area of research: rank-order consistency and mean-level consistency. Rank-order consistency indicates the relative placement of individuals within a group. Mean-level consistency indicates whether groups increase or decrease on certain traits throughout the lifetime.Findings from these studies indicate that, consistent with adult personality trends, youth personality becomes increasingly more stable in terms of rank-order throughout childhood. Unlike adult personality research, which indicates that people become agreeable, conscientious, and emotionally stable with age, some findings in youth personality research have indicated that mean-levels of agreeableness, conscientiousness, and openness to experience decline from late childhood to late adolescence. The disruption hypothesis, which proposes that biological, social, and psychological changes experienced during youth result in temporary dips in maturity, has been proposed to explain these findings.
In Big Five studies, extraversion has been associated with surgency. Children with high Extraversion are energetic, talkative, social, and dominant with children and adults; whereas, children with low Extraversion tend to be quiet, calm, inhibited, and submissive to other children and adults. Individual differences in Extraversion first manifest in infancy as varying levels of positive emotionality. These differences in turn predict social and physical activity during later childhood and may represent, or be associated with, the behavioral activation system. In children, Extraversion/Positive Emotionality includes four sub-traits: three traits that are similar to the previously described traits of temperament – activity, sociability, shyness, and the trait of dominance. Activity: Similarly to findings in temperament research, children with high activity tend to have high energy levels and more intense and frequent motor activity compared to their peers. Salient differences in activity reliably manifest in infancy, persist through adolescence, and fade as motor activity decreases in adulthood or potentially develops into talkativeness. Dominance: Children with high dominance tend to influence the behavior of others, particularly their peers, to obtain desirable rewards or outcomes. Such children are generally skilled at organizing activities and games and deceiving others by controlling their nonverbal behavior. Shyness: Children with high shyness are generally socially withdrawn, nervous, and inhibited around strangers. In time, such children may become fearful even around "known others", especially if their peers reject them. Similar pattern was described in temperament longitudinal studies of shyness Sociability: Children with high sociability generally prefer to be with others rather than alone. During middle childhood, the distinction between low sociability and high shyness becomes more pronounced, particularly as children gain greater control over how and where they spend their time.
Many studies of longitudinal data, which correlate people's test scores over time, and cross-sectional data, which compare personality levels across different age groups, show a high degree of stability in personality traits during adulthood, especially Neuroticism trait that is often regarded as a temperament trait similarly to longitudinal research in temperament for the same traits. It is shown that the personality stabilizes for working-age individuals within about four years after starting working. There is also little evidence that adverse life events can have any significant impact on the personality of individuals. More recent research and meta-analyses of previous studies, however, indicate that change occurs in all five traits at various points in the lifespan. The new research shows evidence for a maturation effect. On average, levels of agreeableness and conscientiousness typically increase with time, whereas extraversion, neuroticism, and openness tend to decrease. Research has also demonstrated that changes in Big Five personality traits depend on the individual's current stage of development. For example, levels of agreeableness and conscientiousness demonstrate a negative trend during childhood and early adolescence before trending upwards during late adolescence and into adulthood. In addition to these group effects, there are individual differences: different people demonstrate unique patterns of change at all stages of life.In addition, some research (Fleeson, 2001) suggests that the Big Five should not be conceived of as dichotomies (such as extraversion vs. introversion) but as continua. Each individual has the capacity to move along each dimension as circumstances (social or temporal) change. He is or she is therefore not simply on one end of each trait dichotomy but is a blend of both, exhibiting some characteristics more often than others:Research regarding personality with growing age has suggested that as individuals enter their elder years (79–86), those with lower IQ see a raise in extraversion, but a decline in conscientiousness and physical well being.Research by Cobb-Clark and Schurer indicates that personality traits are generally stable among adult workers. The research done on personality also mirrors previous results on locus of control.
While personality is mostly stable in adulthood, some diseases can alter personality. Gradual impairment of memory is the hallmark feature of Alzheimer's disease, but changes in personality also commonly occur. A review of personality change in Alzheimer's disease found a characteristic pattern of personality change in patients with Alzheimer's disease: a large decrease in Conscientiousness of two to three standard deviations, a decrease in Extraversion of one to two standard deviations, a reduction in Agreeableness of less than one standard deviation, and an increase in Neuroticism of between one and two standard deviations.
Cross-cultural research has shown some patterns of gender differences on responses to the NEO-PI-R and the Big Five Inventory. For example, women consistently report higher Neuroticism, Agreeableness, warmth (an extraversion facet) and openness to feelings, and men often report higher assertiveness (a facet of extraversion) and openness to ideas as assessed by the NEO-PI-R.A study of gender differences in 55 nations using the Big Five Inventory found that women tended to be somewhat higher than men in neuroticism, extraversion, agreeableness, and conscientiousness. The difference in neuroticism was the most prominent and consistent, with significant differences found in 49 of the 55 nations surveyed. Gender differences in personality traits are largest in prosperous, healthy, and more gender-egalitarian cultures. A plausible explanation for this is that acts by women in individualistic, egalitarian countries are more likely to be attributed to their personality, rather than being attributed to ascribed gender roles within collectivist, traditional countries. Differences in the magnitude of sex differences between more or less developed world regions were due to differences between men, not women, in these respective regions. That is, men in highly developed world regions were less neurotic, extraverted, conscientious and agreeable compared to men in less developed world regions. Women, on the other hand tended not to differ in personality traits across regions. The authors of this study speculated that resource-poor environments (that is, countries with low levels of development) may inhibit the development of gender differences, whereas resource-rich environments facilitate them. This may be because males require more resources than females in order to reach their full developmental potential. The authors also argued that due to different evolutionary pressures, men may have evolved to be more risk taking and socially dominant, whereas women evolved to be more cautious and nurturing. Ancient hunter-gatherer societies may have been more egalitarian than later agriculturally oriented societies. Hence, the development of gender inequalities may have acted to constrain the development of gender differences in personality that originally evolved in hunter-gatherer societies. As modern societies have become more egalitarian, again, it may be that innate sex differences are no longer constrained and hence manifest more fully than in less-developed cultures. Currently, this hypothesis remains untested, as gender differences in modern societies have not been compared with those in hunter-gatherer societies.
Frank Sulloway argues that firstborns are more conscientious, more socially dominant, less agreeable, and less open to new ideas compared to laterborns. Large-scale studies using random samples and self-report personality tests, however, have found milder effects than Sulloway claimed, or no significant effects of birth order on personality. A study using the Project Talent data, which is a large-scale representative survey of American high-schoolers, with 272,003 eligible targets, found statistically significant but very small effects (the average absolute correlation between birth order and personality was .02) of birth order on personality, such that first borns were slightly more conscientious, dominant, and agreeable, while also being less neurotic and less sociable. Parental SES and participant gender had much larger correlations with personality. In 2002, the journal of psychology posted a Big Five Personality Trait Difference; Researchers explored relationship between the five factor model and the Universal-Diverse Orientation (UDO) in counselor trainees. (Thompson, R., Brossart, D., and Mivielle, A., 2002) UDO is known as one social attitude that produces a strong awareness and/or acceptance towards the similarities and differences amongst individuals. (Miville, M., Romas, J., Johnson, J., and Lon, R. 2002) The study has shown the counselor trainees that are more open to the idea of creative expression (a facet of Openness to Experience, Openness to Aesthetics) amongst individuals are more likely to work with a diverse group of clients, and feel comfortable in their role.
The Big Five have been pursued in a variety of languages and cultures, such as German, Chinese, and Indian. For example, Thompson has claimed to find the Big Five structure across several cultures using an international English language scale. Cheung, van de Vijver, and Leong (2011) suggest, however, that the Openness factor is particularly unsupported in Asian countries and that a different fifth factor is identified.Recent work has found relationships between Geert Hofstede's cultural factors, Individualism, Power Distance, Masculinity, and Uncertainty Avoidance, with the average Big Five scores in a country. For instance, the degree to which a country values individualism correlates with its average extraversion, whereas people living in cultures which are accepting of large inequalities in their power structures tend to score somewhat higher on conscientiousness. Personality differences around the world might even have contributed to the emergence of different political systems. A recent study has found that countries' average personality trait levels are correlated with their political systems: countries with higher average trait Openness tended to have more democratic institutions, an association that held even after factoring out other relevant influences such as economic development.Attempts to replicate the Big Five in other countries with local dictionaries have succeeded in some countries but not in others. Apparently, for instance, Hungarians do not appear to have a single agreeableness factor. Other researchers have found evidence for agreeableness but not for other factors. It is important to recognize that individual differences in traits are relevant in a specific cultural context, and that the traits do not have their effects outside of that context.
As of 2002, there were over fifty published studies relating the FFM to personality disorders. Since that time, quite a number of additional studies have expanded on this research base and provided further empirical support for understanding the DSM personality disorders in terms of the FFM domains.In her review of the personality disorder literature published in 2007, Lee Anna Clark asserted that "the five-factor model of personality is widely accepted as representing the higher-order structure of both normal and abnormal personality traits". However, other researchers disagree that this model is widely accepted (see the section Critique below) and suggest that it simply replicates early temperament research. Noticeably, FFM publications never compare their findings to temperament models even though temperament and mental disorders (especially personality disorders) are thought to be based on the same neurotransmitter imbalances, just to varying degrees.The five-factor model was claimed to significantly predict all ten personality disorder symptoms and outperform the Minnesota Multiphasic Personality Inventory (MMPI) in the prediction of borderline, avoidant, and dependent personality disorder symptoms. However, most predictions related to an increase in Neuroticism and a decrease in Agreeableness, and therefore did not differentiate between the disorders very well.
Five major models have been posed to explain the nature of the relationship between personality and mental illness. There is currently no single "best model", as each of them has received at least some empirical support. It is also important to note that these models are not mutually exclusive – more than one may be operating for a particular individual and various mental disorders may be explained by different models. The Vulnerability/Risk Model: According to this model, personality contributes to the onset or etiology of various common mental disorders. In other words, pre-existing personality traits either cause the development of CMDs directly or enhance the impact of causal risk factors. There is strong support for neuroticism being a robust vulnerability factor. The Pathoplasty Model: This model proposes that premorbid personality traits impact the expression, course, severity, and/or treatment response of a mental disorder. An example of this relationship would be a heightened likelihood of committing suicide for a depressed individual who also has low levels of constraint. The Common Cause Model: According to the common cause model, personality traits are predictive of CMDs because personality and psychopathology have shared genetic and environmental determinants which result in non-causal associations between the two constructs. The Spectrum Model: This model proposes that associations between personality and psychopathology are found because these two constructs both occupy a single domain or spectrum and psychopathology is simply a display of the extremes of normal personality function. Support for this model is provided by an issue of criterion overlap. For instance, two of the primary facet scales of neuroticism in the NEO-PI-R are "depression" and "anxiety". Thus the fact that diagnostic criteria for depression, anxiety, and neuroticism assess the same content increases the correlations between these domains. The Scar Model: According to the scar model, episodes of a mental disorder 'scar' an individual's personality, changing it in significant ways from premorbid functioning. An example of a scar effect would be a decrease in openness to experience following an episode of PTSD.
To examine how the Big Five personality traits are related to subjective health outcomes (positive and negative mood, physical symptoms, and general health concern) and objective health conditions (chronic illness, serious illness, and physical injuries), a study, conducted by Jasna Hudek-Knezevic and Igor Kardum, from a sample of 822 healthy volunteers (438 women and 384 men). As a result, out of the Big Five personality traits, neuroticism was found most related to worse subjective health outcomes and optimistic control to better subjective health outcomes. When relating to objective health conditions, connections drawn were presented weak, except for neuroticism significantly predicted chronic illness, whereas optimistic control was more closely related to physical injuries caused by accident.Being highly conscientious may add as much as five years to one's life. The Big Five personality traits also predict positive health outcomes. In an elderly Japanese sample, conscientiousness, extraversion, and openness were related to lower risk of mortality.Higher conscientiousness is associated with lower obesity risk. In already obese individuals, higher conscientiousness is associated with a higher likelihood of becoming non-obese over a 5-year period.
Personality plays an important role in academic achievement. A study of 308 undergraduates who completed the Five Factor Inventory Processes and reported their GPA suggested that conscientiousness and agreeableness have a positive relationship with all types of learning styles (synthesis-analysis, methodical study, fact retention, and elaborative processing), whereas neuroticism shows an inverse relationship. Moreover, extraversion and openness were proportional to elaborative processing. The Big Five personality traits accounted for 14% of the variance in GPA, suggesting that personality traits make some contributions to academic performance. Furthermore, reflective learning styles (synthesis-analysis and elaborative processing) were able to mediate the relationship between openness and GPA. These results indicate that intellectual curiosity significantly enhances academic performance if students combine their scholarly interest with thoughtful information processing.A recent study of Israeli high-school students found that those in the gifted program systematically scored higher on openness and lower on neuroticism than those not in the gifted program. While not a measure of the Big Five, gifted students also reported less state anxiety than students not in the gifted program. Specific Big Five personality traits predict learning styles in addition to academic success. GPA and exam performance are both predicted by conscientiousness neuroticism is negatively related to academic success openness predicts utilizing synthesis-analysis and elaborative-processing learning styles neuroticism negatively correlates with learning styles in general openness and extraversion both predict all four learning styles.Studies conducted on college students have concluded that hope, which is linked to agreeableness, has a positive effect on psychological well being. Individuals high in neurotic tendencies are less likely to display hopeful tendencies and are negatively associated with well-being. Personality can sometimes be flexible and measuring the big five personality for individuals as they enter certain stages of life may predict their educational identity. Recent studies have suggested the likelihood of an individual's personality affecting their educational identity.
Learning styles have been described as "enduring ways of thinking and processing information".In 2008, the Association for Psychological Science (APS) commissioned a report whose conclusion indicates that no significant evidence exists to make the conclusion that learning-style assessments should be included in the education system. Thus it is premature, at best, to conclude that the evidence linking the Big Five to "learning styles", or "learning styles" to learning itself, is valid. However, the APS also suggested in their report that all existing learning styles have not been exhausted and that there could exist learning styles that have the potential to be worthy of being included in educational practices. There are studies that conclude that personality and thinking styles may be intertwined in ways that link thinking styles to the Big Five personality traits. There is no general consensus on the number or specifications of particular learning styles, but there have been many different proposals. As one example, Schmeck, Ribich, and Ramanaiah (1997) defined four types of learning styles: synthesis analysis methodical study fact retention elaborative processingWhen all four facets are implicated within the classroom, they will each likely improve academic achievement. This model asserts that students develop either agentic/shallow processing or reflective/deep processing. Deep processors are more often than not found to be more conscientious, intellectually open, and extraverted when compared to shallow processors. Deep processing is associated with appropriate study methods (methodical study) and a stronger ability to analyze information (synthesis analysis), whereas shallow processors prefer structured fact retention learning styles and are better suited for elaborative processing. The main functions of these four specific learning styles are as follows: Openness has been linked to learning styles that often lead to academic success and higher grades like synthesis analysis and methodical study. Because conscientiousness and openness have been shown to predict all four learning styles, it suggests that individuals who possess characteristics like discipline, determination, and curiosity are more likely to engage in all of the above learning styles.According to the research carried out by Komarraju, Karau, Schmeck & Avdic (2011), conscientiousness and agreeableness are positively related with all four learning styles, whereas neuroticism was negatively related with those four. Furthermore, extraversion and openness were only positively related to elaborative processing, and openness itself correlated with higher academic achievement.In addition, a previous study by Mikael Jensen has shown relationships between The Big Five personality traits, learning, and academic achievement. According to psychologist Jensen, all personality traits, except neuroticism, are associated with learning goals and motivation. Openness and conscientiousness influence individuals to learn to a high degree unrecognized, while extraversion and agreeableness have similar effects. Conscientiousness and neuroticism also influence individuals to perform well in front of others for a sense of credit and reward, while agreeableness forces individuals to avoid this strategy of learning. As a result of Jensen's study, it is likely that individuals who score high on the agreeableness trait will learn just to perform well in front of others.Besides openness, all Big Five personality traits helped predict the educational identity of students. Based on these findings, scientists are beginning to see that there might be a large influence of the Big Five traits on academic motivation that then leads to predicting a student's academic performance.Some authors suggested that Big Five personality traits combined with learning styles can help predict some variations in the academic performance and the academic motivation of an individual which can then influence their academic achievements. This may be seen because individual differences in personality represent stable approaches to information processing. For instance, conscientiousness has consistently emerged as a stable predictor of success in exam performance, largely because conscientious students experience fewer study delays. The reason conscientiousness shows a positive association with the four learning styles is because students with high levels of conscientiousness develop focused learning strategies and appear to be more disciplined and achievement-oriented.
It is believed that the Big Five traits are predictors of future performance outcomes. Job outcome measures include job and training proficiency and personnel data. However, research demonstrating such prediction has been criticized, in part because of the apparently low correlation coefficients characterizing the relationship between personality and job performance. In a 2007 article co-authored by six current or former editors of psychological journals, Dr. Kevin Murphy, Professor of Psychology at Pennsylvania State University and Editor of the Journal of Applied Psychology (1996–2002), states: The problem with personality tests is ... that the validity of personality measures as predictors of job performance is often disappointingly low. The argument for using personality tests to predict performance does not strike me as convincing in the first place. Such criticisms were put forward by Walter Mischel, whose publication caused a two-decades' long crisis in personality psychometrics. However, later work demonstrated (1) that the correlations obtained by psychometric personality researchers were actually very respectable by comparative standards, and (2) that the economic value of even incremental increases in prediction accuracy was exceptionally large, given the vast difference in performance by those who occupy complex job positions.There have been studies that link national innovation to openness to experience and conscientiousness. Those who express these traits have showed leadership and beneficial ideas towards the country of origin.Some businesses, organizations, and interviewers assess individuals based on the Big Five personality traits. Research has suggested that individuals who are considered leaders typically exhibit lower amounts of neurotic traits, maintain higher levels of openness (envisioning success), balanced levels of conscientiousness (well-organized), and balanced levels of extraversion (outgoing, but not excessive). Further studies have linked professional burnout to neuroticism, and extraversion to enduring positive work experience. When it comes to making money, research has suggested that those who are high in agreeableness (especially men) are not as successful in accumulating income.Some research suggests that vocational outcomes are correlated to Big Five personality traits. Conscientiousness predicts job performance in general. Conscientiousness is considered as top-ranked in overall job performance, research further categorized the Big 5 behaviors into 3 perspectives: task performance, organizational citizenship behavior, and counterproductive work behavior. Task performance is the set of activity that a worker is hired to complete, and results showed that Extraversion ranked second after the Conscientiousness, with Emotional Stability tied with Agreeableness ranked third. For organizational citizenship behavior, relatively less tied to the specific task core but benefits an organization by contributing to its social and psychological environment, Agreeableness and Emotional Stability ranked second and third. Lastly, Agreeableness tied with Conscientiousness as top ranked for Counterproductive work behavior, which refers to intentional behavior that is counter to the legitimate interests of the organization or its members.In addition, research has demonstrated that agreeableness is negatively related to salary. Those high in agreeableness make less, on average, than those low in the same trait. Neuroticism is also negatively related to salary while conscientiousness and extraversion are positive predictors of salary. Occupational self-efficacy has also been shown to be positively correlated with conscientiousness and negatively correlated with neuroticism. Significant predictors of career-advancement goals are: extraversion, conscientiousness, and agreeableness. Some research has also suggested that the Conscientiousness of a supervisor is positively associated with an employee's perception of abusive supervision. While others have suggested that those with low agreeableness and high neuroticism are traits more related to abusive supervision.A 2019 study of Canadian adults found conscientiousness to be positively associated with wages, while agreeableness, extraversion, and neuroticism were negatively associated with wages. In the United States, by contrast, no negative correlation between extraversion and wages has been found. Also, the magnitudes found for agreeableness and conscientiousness in this study were higher for women than for men (i.e. there was a higher negative penalty for greater agreeableness in women, as well as a higher positive reward for greater conscientiousness).Research designed to investigate the individual effects of Big Five personality traits on work performance via worker completed surveys and supervisor ratings of work performance has implicated individual traits in several different work roles performances. A "work role" is defined as the responsibilities an individual has while they are working. Nine work roles have been identified, which can be classified in three broader categories: proficiency (the ability of a worker to effectively perform their work duties), adaptivity (a workers ability to change working strategies in response to changing work environments), and proactivity (extent to which a worker will spontaneously put forth effort to change the work environment). These three categories of behavior can then be directed towards three different levels: either the individual, team, or organizational level leading to the nine different work role performance possibilities. Openness is positively related to proactivity at the individual and the organizational levels and is negatively related to team and organizational proficiency. These effects were found to be completely independent of one another. Agreeableness is negatively related to individual task proactivity. Extraversion is negatively related to individual task proficiency. Conscientiousness is positively related to all forms of work role performance. Neuroticism is negatively related to all forms of work role performance.Two theories have been integrated in an attempt to account for these differences in work role performance. Trait activation theory posits that within a person trait levels predict future behavior, that trait levels differ between people, and that work-related cues activate traits which leads to work relevant behaviors. Role theory suggests that role senders provide cues to elicit desired behaviors. In this context, role senders (i.e.: supervisors, managers, et cetera) provide workers with cues for expected behaviors, which in turn activates personality traits and work relevant behaviors. In essence, expectations of the role sender lead to different behavioral outcomes depending on the trait levels of individual workers and because people differ in trait levels, responses to these cues will not be universal.
The Big Five model of personality was used for attempts to predict satisfaction in romantic relationships, relationship quality in dating, engaged, and married couples.Dating couples Self-reported relationship quality is negatively related to partner-reported neuroticism and positively related to both self and partner-reported conscientiousnessEngaged couples Self-reported relationship quality was higher among those high in partner-reported openness, agreeableness and conscientiousness. Self-reported relationship quality was higher among those high in self-reported extraversion and agreeableness. Self-reported relationship quality is negatively related to both self and partner-reported neuroticism Observers rated the relationship quality higher if the participating partner's self-reported extraversion was highMarried couples High self-reported neuroticism, extraversion, and agreeableness are related to high levels of self-reported relationship quality Partner-reported agreeableness is related to observed relationship quality.These reports are, however, rare and not conclusive.
The Big Five Personality Model also has applications in the study of political psychology. Studies have been finding links between the big five personality traits and political identification. It has been found by several studies that individuals who score high in Conscientiousness are more likely to possess a right-wing political identification. On the opposite end of the spectrum, a strong correlation was identified between high scores in Openness to Experience and a left-leaning ideology. While the traits of agreeableness, extraversion, and neuroticism have not been consistently linked to either conservative or liberal ideology, with studies producing mixed results, such traits are promising when analyzing the strength of an individual's party identification. However, correlations between the Big Five and political beliefs, while present, tend to be small, with one study finding correlations ranged from 0.14 to 0.24.
The predictive effects of the Big Five personality traits relate mostly to social functioning and rules-driven behavior and are not very specific for prediction of particular aspects of behavior. For example, it was noted that high neuroticism precedes the development of all common mental disorders and is not attributed with personality by all temperament researchers. Further evidence is required to fully uncover the nature and differences between personality traits, temperament and life outcomes. Social and contextual parameters also play a role in outcomes and the interaction between the two is not yet fully understood.
Several measures of the Big Five exist: International Personality Item Pool (IPIP) NEO-PI-R The Ten-Item Personality Inventory (TIPI) and the Five Item Personality Inventory (FIPI) are very abbreviated rating forms of the Big Five personality traits. Self-descriptive sentence questionnaires Lexical questionnaires Self-report questionnaires Relative-scored Big 5 measureThe most frequently used measures of the Big Five comprise either items that are self-descriptive sentences or, in the case of lexical measures, items that are single adjectives. Due to the length of sentence-based and some lexical measures, short forms have been developed and validated for use in applied research settings where questionnaire space and respondent time are limited, such as the 40-item balanced International English Big-Five Mini-Markers or a very brief (10 item) measure of the Big Five domains. Research has suggested that some methodologies in administering personality tests are inadequate in length and provide insufficient detail to truly evaluate personality. Usually, longer, more detailed questions will give a more accurate portrayal of personality. The five factor structure has been replicated in peer reports. However, many of the substantive findings rely on self-reports. Much of the evidence on the measures of the Big 5 relies on self-report questionnaires, which makes self-report bias and falsification of responses difficult to deal with and account for. It has been argued that the Big Five tests do not create an accurate personality profile because the responses given on these tests are not true in all cases. For example, questionnaires are answered by potential employees who might choose answers that paint them in the best light.Research suggests that a relative-scored Big Five measure in which respondents had to make repeated choices between equally desirable personality descriptors may be a potential alternative to traditional Big Five measures in accurately assessing personality traits, especially when lying or biased responding is present. When compared with a traditional Big Five measure for its ability to predict GPA and creative achievement under both normal and "fake good"-bias response conditions, the relative-scored measure significantly and consistently predicted these outcomes under both conditions; however, the Likert questionnaire lost its predictive ability in the faking condition. Thus, the relative-scored measure proved to be less affected by biased responding than the Likert measure of the Big Five. Andrew H. Schwartz analyzed 700 million words, phrases, and topic instances collected from the Facebook messages of 75,000 volunteers, who also took standard personality tests, and found striking variations in language with personality, gender, and age.
The proposed Big Five model has been subjected to considerable critical scrutiny in a number of published studies. One prominent critic of the model has been Jack Block at the University of California, Berkeley. In response to Block, the model was defended in a paper published by Costa and McCrae. This was followed by a number of published critical replies from Block.It has been argued that there are limitations to the scope of the Big Five model as an explanatory or predictive theory. It has also been argued that measures of the Big Five account for only 56% of the normal personality trait sphere alone (not even considering the abnormal personality trait sphere). Also, the static Big Five is not theory-driven, it is merely a statistically-driven investigation of certain descriptors that tend to cluster together often based on less than optimal factor analytic procedures. Measures of the Big Five constructs appear to show some consistency in interviews, self-descriptions and observations, and this static five-factor structure seems to be found across a wide range of participants of different ages and cultures. However, while genotypic temperament trait dimensions might appear across different cultures, the phenotypic expression of personality traits differs profoundly across different cultures as a function of the different socio-cultural conditioning and experiential learning that takes place within different cultural settings.Moreover, the fact that the Big Five model was based on lexical hypothesis, (i.e. on the verbal descriptors of individual differences) indicated strong methodological flaws in this model, especially related to its main factors, Extraversion and Neuroticism. First, there is a natural pro-social bias of language in people's verbal evaluations. After all, language is an invention of group dynamics that was developed to facilitate socialization, the exchange of information and to synchronize group activity. This social function of language therefore creates a sociability bias in verbal descriptors of human behavior: there are more words related to social than physical or even mental aspects of behavior. The sheer number of such descriptors will cause them to group into a largest factor in any language, and such grouping has nothing to do with the way that core systems of individual differences are set up. Second, there is also a negativity bias in emotionality (i.e. most emotions have negative affectivity), and there are more words in language to describe negative rather than positive emotions. Such asymmetry in emotional valence creates another bias in language. Experiments using the lexical hypothesis approach indeed demonstrated that the use of lexical material skews the resulting dimensionality according to a sociability bias of language and a negativity bias of emotionality, grouping all evaluations around these two dimensions. This means that the two largest dimensions in the Big Five model might be just an artifact of the lexical approach that this model employed.
One common criticism is that the Big Five does not explain all of human personality. Some psychologists have dissented from the model precisely because they feel it neglects other domains of personality, such as religiosity, manipulativeness/machiavellianism, honesty, sexiness/seductiveness, thriftiness, conservativeness, masculinity/femininity, snobbishness/egotism, sense of humour, and risk-taking/thrill-seeking. Dan P. McAdams has called the Big Five a "psychology of the stranger", because they refer to traits that are relatively easy to observe in a stranger; other aspects of personality that are more privately held or more context-dependent are excluded from the Big Five.In many studies, the five factors are not fully orthogonal to one another; that is, the five factors are not independent. Orthogonality is viewed as desirable by some researchers because it minimizes redundancy between the dimensions. This is particularly important when the goal of a study is to provide a comprehensive description of personality with as few variables as possible.
Factor analysis, the statistical method used to identify the dimensional structure of observed variables, lacks a universally recognized basis for choosing among solutions with different numbers of factors. A five factor solution depends on some degree of interpretation by the analyst. A larger number of factors may underlie these five factors. This has led to disputes about the "true" number of factors. Big Five proponents have responded that although other solutions may be viable in a single dataset, only the five factor structure consistently replicates across different studies.Moreover, the factor analysis that this model is based on is a linear method incapable of capturing nonlinear, feedback and contingent relationships between core systems of individual differences.
A frequent criticism is that the Big Five is not based on any underlying theory; it is merely an empirical finding that certain descriptors cluster together under factor analysis. Although this does not mean that these five factors do not exist, the underlying causes behind them are unknown. Jack Block's final published work before his death in January 2010 drew together his lifetime perspective on the five-factor model.He summarized his critique of the model in terms of: the atheoretical nature of the five-factors. their "cloudy" measurement. the model's inappropriateness for studying early childhood. the use of factor analysis as the exclusive paradigm for conceptualizing personality. the continuing non-consensual understandings of the five-factors. the existence of unrecognized but successful efforts to specify aspects of character not subsumed by the five-factors.He went on to suggest that repeatedly observed higher order factors hierarchically above the proclaimed Big Five personality traits may promise deeper biological understanding of the origins and implications of these superfactors.
It has been noted that even though early lexical studies in the English language indicated five large groups of personality traits, more recent, and more comprehensive, cross-language studies have provided evidence for six large groups rather than five. These six groups forms the basis of the HEXACO model of personality structure. Based on these findings it has been suggested that the Big Five system should be replaced by HEXACO, or revised to better align with lexical evidence.
Big Five personality traits and culture Core self-evaluations Dark triad DISC assessment Facet Genomics of personality traits Goal orientation HEXACO model of personality structure List of U.S. states ranked per five-factor model personality trait Moral foundations theory Myers–Briggs Type Indicator Personality psychology Szondi test Trait theory
Ocean has lived in Sunningdale, Berkshire, England, with his wife Judy, since 1978. They have three children. His son played rugby sevens at the 2014 Commonwealth Games for Barbados.Ocean decided to become vegetarian after the loss of his mother in 1989, who died from ovarian cancer.
Billy Ocean has been nominated three times for a Grammy Award, with one win.
Lists of UK Singles Chart number ones List of Billboard number-one singles List of artists who reached number one in the United States List of Billboard number-one dance club songs List of artists who reached number one on the U.S. Dance Club Songs chart List of Euro disco artists List of Eastern Caribbean people
Media related to Billy Ocean at Wikimedia Commons Billy Ocean official website Billy Ocean at AllMusic
Captain America is a superhero appearing in American comic books published by Marvel Comics. Created by cartoonists Joe Simon and Jack Kirby, the character first appeared in Captain America Comics #1 (cover dated March 1941) from Timely Comics, a predecessor of Marvel Comics. Captain America was designed as a patriotic supersoldier who often fought the Axis powers of World War II and was Timely Comics' most popular character during the wartime period. The popularity of superheroes waned following the war, and the Captain America comic book was discontinued in 1950, with a short-lived revival in 1953. Since Marvel Comics revived the character in 1964, Captain America has remained in publication. The character wears a costume bearing an American flag motif, and he utilizes a nearly indestructible shield that he throws as a projectile. Captain America is the alter ego of Steve Rogers, a frail young man enhanced to the peak of human perfection by an experimental serum to aid the United States government's efforts in World War II. Near the end of the war, he was trapped in ice and survived in suspended animation until he was revived in modern times. Although Captain America often struggles to maintain his ideals as a man out of his time, he remains a highly respected figure in his community, which includes becoming the long-time leader of the Avengers. Captain America was the first Marvel Comics character to appear in media outside comics with the release of the 1944 movie serial, Captain America. Since then, the character has been featured in other films and television series. In the Marvel Cinematic Universe (MCU), the character is portrayed by Chris Evans. Captain America was ranked sixth on IGN's "Top 100 Comic Book Heroes of All Time" in 2011, second in their list of "The Top 50 Avengers" in 2012, and second in their "Top 25 best Marvel superheroes" list in 2014.
Captain America Comics #1 — cover-dated March 1941 and on sale December 20, 1940, a year before the attack on Pearl Harbor, but a full year into World War II — showed the protagonist punching Nazi leader Adolf Hitler; it sold nearly one million copies. While most readers responded favorably to the comic, some took objection. Simon noted, "When the first issue came out we got a lot of ... threatening letters and hate mail. Some people really opposed what Cap stood for." The threats, which included menacing groups of people loitering out on the street outside of the offices, proved so serious that police protection was posted with New York Mayor Fiorello La Guardia personally contacting Simon and Kirby to give his support.Though preceded as a "patriotically themed superhero" by MLJ's The Shield, Captain America immediately became the most prominent and enduring of that wave of superheroes introduced in American comic books prior to and during World War II, as evidenced by the unusual move at the time of premiering the character in his own title instead of an anthology title first. This popularity drew the attention and a complaint from MLJ that the character's triangular shield too closely resembled the chest symbol of their Shield character. In response, Goodman had Simon and Kirby create a distinctive round shield for issue 2, which went on to become an iconic element of the character. With his sidekick Bucky, Captain America faced Nazis, Japanese, and other threats to wartime America and the Allies. Stanley Lieber, now better known as Stan Lee, contributed to the character in issue #3 in the filler text story "Captain America Foils the Traitor's Revenge", which introduced the character's use of his shield as a returning throwing weapon. Captain America soon became Timely's most popular character and even had a fan-club called the "Sentinels of Liberty".Circulation figures remained close to a million copies per month after the debut issue, which outstripped even the circulation of news magazines such as Time during the period. The character was widely imitated by other comics publishers, with around 40 red-white-and-blue patriotic heroes debuting in 1941 alone. After the Simon and Kirby team moved to DC Comics in late 1941, having produced Captain America Comics through issue #10 (January 1942), Al Avison and Syd Shores became regular pencillers of the celebrated title, with one generally inking over the other. The character was featured in All Winners Comics #1–19 (Summer 1941 – Fall 1946), Marvel Mystery Comics #80–84 and #86–92, USA Comics #6–17 (Dec. 1942 – Fall 1945), and All Select Comics #1–10 (Fall 1943 – Summer 1946). In the post-war era, with the popularity of superheroes fading, Captain America led Timely's first superhero team, the All-Winners Squad, in its two published adventures, in All Winners Comics #19 and #21 (Fall–Winter 1946; there was no issue #20). After Bucky was shot and wounded in a 1948 Captain America story, he was succeeded by Captain America's girlfriend, Betsy Ross, who became the superheroine Golden Girl. Captain America Comics ran until issue #73 (July 1949), at which time the series was retitled Captain America's Weird Tales for two issues, with the finale being a horror/suspense anthology issue with no superheroes. Atlas Comics attempted to revive its superhero titles when it reintroduced Captain America, along with the original Human Torch and the Sub-Mariner, in Young Men #24 (Dec. 1953). Billed as "Captain America, Commie Smasher!" Captain America appeared during the next year in Young Men #24–28 and Men's Adventures #27–28, as well as in issues #76–78 of an eponymous title. Atlas' attempted superhero revival was a commercial failure, and the character's title was canceled with Captain America #78 (Sept. 1954).
In 1966, Joe Simon sued the owners of Marvel Comics, asserting that he—not Marvel—was legally entitled to renew the copyright upon the expiration of the original 28-year term. The two parties settled out of court, with Simon agreeing to a statement that the character had been created under terms of employment by the publisher, and therefore it was work for hire owned by them.In 1999, Simon filed to claim the copyright to Captain America under a provision of the Copyright Act of 1976, which allowed the original creators of works that had been sold to corporations to reclaim them after the original 56-year copyright term (but not the longer term enacted by the new legislation) had expired. Marvel Entertainment challenged the claim, arguing that the settlement of Simon's 1966 suit made the character ineligible for termination of the copyright transfer. Simon and Marvel settled out of court in 2003, in a deal that paid Simon royalties for merchandising and licensing use of the character.
Steven Grant Rogers was born in the Lower East Side of Manhattan, New York City, in 1920 to poor Irish immigrants, Sarah and Joseph Rogers. Joseph died when Steve was a child, and Sarah died of pneumonia while Steve was a teen. By early 1940, before America's entry into World War II, Rogers is a tall, scrawny fine arts student specializing in illustration and a comic book writer and artist. Disturbed by the devastation of Europe by the Nazis, Rogers attempts to enlist but is rejected due to his frail body. His resolution attracts the notice of U.S. Army General Chester Phillips and "Project: Rebirth". Rogers is used as a test subject for the Super-Soldier project, receiving a special serum made by "Dr. Josef Reinstein", later retroactively changed to a code name for the scientist Abraham Erskine.The serum is a success and transforms Steve Rogers into a nearly perfect human being with peak strength, agility, stamina, and intelligence. The success of the program leaves Erskine wondering about replicating the experiment on other human beings. The process itself has been inconsistently detailed: While in the original material Rogers is shown receiving injections of the Super-Serum, when the origin was retold in the 1960s, the Comic Code Authority had already put a veto over graphic description of drug intake and abuse, and thus the Super-Serum was retconned into an oral formula.Erskine refused to write down every crucial element of the treatment, leaving behind a flawed, imperfect knowledge of the steps. Thus, when the Nazi spy Heinz Kruger killed him, Erskine's method of creating new Super-Soldiers died. Captain America, in his first act after his transformation, avenges Erskine. In the 1941 origin story and in Tales of Suspense #63, Kruger dies when running into machinery but is not killed by Rogers; in the Captain America #109 and #255 revisions, Rogers causes the spy's death by punching him into machinery.Unable to create new Super-Soldiers and willing to hide the Project Rebirth fiasco, the American government casts Rogers as a patriotic superhero, able to counter the menace of the Red Skull as a counter-intelligence agent. He is supplied with a patriotic uniform of his own design, a bulletproof shield, a personal side arm, and the codename Captain America, while posing as a clumsy infantry private at Camp Lehigh in Virginia. He forms a friendship with the camp's teenage mascot, James Buchanan "Bucky" Barnes.Barnes learns of Rogers' dual identity and offers to keep the secret if he can become Captain America's sidekick. During their adventures, Franklin D. Roosevelt presents Captain America with a new shield, forged from an alloy of steel and vibranium, fused by an unknown catalyst, so effective that it replaces his own firearm. Throughout World War II, Captain America and Bucky fight the Nazi menace both on their own and as members of the superhero team the Invaders as seen in the 1970s comic of the same name. Captain America fights in numerous battles in World War II, primarily as a member of 1st Battalion, 26th Infantry Regiment "Blue Spaders". Captain America battles a number of criminal menaces on American soil, including a wide variety of costumed villains: the Wax Man, the Hangman, the Fang, the Black Talon, and the White Death, among others. In addition to Bucky, Captain America was occasionally assisted by the Sentinels of Liberty. Sentinels of Liberty was the title given to members of the Captain America Comics fan club who Captain America sometimes addressed as an aside, or as characters in the Captain America Comics stories. In late April 1945, during the closing days of World War II, Captain America and Bucky try to stop the villainous Baron Zemo from destroying an experimental drone plane. Zemo launches the plane with an armed explosive on it with Rogers and Barnes in hot pursuit. The pair reaches the plane just before takeoff. When Bucky tries to defuse the bomb, it explodes in mid-air. Rogers is hurled into the freezing waters of the North Atlantic. Both are presumed dead, though it is later revealed that neither had died.
Captain America appeared in comics for the next few years, changing from World War II-era hero fighting the Nazis to confronting the United States' newest enemy, Communism. The revival of the character in the mid-1950s was short-lived, and events during that time period are later retconned to show that multiple people operated using the code name to explain the changes in the character. These post World War II successors are listed as William Naslund and Jeffrey Mace. They are assisted by Fred Davis continuing the role of Bucky. The last of these other official Captains, William Burnside, was a history graduate enamored with the Captain America mythos, having his appearance surgically altered to resemble Rogers and legally changing his name to "Steve Rogers", becoming the new "1950s Captain America". He administered to himself and his pupil James "Jack" Monroe a flawed, incomplete copy of the Super-Serum, which made no mention about the necessary Vita-Ray portion of the treatment. As a result, while Burnside and Monroe became the new Captain America and Bucky, they became violently paranoid, often raving about innocent people being communist sympathizers during the height of the Red Scare of the 1950s. Their insanity forced the U.S. government to place them in indefinite cryogenic storage until they could be cured of their mental illness. Monroe would later be cured and assume the Nomad identity.
Years later, the superhero team the Avengers discovers Steve Rogers' body in the North Atlantic. After he revives, they piece together that Rogers has been preserved in a block of ice since 1945, surviving because of his enhancements from Project: Rebirth. The block began to melt after the Sub-Mariner, enraged that an Inuit tribe is worshipping the frozen figure, throws it into the ocean. Rogers accepts membership in the Avengers, and his experience in individual combat service and his time with the Invaders makes him a valuable asset. He quickly assumes leadership and has typically returned to that position throughout the team's history. Captain America is plagued by guilt for having been unable to prevent Bucky's death. Although he takes the young Rick Jones (who closely resembles Bucky) under his tutelage, he refuses for some time to allow Jones to take up the Bucky identity, not wishing to be responsible for another youth's death. Insisting that his hero move on from that loss, Jones convinces Rogers to let him don the Bucky costume, but this partnership lasts only a short time; a disguised Red Skull, impersonating Rogers with the help of the Cosmic Cube, drives Jones away. Rogers reunites with his old war comrade Nick Fury, who is similarly well-preserved due to the "Infinity Formula". As a result, Rogers regularly undertakes missions for the security agency S.H.I.E.L.D., for which Fury is public director. Through Fury, Rogers befriends Sharon Carter, a S.H.I.E.L.D. agent, with whom he eventually begins a romantic relationship. Rogers later meets and trains Sam Wilson, who becomes the superhero the Falcon, the first African-American superhero in mainstream comic books. The characters established an enduring friendship and adventuring partnership, sharing the series title for some time as Captain America and the Falcon. The two later encounter the revived but still insane 1950s Captain America. Although Rogers and the Falcon defeat the faux Rogers and Jack Monroe, Rogers becomes deeply disturbed that he could have suffered his counterpart's fate. During this period, Rogers temporarily gains super strength.The series dealt with the Marvel Universe's version of the Watergate scandal, making Rogers so uncertain about his role that he abandons his Captain America identity in favor of one called Nomad, emphasizing the word's meaning as "man without a country". During this time, several men unsuccessfully assume the Captain America identity. Rogers eventually re-assumes it after coming to consider that the identity could be a symbol of American ideals and not its government; it's a personal conviction epitomized when he later confronted a corrupt Army officer attempting to manipulate him by appealing to his loyalty, "I'm loyal to nothing, General ... except the [American] Dream." Jack Monroe, cured of his mental instability, later takes up the Nomad alias. Sharon Carter is believed to have been killed while under the mind control of Dr. Faustus.
The 1980s included a run by writer Roger Stern and artist John Byrne. Stern had Rogers consider a run for President of the United States in Captain America #250 (June 1980), an idea originally developed by Roger McKenzie and Don Perlin. Stern, in his capacity as editor of the title, originally rejected the idea but later changed his mind about the concept. McKenzie and Perlin received credit for the idea on the letters page at Stern's insistence. Stern additionally introduced a new love interest, law student Bernie Rosenthal, in Captain America #248 (Aug. 1980).Writer J. M. DeMatteis revealed the true face and full origin of the Red Skull in Captain America #298–300, and had Captain America take on Jack Monroe, Nomad, as a partner for a time. The heroes gathered by the Beyonder elect Rogers as leader during their stay on Battleworld. Homophobia is dealt with as Rogers runs into a childhood friend named Arnold Roth who is gay.Mark Gruenwald became the writer of the series with issue #307 (July 1985) and wrote 137 issues for 10 consecutive years from until #443 (Sept. 1995), the most issues by any single author in the character's history. Gruenwald created several new foes, including Crossbones and the Serpent Society. Other Gruenwald characters included Diamondback, Super Patriot, and Demolition Man. Gruenwald explored numerous political and social themes as well, such as extreme idealism when Captain America fights the anti-nationalist terrorist Flag-Smasher; and vigilantism when he hunts the murderous Scourge of the Underworld.Rogers receives a large back-pay reimbursement dating back to his disappearance at the end of World War II, and a government commission orders him to work directly for the U.S. government. Already troubled by the corruption he had encountered with the Nuke incident in New York City, Rogers chooses instead to resign his identity, and then takes the alias of "the Captain". A replacement Captain America, John Walker, struggles to emulate Rogers' ideals until pressure from hidden enemies helps to drive Walker insane. Rogers returns to the Captain America identity while a recovered Walker becomes the U.S. Agent.Sometime afterward, Rogers avoids the explosion of a methamphetamine lab, but the drug triggers a chemical reaction in the Super Soldier Serum in his system. To combat the reaction, Rogers has the serum removed from his body and trains constantly to maintain his physical condition. A retcon later establishes that the serum was not a drug per se, which would have metabolized out of his system, but in fact a virus-like organism that effected a biochemical and genetic change. This additionally explained how nemesis the Red Skull, who at the time inhabited a body cloned from Rogers' cells, has the formula in his body. Because of his altered biochemistry, Rogers' body begins to deteriorate, and for a time he must wear a powered exoskeleton and is eventually placed again in suspended animation. During this time, he is given a transfusion of blood from the Red Skull, which cures his condition and stabilizes the Super-Soldier virus in his system. Captain America returns to crime fighting and the Avengers.Following Gruenwald's departure from the series, Mark Waid took over and resurrected Sharon Carter as Cap's love interest. The title was then relaunched under Rob Liefeld as Cap became part of the Heroes Reborn universe for 13 issues before another relaunch restored Waid to the title in an arc that saw Cap lose his shield for a time using an energy based shield as a temporary replacement. Following Waid's run, Dan Jurgens took over and introduced new foe Protocide, a failed recipient of the Super Soldier Serum prior to the experiment that successfully created Rogers. Some time after this, Rogers' original shield was retrieved, but subtle damage sustained during the battle with the Beyonder resulted in it being shattered and a 'vibranium cancer' being triggered that would destroy all vibranium in the world, with Rogers nearly being forced to destroy the shield before a confrontation with the villain Klaw saw Klaw's attacks unwittingly repair the shield's fractured molecular bonds and negate cancer.
In the aftermath of the September 11 terrorist attacks, Rogers reveals his identity to the world and establishes a residence in the Red Hook neighborhood of Brooklyn, New York, as seen in Captain America vol. 4, #1–7 (June 2002 – Feb. 2003). Following the disbandment of the Avengers in the "Avengers Disassembled" story arc, Rogers, now employed by S.H.I.E.L.D., discovers Bucky is alive, having been saved and deployed by the Soviets as the Winter Soldier. Rogers resumes his on-again, off-again relationship with S.H.I.E.L.D. agent Sharon Carter. After a mass supervillain break-out of the Raft, Rogers and Tony Stark assemble a new team of Avengers to hunt the escapees. In the 2006–2007 company-wide story arc "Civil War", Rogers opposes the new mandatory federal registration of super-powered beings, and leads the underground anti-registration movement. After significant rancor and danger to the public as the two sides clash, Captain America voluntarily surrenders and orders the Anti-Registration forces to stand down, feeling that the fight has reached a point where the principle originally cited by the anti-registration forces has been lost.In the story arc "The Death of Captain America", Rogers is fatally shot by Sharon Carter, whose actions are manipulated by the villain Dr. Faustus. The miniseries Fallen Son: The Death of Captain America #1–5 (June–Aug. 2007) examines the reaction of the stunned superhero community to Rogers' assassination, with each of the five issues focusing a different character's reaction. Bucky takes on the mantle of Captain America, per Rogers' antemortem request.Captain America: Reborn #1 (Aug. 2009) reveals that Rogers did not die, as the gun Sharon Carter had been hypnotized into firing at Rogers caused his consciousness to phase in and out of space and time, appearing at various points in his lifetime. Although Rogers manages to relay a message to the future by giving a time-delayed command to the Vision during the Kree-Skrull War, the Skull returns Rogers to the present, where he takes control of Rogers' mind and body. Rogers eventually regains control, and, with help from his allies, defeats the Skull. In the subsequent one-shot comic Captain America: Who Will Wield the Shield?, Rogers formally grants Bucky his Captain America shield and asks him to continue as Captain America. The President of the United States grants Rogers a full pardon for his anti-registration actions.
Following the company-wide "Dark Reign" and "Siege" story arcs, the Steve Rogers character became part of the "Heroic Age" arc.The President of the United States appoints Rogers, in his civilian identity, as "America's top cop" and head of the nation's security, replacing Norman Osborn as the tenth Executive Director of S.H.I.E.L.D.. The Superhuman Registration Act is repealed and Rogers re-establishes the superhero team the Avengers, spearheaded by Iron Man, Thor, and Bucky as Captain America. In the miniseries Steve Rogers: Super Soldier, he encounters Jacob Erskine, the grandson of Professor Abraham Erskine and the son of Tyler Paxton, one of Rogers' fellow volunteers in the Super-Soldier program. Shortly afterward, Rogers becomes leader of the Secret Avengers, a black-ops superhero team.During the Fear Itself storyline, Steve Rogers is present when the threat of the Serpent is known. Following the apparent death of Bucky at the hands of Sin (in the form of Skadi), Steve Rogers ends up changing into his Captain America uniform. When the Avengers and the New Avengers are fighting Skadi, the Serpent ends up joining the battle and breaks Captain America's shield with his bare hands. Captain America and the Avengers teams end up forming a militia for a last stand against the forces of the Serpent. When it comes to the final battle, Captain America uses Thor's hammer to fight Skadi until Thor manages to kill the Serpent. In the aftermath of the battle, Iron Man presents him with his reforged shield, now stronger for its uru-infused enhancements despite the scar it bears. It is then revealed that Captain America, Nick Fury, and Black Widow are the only ones who know that Bucky actually survived the fight with Skadi as Bucky resumes his identity as Winter Soldier.During the "Spider-Island" storyline, Captain America had been captured turned into the Spider King by Spider Queen and Jackal. He was restored to normal following his fight with Venom.</ref>The Amazing Spider-Man #670 (September 2011). Marvel Comics.</ref> In the Avengers vs. X-Men story arc, Captain America attempts to apprehend Hope Summers of the X-Men. She is the targeted vessel for the Phoenix Force, a destructive cosmic entity. Captain America believes that this Phoenix Force is too dangerous to entrust in one person and seeks to prevent Hope from having it. Cyclops and the X-Men believe that the Phoenix Force will save their race, and oppose Captain America's wishes. The result is a series of battles that eventually take both teams to the blue area of the moon. The Phoenix Force eventually possesses the five X-Men present, leaving the Avengers at an extreme disadvantage. The Phoenix Five, who become corrupted by the power of the Phoenix, are eventually defeated and scattered, with Cyclops imprisoned for turning the world into a police state and murdering Charles Xavier after being pushed too far, only for him to note that, in the end, he was proven right about the Phoenix's intentions. From there, Captain America proceeds to assemble the Avengers Unity Squad, a new team of Avengers composed of both classic Avengers and X-Men.After Cyclops was incarcerated, and Steve accepted the Avengers should have done more to help mutants, and allowed the world to hate them, he started planning a new sub-team of Avengers in the hopes of unifying mutant and humankind alike. He chose Havok to lead his team and become the new face to represent mutants as Professor X and Cyclops once were.Their first threat was the return of the Red Skull- more specifically, a clone of the Skull created in 1942 and kept in stasis in the event of the original's death- who usurped Professor X's body to provide himself with telepathic powers, which he would use to provoke citizens of New York into a mass assault against mutants, or anyone who could be one, and force the Scarlet Witch and Rogue to allow themselves to be attacked. With the help of the S-Man Honest John, he managed to even manipulate Thor.The Red Skull's skills were still erratic, and could not completely control Captain America, an attack against him was enough of a distraction to lose control of Rogue and the Scarlet Witch. After being overpowered by the rest of the Uncanny Avengers, the Red Skull escapes, but promises to return. In the aftermath, both Rogue and the Scarlet Witch joined the team.During a battle with an enemy called the Iron Nail, the Super Soldier Serum within Rogers's body was neutralized, causing him to age rapidly to match his chronological age of over 90 years. No longer able to take part in field missions but retaining his sharp mind, Rogers decided to take on a role as mission coordinator, organizing the Avengers' plans of attack from the mansion, while appointing Sam Wilson as his official "replacement" as Captain America.When various Avengers and X-Men were inverted into villains and several villains inverted into heroism due to a miscast spell by the Scarlet Witch and Doctor Doom, Rogers not only coordinated the efforts of Spider-Man and the inverted villains, now called the "Astonishing Avengers", but also donned his old armor to battle the inverted Falcon, until the heroes and villains could be returned to normal with the aid of the White Skull (the inverted Red Skull).During the "Time Runs Out" storyline, Steve Rogers wears armor when he confronts Iron Man. The ensuing fight between the two old friends led Steve Rogers to force Iron Man to admit that he had lied to him and all of their allies, when he had known about the incursions between alternate Earths all along, but Iron Man also confessed that he wouldn't change a thing. The final incursion started and Earth-1610 started approaching Earth-616 while Iron Man and Steve Rogers kept fighting. Earth-1610's S.H.I.E.L.D. launched a full invasion to destroy Earth-616, where Tony Stark and Steve Rogers were crushed by a Helicarrier.As part of the All-New, All-Different Marvel, Steve Rogers became the new Chief of Civilian Oversight for S.H.I.E.L.D. He returned to the Uncanny Avengers where the team is now using the Schaefer Theater as their headquarters.Steve Rogers later has an encounter with an alternate Logan from Earth-807128. After defeating Logan and bringing him to Alberta, Canada, Rogers tried to "reassure" Logan that this was not "his" past by showing him the adamantium-frozen body of Earth-616's Logan. This sight reminds Logan of the need to enjoy being alive rather than brooding over the ghosts of his past. Although he told Steve Rogers what he had experienced in his timeline, Logan declined Steve's offer of help.
During the 2016 "Avengers: Standoff!" storyline, Steve Rogers learns from Rick Jones that S.H.I.E.L.D. has established Pleasant Hill, a gated community where they use Kobik to transform villains into ordinary citizens. When Rogers is brought to Pleasant Hill, he confronts Maria Hill about the Kobik project. Their argument is interrupted when Baron Helmut Zemo and Fixer restore the inmates to normal. After Hill is injured, Rogers convinces Zemo to let Hill get medical attention. Rogers is then escorted to Dr. Erik Selvig's clinic by Father Patrick. Selvig tells Rogers that Kobik is at the Pleasant Hill Bowling Alley. During an attempt to reason with Kobik, Rogers is attacked by Crossbones. Before Rogers can be killed, Kobik uses her abilities to restore him back to his prime. Declaring that "It's good to be back," Steve defeats Crossbones as Captain America and the Winter Soldier catch up with him. They resume their search for Kobik, and discover that Baron Zemo had Fixer invent a device that would make Kobik subservient to them. Rogers rallies the heroes so that they can take the fight to Zemo. In the aftermath of the incident, Steve and Sam plan to keep what happened at Pleasant Hill under wraps for the time being.In Captain America: Steve Rogers #1 (July 2016), the final panel apparently revealed that Rogers has been a Hydra double-agent since his early youth. This is subsequently revealed to be the result of Kobik's restoration of Rogers' youth, as she had been taught by the Red Skull that Hydra was good for the world, and having the mind of a four-year-old child, Kobik changed reality so that Rogers would be the greatest man he could be: believing Hydra to be good, Kobik permanently altered his memories so that Rogers believed that he had always been a member of Hydra. Some of Rogers' original heroic attributes remain intact, such as covering the death of another Hydra member within S.H.I.E.L.D., Erik Selvig, as well as knowing of Jack Flag's tragic life and his immortality, which is why Steve pushes him from Zemo's airplane (resulting in coma, not death). Additionally, it is revealed that Rogers' abusive father, Joseph, was actually killed by Hydra, and that Hydra deceived him into thinking Joseph died of a heart attack. It is also revealed that Rogers witnessed his mother, Sarah, being killed by Sinclair's Hydra goons and kidnapped him, which is the reason why Steve held a grudge towards Hydra's evilness and plans to kill the Red Skull's clone and restore Hydra's lost honor. As part of his long-term plans, Steve further compromised Sam Wilson's current image as 'the' Captain America by using his greater familiarity with the shield to deliberately put Wilson in a position where he would be unable to use the shield to save a senator from Flag-Smasher, with the final goal of demoralizing Sam to the point where he will return the shield to Rogers of his own free will, not wanting to kill Wilson and risk creating a martyr.During the 2016 "Civil War II" storyline, with the discovery of new Inhuman Ulysses – who has the ability to "predict" the future by calculating complex patterns – Rogers has set out to prevent Ulysses from learning of his true plans and allegiance. Rogers does this by "forcing" certain predictions on him, such as anonymously providing Bruce Banner with new gamma research to provoke a vision that would drive the Avengers to kill Banner, although this plan has apparently backfired with a recent vision showing the new Spider-Man standing over the dead Steve Rogers. Despite this revelation, Rogers presents himself as the voice of reason by allowing Spider-Man to flee with Thor. This inspires doubt in Tony Stark for his current stance by suggesting that he is just acting against Danvers because he does not like being top dog. He then goes to Washington, D.C., the location seen in Ulysses' vision, to talk to Spider-Man, who was trying to understand the vision like he was. When Captain Marvel attempts to arrest Spider-Man, Tony, wearing the War Machine armor, confronts her and the two begin to fight.Later, Rogers goes to Sokovia and joins forces with Black Widow to liberate freedom fighters from a prison so they can reclaim their country. After that, he goes to his base where Doctor Selvig expresses concern of his plan to kill the Red Skull. He then reveals that he has Baron Zemo in a cell, planning to recruit him. He eventually kills the Skull after the villain is captured by the Unity Squad and the Xavier brain fragment extracted by the Beast, Rogers throwing the Skull out of a window over a cliff after Sin and Crossbones affirm their new allegiance to Rogers.In the 2017 "Secret Empire" storyline, Rogers, as the head of S.H.I.E.L.D, uses a subsequent alien invasion and a mass supervillain assault in order to seize control of the United States. He neutralizes the superheroes that might oppose him, and seeks the Cosmic Cube to bring about a reality in which Hydra won World War II. When Rick smuggles information about the Cube's rewriting of Rogers' reality to the remaining free Avengers, a disheveled, bearded man in a torn World War II army uniform appears who introduces himself as Steve Rogers. As the Avengers and Hydra search for fragments of the shattered Cube, it is revealed that this amnesic Steve Rogers is actually a manifestation of Rogers existing within the Cube itself, created by Kobik's memories of Rogers before he was converted to Hydra, as she comes to recognize that her decision to 'rewrite' Rogers as an agent of Hydra was wrong. Although Hydra Rogers is able to mostly reassemble the Cosmic Cube, Sam Wilson and Bucky are able to use a fragment of the cube to restore the 'memory' of pre-Hydra Rogers in the Cube to corporeal existence, allowing him to defeat his Hydra self, subsequently using the Cube to undo most of the damage caused by Hydra manipulating reality even if the physical damage remains. 'Hydra Cap' continues to exist as a separate entity and is kept trapped in a prison where he is the only inmate, mocking the restored Rogers about the challenge he will face rebuilding his reputation. For himself, Rogers muses that this troubling affair has a silver lining, that this experience will teach everyone not to place such blind trust in another.
Rogers' battle experience and training make him an expert tactician and an excellent field commander, with his teammates frequently deferring to his orders in battle. The Avengers, X-Men, Fantastic Four, and other heroes choose Rogers as their leader during the Secret Wars; Thor says that Rogers is one of the very few mortals he will take orders from, and follow "through the gates of Hades".Rogers' reflexes and senses are extraordinarily keen. He has blended Aikido, Boxing, Judo, Karate, Jujutsu, Kickboxing, and gymnastics into his own unique fighting style and is a master of multiple martial arts. Years of practice with his near-indestructible shield make him able to aim and throw it with almost unerring accuracy. His skill with his shield is such that he can attack multiple targets in succession with a single throw or even cause a boomerang-like return from a throw to attack an enemy from behind. In canon, he is regarded by other skilled fighters as one of the best hand-to-hand combatants in the Marvel Universe, limited only by his human physique. Although the Super Soldier Serum is an important part of his strength, Rogers has shown himself still sufficiently capable against stronger opponents, even when the serum has been deactivated reverting him to his pre-Captain America physique.Stan Lee claimed that he'd "always been fascinated by the fact that, although Captain America has the least spectacular super-power of all, the mantle of leadership falls naturally upon him, as though he was born to command... Cap is one of the hardest hero characters to write, because the writer cannot use some exotic super-power to make his episodes seem colorful... All he has to serve him are his extraordinary combat skills, his shield, and his unquenchable love for freedom and justice."Rogers has vast U.S. military knowledge and is often shown to be familiar with ongoing, classified Defense Department operations. He is an expert in combat strategy, survival, acrobatics, parkour, military strategy, piloting, and demolitions. Despite his high profile as one of the world's most popular and recognizable superheroes, Rogers has a broad understanding of the espionage community, largely through his ongoing relationship with S.H.I.E.L.D.
Steve Rodgers is often considered to be the pinnacle of human potential and operates at peak physical performance due to his enhancement via the Super Soldier Serum. The Super Soldier Serum enhances all of his metabolic functions and prevents the build-up of fatigue poisons in his muscles, giving him endurance far in excess of an ordinary human being. This accounts for many of his extraordinary feats, including bench pressing 1,100 pounds (500 kg) as a warm-up and running a mile (1.6 km) in less than a minute (60 mph/97 km/h, nearly twice the maximum speed achieved by the best human sprinters). Furthermore, his enhancements are the reason why he was able to survive being frozen in suspended animation for decades. He is highly resistant to hypnosis or gases that could limit his focus. The secrets of creating a super-soldier were lost with the death of its creator, Dr. Abraham Erskine. In the ensuing decades there have been numerous attempts to recreate Erskine's treatment, only to have them end in failure. Even worse, the attempts have instead often created psychopathic supervillains of which Captain America's 1950s imitator and Nuke are the most notorious examples.
Captain America has used multiple shields throughout his history, the most prevalent of which is a nigh-indestructible disc-shaped shield made from a unique combination of Vibranium, Steel alloy, and an unknown third component that has never been duplicated called Proto-Adamantium. The shield was cast by American metallurgist Dr. Myron MacLain, who was contracted by the U.S. government, from orders of President Franklin D. Roosevelt, to create an impenetrable substance to use for tanks during World War II. This alloy was created by accident and never duplicated, although efforts to reverse-engineer it resulted in the discovery of adamantium.Captain America often uses his shield as an offensive throwing weapon. The first instance of Captain America's trademark ricocheting shield-toss occurs in Stan Lee's first comics writing, the two-page text story "Captain America Foils the Traitor's Revenge" in Captain America Comics #3 (May 1941). The legacy of the shield among other comics characters includes the time-traveling mutant superhero Cable telling Captain America that his shield still exists in one of the possible futures; Cable carries it into battle and brandishes it as a symbol.When without his trademark shield, Captain America sometimes uses other shields made from less durable metals such as steel, or even a photonic energy shield designed to mimic a vibranium matrix. Rogers, having relinquished his regular shield to Barnes, carried a variant of the energy shield which can be used with either arm, and used to either block attacks or as an improvised offensive weapon able to cut through metal with relative ease. Much like his Vibranium shield, the energy shield can be thrown, including ricocheting off multiple surfaces and returning to his hand.Captain America's uniform is made of a fire-retardant material, and he wears a lightweight, bulletproof duralumin scale armor beneath his uniform for added protection. Originally, Rogers' mask was a separate piece of material, but an early engagement had it dislodged, thus almost exposing his identity. To prevent a recurrence of the situation, Rogers modified the mask with connecting material to his uniform, an added benefit of which was extending his armor to cover his previously exposed neck. As a member of the Avengers, Rogers has an Avengers priority card, which serves as a communications device. Captain America has used a custom specialized motorcycle, modified by the S.H.I.E.L.D. weapons laboratory, as well as a custom-built battle van, constructed by the Wakanda Design Group with the ability to change its color for disguise purposes (red, white and blue), and fitted to store and conceal the custom motorcycle in its rear section with a frame that allows Rogers to launch from the vehicle riding it.
Captain America has faced numerous foes in over 70 years of published adventures. Many of his recurring foes embody ideologies contrary to the American values that Captain America is shown to strive for and believes in. Some examples of these opposing values are Nazism (Red Skull, Baron Zemo), neo-Nazism (Crossbones, Doctor Faustus), technocratic fascism (AIM, Arnim Zola), Communism (Aleksander Lukin), amoral capitalism (Roxxon Energy Corporation), anti-patriotism (Flag Smasher) and international and domestic terrorism (Hydra).
"Captain America" is the name of several fictional characters appearing in American comic books published by Marvel Comics. The first and primary character is Steve Rogers, who was created by Joe Simon and Jack Kirby. Other characters have adopted the alias over the years, most notably Bucky Barnes and Sam Wilson.
Captain Steven Rogers, the 18th century Earth-616 ancestor of the World War 2 Super-Soldier serum recipient, wore a colorful costume and carried a round cast iron shield.
The Marvel 1602 limited series presents an alternative history, Earth-311, in which a Captain America from the late 21st century is transported to the year 1602 after the Purple Man takes over the world – his enemy wanting to dispose of Rogers in such a way that there is nothing left of him in the present to inspire others – where he assumes the identity of Rojhaz a white Native American who is presumed by the Europeans to be of Welsh ancestry. His arrival causes numerous alterations in reality, causing analogues of various Marvel Universe characters to appear in the 17th century instead, speculated by Uatu to be the result of the universe attempting to generate a means of repairing the damage caused to reality. Rogers refuses to return to the future because he wants to nurture a new United States free of prejudice from its very beginnings, but the 1602 version of Nick Fury forces him to return, accompanying him on the journey. Rogers noted that in his version of the late 21st century, he was the last true superhero and was left alone fighting his own country – the United States – which had fallen under the rule of a tyrannical life-term President.
In the Age of X reality, Rogers was the leader of the Avengers, here a strike team intended to hunt down mutants. Although he initially believed in his mission to contain the danger that mutants could pose to the world, an encounter with a mutant 'nursery' protecting young children forced Rogers to recognize that he was on the wrong side, he and his team subsequently sacrificing themselves to stop the psychotic Hulk from launching a bioweapon at the mutant stronghold. Rogers' memories were 'stored' by Legacy, a mutant who was able to convey his plan of using various mutants to generate force fields around the facility to cut it off from the outside world.
In the Amalgam Comics universe, Captain America is combined with DC's Superman to create Super-Soldier. In this reality, Clark Kent is given a Super-Soldier serum created from DNA harvested from the body of a dead baby Kal-El. The serum gives him the powers of the main universe Superman. Frozen in ice after a battle with Ultra-Metallo at the end of World War II, Super-Soldier is revived decades later and continues his fight for justice.
In Bishop's future the Witness, a future version of Gambit, possesses Captain America's shattered shield.
The five-issue limited series Bullet Points, written by J. Michael Straczynski and illustrated by Tommy Lee Edwards, tells of an alternative reality in which Doctor Erskine is killed the day before implementing the Captain America program. Steve Rogers, still frail, volunteers for the 'Iron Man' program, which bonds him to a robotic weapons-suit. He uses this to achieve victories against the Axis. Years after the end of the war, Rogers is killed in a battle with Peter Parker, who is the Hulk of that reality.
A member of the Captain Britain Corps, Captain Colonies (Stephen Rogers) appears in Excalibur #44. His name, combined with his membership in the Captain Britain Corps imply that in his universe, the Thirteen Colonies did not declare independence to form the United States as they did in our own universe (and most of the other Marvel universes) but instead remain part of Britain.
The 2014 mobile game Marvel: Contest of Champions includes an exclusive version of Captain America named Civil Warrior. This version of Steve Rogers, set in Earth-TRN634, killed Tony Stark during the Civil War. Rogers then incorporated Stark's armor into his uniform, and uses a modified shield containing a version of the ARC reactor.
The daughter of Luke Cage and Jessica Jones, Dani Cage operates as Captain America in an alternate future where New York City has been flooded. She uses the magnetic components Steve once used on the shield in order to better control it, and has the abilities of both her parents. She first appears in Ultron Forever, and returns to the present as a member of the U.S.Avengers.
Captain America appears in the Marvel/DC crossover DC vs. Marvel. He first appears fighting with HYDRA before being summoned to the DC Earth. He is later shown in a brawl with Bane, winning when he throws his shield so that it strikes Bane in the back of the head before Bane can break his back. He is then seen fighting with Batman in the sewers of Manhattan. After a pitched hand-to-hand standoff, they realize that neither one of them can gain an advantage over the other. Afterward, they team up with each other to stop the entities, the fundamental similarities between the two unique men who trained themselves to the peak of human development—and their lack of interest in 'proving' their superiority over their counterpart forcing the Brothers to halt their conflict.
In the 7th issue in the series, Deadpool visits a world where Captain America is known as General America, and is after a female version of Deadpool called Lady Deadpool. Deadpool intervenes and sends Headpool (the zombie version) after him, and Headpool bites him on the arm. To prevent the zombie plague from affecting that Earth, Deadpool cuts off Cap's arm and leaves with it. In promos for Deadpool Corps, General America is shown to have a robotic arm.
In the Exiles arc "A World Apart", the Earth was conquered by the Skrulls in the nineteenth century. Captain America has become a gladiator known as the Captain, fighting for the Skrulls against other superhumans in contents. He is defeated by Mimic, who, disgusted at Captain America having become nothing but a puppet to the Skrulls rather than the symbol he should be to others, uses Cyclops's optic blasts.In "Forever Avengers", the Exiles visit a timeline where Captain America was turned into a vampire by Baron Blood. He later turns the Avengers into vampires and becomes the new Vampire King. The now Cursed Avengers (composed of Hawkeye, Wasp, Giant-Man, Falcon and Polaris) plan to turn New York's population into zombies, but their plans are thwarted by the Exiles with the help of that Earth's Union Jack Kenneth Crichton. One of the Exiles, Sunfire, is bitten by a vampire. Before she can completely turn, Baron Crichton destroys Captain America and reveals himself to be the grandnephew of the original Baron Blood and a vampire as well, and becomes the newest King of the Vampire by blood right.
Captain America is the leader of the Avengers in the JLA/Avengers limited series, in which the two super teams travel to each other's universe. His mind affected by subtle incompatibilities between the two universes, he sees the Justice League as overlords who demand praise and worship in return for heroic actions. He especially gets angry at Superman, who (likewise affected) sees the Avengers as heroes who do not do enough and have let their world down. After Cap and Batman battle to a standstill, the two team up to solve the mystery of the game. Using an inter-dimensional vehicle that allows them to reach the Grandmaster's headquarters, they discover that the Avengers are fighting for Krona. Their intervention in the last battle, where Cap makes sure that Batman can get the cube so the JLA wins the game, causes the villain Krona to go mad and attack the Grandmaster. The Grandmaster causes the two universes to merge, imprisoning Krona between them. Cap, still subconsciously aware of the reality changes, attacks Superman, who is also subconsciously aware of the changes. This shatters the fixed reality, freeing Krona. Cap and Superman again argue, but are stopped by Wonder Woman. The two teams find the Grandmaster, who reveals their true realities. Despite seeing shocking revelations, the two teams decide to face Krona. Cap leads the teams as a battle tactician at Superman's suggestion, communicating orders through the Martian Manhunter's telepathy, and gives Superman his shield. After the two teams defeat Krona and restore their universes, Cap and Superman salute each other as they are transported back to their own dimensions, saying that they fight on.
A future incarnation of Captain America, known as Commander A, is a major character in the Captain America Corps limited series, and is stated to be of mixed Japanese, African-American, Latino, and Native American descent. He is also implied to be a descendant of Luke Cage. He wields two energy force-field shields, similar to the one that Steve Rogers used once when he temporarily lost his vibranium shield.
The two-issue limited series The Last Avengers Story (November–December 1995) tells of a possible alternative future for Captain America and the Avengers. Appalled with the American government after the "Villain Massacre", Captain America leaves his life as a superhero and runs for president. His presidency is a large success, but he is shot and seemingly killed in his third term, causing the other heroes to lose faith. However, Cap is not dead, but placed in suspended animation in a secret location until the technology to heal him can be developed. Using a sophisticated series of computer monitors, Captain America watches his friends win their final battle and records it for historical purposes.
In the Spider-Ham comic books, the funny animal version of Captain America is Captain Americat (Steve Mouser) an anthropomorphic cat who works for the Daily Beagle.
Two younger versions of Captain America were created by writer/artist Skottie Young. The first appears in the 2015 Secret Wars tie-in, Giant Size Little Marvel, written and illustrated by Young. In the Battleworld town of Marville, the mainstream superheroes are all elementary school age children, using their superpowers to engage in very destructive roughhousing. This Captain America is still the leader of the Avengers, though their headquarters are in a tree house instead of Avengers Mansion. As in the mainstream "Avengers vs. X-Men" storyline, Captain America faces off against Cyclops and the X-Men, only this time in an attempt to get two new kids on the block to join their respective group.An even younger version of Captain America appears in A-Babies vs X-Babies, a 2012 Skottie Young scripted story, illustrated by Gurihiru. In this story, Captain America and his fellow superheroes are all babies, but still superpowered. When baby Captain America's favorite stuffed bear Bucky goes missing, he assembles his baby Avengers and battles the baby X-Men for its return. This issue and the four Giant Size Little Marvel issues were collected into the Giant Size Little Marvel 2016 trade edition (ISBN 978-0785198703).
In Marvel 2099 a man masquerading as the original Captain America became ruler of the U.S. after a successful coup deposed Doom 2099. The man was killed when Doom 2099 dropped nano-machines on the Red House. The real Captain America appears in 2099: Manifest Destiny and takes up the role of Thor before giving Mjolnir to Spider-Man 2099.In Secret Wars, a new version of Captain America was created by Alchemax and resides in the Battleworld domain of 2099. Roberta Mendez was forcefully subjected to take the Super-Soldier Serum by her husband, Harry and became the leader of Alchemax's Avengers. Roberta and Captain America are two different personas of the same woman, with Roberta unknowing of her counterpart. She physically and mentally becomes Captain America if her trigger words, "Avengers Assemble", are said, and she reverts to Roberta if someone says "Dismissed". In the Secret Wars title, Captain America goes against Miguel Stone's orders to treat the Defenders as criminals and worked with the Defenders and Avengers to stop Baron Mordo and the Dweller-In-Darkness.Following Secret Wars, Roberta is transported to the prime Marvel Universe with hallucinations of her past life. She was a supporting character in the All-New, All-Different Marvel Spider-Man 2099 comic, where she was an employee at Parker Industries with Miguel O'Hara as her boss. After Roberta's powers resurface again, she becomes a recurring ally for Spider-Man 2099. During the Civil War II storyline, Roberta goes back to 2099 to find her family, despite Miguel's warnings. The Public Eye attempt to arrest her, until she is rescued by Ravage 2099. In the present, Miguel receives a call from Peter Parker, who tells him of a vision the Inhuman Ulysses had of the future: the death of Roberta Mendez. He goes back to 2099. Roberta learns from Ravage about the Anti-Powers Act, a law outlawing superpowers. Roberta and Ravage are taken to the downtown area by Hawkeye 2099, where they meet the remaining heroes. Spider-Man convinces Doctor Strange 2099 to help him out in exchange for his help in eliminating the A.P.A. Meanwhile, the CEO of Alchemax calls on Power Pack to defeat the heroes. Upon finding Roberta, Strange takes Spider-Man downtown, while Roberta leaves to find her husband upon learning his location. Roberta finds her husband Harry, who claims that she died and that they do not have kids, and gets captured by Power Pack. After Strange reveals that the CEO of Alchemax is J. Jonah Jameson, Spider-Man rallies the heroes to launch an assault on S.H.I.E.L.D. HQ and rescue Roberta. In the process, they discover that "Jameson" and "Power Pack" are actually Skrull impostors. Spider-Man and Roberta then go back to 2016 to restore the timeline. In the book's ending, Roberta and Miguel's son save Miguel from death and return to 2099 on New Year's Eve. Thanks to Miguel's sacrifice, Roberta's family history is restored. In other mediaCaptain America 2099 (Roberta Mendez) appears in Marvel: Future Fight, as alternative costume to Captain America. Captain America 2099 (Roberta Mendez) appears as a playable character in Lego Marvel Super Heroes 2.
In the Marvel Apes Universe, Captain America leads the Ape-vengers (which contain a lot of reformed supervillains). Secretly, he is a vampire along with his version of the Invaders, and plots to enter the 616 universe for sustenance. To accomplish this, he has already killed his world's version of Mr. Fantastic. However, it is revealed that the vampire Captain America was really Baron Blood, who took on Cap's form and increased his strength through the Super-Soldier Serum inside him. The real America was still frozen in ice up to the modern era, and helped the Gibbon, Wolverine, and Speedball fight off the vampire Namor. Afterwards, they stop Baron Blood. This version of Captain America turns out to be nearly as brutal as his impersonator; for example he is willing to kill Spider-Monkey for the 'crime' of helping innocent dimensional travelers.
In the Marvel Mangaverse reality, the original Captain America is decapitated and killed by Doctor Doom, but Carol Danvers assumes the identity. This is done mostly out of a desire of self-defense, but she is encouraged to keep it for the foreseeable future by Sharon Carter. The original Mangaverse Captain America is both the leader of the Avengers and the President of the United States. His costume gives him the power to generate and manipulate energy shields.
In the 2005–2006 miniseries Marvel Zombies, and the follow-up 2007 Marvel Zombies vs. The Army of Darkness, Captain America is known as Colonel America and once served as the President of the United States. He is among the superheroes infected, along with his other fellow Avengers, by the zombified Sentry. Colonel America is responsible for infecting Spider-Man in Marvel Zombies vs. The Army Of Darkness by biting him on the shoulder. He is apparently killed by a zombie Red Skull, who rips off his left arm and scoops his exposed brains out before he himself is decapitated by a zombified Spider-Man. Zombie Ant-Man then steps on the Red Skull. As his intellect was partly retained in the remaining portion of his brain, he was transplanted into Black Panther's son T'Channa's dead body, and given a mechanical left arm. The transplant is successful, but the resulting brain damage turns Colonel America into a battle-crazed zombie leader, manageable but unable to focus on anything that is not related to war, confrontation, and battle. Colonel America (Steve Rogers/T'Channa) also has a role in Marvel Zombies Return, where he was transported to Earth-Z.Marvel Zombies 3 features a zombie version called "Captain Mexica", who comes from an alternate universe in which the Aztec Empire in Mexico never fell. He is killed after Machine Man cuts him in half.
In the alternative reality MC2 universe, Captain America leads the original Avengers on a mission to an alternative reality, which claims the majority of the team. He stays behind to aid the rebels in that reality, thus adding to the list of the dead / missing in action. The next iteration of MC2 Avengers aids him in A-Next #10-11, at the end of which he gives American Dream the shield that had belonged to that universe's Captain America. Captain America and Thunderstrike return to their home universe to aid in the fight against Seth in Spider-Girl #59.In the 2005 limited series Last Hero Standing, the MC2 Captain America is fatally injured leading a group of young heroes in battle against the Norse god Loki. Thor uses his power to transform Captain America into a new star. In the sequel, Last Planet Standing, Galactus states that this new star is the key to his escaping his world-devouring hunger.
In this potential future, all the Marvel Universe superheroes were killed when the supervillains combined forces. The villains then conquer and divide up control of the United States. Captain America is shown in a flashback as having been killed by the Red Skull in the ruins of the U.S. Capitol. The Red Skull subsequently takes Cap's costume and wears it as President of America.
In an alternate universe where World War II is still raging, Steve Rogers and Professor Erskine are both assassinated before the Super-Soldier Serum is administered, so Peggy Carter steps up to participate in Project: Rebirth. Although British, she takes up the shield and American flag to fight as Captain America. In this universe, Becky Barnes serves alongside Captain Peggy.The concept of Peggy Carter serving as Captain America was created for the game Marvel Puzzle Quest for Captain America's 75th anniversary. She was adapted into the third series of the comic Exiles.Peggy Carter also appears as a version of Captain America (named Captain Britain) in the first episode of the Marvel Studios animated series What If...? In this version, Peggy takes the Super-Soldier Serum, while Steve Rogers later joins the fight with an armored suit built by Howard Stark and becomes Iron Man.
Captain America is a S.H.I.E.L.D. agent on Earth-65, who apprehends Spider-Gwen during her battle with the Lizard (this reality's Peter Parker). This Captain America is an African American woman named Samantha Wilson a genderbent version of Sam Wilson/Falcon. During the 1940s, Samantha volunteered for Project: Rebirth after other test subjects were shot and killed or badly injured by Nazis. She became trapped in an alternate dimension after seemingly sacrificing herself to stop Arnim Zola, but later managed to return home to find that 75 years had passed. Steve Rogers would go on to become a famous comic creator, who writes stories of Samantha's dimensional journeys that he saw in his dreams, which Sam confirmed as being accurate.
In this retelling of Spider-Island as part of the "Secret Wars" storyline, Captain America and the other heroes are mutated into monster spiders and he is still the Spider Queen's "Spider King" in the Battleworld domain of Spider-Island. However, Agent Venom gives Captain America the Godstone and turns him into a Man-Wolf (as an homage to the time when Captain America was a werewolf called Capwolf), releasing Steve from the Spider Queen's control. He uses his new form to fight for the resistance.
Spider-Man: Life Story takes place in an alternate continuity where characters naturally age after Peter Parker debuts as Spider-Man in 1962. In 1966, Captain America is pressured by the public to join the efforts in Vietnam and decides to go to see the conflict for himself. A year later, American soldiers label Steve as a traitor when he decides to protect a Vietnamese village. Captain America also gets himself involved in the Superhuman Civil War in the 2000s. In the 2010s, it is unknown if he is dead or in hiding after Doctor Doom took over the planet.
In the 2003 limited series Truth: Red, White & Black, black soldiers act as test subjects for the WWII Super-Soldier program of 1942. Most of the subjects die, or become deformed with the exception of one, Isaiah Bradley. Isaiah substitutes for Captain America on an assignment, discovering Jewish concentration camp detainees subjected to experiments.In Captain America (vol. 4) #28 (August 2004), an Isaiah Bradley from an alternative Earth became Captain America and never married. Later, he is elected president and serves two terms. He travels back in time, accidentally crossing to Earth-616, and brings the mainstream Captain America and Rebecca Quan forward into his own time to prevent his daughter, Rebecca "Becky" Barnes, from traveling to Earth-616.
In addition to the WWII era hero, a 1960s version of Captain America (a.k.a. "Captain America of the Vietnam War") exists as an Ultimate Marvel Universe parallel to the William Burnside/Captain America of the 1950s, who succeeded Rogers in the role after he is accidentally frozen. The 1960s Captain America is in fact Frank Simpson, better known in the Earth-616 Marvel Universe as Nuke. As scientists were unable to recreate the Super-Soldier Serum, they used cybernetics and steroids to enhance Simpson, which eventually eroded his sanity.
In an alternate future of the Ultimate Universe, Scott Summers assumes the mantle of Captain America after Steve Rogers dies and leads a small team of X-Men to fight for mutant justice.
Steve Rogers is selected for the Weapon X program. He is given a procedure similar to Wolverine's that bonds vibranium to his skeleton. He is given the code name Vibram.
Central America (Spanish: América Central, pronounced [aˈmeɾika senˈtɾal] (listen), Centroamérica pronounced [sentɾoaˈmeɾika] (listen)) is sometimes defined as a subregion of the Americas. This region is bordered by Mexico to the north, Colombia to the southeast, the Caribbean Sea to the east and the Pacific Ocean to the west and south. Central America consists of seven countries: El Salvador, Costa Rica, Belize, Guatemala, Honduras, Nicaragua and Panama. The combined population of Central America is estimated at 44.53 million (2016).Central America is a part of the Mesoamerican biodiversity hotspot, which extends from northern Guatemala to central Panama. Due to the presence of several active geologic faults and the Central America Volcanic Arc, there is a great deal of seismic activity in the region, such as volcanic eruptions and earthquakes, which has resulted in death, injury and property damage. In the Pre-Columbian era, Central America was inhabited by the indigenous peoples of Mesoamerica to the north and west and the Isthmo-Colombian peoples to the south and east. Following the Spanish expedition of Christopher Columbus' voyages to the Americas, Spain began to colonize the Americas. From 1609 to 1821, the majority of Central American territories (except for what would become Belize and Panama, and including the modern Mexican state of Chiapas) were governed by the viceroyalty of New Spain from Mexico City as the Captaincy General of Guatemala. On 24 August 1821, Spanish Viceroy Juan de O'Donojú signed the Treaty of Córdoba, which established New Spain's independence from Spain. On 15 September 1821, the Act of Independence of Central America was enacted to announce Central America's separation from the Spanish Empire and provide for the establishment of a new Central American state. Some of New Spain's provinces in the Central American region (i.e. what would become Guatemala, Honduras, El Salvador, Nicaragua and Costa Rica) were annexed to the First Mexican Empire; however, in 1823 they seceded from Mexico to form the Federal Republic of Central America until 1838. In 1838, Nicaragua, Honduras, Costa Rica and Guatemala became the first of Central America's seven states to become independent autonomous countries, followed by El Salvador in 1841, Panama in 1903 and Belize in 1981. Despite the dissolution of the Federal Republic of Central America, there is anecdotal evidence that demonstrates that Salvadorans, Panamanians, Costa Ricans, Guatemalans, Hondurans and Nicaraguans continue to maintain a Central American identity. For instance, Central Americans sometimes refer to their nations as if they were provinces of a Central American state. It is not unusual to write "C.A." after the country's name in formal and informal contexts. Governments in the region sometimes reinforce this sense of belonging to Central America in its citizens. Belizeans are usually identified as culturally West Indian rather than Central American.
"Central America" may mean different things to various people, based upon different contexts: The United Nations geoscheme for the Americas defines the region as all states of mainland North America south of the United States and specifically includes all of Mexico. Middle America is usually thought to comprise Mexico to the north of the 7 states of Central America as well as Colombia and Venezuela to the south. Usually, the whole of the Caribbean to the northeast, and sometimes the Guyanas, are also included. According to one source, the term "Central America" was used as a synonym for "Middle America" at least as recently as 1962. In Ibero-America (Spanish and Portuguese speaking American countries), the Americas is considered a single continent, and Central America is considered a subregion of the Americas comprising the seven countries south of Mexico and north of Colombia. For the people living in the five countries formerly part of the Federal Republic of Central America there is a distinction between the Spanish language terms "América Central" and "Centroamérica". While both can be translated into English as "Central America", "América Central" is generally used to refer to the geographical area of the seven countries between Mexico and Colombia, while "Centroamérica" is used when referring to the former members of the Federation emphasizing the shared culture and history of the region. In Portuguese as a rule and occasionally in Spanish and other languages, the entirety of the Antilles is often included in the definition of Central America. Indeed, the Dominican Republic is a full member of the Central American Integration System.
In the Pre-Columbian era, the northern areas of Central America were inhabited by the indigenous peoples of Mesoamerica. Most notable among these were the Mayans, who had built numerous cities throughout the region, and the Aztecs, who had created a vast empire. The pre-Columbian cultures of eastern El Salvador, eastern Honduras, Caribbean Nicaragua, most of Costa Rica and Panama were predominantly speakers of the Chibchan languages at the time of European contact and are considered by some culturally different and grouped in the Isthmo-Colombian Area. Following the Spanish expedition of Christopher Columbus's voyages to the Americas, the Spanish sent many expeditions to the region, and they began their conquest of Maya territory in 1523. Soon after the conquest of the Aztec Empire, Spanish conquistador Pedro de Alvarado commenced the conquest of northern Central America for the Spanish Empire. Beginning with his arrival in Soconusco in 1523, Alvarado's forces systematically conquered and subjugated most of the major Maya kingdoms, including the K'iche', Tz'utujil, Pipil, and the Kaqchikel. By 1528, the conquest of Guatemala was nearly complete, with only the Petén Basin remaining outside the Spanish sphere of influence. The last independent Maya kingdoms – the Kowoj and the Itza people – were finally defeated in 1697, as part of the Spanish conquest of Petén.In 1538, Spain established the Real Audiencia of Panama, which had jurisdiction over all land from the Strait of Magellan to the Gulf of Fonseca. This entity was dissolved in 1543, and most of the territory within Central America then fell under the jurisdiction of the Audiencia Real de Guatemala. This area included the current territories of Costa Rica, El Salvador, Guatemala, Honduras, Nicaragua, and the Mexican state of Chiapas, but excluded the lands that would become Belize and Panama. The president of the Audiencia, which had its seat in Antigua Guatemala, was the governor of the entire area. In 1609 the area became a captaincy general and the governor was also granted the title of captain general. The Captaincy General of Guatemala encompassed most of Central America, with the exception of present-day Belize and Panama. The Captaincy General of Guatemala lasted for more than two centuries, but began to fray after a rebellion in 1811 which began in the intendancy of San Salvador. The Captaincy General formally ended on 15 September 1821, with the signing of the Act of Independence of Central America. Mexican independence was achieved at virtually the same time with the signing of the Treaty of Córdoba and the Declaration of Independence of the Mexican Empire, and the entire region was finally independent from Spanish authority by 28 September 1821. From its independence from Spain in 1821 until 1823, the former Captaincy General remained intact as part of the short-lived First Mexican Empire. When the Emperor of Mexico abdicated on 19 March 1823, Central America again became independent. On 1 July 1823, the Congress of Central America peacefully seceded from Mexico and declared absolute independence from all foreign nations, and the region formed the Federal Republic of Central America.The Federal Republic of Central America was a representative democracy with its capital at Guatemala City. This union consisted of the provinces of Costa Rica, El Salvador, Guatemala, Honduras, Los Altos, Mosquito Coast, and Nicaragua. The lowlands of southwest Chiapas, including Soconusco, initially belonged to the Republic until 1824, when Mexico annexed most of Chiapas and began its claims to Soconusco. The Republic lasted from 1823 to 1838, when it disintegrated as a result of civil wars. The territory that now makes up Belize was heavily contested in a dispute that continued for decades after Guatemala achieved independence (see History of Belize (1506–1862). Spain, and later Guatemala, considered this land a Guatemalan department. In 1862, Britain formally declared it a British colony and named it British Honduras. It became independent as Belize in 1981.Panama, situated in the southernmost part of Central America on the Isthmus of Panama, has for most of its history been culturally and politically linked to South America. Panama was part of the Province of Tierra Firme from 1510 until 1538 when it came under the jurisdiction of the newly formed Audiencia Real de Panama. Beginning in 1543, Panama was administered as part of the Viceroyalty of Peru, along with all other Spanish possessions in South America. Panama remained as part of the Viceroyalty of Peru until 1739, when it was transferred to the Viceroyalty of New Granada, the capital of which was located at Santa Fé de Bogotá. Panama remained as part of the Viceroyalty of New Granada until the disestablishment of that viceroyalty in 1819. A series of military and political struggles took place from that time until 1822, the result of which produced the republic of Gran Colombia. After the dissolution of Gran Colombia in 1830, Panama became part of a successor state, the Republic of New Granada. From 1855 until 1886, Panama existed as Panama State, first within the Republic of New Granada, then within the Granadine Confederation, and finally within the United States of Colombia. The United States of Colombia was replaced by the Republic of Colombia in 1886. As part of the Republic of Colombia, Panama State was abolished and it became the Isthmus Department. Despite the many political reorganizations, Colombia was still deeply plagued by conflict, which eventually led to the secession of Panama on 3 November 1903. Only after that time did some begin to regard Panama as a North or Central American entity.By the 1930s the United Fruit Company owned 14,000 square kilometres (3.5 million acres) of land in Central America and the Caribbean and was the single largest land owner in Guatemala. Such holdings gave it great power over the governments of small countries. That was one of the factors that led to the coining of the phrase banana republic.After more than two hundred years of social unrest, violent conflict, and revolution, Central America today remains in a period of political transformation. Poverty, social injustice, and violence are still widespread. Nicaragua is the second poorest country in the western hemisphere (only Haiti is poorer).
Central America is a tapering isthmus running from the southern extent of Mexico to the northwestern portion of South America. The Pacific Ocean lies to the southwest, the Caribbean Sea lies to the northeast, and the Gulf of Mexico lies to the north. Some physiographists define the Isthmus of Tehuantepec as the northern geographic border of Central America, while others use the northwestern borders of Belize and Guatemala. From there, the Central American land mass extends southeastward to the Atrato River, where it connects to the Pacific Lowlands in northwestern South America. Of the many mountain ranges within Central America, the longest are the Sierra Madre de Chiapas, the Cordillera Isabelia and the Cordillera de Talamanca. At 4,220 meters (13,850 ft), Volcán Tajumulco is the highest peak in Central America. Other high points of Central America are as listed in the table below: Between the mountain ranges lie fertile valleys that are suitable for the raising of livestock and for the production of coffee, tobacco, beans and other crops. Most of the population of Honduras, Costa Rica and Guatemala lives in valleys.Trade winds have a significant effect upon the climate of Central America. Temperatures in Central America are highest just prior to the summer wet season, and are lowest during the winter dry season, when trade winds contribute to a cooler climate. The highest temperatures occur in April, due to higher levels of sunlight, lower cloud cover and a decrease in trade winds.
Central America is part of the Mesoamerican biodiversity hotspot, boasting 7% of the world's biodiversity. The Pacific Flyway is a major north–south flyway for migratory birds in the Americas, extending from Alaska to Tierra del Fuego. Due to the funnel-like shape of its land mass, migratory birds can be seen in very high concentrations in Central America, especially in the spring and autumn. As a bridge between North America and South America, Central America has many species from the Nearctic and the Neotropical realms. However the southern countries (Costa Rica and Panama) of the region have more biodiversity than the northern countries (Guatemala and Belize), meanwhile the central countries (Honduras, Nicaragua and El Salvador) have the least biodiversity. The table below shows recent statistics: Over 300 species of the region's flora and fauna are threatened, 107 of which are classified as critically endangered. The underlying problems are deforestation, which is estimated by FAO at 1.2% per year in Central America and Mexico combined, fragmentation of rainforests and the fact that 80% of the vegetation in Central America has already been converted to agriculture.Efforts to protect fauna and flora in the region are made by creating ecoregions and nature reserves. 36% of Belize's land territory falls under some form of official protected status, giving Belize one of the most extensive systems of terrestrial protected areas in the Americas. In addition, 13% of Belize's marine territory are also protected. A large coral reef extends from Mexico to Honduras: the Mesoamerican Barrier Reef System. The Belize Barrier Reef is part of this. The Belize Barrier Reef is home to a large diversity of plants and animals, and is one of the most diverse ecosystems of the world. It is home to 70 hard coral species, 36 soft coral species, 500 species of fish and hundreds of invertebrate species. So far only about 10% of the species in the Belize barrier reef have been discovered.
From 2001 to 2010, 5,376 square kilometers (2,076 sq mi) of forest were lost in the region. In 2010 Belize had 63% of remaining forest cover, Costa Rica 46%, Panama 45%, Honduras 41%, Guatemala 37%, Nicaragua 29%, and El Salvador 21%. Most of the loss occurred in the moist forest biome, with 12,201 square kilometers (4,711 sq mi). Woody vegetation loss was partially set off by a gain in the coniferous forest biome with 4,730 square kilometers (1,830 sq mi), and a gain in the dry forest biome at 2,054 square kilometers (793 sq mi). Mangroves and deserts contributed only 1% to the loss in forest vegetation. The bulk of the deforestation was located at the Caribbean slopes of Nicaragua with a loss of 8,574 square kilometers (3,310 sq mi) of forest in the period from 2001 to 2010. The most significant regrowth of 3,050 square kilometers (1,180 sq mi) of forest was seen in the coniferous woody vegetation of Honduras.The Central American pine-oak forests ecoregion, in the tropical and subtropical coniferous forests biome, is found in Central America and southern Mexico. The Central American pine-oak forests occupy an area of 111,400 square kilometers (43,000 sq mi), extending along the mountainous spine of Central America, extending from the Sierra Madre de Chiapas in Mexico's Chiapas state through the highlands of Guatemala, El Salvador, and Honduras to central Nicaragua. The pine-oak forests lie between 600–1,800 metres (2,000–5,900 ft) elevation, and are surrounded at lower elevations by tropical moist forests and tropical dry forests. Higher elevations above 1,800 metres (5,900 ft) are usually covered with Central American montane forests. The Central American pine-oak forests are composed of many species characteristic of temperate North America including oak, pine, fir, and cypress. Laurel forest is the most common type of Central American temperate evergreen cloud forest, found in almost all Central American countries, normally more than 1,000 meters (3,300 ft) above sea level. Tree species include evergreen oaks, members of the laurel family, and species of Weinmannia, Drimys, and Magnolia. The cloud forest of Sierra de las Minas, Guatemala, is the largest in Central America. In some areas of southeastern Honduras there are cloud forests, the largest located near the border with Nicaragua. In Nicaragua, cloud forests are situated near the border with Honduras, but many were cleared to grow coffee. There are still some temperate evergreen hills in the north. The only cloud forest in the Pacific coastal zone of Central America is on the Mombacho volcano in Nicaragua. In Costa Rica, there are laurel forests in the Cordillera de Tilarán and Volcán Arenal, called Monteverde, also in the Cordillera de Talamanca. The Central American montane forests are an ecoregion of the tropical and subtropical moist broadleaf forests biome, as defined by the World Wildlife Fund. These forests are of the moist deciduous and the semi-evergreen seasonal subtype of tropical and subtropical moist broadleaf forests and receive high overall rainfall with a warm summer wet season and a cooler winter dry season. Central American montane forests consist of forest patches located at altitudes ranging from 1,800–4,000 metres (5,900–13,100 ft), on the summits and slopes of the highest mountains in Central America ranging from Southern Mexico, through Guatemala, El Salvador, and Honduras, to northern Nicaragua. The entire ecoregion covers an area of 13,200 square kilometers (5,100 sq mi) and has a temperate climate with relatively high precipitation levels.
Ecoregions are not only established to protect the forests themselves but also because they are habitats for an incomparably rich and often endemic fauna. Almost half of the bird population of the Talamancan montane forests in Costa Rica and Panama are endemic to this region. Several birds are listed as threatened, most notably the resplendent quetzal (Pharomacrus mocinno), three-wattled bellbird (Procnias tricarunculata), bare-necked umbrellabird (Cephalopterus glabricollis), and black guan (Chamaepetes unicolor). Many of the amphibians are endemic and depend on the existence of forest. The golden toad that once inhabited a small region in the Monteverde Reserve, which is part of the Talamancan montane forests, has not been seen alive since 1989 and is listed as extinct by IUCN. The exact causes for its extinction are unknown. Global warming may have played a role, because the development of that frog is typical for this area may have been compromised. Seven small mammals are endemic to the Costa Rica-Chiriqui highlands within the Talamancan montane forest region. Jaguars, cougars, spider monkeys, as well as tapirs, and anteaters live in the woods of Central America. The Central American red brocket is a brocket deer found in Central America's tropical forest.
Central America is geologically very active, with volcanic eruptions and earthquakes occurring frequently, and tsunamis occurring occasionally. Many thousands of people have died as a result of these natural disasters. Most of Central America rests atop the Caribbean Plate. This tectonic plate converges with the Cocos, Nazca, and North American plates to form the Middle America Trench, a major subduction zone. The Middle America Trench is situated some 60–160 kilometers (37–99 mi) off the Pacific coast of Central America and runs roughly parallel to it. Many large earthquakes have occurred as a result of seismic activity at the Middle America Trench. For example, subduction of the Cocos Plate beneath the North American Plate at the Middle America Trench is believed to have caused the 1985 Mexico City earthquake that killed as many as 40,000 people. Seismic activity at the Middle America Trench is also responsible for earthquakes in 1902, 1942, 1956, 1982, 1992, 2001, 2007, 2012, 2014, and many other earthquakes throughout Central America. The Middle America Trench is not the only source of seismic activity in Central America. The Motagua Fault is an onshore continuation of the Cayman Trough which forms part of the tectonic boundary between the North American Plate and the Caribbean Plate. This transform fault cuts right across Guatemala and then continues offshore until it merges with the Middle America Trench along the Pacific coast of Mexico, near Acapulco. Seismic activity at the Motagua Fault has been responsible for earthquakes in 1717, 1773, 1902, 1976, 1980, and 2009. Another onshore continuation of the Cayman Trough is the Chixoy-Polochic Fault, which runs parallel to, and roughly 80 kilometers (50 mi) to the north, of the Motagua Fault. Though less active than the Motagua Fault, seismic activity at the Chixoy-Polochic Fault is still thought to be capable of producing very large earthquakes, such as the 1816 earthquake of Guatemala.Managua, the capital of Nicaragua, was devastated by earthquakes in 1931 and 1972. Volcanic eruptions are also common in Central America. In 1968 the Arenal Volcano, in Costa Rica, erupted killing 87 people as the 3 villages of Tabacon, Pueblo Nuevo and San Luis were buried under pyroclastic flows and debris. Fertile soils from weathered volcanic lava have made it possible to sustain dense populations in the agriculturally productive highland areas.
The official language majority in all Central American countries is Spanish, except in Belize, where the official language is English. Mayan languages constitute a language family consisting of about 26 related languages. Guatemala formally recognized 21 of these in 1996. Xinca and Garifuna are also present in Central America.
This region of the continent is very rich in terms of ethnic groups. The majority of the population is mestizo, with sizable Mayan and African descendent populations present, including Xinca and Garifuna minorities. The immigration of Arabs, Jews, Chinese, Europeans and others brought additional groups to the area.
The predominant religion in Central America is Christianity (95.6%). Beginning with the Spanish colonization of Central America in the 16th century, Roman Catholicism became the most popular religion in the region until the first half of the 20th century. Since the 1960s, there has been an increase in other Christian groups, particularly Protestantism, as well as other religious organizations, and individuals identifying themselves as having no religion.
Central American music Central American cuisine List of cuisines of the Americas – Central American cuisine
Central American Games Central American and Caribbean Games 1926 Central American and Caribbean Games – the first time this event occurred Central American Football Union Surfing
Central America is currently undergoing a process of political, economic and cultural transformation that started in 1907 with the creation of the Central American Court of Justice. In 1951 the integration process continued with the signature of the San Salvador Treaty, which created the ODECA, the Organization of Central American States. However, the unity of the ODECA was limited by conflicts between several member states. In 1991, the integration agenda was further advanced by the creation of the Central American Integration System (Sistema para la Integración Centroamericana, or SICA). SICA provides a clear legal basis to avoid disputes between the member states. SICA membership includes the 7 nations of Central America plus the Dominican Republic, a state that is traditionally considered part of the Caribbean. On 6 December 2008, SICA announced an agreement to pursue a common currency and common passport for the member nations. No timeline for implementation was discussed. Central America already has several supranational institutions such as the Central American Parliament, the Central American Bank for Economic Integration and the Central American Common Market. On 22 July 2011, President Mauricio Funes of El Salvador became the first president pro tempore to SICA. El Salvador also became the headquarters of SICA with the inauguration of a new building.
Until recently, all Central American countries have maintained diplomatic relations with Taiwan instead of China. President Óscar Arias of Costa Rica, however, established diplomatic relations with China in 2007, severing formal diplomatic ties with Taiwan. After breaking off relations with the Republic of China in 2017, Panama established diplomatic relations with the People's Republic of China. In August 2018, El Salvador also severed ties with Taiwan to formally start recognizing the People's Republic of China as sole China, a move many considered lacked transparency due to its abruptness and reports of the Chinese government's desires to invest in the department of La Union while also promising to fund the ruling party's reelection campaign. President of El Salvador, Nayib Bukele, broke diplomatic relations with Taiwan and establish better ones with China.
The Central American Parliament (also known as PARLACEN) is a political and parliamentary body of SICA. The parliament started around 1980, and its primary goal was to resolve conflicts in Nicaragua, Guatemala, and El Salvador. Although the group was disbanded in 1986, ideas of unity of Central Americans still remained, so a treaty was signed in 1987 to create the Central American Parliament and other political bodies. Its original members were Guatemala, El Salvador, Nicaragua and Honduras. The parliament is the political organ of Central America, and is part of SICA. New members have since then joined including Panama and the Dominican Republic. Costa Rica is not a member State of the Central American Parliament and its adhesion remains as a very unpopular topic at all levels of the Costa Rican society due to existing strong political criticism towards the regional parliament, since it is regarded by Costa Ricans as a menace to democratic accountability and effectiveness of integration efforts. Excessively high salaries for its members, legal immunity of jurisdiction from any member State, corruption, lack of a binding nature and effectiveness of the regional parliament's decisions, high operative costs and immediate membership of Central American Presidents once they leave their office and presidential terms, are the most common reasons invoked by Costa Ricans against the Central American Parliament.
Signed in 2004, the Central American Free Trade Agreement (CAFTA) is an agreement between the United States, Costa Rica, El Salvador, Guatemala, Honduras, Nicaragua, and the Dominican Republic. The treaty is aimed at promoting free trade among its members. Guatemala has the largest economy in the region. Its main exports are coffee, sugar, bananas, petroleum, clothing, and cardamom. Of its 10.29 billion dollar annual exports, 40.2% go to the United States, 11.1% to neighboring El Salvador, 8% to Honduras, 5.5% to Mexico, 4.7% to Nicaragua, and 4.3% to Costa Rica.The region is particularly attractive for companies (especially clothing companies) because of its geographical proximity to the[United States], very low wages and considerable tax advantages. In addition, the decline in the prices of coffee and other export products and the structural adjustment measures promoted by the international financial institutions have partly ruined agriculture, favouring the emergence of maquiladoras. This sector accounts for 42 per cent of total exports from El Salvador, 55 per cent from Guatemala, and 65 per cent from Honduras. However, its contribution to the economies of these countries is disputed; raw materials are imported, jobs are precarious and low-paid, and tax exemptions weaken public finances.They are also criticised for the working conditions of employees: insults and physical violence, abusive dismissals (especially of pregnant workers), working hours, non-payment of overtime. According to Lucrecia Bautista, coordinator of the maquilas sector of the audit firm Coverco, labour law regulations are regularly violated in maquilas and there is no political will to enforce their application. In the case of infringements, the labour inspectorate shows remarkable leniency. It is a question of not discouraging investors. Trade unionists are subject to pressure, and sometimes to kidnapping or murder. In some cases, business leaders have used the services of the maras. Finally, black lists containing the names of trade unionists or political activists are circulating in employers' circles.Economic growth in Central America is projected to slow slightly in 2014–15, as country-specific domestic factors offset the positive effects from stronger economic activity in the United States.
Tourism in Belize has grown considerably in more recent times, and it is now the second largest industry in the nation. Belizean Prime Minister Dean Barrow has stated his intention to use tourism to combat poverty throughout the country. The growth in tourism has positively affected the agricultural, commercial, and finance industries, as well as the construction industry. The results for Belize's tourism-driven economy have been significant, with the nation welcoming almost one million tourists in a calendar year for the first time in its history in 2012. Belize is also the only country in Central America with English as its official language, making this country a comfortable destination for English-speaking tourists.Costa Rica is the most visited nation in Central America. Tourism in Costa Rica is one of the fastest growing economic sectors of the country, having become the largest source of foreign revenue by 1995. Since 1999, tourism has earned more foreign exchange than bananas, pineapples and coffee exports combined. The tourism boom began in 1987, with the number of visitors up from 329,000 in 1988, through 1.03 million in 1999, to a historical record of 2.43 million foreign visitors and $1.92-billion in revenue in 2013. In 2012 tourism contributed with 12.5% of the country's GDP and it was responsible for 11.7% of direct and indirect employment.Tourism in Nicaragua has grown considerably recently, and it is now the second largest industry in the nation. Nicaraguan President Daniel Ortega has stated his intention to use tourism to combat poverty throughout the country. The growth in tourism has positively affected the agricultural, commercial, and finance industries, as well as the construction industry. The results for Nicaragua's tourism-driven economy have been significant, with the nation welcoming one million tourists in a calendar year for the first time in its history in 2010.
Central America. Columbia Encyclopedia, 6th ed. 2001–6. New York: Columbia University Press. American Heritage Dictionaries, Central America. WordNet Princeton University: Central America. Central America. Columbia Gazetteer of the World Online. 2006. New York: Columbia University Press. Hernández, Consuelo (2009). Reconstruyendo a Centroamérica a través de la poesía. Voces y perspectivas en la poesia latinoamericana del siglo XX. Madrid: Visor.
Central America Video Links from the Dean Peter Krogh Foreign Affairs Digital Archives Central America country pages Teaching Central America
In the wealthy African nation of Zamunda, crown prince Akeem Joffer grows weary of his pampered lifestyle on his 21st birthday and wishes to do more for himself. When his parents, King Jaffe and Queen Aoleon, present him with an arranged bride-to-be, Akeem takes action. Seeking an independent woman who loves him for himself and not his social status, Akeem and his best friend/personal aide, Semmi, travel to the New York City borough of Queens and rent a squalid tenement in the neighborhood of Long Island City under the guise of poor foreign students. Beginning their search for Akeem's bride, they end up being invited by some locals to a rally that is raising money for the inner city. During the rally, Akeem encounters Lisa McDowell, who possesses all the qualities he is looking for and, upon his insistence, he and Semmi get entry-level jobs working at the local fast-food restaurant called McDowell's, a McDonald's knockoff owned by widower Cleo McDowell, Lisa's father. Akeem's attempts to win Lisa's love are complicated by Lisa's lazy and obnoxious boyfriend, Darryl Jenks (Eriq La Salle), whose father owns Soul Glo (a Jheri curl–like hairstyling aid). After Darryl announces their engagement -— without Lisa's consent -— to their families, she starts dating Akeem, who claims that he comes from a family of poor goat herders. Meanwhile, although Akeem thrives on hard work and learning how commoners live, Semmi is not comfortable with living in such meager conditions. After a dinner date with Lisa is thwarted when Semmi furnishes their apartment with a hot tub and other luxuries, Akeem confiscates his money and donates it to two homeless men. Semmi wires a telegraph to King Jaffe for more money, prompting the Joffers to travel to Queens and expose his identity as a prince. Cleo, initially disapproving of Akeem as he did not want to see his daughter with a poor man, becomes ecstatic when he discovers that Akeem is actually an extremely wealthy prince after being introduced to the Joffers. When Akeem discovers that his parents have arrived in the United States, he and Lisa take shelter at the McDowell residence where Cleo welcomes them. After Cleo's bond with Akeem is ruined by Darryl's unexpected arrival, Lisa later becomes angry and confused that Akeem lied to her about his identity. Akeem explains that he wanted her to love him for who, not what, he is, even offering to renounce his throne; but Lisa, still hurt and angry, refuses to marry him. Despondent, Akeem resigns himself to the arranged marriage, but as they leave, Jaffe is reprimanded by Aoleon for clinging to outdated traditions instead of thinking of his son's happiness. At the wedding procession, a still-heartbroken Akeem becomes surprised when his veiled bride-to-be is Lisa herself. Following the ceremony, they ride happily in a carriage to the cheers of Zamundans. Witnessing such splendor, Lisa is both surprised and touched by the fact that Akeem would have given it up just for her. Akeem offers again to abdicate if she does not want this life, but Lisa playfully declines.
Paramount cancelled press screenings of the film after initial negative reactions to a press screening in New York.
Coming To America received positive reviews upon release. Review-aggregation website Rotten Tomatoes gives the film a score of 65%, based on reviews from 43 critics with an average rating of 5.81/10. The website's critical consensus reads, "Eddie Murphy was in full control at this point, starkly evident in Coming to America's John Landis' coasting direction." On Metacritic, the film holds a weighted average score of 47 out of 100, based on 16 reviews, indicating "mixed or average reviews". Audiences polled by CinemaScore gave the film an average grade of "A" on an A+ to F scale.Sheila Benson in the Los Angeles Times called it a "hollow and wearying Eddie Murphy fairy tale" and bemoans, "That an Eddie Murphy movie would come to this." Vincent Canby in The New York Times was also critical of the writing, calling it a "possibly funny idea" but suggesting the screenplay had escaped before it was ready. Canby viewed the film as essentially a romantic comedy but said the romantic elements fell flat, and the film instead goes for broad slapstick. Siskel & Ebert had mixed opinions on the film. Siskel enjoyed the acting from Murphy and Hall but Ebert was disappointed that Murphy did not bring his usual more lively performance, and Ebert was also critical of the unoriginal script.
The film was nominated for two Oscars: Best Costume Design by Deborah Nadoolman Landis and Best Makeup by Rick Baker, who designed the makeup effects for both Murphy's and Arsenio Hall's multiple supporting characters.
The film was the subject of the Buchwald v. Paramount civil suit, which the humorist Art Buchwald filed in 1990 against the film's producers on the grounds that the film's idea was stolen from his 1982 script treatment about a rich, despotic African potentate who comes to America for a state visit. Paramount had optioned the treatment from Buchwald, and John Landis was attached as director and Eddie Murphy as the lead, but after two years of development hell Paramount abandoned the project in March 1985. In 1987, Paramount began working on Coming to America based on a story by Eddie Murphy. Buchwald won the breach of contract action and the court ordered monetary damages. The parties later settled the case out-of-court before an appeal going to trial.
The jingle for the commercial for the fictional product Soul Glo was composed by Nile Rodgers, who has suggested it is his "proudest moment". Vocals were provided by Christopher Max.
A television pilot of a weekly sitcom version of the film was produced for CBS, following the film's success, starring Tommy Davidson as Prince Tariq, and Paul Bates reprising his role as Oha. The pilot went unsold, but was televised on July 4, 1989 as part of the CBS Summer Playhouse pilot anthology series.
The movie has been described as having a "cult following" years after its release, despite negative press, with one of the highest-grossing box office of the year it was released, as well as one of the highest-grossing films featuring African-Americans.
In early 2017, an announcement was publicized which addressed the impending production of a sequel to the film. Kevin Misher was named as producer, and Sheffield and Blaustein, the original screenwriters, were also attached to the project. However, a possible participation of lead actors Eddie Murphy and Arsenio Hall was left undefined.On January 11, 2019, it was announced that the sequel will be moving forward with Murphy reprising his role and Craig Brewer as director (having previously worked with Murphy on the Netflix film Dolemite Is My Name). Arsenio Hall, Shari Headley, John Amos, Paul Bates and James Earl Jones are expected to return for the sequel as well. Wesley Snipes signed on for a role in the film. Leslie Jones and rapper Rick Ross are also joining the cast in an undisclosed role.The film was scheduled to be theatrically released on December 18, 2020, but due to the COVID-19 pandemic, Amazon Studios bought the distribution rights and will release it digitally on Prime Video on the exact same release date.
Coming to America on IMDb Paramount Movies Coming to America at Box Office Mojo Coming to America at Rotten Tomatoes Filming Locations Coming to America at the American Film Institute Catalog
Conference USA (C-USA or CUSA) is an intercollegiate athletic conference whose current member institutions are located within the Southern United States. The conference participates in the NCAA's Division I in all sports. C-USA's offices are located in Dallas, Texas.
C-USA was founded in 1995 by the merger of the Metro Conference and Great Midwest Conference, two Division I conferences that did not sponsor football. However, the merger did not include either Great Midwest member Dayton or Metro members VCU and Virginia Tech. Since this left an uneven number of schools in the conference, Houston of the dissolving Southwest Conference was extended an invitation and agreed to join following the SWC's disbanding at the end of the 1995–96 academic year. The conference immediately started competition in all sports, except football which started in 1996. Being the result of a merger, C-USA was originally a sprawling, large league that stretched from Florida to Missouri, Wisconsin to Texas. Many of its original schools were located in major urban centers and had strong basketball traditions, which helped establish the league on a national basis.
On November 27, 2012, it was announced that Tulane would leave the conference to join the Big East in all sports, and East Carolina would join the Big East for football only (ECU's membership was upgraded to all-sports in March 2013 after the Big East's non-football members, save for ACC-bound Notre Dame, announced they were leaving to form a new conference which took the Big East name, leaving the football-playing members to become the American Athletic Conference). Conference USA responded by adding Middle Tennessee and Florida Atlantic, both from the Sun Belt. On April 1, 2013, Conference USA announced they were adding Western Kentucky, also from the Sun Belt, to offset Tulsa's departure to The American in all sports which was confirmed the next day.Citing financial difficulties, the UAB football program was shut down on December 2, 2014. According to Conference USA bylaws, member schools must sponsor football. In January 2015, UAB announced an independent re-evaluation of the program and the finances involved, leaving open a possible resumption of the program as early as the 2016 season. On January 29, 2015, the conference announced that there was no time pressure in making a decision regarding UAB's future membership. The conference also stated that it would wait for the results of the new study before any further discussions on the subject. On June 1, UAB announced that it would reinstate football effective with the 2016 season, presumably keeping the school in C-USA for the immediate future. The return of football was later pushed back to 2017. The Blazers won the 2018 conference championship their second year back.
Commissioner Britton Banowsky stepped down on September 15, 2015 to become the head of the College Football Playoff Foundation. Executive associate commissioner and chief operating officer Judy MacLeod was subsequently named interim commissioner. On October 26 MacLeod was named the conference's third official commissioner, also becoming the first woman to head an FBS conference.
In 2019, Conference USA inducted its first Hall of Fame class, comprising 20 student-athletes, three coaches, and two administrators. The inductees included former University of Cincinnati basketball player Kenyon Martin, baseball player Kevin Youkilis, and men’s basketball head coach Bob Huggins.
Notes
In this table, all dates reflect the calendar year of entry into Conference USA, which for spring sports is the year before the start of competition.
Notes
In this table, all dates reflect each school's actual entry into and departure from Conference USA. For spring sports, the joining date is the calendar year before the start of competition. For fall sports, the departure date is the calendar year after the last season of competition. Notes
Full members (all-sports) Full members (non-football) Affiliate members (football-only) Affiliate member (other sport)Other Conference Other Conference
Michael Slive 1995–2002 Britton Banowsky 2002–2015 Judy MacLeod 2015–present
Conference USA sponsors championship competition in nine men's and ten women's NCAA sanctioned sports. Two schools are affiliate members for men's soccer.
Men's varsity sports not sponsored by Conference USA which are played by current full C-USA members:
Women's varsity sports not sponsored by Conference USA which are played by current full C-USA members:
This list goes through the 2012–13 season.
No team has won an NCAA team championship as a member of C-USA. However, the following C-USA teams have won national championships when they were not affiliated with C-USA:
Notes
In 2016, C-USA began a long-term television contract with lead partners ESPN and CBS Sports Network, with ESPN carrying 5 football games and the football championship game; and CBSSN carrying 6 football games, 5 basketball games, and both the men's and women's basketball championship games. C-USA also renewed and expanded its partnership with American Sports Network; owned and operated by Sinclair Broadcast Group, ASN will carry between 15 and 30 football games; between 13 and 55 men's basketball games; and between 2 and 5 women's basketball games. ASN will also carry 10 events in other C-USA sports.The conference also entered into a contract with beIN Sports for 10 football games (marking the first domestic American football rights the network has ever acquired, and the first broadcast rights deal it had ever entered into with a college conference), 10 men's and 10 women's basketball games, 12 baseball and 12 softball games, 10 men's and 10 women's soccer games (excluding conference men's soccer games at Kentucky and South Carolina, covered by their primary conference's contract), and 10 women's volleyball games.The total values of the 2016 contracts are notably lower than those of the previous contracts (which included Fox Sports).Men's soccer associate members Kentucky and South Carolina have an agreement with their primary conference for other sports to carry all home matches online through the SEC Network service, including all Conference USA conference matches. ESPN and the SEC Network will have first rights to all C-USA home men's soccer matches featuring both schools. In 2017 American Sports Network and Campus Insiders merged creating Stadium. Stadium's C-USA content will be available to stream on Twitter and Pluto TV. In 2017 Stadium completed a deal with Facebook to exclusively stream some C-USA football games. In 2017 C-USA entered an agreement with the streaming subscription service FloSports to stream three football games.
In 2016 C-USA partnered with SIDEARM Sports to create a subscription based streaming service named CUSA.tv. In a statement C-USA Commissioner Judy MacLeod said. "Thanks to our partnership with SIDEARM Sports, this new site showcases a clean modern look with easy access to information and we are proud to offer live content and original feature stories through our CUSA.tv." Various sports including football, basketball, and baseball will exclusively air on CUSA.tv when they are not picked up by other networks.
One of the current member schools, Rice University is a member of the Association of American Universities (AAU), an organization of 62 leading research universities in the United States and Canada. Six of the Conference's fourteen members are doctorate-granting universities with "very high research activity," the highest classification given by the Carnegie Foundation for the Advancement of Teaching. A majority of the Conference's members are ranked as Tier One National Universities in U.S. News and World Report's 2021 Best Colleges rankings. Notes
Official website
The following table lists estimates for the population of India (including what are now Pakistan and Bangladesh) from prehistory up until 1820. It includes estimates and growth rates according to five different economic historians, along with interpolated estimates and overall aggregate averages derived from their estimates. The population grew from the South Asian Stone Age in 10,000 BC to the Maurya Empire in 200 BC at a steadily increasing growth rate, before population growth slowed down in the classical era up to 500 AD, and then became largely stagnant during the early medieval era up to 1000 AD. The population growth rate then increased in the late medieval era (during the Delhi Sultanate) from 1000 to 1500.India's population growth rate under the Mughal Empire (16th–18th centuries) was higher than during any previous period in Indian history. Under the Mughal Empire, India experienced an unprecedented economic and demographic upsurge, due to Mughal agrarian reforms that intensified agricultural production, proto-industrialization that established India as the most important centre of manufacturing in international trade, and a relatively high degree of urbanisation for its time; 15% of the population lived in urban centres, higher than the percentage of the population in 19th-century British India and contemporary Europe up until the 19th century.Under the reign of Akbar (reigned 1556–1605) in 1600, the Mughal Empire's urban population was up to 17 million people, larger than the urban population in Europe. By 1700, Mughal India had an urban population of 23 million people, larger than British India's urban population of 22.3 million in 1871. Nizamuddin Ahmad (1551–1621) reported that, under Akbar's reign, Mughal India had 120 large cities and 3,200 townships. A number of cities in India had a population between a quarter-million and half-million people, with larger cities including Agra (in Agra Subah) with up to 800,000 people and Dhaka (in Bengal Subah) with over 1 million people. Mughal India also had a large number of villages, with 455,698 villages by the time of Aurangzeb (reigned 1658–1707).In the early 18th century, the average life expectancy in Mughal India was 35 years. In comparison, the average life expectancy for several European nations in the 18th century were 34 years in early modern England, up to 30 years in France, and about 25 years in Prussia.
The total fertility rate is the number of children born per woman. It is based on fairly good data for the entire years. Sources: Our World In Data and Gapminder Foundation. Life expectancy from 1881 to 1950 The population of India under the British Raj (including what are now Pakistan and Bangladesh) according to censuses: Studies of India's population since 1881 have focused on such topics as total population, birth and death rates, growth rates, geographic distribution, literacy, the rural and urban divide, cities of a million, and the three cities with populations over eight million: Delhi, Greater Mumbai (Bombay), and Kolkata (Calcutta).Mortality rates fell in the period 1920–45, primarily due to biological immunisation. Other factors included rising incomes, better living conditions, improved nutrition, a safer and cleaner environment, and better official health policies and medical care.
The table below summarises India's demographics (excluding the Mao-Maram, Paomata and Purul subdivisions of Senapati District of Manipur state due to cancellation of census results) according to religion at the 2011 census in per cent. The data is "unadjusted" (without excluding Assam and Jammu and Kashmir); the 1981 census was not conducted in Assam and the 1991 census was not conducted in Jammu and Kashmir. Missing citing/reference for "Changes in religious demagraphics over time" table below. Characteristics of religious groups
The table below represents the infant mortality rate trends in India, based on sex, over the last 15 years. In the urban areas of India, average male infant mortality rates are slightly higher than average female infant mortality rates. Some activists believe India's 2011 census shows a serious decline in the number of girls under the age of seven – activists posit that eight million female fetuses may have been aborted between 2001 and 2011. These claims are controversial. Scientists who study human sex ratios and demographic trends suggest that birth sex ratio between 1.08 and 1.12 can be because of natural factors, such as the age of mother at baby's birth, age of father at baby's birth, number of babies per couple, economic stress, endocrinological factors, etc. The 2011 census birth sex ratio in India, of 917 girls to 1000 boys, is similar to 870–930 girls to 1000 boys birth sex ratios observed in Japanese, Chinese, Cuban, Filipino and Hawaiian ethnic groups in the United States between 1940 and 2005. They are also similar to birth sex ratios below 900 girls to 1000 boys observed in mothers of different age groups and gestation periods in the United States.
41.03% of the Indians speak Hindi while the rest speak Assamese, Bengali, Gujarati, Maithili, Kannada, Malayalam, Marathi, Odia, Punjabi, Tamil, Telugu, Urdu and a variety of other languages. There are a total of 122 languages and 234 mother tongues. The 22 languages are Languages specified in the Eighth Schedule of Indian Constitution and 100 non-specified languages. The table immediately below excludes Mao-Maram, Paomata and Purul subdivisions of Senapati District of Manipur state due to cancellation of census results.
Source: UN World Population Prospects
From the Demographic Health Survey:
Caste and community statistics as recorded from "Socially and Educationally Backward Classes Commission" (SEBC) or Mandal Commission of 1979. This was completed in 1983.There has not yet been a proper consensus on contemporary figures.The following data is from the Mandal report:
India is projected to overtake China as the world's most populous nation by 2027. Note that these projections make assumptions about future fertility and death rates which may not turn out to be correct in the event. Fertility rates also vary from region to region, with some higher than the national average and some lower of China.
In millions
The national Census of India does not recognise racial or ethnic groups within India, but recognises many of the tribal groups as Scheduled Castes and Tribes (see list of Scheduled Tribes in India). According to a 2009 study published by Reich et al., the modern Indian population is composed of two genetically divergent and heterogeneous populations which mixed in ancient times (about 1,200–3,500 BP), known as Ancestral North Indians (ANI) and Ancestral South Indians (ASI). ASI corresponds to the Dravidian-speaking population of southern India, whereas ANI corresponds to the Indo-Aryan-speaking population of northern India. 700,000 people from the United States of any race live in India. Between 300,000 and 1 million Anglo-Indians live in India.For a list of ethnic groups in the Republic of India (as well as neighbouring countries) see ethnic groups of the Indian subcontinent.
Numerous genomic studies have been conducted in the last 15 years to seek insights into India's demographic and cultural diversity. These studies paint a complex and conflicting picture. In a 2003 study, Basu, Majumder et al. have concluded on the basis of results obtained from mtDNA, Y-chromosome and autosomal markers that "(1) there is an underlying unity of female lineages in India, indicating that the initial number of female settlers may have been small; (2) the tribal and the caste populations are highly differentiated; (3) the Austroasiatic tribals are the earliest settlers in India, providing support to one anthropological hypothesis while refuting some others; (4) a major wave of humans entered India through the northeast; (5) the Tibeto-Burman tribals share considerable genetic commonalities with the Austroasiatic tribals, supporting the hypothesis that they may have shared a common habitat in southern China, but the two groups of tribals can be differentiated on the basis of Y-chromosomal haplotypes; (6) the Dravidian speaking populations were possibly widespread throughout India but are regulated to South India now ; (7) formation of populations by fission that resulted in founder and drift effects have left their imprints on the genetic structures of contemporary populations; (8) the upper castes show closer genetic affinities with Central Asian populations, although those of southern India are more distant than those of northern India; (9) historical gene flow into India has contributed to a considerable obliteration of genetic histories of contemporary populations so that there is at present no clear congruence of genetic and geographical or sociocultural affinities." In a later 2010 review article, Majumder affirms some of these conclusions, introduces and revises some other. The ongoing studies, concludes Majumder, suggest India has served as the major early corridor for geographical dispersal of modern humans from out-of-Africa. The archaeological and genetic traces of the earliest settlers in India has not provided any conclusive evidence. The tribal populations of India are older than the non-tribal populations. The autosomal differentiation and genetic diversity within India's caste populations at 0.04 is significantly lower than 0.14 for continental populations and 0.09 for 31 world population sets studied by Watkins et al., suggesting that while tribal populations were differentiated, the differentiation effects within India's caste population was less than previously thought. Majumder also concludes that recent studies suggest India has been a major contributor to the gene pool of southeast Asia. Another study covering a large sample of Indian populations allowed Watkins et al. to examine eight Indian caste groups and four endogamous south Indian tribal populations. The Indian castes data show low between-group differences, while the tribal Indian groups show relatively high between-group differentiation. This suggests that people between Indian castes were not reproductively isolated, while Indian tribal populations experienced reproductive isolation and drift. Furthermore, the genetic fixation index data shows historical genetic differentiation and segregation between Indian castes population is much smaller than those found in east Asia, Africa and other continental populations; while being similar to the genetic differentiation and segregation observed in European populations. In 2006, Sahoo et al. reported their analysis of genomic data on 936 Y-chromosomes representing 32 tribal and 45 caste groups from different regions of India. These scientists find that the haplogroup frequency distribution across the country, between different caste groups, was found to be predominantly driven by geographical, rather than cultural determinants. They conclude there is clear evidence for both large-scale immigration into ancient India of Sino-Tibetan speakers and language change of former Austroasiatic speakers, in the northeast Indian region. The genome studies conducted up until 2010 have been on relatively small population sets. Many are from just one southeastern state of Andhra Pradesh (including Telangana, which was part of the state until June 2014). Thus, any conclusions on demographic history of India must be interpreted with caution. A larger national genome study with demographic growth and sex ratio balances may offer further insights on the extent of genetic differentiation and segregation in India over the millenniums.
2011 census of India National Commission on Population
List of most populous cities in India List of most populous metropolitan areas in India List of million-plus urban agglomerations in India List of states and union territories of India by population
Census of India; government site with detailed data from 2001 census Population of India as per Census India 2011 Census of India map generator; generates maps based on 2001 census figures Demographic data for India; provides sources of demographic data for India 2001 maps; provides maps of social, economic and demographic data of India in 2001 Population of India 2011 map; distribution of population amongst states and union territories India's Demographic Outlook: Implications and Trends United Nations "World Population Prospects": Country Profile – India Aggregated demographic statistics from Indian and global data sources Demographic statistics for India – online on Bluenomics India comparing with China population projection graph Based on data from database of UN Population Division
Dinosaurs are a diverse group of reptiles of the clade Dinosauria. They first appeared during the Triassic period, between 243 and 233.23 million years ago, although the exact origin and timing of the evolution of dinosaurs is the subject of active research. They became the dominant terrestrial vertebrates after the Triassic–Jurassic extinction event 201.3 million years ago; their dominance continued throughout the Jurassic and Cretaceous periods. The fossil record shows that birds are modern feathered dinosaurs, having evolved from earlier theropods during the Late Jurassic epoch, and are the only dinosaur lineage to survive the Cretaceous–Paleogene extinction event approximately 66 million years ago. Dinosaurs can therefore be divided into avian dinosaurs, or birds; and non-avian dinosaurs, which are all dinosaurs other than birds. Dinosaurs are a varied group of animals from taxonomic, morphological and ecological standpoints. Birds, at over 10,000 living species, are the most diverse group of vertebrates besides perciform fish. Using fossil evidence, paleontologists have identified over 500 distinct genera and more than 1,000 different species of non-avian dinosaurs. Dinosaurs are represented on every continent by both extant species (birds) and fossil remains. Through the first half of the 20th century, before birds were recognized to be dinosaurs, most of the scientific community believed dinosaurs to have been sluggish and cold-blooded. Most research conducted since the 1970s, however, has indicated that all dinosaurs were active animals with elevated metabolisms and numerous adaptations for social interaction. Some were herbivorous, others carnivorous. Evidence suggests that all dinosaurs were egg-laying; and that nest-building was a trait shared by many dinosaurs, both avian and non-avian. While dinosaurs were ancestrally bipedal, many extinct groups included quadrupedal species, and some were able to shift between these stances. Elaborate display structures such as horns or crests are common to all dinosaur groups, and some extinct groups developed skeletal modifications such as bony armor and spines. While the dinosaurs' modern-day surviving avian lineage (birds) are generally small due to the constraints of flight, many prehistoric dinosaurs (non-avian and avian) were large-bodied—the largest sauropod dinosaurs are estimated to have reached lengths of 39.7 meters (130 feet) and heights of 18 m (59 ft) and were the largest land animals of all time. Still, the idea that non-avian dinosaurs were uniformly gigantic is a misconception based in part on preservation bias, as large, sturdy bones are more likely to last until they are fossilized. Many dinosaurs were quite small: Xixianykus, for example, was only about 50 centimeters (20 inches) long. Since the first dinosaur fossils were recognized in the early 19th century, mounted fossil dinosaur skeletons have been major attractions at museums around the world, and dinosaurs have become an enduring part of world culture. The large sizes of some dinosaur groups, as well as their seemingly monstrous and fantastic nature, have ensured dinosaurs' regular appearance in best-selling books and films, such as Jurassic Park. Persistent public enthusiasm for the animals has resulted in significant funding for dinosaur science, and new discoveries are regularly covered by the media.
The taxon 'Dinosauria' was formally named in 1841 by paleontologist Sir Richard Owen, who used it to refer to the "distinct tribe or sub-order of Saurian Reptiles" that were then being recognized in England and around the world. The term is derived from Ancient Greek δεινός (deinos) 'terrible, potent or fearfully great', and σαῦρος (sauros) 'lizard or reptile'. Though the taxonomic name has often been interpreted as a reference to dinosaurs' teeth, claws, and other fearsome characteristics, Owen intended it to also evoke their size and majesty.Other prehistoric animals, including pterosaurs, mosasaurs, ichthyosaurs, plesiosaurs, and Dimetrodon, while often popularly conceived of as dinosaurs, are not taxonomically classified as dinosaurs. Pterosaurs are distantly related to dinosaurs, being members of the clade Ornithodira. The other groups mentioned are, like dinosaurs and pterosaurs, members of Sauropsida (the reptile and bird clade), except Dimetrodon (which is a synapsid).
Under phylogenetic nomenclature, dinosaurs are usually defined as the group consisting of the most recent common ancestor (MRCA) of Triceratops and modern birds (Neornithes), and all its descendants. It has also been suggested that Dinosauria be defined with respect to the MRCA of Megalosaurus and Iguanodon, because these were two of the three genera cited by Richard Owen when he recognized the Dinosauria. Both definitions result in the same set of animals being defined as dinosaurs: "Dinosauria = Ornithischia + Saurischia", encompassing ankylosaurians (armored herbivorous quadrupeds), stegosaurians (plated herbivorous quadrupeds), ceratopsians (herbivorous quadrupeds with horns and frills), ornithopods (bipedal or quadrupedal herbivores including "duck-bills"), theropods (mostly bipedal carnivores and birds), and sauropodomorphs (mostly large herbivorous quadrupeds with long necks and tails).Birds are now recognized as being the sole surviving lineage of theropod dinosaurs. In traditional taxonomy, birds were considered a separate class that had evolved from dinosaurs, a distinct superorder. However, a majority of contemporary paleontologists concerned with dinosaurs reject the traditional style of classification in favor of phylogenetic taxonomy; this approach requires that, for a group to be natural, all descendants of members of the group must be included in the group as well. Birds are thus considered to be dinosaurs and dinosaurs are, therefore, not extinct. Birds are classified as belonging to the subgroup Maniraptora, which are coelurosaurs, which are theropods, which are saurischians, which are dinosaurs.Research by Matthew G. Baron, David B. Norman, and Paul M. Barrett in 2017 suggested a radical revision of dinosaurian systematics. Phylogenetic analysis by Baron et al. recovered the Ornithischia as being closer to the Theropoda than the Sauropodomorpha, as opposed to the traditional union of theropods with sauropodomorphs. They resurrected the clade Ornithoscelida to refer to the group containing Ornithischia and Theropoda. Dinosauria itself was re-defined as the last common ancestor of Triceratops horridus, Passer domesticus and Diplodocus carnegii, and all of its descendants, to ensure that sauropods and kin remain included as dinosaurs.
Using one of the above definitions, dinosaurs can be generally described as archosaurs with hind limbs held erect beneath the body. Many prehistoric animal groups are popularly conceived of as dinosaurs, such as ichthyosaurs, mosasaurs, plesiosaurs, pterosaurs, and pelycosaurs (especially Dimetrodon), but are not classified scientifically as dinosaurs, and none had the erect hind limb posture characteristic of true dinosaurs. Dinosaurs were the dominant terrestrial vertebrates of the Mesozoic Era, especially the Jurassic and Cretaceous periods. Other groups of animals were restricted in size and niches; mammals, for example, rarely exceeded the size of a domestic cat, and were generally rodent-sized carnivores of small prey.Dinosaurs have always been an extremely varied group of animals; according to a 2006 study, over 500 non-avian dinosaur genera have been identified with certainty so far, and the total number of genera preserved in the fossil record has been estimated at around 1850, nearly 75% of which remain to be discovered. An earlier study predicted that about 3,400 dinosaur genera existed, including many that would not have been preserved in the fossil record. By September 17, 2008, 1,047 different species of dinosaurs had been named.In 2016, the estimated number of dinosaur species that existed in the Mesozoic was estimated to be 1,543–2,468. Some are herbivorous, others carnivorous, including seed-eaters, fish-eaters, insectivores, and omnivores. While dinosaurs were ancestrally bipedal (as are all modern birds), some prehistoric species were quadrupeds, and others, such as Anchisaurus and Iguanodon, could walk just as easily on two or four legs. Cranial modifications like horns and crests are common dinosaurian traits, and some extinct species had bony armor. Although known for large size, many Mesozoic dinosaurs were human-sized or smaller, and modern birds are generally small in size. Dinosaurs today inhabit every continent, and fossils show that they had achieved global distribution by at least the Early Jurassic epoch. Modern birds inhabit most available habitats, from terrestrial to marine, and there is evidence that some non-avian dinosaurs (such as Microraptor) could fly or at least glide, and others, such as spinosaurids, had semiaquatic habits.
While recent discoveries have made it more difficult to present a universally agreed-upon list of dinosaurs' distinguishing features, nearly all dinosaurs discovered so far share certain modifications to the ancestral archosaurian skeleton, or are clear descendants of older dinosaurs showing these modifications. Although some later groups of dinosaurs featured further modified versions of these traits, they are considered typical for Dinosauria; the earliest dinosaurs had them and passed them on to their descendants. Such modifications, originating in the most recent common ancestor of a certain taxonomic group, are called the synapomorphies of such a group.A detailed assessment of archosaur interrelations by Sterling Nesbitt confirmed or found the following twelve unambiguous synapomorphies, some previously known: in the skull, a supratemporal fossa (excavation) is present in front of the supratemporal fenestra, the main opening in the rear skull roof epipophyses, obliquely backward-pointing processes on the rear top corners, present in the anterior (front) neck vertebrae behind the atlas and axis, the first two neck vertebrae apex of a deltopectoral crest (a projection on which the deltopectoral muscles attach) located at or more than 30% down the length of the humerus (upper arm bone) radius, a lower arm bone, shorter than 80% of humerus length fourth trochanter (projection where the caudofemoralis muscle attaches on the inner rear shaft) on the femur (thigh bone) is a sharp flange fourth trochanter asymmetrical, with distal, lower, margin forming a steeper angle to the shaft on the astragalus and calcaneum, upper ankle bones, the proximal articular facet, the top connecting surface, for the fibula occupies less than 30% of the transverse width of the element exoccipitals (bones at the back of the skull) do not meet along the midline on the floor of the endocranial cavity, the inner space of the braincase in the pelvis, the proximal articular surfaces of the ischium with the ilium and the pubis are separated by a large concave surface (on the upper side of the ischium a part of the open hip joint is located between the contacts with the pubic bone and the ilium) cnemial crest on the tibia (protruding part of the top surface of the shinbone) arcs anterolaterally (curves to the front and the outer side) distinct proximodistally oriented (vertical) ridge present on the posterior face of the distal end of the tibia (the rear surface of the lower end of the shinbone) concave articular surface for the fibula of the calcaneum (the top surface of the calcaneum, where it touches the fibula, has a hollow profile)Nesbitt found a number of further potential synapomorphies and discounted a number of synapomorphies previously suggested. Some of these are also present in silesaurids, which Nesbitt recovered as a sister group to Dinosauria, including a large anterior trochanter, metatarsals II and IV of subequal length, reduced contact between ischium and pubis, the presence of a cnemial crest on the tibia and of an ascending process on the astragalus, and many others. A variety of other skeletal features are shared by dinosaurs. However, because they are either common to other groups of archosaurs or were not present in all early dinosaurs, these features are not considered to be synapomorphies. For example, as diapsids, dinosaurs ancestrally had two pairs of Infratemporal fenestrae (openings in the skull behind the eyes), and as members of the diapsid group Archosauria, had additional openings in the snout and lower jaw. Additionally, several characteristics once thought to be synapomorphies are now known to have appeared before dinosaurs, or were absent in the earliest dinosaurs and independently evolved by different dinosaur groups. These include an elongated scapula, or shoulder blade; a sacrum composed of three or more fused vertebrae (three are found in some other archosaurs, but only two are found in Herrerasaurus); and a perforate acetabulum, or hip socket, with a hole at the center of its inside surface (closed in Saturnalia tupiniquim, for example). Another difficulty of determining distinctly dinosaurian features is that early dinosaurs and other archosaurs from the Late Triassic epoch are often poorly known and were similar in many ways; these animals have sometimes been misidentified in the literature. Dinosaurs stand with their hind limbs erect in a manner similar to most modern mammals, but distinct from most other reptiles, whose limbs sprawl out to either side. This posture is due to the development of a laterally facing recess in the pelvis (usually an open socket) and a corresponding inwardly facing distinct head on the femur. Their erect posture enabled early dinosaurs to breathe easily while moving, which likely permitted stamina and activity levels that surpassed those of "sprawling" reptiles. Erect limbs probably also helped support the evolution of large size by reducing bending stresses on limbs. Some non-dinosaurian archosaurs, including rauisuchians, also had erect limbs but achieved this by a "pillar-erect" configuration of the hip joint, where instead of having a projection from the femur insert on a socket on the hip, the upper pelvic bone was rotated to form an overhanging shelf.
Dinosaurs diverged from their archosaur ancestors during the Middle to Late Triassic epochs, roughly 20 million years after the devastating Permian–Triassic extinction event wiped out an estimated 96% of all marine species and 70% of terrestrial vertebrate species approximately 252 million years ago. Radiometric dating of the rock formation that contained fossils from the early dinosaur genus Eoraptor at 231.4 million years old establishes its presence in the fossil record at this time. Paleontologists think that Eoraptor resembles the common ancestor of all dinosaurs; if this is true, its traits suggest that the first dinosaurs were small, bipedal predators. The discovery of primitive, dinosaur-like ornithodirans such as Marasuchus and Lagerpeton in Argentinian Middle Triassic strata supports this view; analysis of recovered fossils suggests that these animals were indeed small, bipedal predators. Dinosaurs may have appeared as early as 243 million years ago, as evidenced by remains of the genus Nyasasaurus from that period, though known fossils of these animals are too fragmentary to tell if they are dinosaurs or very close dinosaurian relatives. Paleontologist Max C. Langer et al. (2018) determined that Staurikosaurus from the Santa Maria Formation dates to 233.23 million years ago, making it older in geologic age than Eoraptor.When dinosaurs appeared, they were not the dominant terrestrial animals. The terrestrial habitats were occupied by various types of archosauromorphs and therapsids, like cynodonts and rhynchosaurs. Their main competitors were the pseudosuchia, such as aetosaurs, ornithosuchids and rauisuchians, which were more successful than the dinosaurs. Most of these other animals became extinct in the Triassic, in one of two events. First, at about 215 million years ago, a variety of basal archosauromorphs, including the protorosaurs, became extinct. This was followed by the Triassic–Jurassic extinction event (about 201 million years ago), that saw the end of most of the other groups of early archosaurs, like aetosaurs, ornithosuchids, phytosaurs, and rauisuchians. Rhynchosaurs and dicynodonts survived (at least in some areas) at least as late as early-mid Norian and late Norian or earliest Rhaetian stages, respectively, and the exact date of their extinction is uncertain. These losses left behind a land fauna of crocodylomorphs, dinosaurs, mammals, pterosaurians, and turtles. The first few lines of early dinosaurs diversified through the Carnian and Norian stages of the Triassic, possibly by occupying the niches of the groups that became extinct. Also notably, there was a heightened rate of extinction during the Carnian Pluvial Event.
Dinosaur evolution after the Triassic follows changes in vegetation and the location of continents. In the Late Triassic and Early Jurassic, the continents were connected as the single landmass Pangaea, and there were a worldwide dinosaur fauna mostly composed of coelophysoid carnivores and early sauropodomorph herbivores. Gymnosperm plants (particularly conifers), a potential food source, radiated in the Late Triassic. Early sauropodomorphs did not have sophisticated mechanisms for processing food in the mouth, and so must have employed other means of breaking down food farther along the digestive tract. The general homogeneity of dinosaurian faunas continued into the Middle and Late Jurassic, where most localities had predators consisting of ceratosaurians, spinosauroids, and carnosaurians, and herbivores consisting of stegosaurian ornithischians and large sauropods. Examples of this include the Morrison Formation of North America and Tendaguru Beds of Tanzania. Dinosaurs in China show some differences, with specialized sinraptorid theropods and unusual, long-necked sauropods like Mamenchisaurus. Ankylosaurians and ornithopods were also becoming more common, but prosauropods had become extinct. Conifers and pteridophytes were the most common plants. Sauropods, like the earlier prosauropods, were not oral processors, but ornithischians were evolving various means of dealing with food in the mouth, including potential cheek-like organs to keep food in the mouth, and jaw motions to grind food. Another notable evolutionary event of the Jurassic was the appearance of true birds, descended from maniraptoran coelurosaurians. By the Early Cretaceous and the ongoing breakup of Pangaea, dinosaurs were becoming strongly differentiated by landmass. The earliest part of this time saw the spread of ankylosaurians, iguanodontians, and brachiosaurids through Europe, North America, and northern Africa. These were later supplemented or replaced in Africa by large spinosaurid and carcharodontosaurid theropods, and rebbachisaurid and titanosaurian sauropods, also found in South America. In Asia, maniraptoran coelurosaurians like dromaeosaurids, troodontids, and oviraptorosaurians became the common theropods, and ankylosaurids and early ceratopsians like Psittacosaurus became important herbivores. Meanwhile, Australia was home to a fauna of basal ankylosaurians, hypsilophodonts, and iguanodontians. The stegosaurians appear to have gone extinct at some point in the late Early Cretaceous or early Late Cretaceous. A major change in the Early Cretaceous, which would be amplified in the Late Cretaceous, was the evolution of flowering plants. At the same time, several groups of dinosaurian herbivores evolved more sophisticated ways to orally process food. Ceratopsians developed a method of slicing with teeth stacked on each other in batteries, and iguanodontians refined a method of grinding with dental batteries, taken to its extreme in hadrosaurids. Some sauropods also evolved tooth batteries, best exemplified by the rebbachisaurid Nigersaurus. There were three general dinosaur faunas in the Late Cretaceous. In the northern continents of North America and Asia, the major theropods were tyrannosaurids and various types of smaller maniraptoran theropods, with a predominantly ornithischian herbivore assemblage of hadrosaurids, ceratopsians, ankylosaurids, and pachycephalosaurians. In the southern continents that had made up the now-splitting Gondwana, abelisaurids were the common theropods, and titanosaurian sauropods the common herbivores. Finally, in Europe, dromaeosaurids, rhabdodontid iguanodontians, nodosaurid ankylosaurians, and titanosaurian sauropods were prevalent. Flowering plants were greatly radiating, with the first grasses appearing by the end of the Cretaceous. Grinding hadrosaurids and shearing ceratopsians became extremely diverse across North America and Asia. Theropods were also radiating as herbivores or omnivores, with therizinosaurians and ornithomimosaurians becoming common.The Cretaceous–Paleogene extinction event, which occurred approximately 66 million years ago at the end of the Cretaceous, caused the extinction of all dinosaur groups except for the neornithine birds. Some other diapsid groups, such as crocodilians, sebecosuchians, turtles, lizards, snakes, sphenodontians, and choristoderans, also survived the event.The surviving lineages of neornithine birds, including the ancestors of modern ratites, ducks and chickens, and a variety of waterbirds, diversified rapidly at the beginning of the Paleogene period, entering ecological niches left vacant by the extinction of Mesozoic dinosaur groups such as the arboreal enantiornithines, aquatic hesperornithines, and even the larger terrestrial theropods (in the form of Gastornis, eogruiids, bathornithids, ratites, geranoidids, mihirungs, and "terror birds"). It is often cited that mammals out-competed the neornithines for dominance of most terrestrial niches but many of these groups co-existed with rich mammalian faunas for most of the Cenozoic Era. Terror birds and bathornithids occupied carnivorous guilds alongside predatory mammals, and ratites are still fairly successful as mid-sized herbivores; eogruiids similarly lasted from the Eocene to Pliocene, only becoming extinct very recently after over 20 million years of co-existence with many mammal groups.
Dinosaurs belong to a group known as archosaurs, which also includes modern crocodilians. Within the archosaur group, dinosaurs are differentiated most noticeably by their gait. Dinosaur legs extend directly beneath the body, whereas the legs of lizards and crocodilians sprawl out to either side.Collectively, dinosaurs as a clade are divided into two primary branches, Saurischia and Ornithischia. Saurischia includes those taxa sharing a more recent common ancestor with birds than with Ornithischia, while Ornithischia includes all taxa sharing a more recent common ancestor with Triceratops than with Saurischia. Anatomically, these two groups can be distinguished most noticeably by their pelvic structure. Early saurischians—"lizard-hipped", from the Greek sauros (σαῦρος) meaning "lizard" and ischion (ἰσχίον) meaning "hip joint"—retained the hip structure of their ancestors, with a pubis bone directed cranially, or forward. This basic form was modified by rotating the pubis backward to varying degrees in several groups (Herrerasaurus, therizinosauroids, dromaeosaurids, and birds). Saurischia includes the theropods (exclusively bipedal and with a wide variety of diets) and sauropodomorphs (long-necked herbivores which include advanced, quadrupedal groups).By contrast, ornithischians—"bird-hipped", from the Greek ornitheios (ὀρνίθειος) meaning "of a bird" and ischion (ἰσχίον) meaning "hip joint"—had a pelvis that superficially resembled a bird's pelvis: the pubic bone was oriented caudally (rear-pointing). Unlike birds, the ornithischian pubis also usually had an additional forward-pointing process. Ornithischia includes a variety of species that were primarily herbivores. Despite the terms "bird hip" and "lizard hip", birds are not part of Ornithischia, but rather Saurischia—birds evolved from earlier dinosaurs with "lizard hips".
The following is a simplified classification of dinosaur groups based on their evolutionary relationships, and organized based on the list of Mesozoic dinosaur species provided by Holtz (2007). A more detailed version can be found at Dinosaur classification. The dagger (†) is used to signify groups with no living members. DinosauriaSaurischia ("lizard-hipped"; includes Theropoda and Sauropodomorpha)†Herrerasauria (early bipedal carnivores) Theropoda (all bipedal; most were carnivorous) †Coelophysoidea (small, early theropods; includes Coelophysis and close relatives) †Dilophosauridae (early crested and carnivorous theropods) †Ceratosauria (generally elaborately horned, the dominant southern carnivores of the Cretaceous) Tetanurae ("stiff tails"; includes most theropods)†Megalosauroidea (early group of large carnivores including the semiaquatic spinosaurids) †Carnosauria (Allosaurus and close relatives, like Carcharodontosaurus) Coelurosauria (feathered theropods, with a range of body sizes and niches)†Compsognathidae (common early coelurosaurs with reduced forelimbs) †Tyrannosauridae (Tyrannosaurus and close relatives; had reduced forelimbs) †Ornithomimosauria ("ostrich-mimics"; mostly toothless; carnivores to possible herbivores) †Alvarezsauroidea (small insectivores with reduced forelimbs each bearing one enlarged claw) Maniraptora ("hand snatchers"; had long, slender arms and fingers)†Therizinosauria (bipedal herbivores with large hand claws and small heads) †Oviraptorosauria (mostly toothless; their diet and lifestyle are uncertain) †Archaeopterygidae (small, winged theropods or primitive birds) †Deinonychosauria (small- to medium-sized; bird-like, with a distinctive toe claw) Avialae (modern birds and extinct relatives)†Scansoriopterygidae (small primitive avialans with long third fingers) †Omnivoropterygidae (large, early short-tailed avialans) †Confuciusornithidae (small toothless avialans) †Enantiornithes (primitive tree-dwelling, flying avialans) Euornithes (advanced flying birds)†Yanornithiformes (toothed Cretaceous Chinese birds) †Hesperornithes (specialized aquatic diving birds) Aves (modern, beaked birds and their extinct relatives) †Sauropodomorpha (herbivores with small heads, long necks, long tails)†Guaibasauridae (small, primitive, omnivorous sauropodomorphs) †Plateosauridae (primitive, strictly bipedal "prosauropods") †Riojasauridae (small, primitive sauropodomorphs) †Massospondylidae (small, primitive sauropodomorphs) †Sauropoda (very large and heavy, usually over 15 m (49 ft) long; quadrupedal)†Vulcanodontidae (primitive sauropods with pillar-like limbs) †Eusauropoda ("true sauropods")†Cetiosauridae ("whale reptiles") †Turiasauria (European group of Jurassic and Cretaceous sauropods) †Neosauropoda ("new sauropods")†Diplodocoidea (skulls and tails elongated; teeth typically narrow and pencil-like) †Macronaria (boxy skulls; spoon- or pencil-shaped teeth)†Brachiosauridae (long-necked, long-armed macronarians) †Titanosauria (diverse; stocky, with wide hips; most common in the Late Cretaceous of southern continents) †Ornithischia ("bird-hipped"; diverse bipedal and quadrupedal herbivores)†Heterodontosauridae (small basal ornithopod herbivores/omnivores with prominent canine-like teeth) †Thyreophora (armored dinosaurs; mostly quadrupeds)†Ankylosauria (scutes as primary armor; some had club-like tails) †Stegosauria (spikes and plates as primary armor)†Neornithischia ("new ornithischians")†Ornithopoda (various sizes; bipeds and quadrupeds; evolved a method of chewing using skull flexibility and numerous teeth) †Marginocephalia (characterized by a cranial growth)†Pachycephalosauria (bipeds with domed or knobby growth on skulls) †Ceratopsia (quadrupeds with frills; many also had horns)
Knowledge about dinosaurs is derived from a variety of fossil and non-fossil records, including fossilized bones, feces, trackways, gastroliths, feathers, impressions of skin, internal organs and soft tissues. Many fields of study contribute to our understanding of dinosaurs, including physics (especially biomechanics), chemistry, biology, and the Earth sciences (of which paleontology is a sub-discipline). Two topics of particular interest and study have been dinosaur size and behavior.
Current evidence suggests that dinosaur average size varied through the Triassic, Early Jurassic, Late Jurassic and Cretaceous. Predatory theropod dinosaurs, which occupied most terrestrial carnivore niches during the Mesozoic, most often fall into the 100 to 1000 kg (220 to 2200 lb) category when sorted by estimated weight into categories based on order of magnitude, whereas recent predatory carnivoran mammals peak in the 10 to 100 kg (22 to 220 lb) category. The mode of Mesozoic dinosaur body masses is between 1 to 10 metric tons (1.1 to 11.0 short tons). This contrasts sharply with the average size of Cenozoic mammals, estimated by the National Museum of Natural History as about 2 to 5 kg (4.4 to 11.0 lb).The sauropods were the largest and heaviest dinosaurs. For much of the dinosaur era, the smallest sauropods were larger than anything else in their habitat, and the largest was an order of magnitude more massive than anything else that has since walked the Earth. Giant prehistoric mammals such as Paraceratherium (the largest land mammal ever) were dwarfed by the giant sauropods, and only modern whales approach or surpass them in size. There are several proposed advantages for the large size of sauropods, including protection from predation, reduction of energy use, and longevity, but it may be that the most important advantage was dietary. Large animals are more efficient at digestion than small animals, because food spends more time in their digestive systems. This also permits them to subsist on food with lower nutritive value than smaller animals. Sauropod remains are mostly found in rock formations interpreted as dry or seasonally dry, and the ability to eat large quantities of low-nutrient browse would have been advantageous in such environments.
Scientists will probably never be certain of the largest and smallest dinosaurs to have ever existed. This is because only a tiny percentage of animals were ever fossilized and most of these remain buried in the earth. Few of the specimens that are recovered are complete skeletons, and impressions of skin and other soft tissues are rare. Rebuilding a complete skeleton by comparing the size and morphology of bones to those of similar, better-known species is an inexact art, and reconstructing the muscles and other organs of the living animal is, at best, a process of educated guesswork.The tallest and heaviest dinosaur known from good skeletons is Giraffatitan brancai (previously classified as a species of Brachiosaurus). Its remains were discovered in Tanzania between 1907 and 1912. Bones from several similar-sized individuals were incorporated into the skeleton now mounted and on display at the Museum für Naturkunde in Berlin; this mount is 12 meters (39 ft) tall and 21.8 to 22.5 meters (72 to 74 ft) long, and would have belonged to an animal that weighed between 30000 and 60000 kilograms (70000 and 130000 lb). The longest complete dinosaur is the 27 meters (89 ft) long Diplodocus, which was discovered in Wyoming in the United States and displayed in Pittsburgh's Carnegie Museum of Natural History in 1907. The longest dinosaur known from good fossil material is the Patagotitan: the skeleton mount in the American Museum of Natural History in New York is 37 meters (121 ft) long. The Museo Municipal Carmen Funes in Plaza Huincul, Argentina, has an Argentinosaurus reconstructed skeleton mount 39.7 meters (130 ft) long. There were larger dinosaurs, but knowledge of them is based entirely on a small number of fragmentary fossils. Most of the largest herbivorous specimens on record were discovered in the 1970s or later, and include the massive Argentinosaurus, which may have weighed 80000 to 100000 kilograms (90 to 110 short tons) and reached length of 30 to 40 meters (98 to 131 ft); some of the longest were the 33.5 meters (110 ft) long Diplodocus hallorum (formerly Seismosaurus), the 33 to 34 meters (108 to 112 ft) long Supersaurus and 37 meters (121 ft) long Patagotitan; and the tallest, the 18 meters (59 ft) tall Sauroposeidon, which could have reached a sixth-floor window. The heaviest and longest dinosaur may have been Maraapunisaurus, known only from a now lost partial vertebral neural arch described in 1878. Extrapolating from the illustration of this bone, the animal may have been 58 meters (190 ft) long and weighed 122400 kg (270000 lb). However, as no further evidence of sauropods of this size has been found, and the discoverer, Edward Drinker Cope, had made typographic errors before, it is likely to have been an extreme overestimation.The largest carnivorous dinosaur was Spinosaurus, reaching a length of 12.6 to 18 meters (41 to 59 ft), and weighing 7 to 20.9 metric tons (7.7 to 23.0 short tons). Other large carnivorous theropods included Giganotosaurus, Carcharodontosaurus and Tyrannosaurus. Therizinosaurus and Deinocheirus were among the tallest of the theropods. The largest ornithischian dinosaur was probably the hadrosaurid Shantungosaurus giganteus which measured 16.6 meters (54 ft). The largest individuals may have weighed as much as 16 metric tons (18 short tons).The smallest dinosaur known is the bee hummingbird, with a length of only 5 centimeters (2.0 in) and mass of around 1.8 g (0.063 oz). The smallest known non-avialan dinosaurs were about the size of pigeons and were those theropods most closely related to birds. For example, Anchiornis huxleyi is currently the smallest non-avialan dinosaur described from an adult specimen, with an estimated weight of 110 g (3.9 oz) and a total skeletal length of 34 centimeters (1.12 ft). The smallest herbivorous non-avialan dinosaurs included Microceratus and Wannanosaurus, at about 60 centimeters (2.0 ft) long each.
Many modern birds are highly social, often found living in flocks. There is general agreement that some behaviors that are common in birds, as well as in crocodiles (birds' closest living relatives), were also common among extinct dinosaur groups. Interpretations of behavior in fossil species are generally based on the pose of skeletons and their habitat, computer simulations of their biomechanics, and comparisons with modern animals in similar ecological niches.The first potential evidence for herding or flocking as a widespread behavior common to many dinosaur groups in addition to birds was the 1878 discovery of 31 Iguanodon bernissartensis, ornithischians that were then thought to have perished together in Bernissart, Belgium, after they fell into a deep, flooded sinkhole and drowned. Other mass-death sites have been discovered subsequently. Those, along with multiple trackways, suggest that gregarious behavior was common in many early dinosaur species. Trackways of hundreds or even thousands of herbivores indicate that duck-billed (hadrosaurids) may have moved in great herds, like the American bison or the African Springbok. Sauropod tracks document that these animals traveled in groups composed of several different species, at least in Oxfordshire, England, although there is no evidence for specific herd structures. Congregating into herds may have evolved for defense, for migratory purposes, or to provide protection for young. There is evidence that many types of slow-growing dinosaurs, including various theropods, sauropods, ankylosaurians, ornithopods, and ceratopsians, formed aggregations of immature individuals. One example is a site in Inner Mongolia that has yielded the remains of over 20 Sinornithomimus, from one to seven years old. This assemblage is interpreted as a social group that was trapped in mud. The interpretation of dinosaurs as gregarious has also extended to depicting carnivorous theropods as pack hunters working together to bring down large prey. However, this lifestyle is uncommon among modern birds, crocodiles, and other reptiles, and the taphonomic evidence suggesting mammal-like pack hunting in such theropods as Deinonychus and Allosaurus can also be interpreted as the results of fatal disputes between feeding animals, as is seen in many modern diapsid predators. The crests and frills of some dinosaurs, like the marginocephalians, theropods and lambeosaurines, may have been too fragile to be used for active defense, and so they were likely used for sexual or aggressive displays, though little is known about dinosaur mating and territorialism. Head wounds from bites suggest that theropods, at least, engaged in active aggressive confrontations.From a behavioral standpoint, one of the most valuable dinosaur fossils was discovered in the Gobi Desert in 1971. It included a Velociraptor attacking a Protoceratops, providing evidence that dinosaurs did indeed attack each other. Additional evidence for attacking live prey is the partially healed tail of an Edmontosaurus, a hadrosaurid dinosaur; the tail is damaged in such a way that shows the animal was bitten by a tyrannosaur but survived. Cannibalism amongst some species of dinosaurs was confirmed by tooth marks found in Madagascar in 2003, involving the theropod Majungasaurus.Comparisons between the scleral rings of dinosaurs and modern birds and reptiles have been used to infer daily activity patterns of dinosaurs. Although it has been suggested that most dinosaurs were active during the day, these comparisons have shown that small predatory dinosaurs such as dromaeosaurids, Juravenator, and Megapnosaurus were likely nocturnal. Large and medium-sized herbivorous and omnivorous dinosaurs such as ceratopsians, sauropodomorphs, hadrosaurids, ornithomimosaurs may have been cathemeral, active during short intervals throughout the day, although the small ornithischian Agilisaurus was inferred to be diurnal.Based on current fossil evidence from dinosaurs such as Oryctodromeus, some ornithischian species seem to have led a partially fossorial (burrowing) lifestyle. Many modern birds are arboreal (tree climbing), and this was also true of many Mesozoic birds, especially the enantiornithines. While some early bird-like species may have already been arboreal as well (including dromaeosaurids such as Microraptor) most non-avialan dinosaurs seem to have relied on land-based locomotion. A good understanding of how dinosaurs moved on the ground is key to models of dinosaur behavior; the science of biomechanics, pioneered by Robert McNeill Alexander, has provided significant insight in this area. For example, studies of the forces exerted by muscles and gravity on dinosaurs' skeletal structure have investigated how fast dinosaurs could run, whether diplodocids could create sonic booms via whip-like tail snapping, and whether sauropods could float.
Modern birds are known to communicate using visual and auditory signals, and the wide diversity of visual display structures among fossil dinosaur groups, such as horns, frills, crests, sails, and feathers, suggests that visual communication has always been important in dinosaur biology. Reconstruction of the plumage color of Anchiornis huxleyi, suggest the importance of color in visual communication in non-avian dinosaurs. The evolution of dinosaur vocalization is less certain. Paleontologist Phil Senter suggests that non-avian dinosaurs relied mostly on visual displays and possibly non-vocal acoustic sounds like hissing, jaw grinding or clapping, splashing and wing beating (possible in winged maniraptoran dinosaurs). He states they were unlikely to have been capable of vocalizing since their closest relatives, crocodilians and birds, use different means to vocalize, the former via the larynx and the latter through the unique syrinx, suggesting they evolved independently and their common ancestor was mute.The earliest remains of a syrinx, which has enough mineral content for fossilization, was found in a specimen of the duck-like Vegavis iaai dated 69-66 million years ago, and this organ is unlikely to have existed in non-avian dinosaurs. However, in contrast to Senter, the researchers have suggested that dinosaurs could vocalize and that the syrinx-based vocal system of birds evolved from a larynx-based one, rather than the two systems evolving independently. A 2016 study suggests that dinosaurs produced closed mouth vocalizations like cooing, which occur in both crocodilians and birds as well as other reptiles. Such vocalizations evolved independently in extant archosaurs numerous times, following increases in body size. The crests of the Lambeosaurini and nasal chambers of ankylosaurids have been suggested to function in vocal resonance, though Senter states that the presence of resonance chambers in some dinosaurs is not necessarily evidence of vocalization as modern snakes have such chambers which intensify their hisses.
All dinosaurs laid amniotic eggs with hard shells made mostly of calcium carbonate. Dinosaur eggs were usually laid in a nest. Most species create somewhat elaborate nests which can be cups, domes, plates, beds scrapes, mounds, or burrows. Some species of modern bird have no nests; the cliff-nesting common guillemot lays its eggs on bare rock, and male emperor penguins keep eggs between their body and feet. Primitive birds and many non-avialan dinosaurs often lay eggs in communal nests, with males primarily incubating the eggs. While modern birds have only one functional oviduct and lay one egg at a time, more primitive birds and dinosaurs had two oviducts, like crocodiles. Some non-avialan dinosaurs, such as Troodon, exhibited iterative laying, where the adult might lay a pair of eggs every one or two days, and then ensured simultaneous hatching by delaying brooding until all eggs were laid.When laying eggs, females grow a special type of bone between the hard outer bone and the marrow of their limbs. This medullary bone, which is rich in calcium, is used to make eggshells. A discovery of features in a Tyrannosaurus rex skeleton provided evidence of medullary bone in extinct dinosaurs and, for the first time, allowed paleontologists to establish the sex of a fossil dinosaur specimen. Further research has found medullary bone in the carnosaur Allosaurus and the ornithopod Tenontosaurus. Because the line of dinosaurs that includes Allosaurus and Tyrannosaurus diverged from the line that led to Tenontosaurus very early in the evolution of dinosaurs, this suggests that the production of medullary tissue is a general characteristic of all dinosaurs. Another widespread trait among modern birds (but see below in regards to fossil groups and extant megapodes) is parental care for young after hatching. Jack Horner's 1978 discovery of a Maiasaura ("good mother lizard") nesting ground in Montana demonstrated that parental care continued long after birth among ornithopods. A specimen of the Mongolian oviraptorid Citipati osmolskae was discovered in a chicken-like brooding position in 1993, which may indicate that they had begun using an insulating layer of feathers to keep the eggs warm. A dinosaur embryo (pertaining to the prosauropod Massospondylus) was found without teeth, indicating that some parental care was required to feed the young dinosaurs. Trackways have also confirmed parental behavior among ornithopods from the Isle of Skye in northwestern Scotland.However, there is ample evidence of precociality or superprecociality among many dinosaur species, particularly theropods. For instance, non-ornithuromorph birds have been abundantly demonstrated to have had slow growth rates, megapode-like egg burying behavior and the ability to fly soon after birth. Both Tyrannosaurus rex and Troodon formosus display juveniles with clear superprecociality and likely occupying different ecological niches than the adults. Superprecociality has been inferred for sauropods.
Because both modern crocodilians and birds have four-chambered hearts (albeit modified in crocodilians), it is likely that this is a trait shared by all archosaurs, including all dinosaurs. While all modern birds have high metabolisms and are "warm-blooded" (endothermic), a vigorous debate has been ongoing since the 1960s regarding how far back in the dinosaur lineage this trait extends. Scientists disagree as to whether non-avian dinosaurs were endothermic, ectothermic, or some combination of both.After non-avian dinosaurs were discovered, paleontologists first posited that they were ectothermic. This supposed "cold-bloodedness" was used to imply that the ancient dinosaurs were relatively slow, sluggish organisms, even though many modern reptiles are fast and light-footed despite relying on external sources of heat to regulate their body temperature. The idea of dinosaurs as ectothermic remained a prevalent view until Robert T. "Bob" Bakker, an early proponent of dinosaur endothermy, published an influential paper on the topic in 1968.Modern evidence indicates that some non-avian dinosaurs thrived in cooler temperate climates and that some early species must have regulated their body temperature by internal biological means (aided by the animals' bulk in large species and feathers or other body coverings in smaller species). Evidence of endothermy in Mesozoic dinosaurs includes the discovery of polar dinosaurs in Australia and Antarctica as well as analysis of blood-vessel structures within fossil bones that are typical of endotherms. Scientific debate continues regarding the specific ways in which dinosaur temperature regulation evolved. In saurischian dinosaurs, higher metabolisms were supported by the evolution of the avian respiratory system, characterized by an extensive system of air sacs that extended the lungs and invaded many of the bones in the skeleton, making them hollow. Early avian-style respiratory systems with air sacs may have been capable of sustaining higher activity levels than those of mammals of similar size and build. In addition to providing a very efficient supply of oxygen, the rapid airflow would have been an effective cooling mechanism, which is essential for animals that are active but too large to get rid of all the excess heat through their skin.Like other reptiles, dinosaurs are primarily uricotelic, that is, their kidneys extract nitrogenous wastes from their bloodstream and excrete it as uric acid instead of urea or ammonia via the ureters into the intestine. In most living species, uric acid is excreted along with feces as a semisolid waste. However, at least some modern birds (such as hummingbirds) can be facultatively ammonotelic, excreting most of the nitrogenous wastes as ammonia. This material, as well as the output of the intestines, emerges from the cloaca. In addition, many species regurgitate pellets, and fossil pellets that may have come from dinosaurs are known from as long ago as the Cretaceous.
The possibility that dinosaurs were the ancestors of birds was first suggested in 1868 by Thomas Henry Huxley. After the work of Gerhard Heilmann in the early 20th century, the theory of birds as dinosaur descendants was abandoned in favor of the idea of their being descendants of generalized thecodonts, with the key piece of evidence being the supposed lack of clavicles in dinosaurs. However, as later discoveries showed, clavicles (or a single fused wishbone, which derived from separate clavicles) were not actually absent; they had been found as early as 1924 in Oviraptor, but misidentified as an interclavicle. In the 1970s, John Ostrom revived the dinosaur–bird theory, which gained momentum in the coming decades with the advent of cladistic analysis, and a great increase in the discovery of small theropods and early birds. Of particular note have been the fossils of the Yixian Formation, where a variety of theropods and early birds have been found, often with feathers of some type. Birds share over a hundred distinct anatomical features with theropod dinosaurs, which are now generally accepted to have been their closest ancient relatives. They are most closely allied with maniraptoran coelurosaurs. A minority of scientists, most notably Alan Feduccia and Larry Martin, have proposed other evolutionary paths, including revised versions of Heilmann's basal archosaur proposal, or that maniraptoran theropods are the ancestors of birds but themselves are not dinosaurs, only convergent with dinosaurs.
Feathers are one of the most recognizable characteristics of modern birds, and a trait that was shared by all other dinosaur groups. Based on the current distribution of fossil evidence, it appears that feathers were an ancestral dinosaurian trait, though one that may have been selectively lost in some species. Direct fossil evidence of feathers or feather-like structures has been discovered in a diverse array of species in many non-avian dinosaur groups, both among saurischians and ornithischians. Simple, branched, feather-like structures are known from heterodontosaurids, primitive neornithischians and theropods, and primitive ceratopsians. Evidence for true, vaned feathers similar to the flight feathers of modern birds has been found only in the theropod subgroup Maniraptora, which includes oviraptorosaurs, troodontids, dromaeosaurids, and birds. Feather-like structures known as pycnofibres have also been found in pterosaurs, suggesting the possibility that feather-like filaments may have been common in the bird lineage and evolved before the appearance of dinosaurs themselves. Research into the genetics of American alligators has also revealed that crocodylian scutes do possess feather-keratins during embryonic development, but these keratins are not expressed by the animals before hatching.Archaeopteryx was the first fossil found that revealed a potential connection between dinosaurs and birds. It is considered a transitional fossil, in that it displays features of both groups. Brought to light just two years after Charles Darwin's seminal On the Origin of Species (1859), its discovery spurred the nascent debate between proponents of evolutionary biology and creationism. This early bird is so dinosaur-like that, without a clear impression of feathers in the surrounding rock, at least one specimen was mistaken for Compsognathus. Since the 1990s, a number of additional feathered dinosaurs have been found, providing even stronger evidence of the close relationship between dinosaurs and modern birds. Most of these specimens were unearthed in the lagerstätte of the Yixian Formation, Liaoning, northeastern China, which was part of an island continent during the Cretaceous. Though feathers have been found in only a few locations, it is possible that non-avian dinosaurs elsewhere in the world were also feathered. The lack of widespread fossil evidence for feathered non-avian dinosaurs may be because delicate features like skin and feathers are not often preserved by fossilization and thus are absent from the fossil record.The description of feathered dinosaurs has not been without controversy; perhaps the most vocal critics have been Alan Feduccia and Theagarten Lingham-Soliar, who have proposed that some purported feather-like fossils are the result of the decomposition of collagenous fiber that underlaid the dinosaurs' skin, and that maniraptoran dinosaurs with vaned feathers were not actually dinosaurs, but convergent with dinosaurs. However, their views have for the most part not been accepted by other researchers, to the point that the scientific nature of Feduccia's proposals has been questioned.In 2016, it was reported that a dinosaur tail with feathers had been found enclosed in amber. The fossil is about 99 million years old.
Because feathers are often associated with birds, feathered dinosaurs are often touted as the missing link between birds and dinosaurs. However, the multiple skeletal features also shared by the two groups represent another important line of evidence for paleontologists. Areas of the skeleton with important similarities include the neck, pubis, wrist (semi-lunate carpal), arm and pectoral girdle, furcula (wishbone), and breast bone. Comparison of bird and dinosaur skeletons through cladistic analysis strengthens the case for the link.
Large meat-eating dinosaurs had a complex system of air sacs similar to those found in modern birds, according to a 2005 investigation led by Patrick M. O'Connor. The lungs of theropod dinosaurs (carnivores that walked on two legs and had bird-like feet) likely pumped air into hollow sacs in their skeletons, as is the case in birds. "What was once formally considered unique to birds was present in some form in the ancestors of birds", O'Connor said. In 2008, scientists described Aerosteon riocoloradensis, the skeleton of which supplies the strongest evidence to date of a dinosaur with a bird-like breathing system. CT scanning of Aerosteon's fossil bones revealed evidence for the existence of air sacs within the animal's body cavity.
Fossils of the troodonts Mei and Sinornithoides demonstrate that some dinosaurs slept with their heads tucked under their arms. This behavior, which may have helped to keep the head warm, is also characteristic of modern birds. Several deinonychosaur and oviraptorosaur specimens have also been found preserved on top of their nests, likely brooding in a bird-like manner. The ratio between egg volume and body mass of adults among these dinosaurs suggest that the eggs were primarily brooded by the male, and that the young were highly precocial, similar to many modern ground-dwelling birds.Some dinosaurs are known to have used gizzard stones like modern birds. These stones are swallowed by animals to aid digestion and break down food and hard fibers once they enter the stomach. When found in association with fossils, gizzard stones are called gastroliths.
The discovery that birds are a type of dinosaur showed that dinosaurs in general are not, in fact, extinct as is commonly stated. However, all non-avian dinosaurs, estimated to have been 628-1078 species, as well as many groups of birds did suddenly become extinct approximately 66 million years ago. It has been suggested that because small mammals, squamata and birds occupied the ecological niches suited for small body size, non-avian dinosaurs never evolved a diverse fauna of small-bodied species, which led to their downfall when large-bodied terrestrial tetrapods were hit by the mass extinction event. Many other groups of animals also became extinct at this time, including ammonites (nautilus-like mollusks), mosasaurs, plesiosaurs, pterosaurs, and many groups of mammals. Significantly, the insects suffered no discernible population loss, which left them available as food for other survivors. This mass extinction is known as the Cretaceous–Paleogene extinction event. The nature of the event that caused this mass extinction has been extensively studied since the 1970s; at present, several related theories are supported by paleontologists. Though the consensus is that an impact event was the primary cause of dinosaur extinction, some scientists cite other possible causes, or support the idea that a confluence of several factors was responsible for the sudden disappearance of dinosaurs from the fossil record.
The asteroid impact hypothesis, which was brought to wide attention in 1980 by Walter Alvarez and colleagues, links the extinction event at the end of the Cretaceous to a bolide impact approximately 66 million years ago. Alvarez et al. proposed that a sudden increase in iridium levels, recorded around the world in the period's rock stratum, was direct evidence of the impact. The bulk of the evidence now suggests that a bolide 5 to 15 kilometers (3.1 to 9.3 miles) wide hit in the vicinity of the Yucatán Peninsula (in southeastern Mexico), creating the approximately 180 km (110 mi) Chicxulub crater and triggering the mass extinction.Scientists are not certain whether dinosaurs were thriving or declining before the impact event. Some scientists propose that the meteorite impact caused a long and unnatural drop in Earth's atmospheric temperature, while others claim that it would have instead created an unusual heat wave. The consensus among scientists who support this hypothesis is that the impact caused extinctions both directly (by heat from the meteorite impact) and also indirectly (via a worldwide cooling brought about when matter ejected from the impact crater reflected thermal radiation from the sun). Although the speed of extinction cannot be deduced from the fossil record alone, various models suggest that the extinction was extremely rapid, being down to hours rather than years.In 2019, scientists drilling into the seafloor off Mexico extracted a unique geologic record of what they believe to be the day a city-sized asteroid smashed into the planet.
Before 2000, arguments that the Deccan Traps flood basalts caused the extinction were usually linked to the view that the extinction was gradual, as the flood basalt events were thought to have started around 68 million years ago and lasted for over 2 million years. However, there is evidence that two thirds of the Deccan Traps were created in only 1 million years about 66 million years ago, and so these eruptions would have caused a fairly rapid extinction, possibly over a period of thousands of years, but still longer than would be expected from a single impact event.The Deccan Traps in India could have caused extinction through several mechanisms, including the release into the air of dust and sulfuric aerosols, which might have blocked sunlight and thereby reduced photosynthesis in plants. In addition, Deccan Trap volcanism might have resulted in carbon dioxide emissions, which would have increased the greenhouse effect when the dust and aerosols cleared from the atmosphere. Before the mass extinction of the dinosaurs, the release of volcanic gases during the formation of the Deccan Traps "contributed to an apparently massive global warming. Some data point to an average rise in temperature of [8 °C (14 °F)] in the last half-million years before the impact [at Chicxulub]."In the years when the Deccan Traps hypothesis was linked to a slower extinction, Luis Alvarez (who died in 1988) replied that paleontologists were being misled by sparse data. While his assertion was not initially well-received, later intensive field studies of fossil beds lent weight to his claim. Eventually, most paleontologists began to accept the idea that the mass extinctions at the end of the Cretaceous were largely or at least partly due to a massive Earth impact. However, even Walter Alvarez has acknowledged that there were other major changes on Earth even before the impact, such as a drop in sea level and massive volcanic eruptions that produced the Indian Deccan Traps, and these may have contributed to the extinctions.
Non-avian dinosaur remains are occasionally found above the Cretaceous–Paleogene boundary. In 2000, paleontologist Spencer G. Lucas et al. reported the discovery of a single hadrosaur right femur in the San Juan Basin, New Mexico, and described it as evidence of Paleocene dinosaurs. The formation in which the bone was discovered has been dated to the early Paleocene epoch, approximately 64.5 million years ago. If the bone was not re-deposited into that stratum by weathering action, it would provide evidence that some dinosaur populations may have survived at least a half-million years into the Cenozoic. Other evidence includes the finding of dinosaur remains in the Hell Creek Formation up to 1.3 m (51 in) above the Cretaceous–Paleogene boundary, representing 40000 years of elapsed time. Similar reports have come from other parts of the world, including China. Many scientists, however, dismissed the supposed Paleocene dinosaurs as re-worked, that is, washed out of their original locations and then re-buried in much later sediments. Direct dating of the bones themselves has supported the later date, with uranium–lead dating methods resulting in a precise age of 64.8 ± 0.9 million years ago. If correct, the presence of a handful of dinosaurs in the early Paleocene would not change the underlying facts of the extinction.
Dinosaur fossils have been known for millennia, although their true nature was not recognized. The Chinese considered them to be dragon bones and documented them as such. For example, Huayang Guo Zhi (華陽國志), a gazetteer compiled by Chang Qu (常璩) during the Western Jin Dynasty (265–316), reported the discovery of dragon bones at Wucheng in Sichuan Province. Villagers in central China have long unearthed fossilized "dragon bones" for use in traditional medicines. In Europe, dinosaur fossils were generally believed to be the remains of giants and other biblical creatures.Scholarly descriptions of what would now be recognized as dinosaur bones first appeared in the late 17th century in England. Part of a bone, now known to have been the femur of a Megalosaurus, was recovered from a limestone quarry at Cornwell near Chipping Norton, Oxfordshire, in 1676. The fragment was sent to Robert Plot, Professor of Chemistry at the University of Oxford and first curator of the Ashmolean Museum, who published a description in his The Natural History of Oxford-shire (1677). He correctly identified the bone as the lower extremity of the femur of a large animal, and recognized that it was too large to belong to any known species. He, therefore, concluded it to be the femur of a huge human, perhaps a Titan or another type of giant featured in legends. Edward Lhuyd, a friend of Sir Isaac Newton, published Lithophylacii Britannici ichnographia (1699), the first scientific treatment of what would now be recognized as a dinosaur when he described and named a sauropod tooth, Rutellum impicatum, that had been found in Caswell, near Witney, Oxfordshire. Between 1815 and 1824, the Rev William Buckland, the first Reader of Geology at the University of Oxford, collected more fossilized bones of Megalosaurus and became the first person to describe a dinosaur in a scientific journal. The second dinosaur genus to be identified, Iguanodon, was discovered in 1822 by Mary Ann Mantell – the wife of English geologist Gideon Mantell. Gideon Mantell recognized similarities between his fossils and the bones of modern iguanas. He published his findings in 1825.The study of these "great fossil lizards" soon became of great interest to European and American scientists, and in 1842 the English paleontologist Richard Owen coined the term "dinosaur". He recognized that the remains that had been found so far, Iguanodon, Megalosaurus and Hylaeosaurus, shared a number of distinctive features, and so decided to present them as a distinct taxonomic group. With the backing of Prince Albert, the husband of Queen Victoria, Owen established the Natural History Museum, London, to display the national collection of dinosaur fossils and other biological and geological exhibits.In 1858, William Parker Foulke discovered the first known American dinosaur, in marl pits in the small town of Haddonfield, New Jersey. (Although fossils had been found before, their nature had not been correctly discerned.) The creature was named Hadrosaurus foulkii. It was an extremely important find: Hadrosaurus was one of the first nearly complete dinosaur skeletons found (the first was in 1834, in Maidstone, England), and it was clearly a bipedal creature. This was a revolutionary discovery as, until that point, most scientists had believed dinosaurs walked on four feet, like other lizards. Foulke's discoveries sparked a wave of interests in dinosaurs in the United States, known as dinosaur mania. Dinosaur mania was exemplified by the fierce rivalry between Edward Drinker Cope and Othniel Charles Marsh, both of whom raced to be the first to find new dinosaurs in what came to be known as the Bone Wars. The feud probably originated when Marsh publicly pointed out that Cope's reconstruction of an Elasmosaurus skeleton was flawed: Cope had inadvertently placed the plesiosaur's head at what should have been the animal's tail end. The fight between the two scientists lasted for over 30 years, ending in 1897 when Cope died after spending his entire fortune on the dinosaur hunt. Unfortunately, many valuable dinosaur specimens were damaged or destroyed due to the pair's rough methods: for example, their diggers often used dynamite to unearth bones. Modern paleontologists would find such methods crude and unacceptable, since blasting easily destroys fossil and stratigraphic evidence. Despite their unrefined methods, the contributions of Cope and Marsh to paleontology were vast: Marsh unearthed 86 new species of dinosaur and Cope discovered 56, a total of 142 new species. Cope's collection is now at the American Museum of Natural History, while Marsh's is on display at the Peabody Museum of Natural History at Yale University.After 1897, the search for dinosaur fossils extended to every continent, including Antarctica. The first Antarctic dinosaur to be discovered, the ankylosaurid Antarctopelta oliveroi, was found on James Ross Island in 1986, although it was 1994 before an Antarctic species, the theropod Cryolophosaurus ellioti, was formally named and described in a scientific journal.Current dinosaur "hot spots" include southern South America (especially Argentina) and China. China, in particular, has produced many exceptional feathered dinosaur specimens due to the unique geology of its dinosaur beds, as well as an ancient arid climate particularly conducive to fossilization.
The field of dinosaur research has enjoyed a surge in activity that began in the 1970s and is ongoing. This was triggered, in part, by John Ostrom's discovery of Deinonychus, an active predator that may have been warm-blooded, in marked contrast to the then-prevailing image of dinosaurs as sluggish and cold-blooded. Vertebrate paleontology has become a global science. Major new dinosaur discoveries have been made by paleontologists working in previously unexploited regions, including India, South America, Madagascar, Antarctica, and most significantly China (the well-preserved feathered dinosaurs in China have further consolidated the link between dinosaurs and their living descendants, modern birds). The widespread application of cladistics, which rigorously analyzes the relationships between biological organisms, has also proved tremendously useful in classifying dinosaurs. Cladistic analysis, among other modern techniques, helps to compensate for an often incomplete and fragmentary fossil record.
One of the best examples of soft-tissue impressions in a fossil dinosaur was discovered in the Pietraroia Plattenkalk in southern Italy. The discovery was reported in 1998, and described the specimen of a small, very young coelurosaur, Scipionyx samniticus. The fossil includes portions of the intestines, colon, liver, muscles, and windpipe of this immature dinosaur.In the March 2005 issue of Science, the paleontologist Mary Higby Schweitzer and her team announced the discovery of flexible material resembling actual soft tissue inside a 68-million-year-old Tyrannosaurus rex leg bone from the Hell Creek Formation in Montana. After recovery, the tissue was rehydrated by the science team. When the fossilized bone was treated over several weeks to remove mineral content from the fossilized bone-marrow cavity (a process called demineralization), Schweitzer found evidence of intact structures such as blood vessels, bone matrix, and connective tissue (bone fibers). Scrutiny under the microscope further revealed that the putative dinosaur soft tissue had retained fine structures (microstructures) even at the cellular level. The exact nature and composition of this material, and the implications of Schweitzer's discovery, are not yet clear.In 2009, a team including Schweitzer announced that, using even more careful methodology, they had duplicated their results by finding similar soft tissue in a duck-billed dinosaur, Brachylophosaurus canadensis, found in the Judith River Formation of Montana. This included even more detailed tissue, down to preserved bone cells that seem even to have visible remnants of nuclei and what seem to be red blood cells. Among other materials found in the bone was collagen, as in the Tyrannosaurus bone. The type of collagen an animal has in its bones varies according to its DNA and, in both cases, this collagen was of the same type found in modern chickens and ostriches.The extraction of ancient DNA from dinosaur fossils has been reported on two separate occasions; upon further inspection and peer review, however, neither of these reports could be confirmed. However, a functional peptide involved in the vision of a theoretical dinosaur has been inferred using analytical phylogenetic reconstruction methods on gene sequences of related modern species such as reptiles and birds. In addition, several proteins, including hemoglobin, have putatively been detected in dinosaur fossils.In 2015, researchers reported finding structures similar to blood cells and collagen fibers, preserved in the bone fossils of six Cretaceous dinosaur specimens, which are approximately 75 million years old.
By human standards, dinosaurs were creatures of fantastic appearance and often enormous size. As such, they have captured the popular imagination and become an enduring part of human culture. The entry of the word "dinosaur" into the common vernacular reflects the animals' cultural importance: in English, "dinosaur" is commonly used to describe anything that is impractically large, obsolete, or bound for extinction.Public enthusiasm for dinosaurs first developed in Victorian England, where in 1854, three decades after the first scientific descriptions of dinosaur remains, a menagerie of lifelike dinosaur sculptures was unveiled in London's Crystal Palace Park. The Crystal Palace dinosaurs proved so popular that a strong market in smaller replicas soon developed. In subsequent decades, dinosaur exhibits opened at parks and museums around the world, ensuring that successive generations would be introduced to the animals in an immersive and exciting way. Dinosaurs' enduring popularity, in its turn, has resulted in significant public funding for dinosaur science, and has frequently spurred new discoveries. In the United States, for example, the competition between museums for public attention led directly to the Bone Wars of the 1880s and 1890s, during which a pair of feuding paleontologists made enormous scientific contributions.The popular preoccupation with dinosaurs has ensured their appearance in literature, film, and other media. Beginning in 1852 with a passing mention in Charles Dickens' Bleak House, dinosaurs have been featured in large numbers of fictional works. Jules Verne's 1864 novel Journey to the Center of the Earth, Sir Arthur Conan Doyle's 1912 book The Lost World, the iconic 1933 film King Kong, the 1954 Godzilla and its many sequels, the best-selling 1990 novel Jurassic Park by Michael Crichton and its 1993 film adaptation are just a few notable examples of dinosaur appearances in fiction. Authors of general-interest non-fiction works about dinosaurs, including some prominent paleontologists, who have often sought to use the animals as a way to educate readers about science in general. Dinosaurs are ubiquitous in advertising; numerous companies have referenced dinosaurs in printed or televised advertisements, either in order to sell their own products or in order to characterize their rivals as slow-moving, dim-witted, or obsolete.
General "DinoDatabase.com" – Hundreds of dinosaurs and dinosaur related topics.Images "The Science and Art of Gregory S. Paul" – Influential paleontologist's anatomy art and paintings. "Scott Hartman's Skeletal Drawing.com" – Professional restorations of numerous dinosaurs, and discussions of dinosaur anatomy. "Dinosaur Discovery - Early Published Images" – A collection of images from early works on dinosaurs at the Linda Hall Library, in support of the exhibition, Paper Dinosaurs, 1824–1969.Video BBC Nature: Prehistoric Life: Dinosaurs – Reconstructions and expert interpretations, including Walking with Dinosaurs. (Archived website last updated by the BBC in October 2014.) BBC Explainer: Dinosaurs – Animation. A complete history in 4 minutes. "The origin, evolution, and extinction of the dinosaurs" – A Stephen L. Brusatte video lecture, April 15, 2014.Popular "The Dino Directory" – A well-illustrated dinosaur directory from the Natural History Museum in London. Dinosaurnews – Dinosaur-related headlines from around the world, including finds and discoveries, and many links. "The Dinosauria" – From University of California Museum of Paleontology. "Zoom Dinosaurs" – From Enchanted Learning. Kids' site, info pages and stats, theories, history. Dinosaur genus list contains data tables on nearly every published Mesozoic dinosaur genus as of January 2011.Technical Palaeontologia Electronica From Coquina Press. Online technical journal.
Phoenician sailors visiting the coast of Spain c. 12th century BC, mistaking the European rabbit for a species from their homeland (the rock hyrax Procavia capensis), gave it the name i-shepan-ham (land or island of hyraxes). The captivity of rabbits as a food source is recorded as early as the 1st century BC, when the Roman writer Pliny the Elder described the use of rabbit hutches, along with enclosures called leporaria. A controversial theory is that a corruption of the rabbit's name used by the Romans became the Latin name for the peninsula, Hispania. In Rome, rabbits were raised in large walled colonies with walls extended underground. According to Pliny, the consumption of unborn and newborn rabbits, called laurices, was considered a delicacy. Evidence for the domestic rabbit is rather late. In the Middle Ages, wild rabbits were often kept for the hunt. Monks in southern France were crossbreeding rabbits at least by the 12th century AD. Domestication was probably a slow process that took place from the Roman period (or earlier) until the 1500s.In the 19th century, as animal fancy in general began to emerge, rabbit fanciers began to sponsor rabbit exhibitions and fairs in Western Europe and the United States. Breeds of various domesticated animals were created and modified for the added purpose of exhibition, a departure from the breeds that had been created solely for food, fur, or wool. The rabbit's emergence as a household pet began during the Victorian era.The keeping of the rabbit as a pet commencing from the 1800s coincides with the first observable skeletal differences between the wild and domestic populations, even though captive rabbits had been exploited for over 2,000 years. Domestic rabbits have been popular in the United States since the late 19th century. What became known as the "Belgian Hare Boom" began with the importation of the first Belgian Hares from England in 1888 and, soon after, the founding of the American Belgian Hare Association, the first rabbit club in America. From 1898 to 1901, many thousands of Belgian Hares were imported to America. Today, the Belgian Hare is one of the rarest breeds, with only 132 specimens found in the United States in a 2015 census. The American Rabbit Breeders Association (ARBA) was founded in 1910 and is the national authority on rabbit raising and rabbit breeds having a uniform Standard of Perfection, registration and judging system. The domestic rabbit continues to be popular as a show animal and pet. Many thousand rabbit shows occur each year and are sanctioned in Canada and the United States by the ARBA. Today, the domesticated rabbit is the third most popular mammalian pet in Britain after dogs and cats.
Rabbits have been, and continue to be, used in laboratory work such as the production of antibodies for vaccines and research of human male reproductive system toxicology. The Environmental Health Perspective, published by the National Institute of Health, states, "The rabbit [is] an extremely valuable model for studying the effects of chemicals or other stimuli on the male reproductive system." According to the Humane Society of the United States, rabbits are also used extensively in the study of bronchial asthma, stroke prevention treatments, cystic fibrosis, diabetes, and cancer. Animal rights activists have opposed animal experimentation for non-medical purposes, such as the testing of cosmetic and cleaning products, which has resulted in decreased use of rabbits in these areas.
As a refinement of the diet of the wild rabbit, the diet of the domestic rabbit is often a function of its purpose. Show rabbits are fed for vibrant health, strong musculoskeletal systems, and—like rabbits intended for the fur trade—optimal coat production and condition. Rabbits intended for the meat trade are fed for swift and efficient production of flesh, while rabbits in research settings have closely controlled diets for specific goals. Nutritional needs of the domestic rabbit may also be focused on developing a physique that allows for the safe delivery of larger litters of healthy kits. Optimizing costs and producing feces that meet local waste regulations may also be factors. The diet of a pet rabbit, too, is geared toward its purpose—as a healthy and long-lived companion. Hay is an essential part of the diet of all rabbits and it is a major component of the commercial food pellets that are formulated for domestic rabbits and available in many areas. Pellets are typically fed to adult rabbits in limited quantities once or twice a day, to mimic their natural behavior and to prevent obesity. It is recommended only a teaspoon to an egg cup full of pellets is fed to adult rabbits each day. Most rabbit pellets are alfalfa-based for protein and fiber, with other grains completing the carbohydrate requirements. "Muesli" style rabbit foods are also available; these contain separate components—e.g., dried carrot, pea flakes and hay pellets as opposed to a uniform pellet. These are not recommended as rabbits will choose favored parts and leave the rest. Muesli style feeds are often lower in fiber than pelleted versions of rabbit food. Additionally numerous studies have found they increase the risk of obesity and dental disease. Minerals and vitamins are added during production of rabbit pellets to meet the nutritional requirements of the domestic rabbit. Along with pellets, many commercial rabbit raisers also feed one or more types of loose hay, for its freshness and important cellulose components. Alfalfa in particular is recommended for the growth needs of young rabbits.
Rabbits are hindgut fermenters and therefore have an enlarged cecum. This allows a rabbit to digest, via fermentation, what it otherwise would not be able to metabolically process. After a rabbit ingests food, the food travels down the esophagus and through a small valve called the cardia. In rabbits, this valve is very well pronounced and makes the rabbit incapable of vomiting. The food enters the stomach after passing through the cardia. Food then moves to the stomach and small intestine, where a majority of nutrient extraction and absorption takes place. Food then passes into the colon and eventually into the cecum. Peristaltic muscle contractions (waves of motion) help to separate fibrous and non-fibrous particles. The non-fibrous particles are then moved backwards up the colon, through the illeo-cecal valve, and into the cecum. Symbiotic bacteria in the cecum help to further digest the non-fibrous particles into a more metabolically manageable substance. After as little as three hours, a soft, fecal "pellet," called a cecotrope, is expelled from the rabbit's anus. The rabbit instinctively eats these grape-like pellets, without chewing, in exchange keeping the mucous coating intact. This coating protects the vitamin- and nutrient-rich bacteria from stomach acid, until it reaches the small intestine, where the nutrients from the cecotrope can be absorbed.The soft pellets contain a sufficiently large portion of nutrients that are critical to the rabbit's health. This soft fecal matter is rich in vitamin B and other nutrients. The process of coprophagy is important to the stability of a rabbit's digestive health because it is one important way that which a rabbit receives vitamin B in a form that is useful to its digestive wellness. Occasionally, the rabbit may leave these pellets lying about its cage; this behavior is harmless and usually related to an ample food supply. When caecal pellets are wet and runny (semi-liquid) and stick to the rabbit and surrounding objects, they are called intermittent soft cecotropes (ISCs). This is different from ordinary diarrhea and is usually caused by a diet too high in carbohydrates or too low in fiber. Soft fruit or salad items such as lettuce, cucumbers and tomatoes are possible causes.
Disease is rare when rabbits are raised in sanitary conditions and provided with adequate care. Rabbits have fragile bones, especially in their spines, and need support on the belly or bottom when they are picked up. Spayed or neutered rabbits kept indoors with proper care may have a lifespan of 8 to 12 years, with mixed-breed rabbits typically living longer than purebred specimens, and dwarf breeds having longer average lifespans than larger breeds. The world record for longest-lived rabbit is 18 years.Rabbits will gnaw on almost anything, including electrical cords (possibly leading to electrocution), potentially poisonous plants, and material like carpet and fabric that may cause life-threatening intestinal blockages, so areas to which they have access need to be pet-proofed.
Rabbit fancier organizations and veterinarians recommend that pet rabbits be made infertile by spaying or neutering by a rabbit-experienced veterinarian. Health advantages of surgically altering a rabbit include increased longevity and (for females) a reduced risk of ovarian and uterine cancers or of endometritis. For both rabbit sexes, spaying or neutering reduces aggression toward other rabbits, as well as territorial marking (especially in males). Rabbits are at high risk for complications from anesthesia and infection of the surgical site is another top concern. Since un-altered animals are not as likely to form agreeable social bonds, spaying and neutering promotes less stressful interactions.
In most jurisdictions, including the United States (except where required by local animal control ordinances), rabbits do not require vaccination. Vaccinations exist for both rabbit hemorrhagic disease and myxomatosis. These vaccinations are usually given annually, two weeks apart. If there is an outbreak of myxomatosis locally, this vaccine can be administered every six months for extra protection. Myxomatosis immunizations are not available in all countries, including Australia, due to fears that immunity will pass on to feral rabbits. However, they are recommended by some veterinarians as prophylactics, where they are legally available. In the UK a combined vaccination exists for myxomatosis and VHD1 made by Nobivac called Myxo-RHD, this is given yearly. Due to increasing cases of VHD2 it is now recommended rabbits receive an additional vaccination for RHD2 one brand for this is filovac, the vaccination is given yearly 2 weeks apart from other vaccinations, it may be given 6 monthly at rabbit believed to be at higher risk.
A rabbit cannot be declawed. Lacking pads on the bottoms of its feet, a rabbit requires its claws for traction. Removing its claws would render it unable to stand.
Coping with stress is a key aspect of rabbit behavior, and this can be traced to part of the brain known as ventral tegmental area (VTA). Dopaminergic neurons in this part of the brain release the hormone dopamine. In rabbits, it is released as part of a coping mechanism while in a heightened state of fear or stress, and has a calming effect. Dopamine has also been found in the rabbit's medial prefrontal cortex, the nucleus accumbens, and the amygdala. Physiological and behavioral responses to human-induced tonic immobility (TI, sometimes termed "trancing" or "playing dead") have been found to be indicative of a fear-motivated stress state, confirming that the promotion of TI to try to increase a bond between rabbits and their owners—thinking the rabbits enjoy it—is misplaced. However, some researchers conclude that inducing TI in rabbits is appropriate for certain procedures, as it holds less risk than anesthesia.
The formation of open sores on the rabbit's hocks, commonly called sore hocks, is a problem that commonly afflicts mostly heavy-weight rabbits kept in cages with wire flooring or soiled solid flooring. The problem is most prevalent in rex-furred rabbits and heavy-weight rabbits (over 9 pounds (4.1 kg)), as well as those with thin foot bristles. The condition results when, over the course of time, the protective bristle-like fur on the rabbit's hocks thins down. Standing urine or other unsanitary cage conditions can exacerbate the problem by irritating the sensitive skin. The exposed skin in turn can result in tender areas or, in severe cases, open sores, which may then become infected and abscessed if not properly cared for.
Gastrointestinal stasis (GI stasis) is a serious and potentially fatal condition that occurs in some rabbits in which gut motility is severely reduced and possibly completely stopped. When untreated or improperly treated, GI stasis can be fatal in as little as 24 hours. GI stasis is the condition of food not moving through the gut as quickly as normal. The gut contents may dehydrate and compact into a hard, immobile mass (impacted gut), blocking the digestive tract of the rabbit. Food in an immobile gut may also ferment, causing significant gas buildup and resultant gas pain for the rabbit. The first noticeable symptom of GI stasis may be that the rabbit suddenly stops eating. Treatment frequently includes intravenous or subcutaneous fluid therapy (rehydration through injection of a balanced electrolyte solution), pain control, possible careful massage to promote gas expulsion and comfort, drugs to promote gut motility, and careful monitoring of all inputs and outputs. The rabbit's diet may also be changed as part of treatment, to include force-feeding to ensure adequate nutrition. Surgery to remove the blockage is not generally recommended and comes with a poor prognosis.Some rabbits are more prone to GI stasis than others. The causes of GI stasis are not completely understood, but common contributing factors are thought to include stress, reduced food intake, low fiber in the diet, dehydration, reduction in exercise or blockage caused by excess fur or carpet ingestion. Stress factors can include changes in housing, transportation, or medical procedures under anesthesia. As many of these factors may occur together (poor dental structure leading to decreased food intake, followed by a stressful veterinary dental procedure to correct the dental problem) establishing a root cause may be difficult.GI stasis is sometimes misdiagnosed as "hair balls" by veterinarians or rabbit keepers not familiar with the condition. While fur is commonly found in the stomach following a fatal case of GI stasis, it is also found in healthy rabbits. Molting and chewing fur can be a predisposing factor in the occurrence of GI stasis, however, the primary cause is the change in motility of the gut.
Dental disease has several causes, namely genetics, inappropriate diet, injury to the jaw, infection, or cancer. Malocclusion: Rabbit teeth are open-rooted and continue to grow throughout their lives. In some rabbits, the teeth are not properly aligned, a condition called malocclusion. Because of the misaligned nature of the rabbit's teeth, there is no normal wear to control the length to which the teeth grow. There are three main causes of malocclusion, most commonly genetic predisposition, injury, or bacterial infection. In the case of congenital malocclusion, treatment usually involves veterinary visits in which the teeth are treated with a dental burr (a procedure called crown reduction or, more commonly, teeth clipping) or, in some cases, permanently removed. In cases of simple malocclusion, a block of wood for the rabbit to chew on can rectify this problem. Molar spurs: These are spurs that can dig into the rabbit's tongue and/or cheek causing pain. These should be filed down by an experienced exotic veterinarian specialised in rabbit care, using a dental burr, for example. Osteoporosis: Rabbits, especially neutered females and those that are kept indoors without adequate natural sunlight, can suffer from osteoporosis, in which holes appear in the skull by X-Ray imaging. This reflects the general thinning of the bone, and teeth will start to become looser in the sockets, making it uncomfortable and painful for the animal to chew hay. The inability to properly chew hay can result in molar spurs, as described above, and weight loss, leading into a downward spiral if not treated promptly. This can be reversible and treatable. A veterinary formulated liquid calcium supplement with vitamin D3 and magnesium can be given mixed with the rabbit's drinking water, once or twice per week, according to the veterinarian's instructions. The molar spurs should also be trimmed down by an experienced exotic veterinarian specialised in rabbit care, once per 1–2 months depending on the case.Signs of dental difficulty include difficulty eating, weight loss and small stools and visibly overgrown teeth. However, there are many other causes of ptyalism, including pain due to other causes.
An over-diagnosed ailment amongst rabbits is respiratory infection, known colloquially as "snuffles". Pasteurella, a bacterium, is usually misdiagnosed and this is known to be a factor in the overuse of antibiotics among rabbits. A runny nose, for instance, can have several causes, among those being high temperature or humidity, extreme stress, environmental pollution (like perfume or incense), or a sinus infection. Options for treating this is removing the pollutant, lowering or raising the temperature accordingly, and medical treatment for sinus infections. Pasteurella does live naturally in a rabbit's respiratory tract, and it can flourish out of control in some cases. In the rare event that happens, antibiotic treatment is necessary. Sneezing can be a sign of environmental pollution (such as too much dust) or a food allergy. Runny eyes and other conjunctival problems can be caused by dental disease or a blockage of the tear duct. Environmental pollution, corneal disease, entropion, distichiasis, or inflammation of the eyes are also causes. This is easy to diagnose as well as treat.
Rabbits are subject to infection by a variety of viruses. Some have had deadly and widespread impact.
Myxomatosis is a virulent threat to all rabbits but not to humans. The intentional introduction of myxomatosis in rabbit-ravaged Australia killed an estimated 500 million feral rabbits between 1950 and 1952. The Australian government will not allow veterinarians to purchase and use the myxomatosis vaccine that would protect domestic rabbits, for fear that this immunity would be spread into the wild via escaped livestock and pets. This potential consequence is also one motivation for the pet-rabbit ban in Queensland.In Australia, rabbits caged outdoors in areas with high numbers of mosquitoes are vulnerable to myxomatosis. In Europe, fleas are the carriers of myxomatosis. In some countries, annual vaccinations against myxomatosis are available.
Rabbit hemorrhagic disease (RHD), also known as viral hemorrhagic disease (VHD) or rabbit calicivirus disease (RCD), is caused by a rabbit-specific calicivirus known as RHDV or RCV. Discovered in 1983, RHD is highly infectious and usually fatal. Initial signs of the disease may be limited to fever and lethargy, until significant internal organ damage results in labored breathing, squealing, bloody mucus, and eventual coma and death. Internally, the infection causes necrosis of the liver and damages other organs, especially the spleen, kidneys, and small intestine. RHD, like myxomatosis, has been intentionally introduced to control feral rabbit populations in Australia and (illegally) in New Zealand, and RHD has, in some areas, escaped quarantine. The disease has killed tens of millions of rabbits in China (unintentionally) as well as Australia, with other epidemics reported in Bolivia, Mexico, South Korea, and continental Europe. Rabbit populations in New Zealand have bounced back after developing a genetic immunity to RHD, and the disease has, so far, had no effect on the genetically divergent native wild rabbits and hares in the Americas. In the United States, an October 2013 USDA document stated:RHD has been found in the United States as recently as 2010, and was detected in Canada in 2011. Thus far, outbreaks have been controlled quickly through quarantine, depopulation, disease tracing, and cleaning and disinfection; however, rabbit losses have been in the thousands. An RHD vaccine exists, but it is not recommended for use where the disease is not widespread in wildlife, as it may hide signs of disease and is not considered a practical response for such a rapidly spreading disease. In the UK, reports of RHD (as recently as February 2018) have been submitted to the British Rabbit Council's online "Notice Board". Vaccines for RHD are available—and mandatory—in the UK.
West Nile virus is another threat to domestic as well as wild rabbits. It is a fatal disease, and while vaccines are available for other species, there are none yet specifically indicated for rabbits.
Wry neck (or head tilt) is a condition in rabbits that can be fatal, due to the resulting disorientation that causes the animal to stop eating and drinking. Inner ear infections or ear mites, as well as diseases or injuries affecting the brain (including stroke) can lead to wry neck. The most common cause, however, is a parasitic microscopic fungus called Encephalitozoon cuniculi (E. cuniculi). Note that: "despite approximately half of all pet rabbits carrying the infection, only a small proportion of these cases ever show any illness". Some vets now recommend treating rabbits against E. cuniculi. The usual drugs for treatment and prevention are the benzimidazole anthelmintics, particularly fenbendazole (also used as a deworming agent in other animal species). In the UK, fenbendazole (under the brand name Panacur Rabbit), is sold over-the-counter in oral paste form as a nine-day treatment. Fenbendazole is particularly recommended for rabbits kept in colonies and as a preventive before mixing new rabbits with each other.
Fly strike, or blowfly strike, (Lucilia sericata) is a condition that occurs when flies (particularly botflies) lay their eggs in a rabbit's damp or soiled fur, or in an open wound. Within 12 hours, the eggs hatch into the larval stage of the fly, known as maggots. Initially small but quickly growing to 15 millimetres (0.59 in) long, maggots can burrow into skin and feed on an animal's tissue, leading to shock and death. The most susceptible rabbits are those in unsanitary conditions, sedentary ones, and those unable to clean their excretory areas. Rabbits with diarrhea should be inspected for fly strike, especially during the summer months. The topical treatment Rearguard® (from Novartis) is approved in the United Kingdom for 10-week-per-application prevention of fly strike.
As of 2017, there were at least 305 breeds of domestic rabbit in 70 countries around the world. The American Rabbit Breeders Association currently recognizes 49 rabbit breeds and the British Rabbit Council recognizes 106. Selective breeding has produced rabbits ranging in size from dwarf to giant. Across the world, rabbits are raised as livestock (in cuniculture) for their meat, pelts, and wool, and also by fanciers and hobbyists as pets. Rabbits have been selectively bred since ancient times to achieve certain desired characteristics. Variations include size and body shape, coat type (including hair length and texture), coat color, ear carriage (erect or lop), and even ear length. As with any animal, domesticated rabbits' temperaments vary in such factors as energy level and novelty seeking. Most genetic defects in the domestic rabbit (such as dental problems in the Holland Lop breed) are due to recessive genes. Genetics are carefully tracked by fanciers who show rabbits, to breed out defects.
Rabbits have been kept as pets in Western nations since the 19th century, but because of the destructive history of feral rabbits in Australia, domestic rabbits are illegal as pets in Queensland. Depending upon its size, a rabbit may be considered a type of pocket pet. Rabbits can bond with humans, can learn to follow simple voice commands and to come when called, and are curious and playful. Rabbits do not make good pets for small children because rabbits are fragile and easily injured by rough handling, can bite when hurt or frightened, and are easily frightened by loud noises and sudden motions. With the right guidance, rabbits can be trained to live indoors perfectly.Rabbits are especially popular as pets in the United States during the Easter season, due to their association with the holiday. However, animal shelters that accept rabbits often complain that during the weeks and months following Easter, there is a rise in unwanted and neglected rabbits that were bought as Easter gifts, especially for children. Similar problems arise in rural areas after county fairs and the like, in jurisdictions where rabbits are legal prizes in fairground games. Thus, there are many humane societies, animal shelters, and rescue groups that have rabbits available for pet adoption. Fancy rabbit breeds are often purchased from pet stores, private breeders, and fanciers.
Rabbits may be kept as small house pets and "rabbit-proofed" spaces reduce the risks associated with their intrinsic need to chew. Rabbits are easily litter box trained and a rabbit that lives indoors may be less exposed to the dangers of predators, parasites, diseases, adverse weather, and pesticides, which in turn increases their lifespan. Rabbits are often compatible with others of their kind, or with birds or guinea pigs, but opinion differs regarding the dangers of housing different species together. For example, while rabbits can synthesize their own Vitamin C, guinea pigs cannot, so the two species should not be fed the same diet. Also, most rabbits tend to be stronger than guinea pigs, so this may cause deliberate or inadvertent injury. Some people consider rabbits a pocket pet even though they are rather large. Keeping a rabbit as a house companion was popularised by Sandy Crook in her 1981 book Your French Lop. In 1983, at the American Family Pet Show in Anaheim, California (attended by 35,000), Crook presented her personal experiences living with an indoor rabbit as evidence of a human-rabbit bond. In the late 1980s, it became more common to litter box train a rabbit and keep it indoors, after the publication of Marinell Harriman's House Rabbit Handbook: How to Live with an Urban Rabbit in 1985. (The book's fifth edition was published in 2013.)As the domestic descendants of wild prey animals, rabbits are alert, timid creatures that startle fairly easily, and many of their behaviors are triggered by the fight-or-flight response to perceived threats. According to the House Rabbit Society, the owner of a pet rabbit can use various behavioral approaches to gain the animal's trust and reduce aggression, though this can be a long and difficult process.In addition, there is evidence to suggest that young rabbits that occupy the periphery of the "litter huddle" obtain less milk from the mother and, as a result, have a lower weight. It has been suggested that this factor may contribute to behavioural differences in litter mates during adolescence.
Not all veterinarians will treat rabbits, and pet owners may have to seek out an Exotic Animal Veterinarian for their rabbit's care. Rabbits need regular checkups at the veterinarian because they may hide signs of illness or disease. Additionally, rabbits need regular maintenance in the form of being able to chew on something and having their nails trimmed regularly.
The advantages of keeping rabbits as pets is that they are clean, smart, cute, soft and have a low carbon footprint. They may or may not react favorably to handling and petting depending on their personality and how they were raised. There are also many different sizes and characteristics available, owing to a long history of breeding. Rabbits are friendly to each other and are often compatible with other pets. Rabbits are herbivores and their diet is relatively simple. Compared to other small animals kept as pets, rabbits are physically robust creatures with strong hind legs that enable them to run fast, and they have powerful teeth. Rabbits should never be picked up by the ears or the "scruff" on the back of their neck because "their skeletons are light compared to their bodies, and they susceptible to trauma from falling, twisting, and kicking". Rabbits breed rapidly and so it is often easy, and affordable, to find one to buy or adopt. Some disadvantages of keeping rabbits as pets is that they may chew many things in the house. Unneutered male rabbits may spray their territory with a strong-smelling urine, unspayed female urine is also pungent, and so the litter box may smell. Rabbits can bite and scratch, and may do so to communicate displeasure, or if ignored; it is a part of normal communication and cannot be stopped entirely. They have to be picked up and handled properly to avoid injury to the rabbit or the owner. They may leave faeces around the house and are not always that conscious of leaving their droppings in the litter box. Rabbits can potentially be aggressive and territorial. Some rabbits may also be unfriendly, and then would be unsuitable as pets for children. Rabbits have a different body language to the most common domestic pets: cats and dogs. If someone wants a rabbit and is only familiar with those pet animals, then they would have to learn a lot about caring for this species and the behaviour of rabbits. They are often compared to guinea pigs but they may be as similar, in care and behaviour, to guinea pigs as they are to cats. Like cats, they are smart and can be litterbox trained. They also use their teeth and claws as weapons of defense and they can jump like a cat. They are quiet like a cat and independent, but they are also quite curious. Another animal they might be compared to is a chinchilla.
Rabbits have been kept as livestock since ancient times for their meat, wool, and fur. In modern times, rabbits are also utilized in scientific research as laboratory animals.
Breeds such as the New Zealand and Californian are frequently utilized for meat in commercial rabbitries. These breeds have efficient metabolisms and grow quickly; they are ready for slaughter by approximately 14 to 16 weeks of age. Rabbit fryers are rabbits that are between 70 and 90 days of age, and weighing between 3 and 5 lb (1 to 2 kg) live weight. Rabbit roasters are rabbits from 90 days to 6 months of age weighing between 5 and 8 lb (2 to 3.5 kg) live weight. Rabbit stewers are rabbits from 6 months on weighing over 8 lb. Any type of rabbit can be slaughtered for meat, but those exhibiting the "commercial" body type are most commonly raised for meat purposes. Dark fryers (any other color but albino whites) are sometimes lower in price than albino fryers because of the slightly darker tinge of the fryer (purely pink carcasses are preferred by consumers) and because the dark hairs are easier to see than if there are residual white hairs on the carcass. There is no difference in skinability.
Rabbits such as the Angora, American Fuzzy Lop, and Jersey Wooly produce wool. However, since the American Fuzzy Lop and Jersey Wooly are both dwarf breeds, only the much larger Angora breeds such as the English Angora, Satin Angora, Giant Angora, and French Angoras are used for commercial wool production. Their long fur is sheared, combed, or plucked (gently pulling loose hairs from the body during molting) and then spun into yarn used to make a variety of products. Angora sweaters can be purchased in many clothing stores and is generally mixed with other types of wool. Rabbit wool, called Angora, is 2.5 times warmer than sheep's wool.
Rabbit breeds that were developed for their fur qualities include the Rex with its plush texture, the Satin with its lustrous color, and the Chinchilla for its exotic pattern. White rabbit fur may be dyed in an array of colors that aren't produced naturally. Rabbits in the fur industry are fed a diet focused for robust coat production and pelts are harvested after the rabbit reaches prime condition, which takes longer than in the meat industry. Rabbit fur is used in local and commercial textile industries throughout the world. China imports much of its rabbit fur from Scandinavia (80%) and some from North America (5%), according to the USDA Foreign Agricultural Service GAIN Report CH7607.
Rabbits have been and continue to be used in laboratory work such as production of antibodies for vaccines and research of human male reproductive system toxicology. In 1972, around 450,000 rabbits were used for experiments in the United States, decreasing to around 240,000 in 2006. The Environmental Health Perspective, published by the National Institute of Health, states, "The rabbit [is] an extremely valuable model for studying the effects of chemicals or other stimuli on the male reproductive system." According to the Humane Society of the United States, rabbits are also used extensively in the study of bronchial asthma, stroke prevention treatments, cystic fibrosis, diabetes, and cancer. The New Zealand White is one of the most commonly used breeds for research and testing. The use of rabbits for the Draize test, a method of testing cosmetics on animals, has been cited as an example of cruelty in animal research by animal rights activists. Albino rabbits are typically used in the Draize tests because they have less tear flow than other animals, and the lack of eye pigment makes the effects easier to visualize.
Rabbits can live outdoors in properly constructed, sheltered hutches, which provide protection from the elements in winter and keep rabbits cool in summer heat. To protect from predators, rabbit hutches are usually situated in a fenced yard, shed, barn, or other enclosed structure, which may also contain a larger pen for exercise. Rabbits in such an environment can alternatively be allowed to roam the secured area freely, and simply be provided with an adapted doghouse for shelter. A more elaborate setup is an artificial warren. However, because of stress related to being inside confined spaces too small for a rabbit, it is recommended that instead of a cage, domestic rabbits free-roam indoors.
Rabbit show jumping, a form of animal sport between rabbits, began in the 1970s and has since become popular in Europe, particularly Sweden and the United Kingdom. When rabbit jumping was first starting out, the rules of competition were the same as horse jumping rules. However, rules were later changed to reflect a rabbit's abilities. The first national championship for rabbit show jumping was held in Stockholm, Sweden in 1987. Any rabbit, regardless of breed, may participate in this kind of competition, as it is based on athletic skill.
Cuniculture Dwarf rabbit Lop rabbit
The American Rabbit Breeders Association – the oldest and largest rabbit specialist organization in the United States The Livestock Conservancy – a registry of the rarest breeds of domestic rabbits World Rabbit Science Association – an international science organization dedicated to rabbit health research The British Rabbit Council – recognized breeds with photographs and more MediRabbit – a site dedicated to spreading the knowledge of rabbit medicine and safe medication in rabbits, for the owner and the vet professional Rabbit Breeds - directory of ARBA-recognized breeds of rabbit Complete Guide of Rabbit Breeds - List of rabbit breeds approved by American Rabbit Breeders Association RabbitPedia.com - Source for information about rabbit care. House Rabbit Society – a US-based educational and advocacy organization for rabbit pet-keepers, founded in 1988 Domestic rabbit at Curlie
An economy (from Greek οίκος – "household" and νέμoμαι – "manage") is an area of the production, distribution and trade, as well as consumption of goods and services by different agents. Understood in its broadest sense, 'The economy is defined as a social domain that emphasize the practices, discourses, and material expressions associated with the production, use, and management of resources'. A given economy is the result of a set of processes that involves its culture, values, education, technological evolution, history, social organization, political structure and legal systems, as well as its geography, natural resource endowment, and ecology, as main factors. These factors give context, content, and set the conditions and parameters in which an economy functions. In other words, the economic domain is a social domain of human practices and transactions. It does not stand alone. Economic agents can be individuals, businesses, organizations, or governments. Economic transactions occur when two groups or parties agree to the value or price of the transacted good or service, commonly expressed in a certain currency. However, monetary transactions only account for a small part of the economic domain. Economic activity is spurred by production which uses natural resources, labor and capital. It has changed over time due to technology (automation, accelerator of process, reduction of cost functions), innovation (new products, services, processes, expanding markets, diversification of markets, niche markets, increases revenue functions) such as, that which produces intellectual property and changes in industrial relations (most notably child labor being replaced in some parts of the world with universal access to education). A market-based economy is one where goods and services are produced and exchanged according to demand and supply between participants (economic agents) by barter or a medium of exchange with a credit or debit value accepted within the network, such as a unit of currency. A command-based economy is one where political agents directly control what is produced and how it is sold and distributed. A green economy is low-carbon, resource efficient and socially inclusive. In a green economy, growth in income and employment is driven by public and private investments that reduce carbon emissions and pollution, enhance energy and resource efficiency, and prevent the loss of biodiversity and ecosystem services. A gig economy is one in which short-term jobs are assigned or chosen via online platforms. New economy is a term that referred to the whole emerging ecosystem where new standards and practices were introduced, usually as a result of technological innovations. The global economy refers to humanity's economic system or systems overall.
Today the range of fields of study examining the economy revolves around the social science of economics, but may include sociology (economic sociology), history (economic history), anthropology (economic anthropology), and geography (economic geography). Practical fields directly related to the human activities involving production, distribution, exchange, and consumption of goods and services as a whole are engineering, management, business administration, applied science, and finance. All professions, occupations, economic agents or economic activities, contribute to the economy. Consumption, saving, and investment are variable components in the economy that determine macroeconomic equilibrium. There are three main sectors of economic activity: primary, secondary, and tertiary. Due to the growing importance of the economical sector in modern times, the term real economy is used by analysts as well as politicians to denote the part of the economy that is concerned with the actual production of goods and services, as ostensibly contrasted with the paper economy, or the financial side of the economy, which is concerned with buying and selling on the financial markets. Alternate and long-standing terminology distinguishes measures of an economy expressed in real values (adjusted for inflation), such as real GDP, or in nominal values (unadjusted for inflation).
As long as someone has been making, supplying and distributing goods or services, there has been some sort of economy; economies grew larger as societies grew and became more complex. Sumer developed a large-scale economy based on commodity money, while the Babylonians and their neighboring city states later developed the earliest system of economics as we think of, in terms of rules/laws on debt, legal contracts and law codes relating to business practices, and private property.The Babylonians and their city state neighbors developed forms of economics comparable to currently used civil society (law) concepts. They developed the first known codified legal and administrative systems, complete with courts, jails, and government records. The ancient economy was mainly based on subsistence farming. The Shekel referred to an ancient unit of weight and currency. The first usage of the term came from Mesopotamia circa 3000 BC. and referred to a specific mass of barley which related other values in a metric such as silver, bronze, copper etc. A barley/shekel was originally both a unit of currency and a unit of weight, just as the British Pound was originally a unit denominating a one-pound mass of silver. For most people, the exchange of goods occurred through social relationships. There were also traders who bartered in the marketplaces. In Ancient Greece, where the present English word 'economy' originated, many people were bond slaves of the freeholders. The economic discussion was driven by scarcity.
The European captures became branches of the European states, the so-called colonies. The rising nation-states Spain, Portugal, France, Great Britain and the Netherlands tried to control the trade through custom duties and (from mercator, lat.: merchant) was a first approach to intermediate between private wealth and public interest. The secularization in Europe allowed states to use the immense property of the church for the development of towns. The influence of the nobles decreased. The first Secretaries of State for economy started their work. Bankers like Amschel Mayer Rothschild (1773–1855) started to finance national projects such as wars and infrastructure. Economy from then on meant national economy as a topic for the economic activities of the citizens of a state.
The first economist in the true modern meaning of the word was the Scotsman Adam Smith (1723–1790) who was inspired partly by the ideas of physiocracy, a reaction to mercantilism and also later Economics student, Adam Mari. He defined the elements of a national economy: products are offered at a natural price generated by the use of competition - supply and demand - and the division of labor. He maintained that the basic motive for free trade is human self-interest. The so-called self-interest hypothesis became the anthropological basis for economics. Thomas Malthus (1766–1834) transferred the idea of supply and demand to the problem of overpopulation. The Industrial Revolution was a period from the 18th to the 19th century where major changes in agriculture, manufacturing, mining, and transport had a profound effect on the socioeconomic and cultural conditions starting in the United Kingdom, then subsequently spreading throughout Europe, North America, and eventually the world. The onset of the Industrial Revolution marked a major turning point in human history; almost every aspect of daily life was eventually influenced in some way. In Europe wild capitalism started to replace the system of mercantilism (today: protectionism) and led to economic growth. The period today is called industrial revolution because the system of Production, production and division of labor enabled the mass production of goods.
The contemporary concept of "the economy" wasn't popularly known until the American Great Depression in the 1930s.After the chaos of two World Wars and the devastating Great Depression, policymakers searched for new ways of controlling the course of the economy. This was explored and discussed by Friedrich August von Hayek (1899–1992) and Milton Friedman (1912–2006) who pleaded for a global free trade and are supposed to be the fathers of the so-called neoliberalism. However, the prevailing view was that held by John Maynard Keynes (1883–1946), who argued for a stronger control of the markets by the state. The theory that the state can alleviate economic problems and instigate economic growth through state manipulation of aggregate demand is called Keynesianism in his honor. In the late 1950s, the economic growth in America and Europe—often called Wirtschaftswunder (ger: economic miracle) —brought up a new form of economy: mass consumption economy. In 1958, John Kenneth Galbraith (1908–2006) was the first to speak of an affluent society. In most of the countries the economic system is called a social market economy.
With the fall of the Iron Curtain and the transition of the countries of the Eastern Bloc towards democratic government and market economies, the idea of the post-industrial society is brought into importance as its role is to mark together the significance that the service sector receives instead of industrialization. Some attribute the first use of this term to Daniel Bell's 1973 book, The Coming of Post-Industrial Society, while others attribute it to social philosopher Ivan Illich's book, Tools for Conviviality. The term is also applied in philosophy to designate the fading of postmodernism in the late 90s and especially in the beginning of the 21st century. With the spread of Internet as a mass media and communication medium especially after 2000-2001, the idea for the Internet and information economy is given place because of the growing importance of e-commerce and electronic businesses, also the term for a global information society as understanding of a new type of "all-connected" society is created. In the late 2000s, the new type of economies and economic expansions of countries like China, Brazil, and India bring attention and interest to different from the usually dominating Western type economies and economic models.
The economy may be considered as having developed through the following phases or degrees of precedence. The ancient economy was mainly based on subsistence farming. The industrial revolution phase lessened the role of subsistence farming, converting it to more extensive and mono-cultural forms of agriculture in the last three centuries. The economic growth took place mostly in mining, construction and manufacturing industries. Commerce became more significant due to the need for improved exchange and distribution of produce throughout the community. In the economies of modern consumer societies phase there is a growing part played by services, finance, and technology—the knowledge economy.In modern economies, these phase precedences are somewhat differently expressed by the three-sector theory. Primary stage/degree of the economy: Involves the extraction and production of raw materials, such as corn, coal, wood and iron. (A coal miner and a fisherman would be workers in the primary degree.) Secondary stage/degree of the economy: Involves the transformation of raw or intermediate materials into goods e.g. manufacturing steel into cars, or textiles into clothing. (A builder and a dressmaker would be workers in the secondary degree.) At this stage the associated industrial economy is also sub-divided into several economic sectors (also called industries). Their separate evolution during the Industrial Revolution phase is dealt with elsewhere. Tertiary stage/degree of the economy: Involves the provision of services to consumers and businesses, such as baby-sitting, cinema and banking. (A shopkeeper and an accountant would be workers in the tertiary degree.) Quaternary stage/degree of the economy: Involves the research and development needed to produce products from natural resources and their subsequent by-products. (A logging company might research ways to use partially burnt wood to be processed so that the undamaged portions of it can be made into pulp for paper.) Note that education is sometimes included in this sector.Other sectors of the developed community include : the public sector or state sector (which usually includes: parliament, law-courts and government centers, various emergency services, public health, shelters for impoverished and threatened people, transport facilities, air/sea ports, post-natal care, hospitals, schools, libraries, museums, preserved historical buildings, parks/gardens, nature-reserves, some universities, national sports grounds/stadiums, national arts/concert-halls or theaters and centers for various religions). the private sector or privately run businesses. the social sector or voluntary sector.
There are a number of concepts associated with the economy, such as these:
The GDP (gross domestic product) of a country is a measure of the size of its economy. The most conventional economic analysis of a country relies heavily on economic indicators like the GDP and GDP per capita. While often useful, GDP only includes economic activity for which money is exchanged.
An informal economy is economic activity that is neither taxed nor monitored by a government, contrasted with a formal economy. The informal economy is thus not included in that government's gross national product (GNP). Although the informal economy is often associated with developing countries, all economic systems contain an informal economy in some proportion. Informal economic activity is a dynamic process that includes many aspects of economic and social theory including exchange, regulation, and enforcement. By its nature, it is necessarily difficult to observe, study, define, and measure. No single source readily or authoritatively defines informal economy as a unit of study. The terms "underground", "under the table" and "off the books" typically refer to this type of economy. The term black market refers to a specific subset of the informal economy. The term "informal sector" was used in many earlier studies, and has been mostly replaced in more recent studies which use the newer term. The informal sector makes up a significant portion of the economies in developing countries but it is often stigmatized as troublesome and unmanageable. However, the informal sector provides critical economic opportunities for the poor and has been expanding rapidly since the 1960s. As such, integrating the informal economy into the formal sector is an important policy challenge.
Economic research is conducted in fields as different as economics, economic sociology, economic anthropology, and economic history.
Aristotle, Politics, Book I-IIX, translated by Benjamin Jowett, Classics.mit.edu Barnes, Peter, Capitalism 3.0, A Guide to Reclaiming the Commons, San Francisco 2006, Whatiseconomy.com Dill, Alexander, Reclaiming the Hidden Assets, Towards a Global Freeware Index, Global Freeware Research Paper 01-07, 2007, Whatiseconomy.com Fehr Ernst, Schmidt, Klaus M., The Economics Of Fairness, Reciprocity and Altruism - experimental Evidence and new Theories, 2005, Discussion PAPER 2005-20, Munich Economics, Whatiseconomy.com Marx, Karl, Engels, Friedrich, 1848, The Communist Manifesto, Marxists.org Stiglitz, Joseph E., Global public goods and global finance: does global governance ensure that the global public interest is served? In: Advancing Public Goods, Jean-Philippe Touffut, (ed.), Paris 2006, pp. 149/164, GSB.columbia.edu Where is the Wealth of Nations? Measuring Capital for the 21st Century. Wealth of Nations Report 2006, Ian Johnson and Francois Bourguignon, World Bank, Washington 2006, Whatiseconomy.com.
In microeconomics, economies of scale are the cost advantages that enterprises obtain due to their scale of operation (typically measured by the amount of output produced), with cost per unit of output decreasing with increasing scale. At the basis of economies of scale there may be technical, statistical, organizational or related factors to the degree of market control. Economies of scale apply to a variety of organizational and business situations and at various levels, such as a production, plant or an entire enterprise. When average costs start falling as output increases, then economies of scale occur. Some economies of scale, such as capital cost of manufacturing facilities and friction loss of transportation and industrial equipment, have a physical or engineering basis. Another source of scale economies is the possibility of purchasing inputs at a lower per-unit cost when they are purchased in large quantities. The economic concept dates back to Adam Smith and the idea of obtaining larger production returns through the use of division of labor. Diseconomies of scale are the opposite. Economies of scale often have limits, such as passing the optimum design point where costs per additional unit begin to increase. Common limits include exceeding the nearby raw material supply, such as wood in the lumber, pulp and paper industry. A common limit for a low cost per unit weight commodities is saturating the regional market, thus having to ship product uneconomic distances. Other limits include using energy less efficiently or having a higher defect rate. Large producers are usually efficient at long runs of a product grade (a commodity) and find it costly to switch grades frequently. They will, therefore, avoid specialty grades even though they have higher margins. Often smaller (usually older) manufacturing facilities remain viable by changing from commodity-grade production to specialty products.Economies of scale must be distinguished from economies stemming from an increase in the production of a given plant. When a plant is used below its optimal production capacity, increases in its degree of utilization bring about decreases in the total average cost of production. As noticed, among the others, by Nicholas Georgescu-Roegen (1966) and Nicholas Kaldor (1972) these economies are not economies of scale.
The simple meaning of economies of scale is doing things more efficiently with increasing size. Common sources of economies of scale are purchasing (bulk buying of materials through long-term contracts), managerial (increasing the specialization of managers), financial (obtaining lower-interest charges when borrowing from banks and having access to a greater range of financial instruments), marketing (spreading the cost of advertising over a greater range of output in media markets), and technological (taking advantage of returns to scale in the production function). Each of these factors reduces the long run average costs (LRAC) of production by shifting the short-run average total cost (SRATC) curve down and to the right. Economies of scale is a concept that may explain real-world phenomena such as patterns of international trade or the number of firms in a market. The exploitation of economies of scale helps explain why companies grow large in some industries. It is also a justification for free trade policies, since some economies of scale may require a larger market than is possible within a particular country—for example, it would not be efficient for Liechtenstein to have its own carmaker if they only sold to their local market. A lone carmaker may be profitable, but even more so if they exported cars to global markets in addition to selling to the local market. Economies of scale also play a role in a "natural monopoly". There is a distinction between two types of economies of scale: internal and external. An industry that exhibits an internal economy of scale is one where the costs of production fall when the number of firms in the industry drops, but the remaining firms increase their production to match previous levels. Conversely, an industry exhibits an external economy of scale when costs drop due to the introduction of more firms, thus allowing for more efficient use of specialized services and machinery.
Some of the economies of scale recognized in engineering have a physical basis, such as the square-cube law, by which the surface of a vessel increases by the square of the dimensions while the volume increases by the cube. This law has a direct effect on the capital cost of such things as buildings, factories, pipelines, ships and airplanes.In structural engineering, the strength of beams increases with the cube of the thickness. Drag loss of vehicles like aircraft or ships generally increases less than proportional with increasing cargo volume, although the physical details can be quite complicated. Therefore, making them larger usually results in less fuel consumption per ton of cargo at a given speed. Heat loss from industrial processes vary per unit of volume for pipes, tanks and other vessels in a relationship somewhat similar to the square-cube law. In some productions, an increase in the size of the plant reduces the average variable cost, thanks to the energy savings resulting from the lower dispersion of heat. Economies of increased dimension are often misinterpreted because of the confusion between indivisibility and three-dimensionality of space. This confusion arises from the fact that three-dimensional production elements, such as pipes and ovens, once installed and operating, are always technically indivisible. However, the economies of scale due to the increase in size do not depend on indivisibility but exclusively on the three-dimensionality of space. Indeed, indivisibility only entails the existence of economies of scale produced by the balancing of productive capacities, considered above; or of increasing returns in the utilisation of a single plant, due to its more efficient use as the quantity produced increases. However, this latter phenomenon has nothing to do with the economies of scale which, by definition, are linked to the use of a larger plant.
At the base of economies of scale there are also returns to scale linked to statistical factors. In fact, the greater of the number of resources involved, the smaller, in proportion, is the quantity of reserves necessary to cope with unforeseen contingencies (for instance, machine spare parts, inventories, circulating capital, etc.).
A larger scale generally determines greater bargaining power over input prices and therefore benefits from pecuniary economies in terms of purchasing raw materials and intermediate goods compared to companies that make orders for smaller amounts. In this case we speak of pecuniary economies, to highlight the fact that nothing changes from the "physical" point of view of the returns to scale. Furthermore, supply contracts entail fixed costs which lead to decreasing average costs if the scale of production increases.
Economies of productive capacity balancing derives from the possibility that a larger scale of production involves a more efficient use of the production capacities of the individual phases of the production process. If the inputs are indivisible and complementary, a small scale may be subject to idle times or to the underutilization of the productive capacity of some sub-processes. A higher production scale can make the different production capacities compatible. The reduction in machinery idle times is crucial in the case of a high cost of machinery. === Economies resulting from the division of labour and the use of superior techniques === A larger scale allows for a more efficient division of labour. The economies of division of labour derive from the increase in production speed, from the possibility of using specialized personnel and adopting more efficient techniques. An increase in the division of labour inevitably leads to changes in the quality of inputs and outputs.
Many administrative and organizational activities are mostly cognitive and, therefore, largely independent of the scale of production. When the size of the company and the division of labour increase, there are a number of advantages due to the possibility of making organizational management more effective and perfecting accounting and control techniques. Furthermore, the procedures and routines that turned out to be the best can be reproduced by managers at different times and places.
Learning and growth economies are at the base of dynamic economies of scale, associated with the process of growth of the scale dimension and not to the dimension of scale per se. Learning by doing implies improvements in the ability to perform and promotes the introduction of incremental innovations with a progressive lowering of average costs. Learning economies are directly proportional to the cumulative production (experience curve). Growth economies occur when a company acquires an advantage by increasing its size. These economies are due to the presence of some resource or competence that is not fully utilized, or to the existence of specific market positions that create a differential advantage in expanding the size of the firms. That growth economies disappear once the scale size expansion process is completed. For example, a company that owns a supermarket chain benefits from an economy of growth if, opening a new supermarket, it gets an increase in the price of the land it owns around the new supermarket. The sale of these lands to economic operators, who wish to open shops near the supermarket, allows the company in question to make a profit, making a profit on the revaluation of the value of building land.
Overall costs of capital projects are known to be subject to economies of scale. A crude estimate is that if the capital cost for a given sized piece of equipment is known, changing the size will change the capital cost by the 0.6 power of the capacity ratio (the point six to the power rule).In estimating capital cost, it typically requires an insignificant amount of labor, and possibly not much more in materials, to install a larger capacity electrical wire or pipe having significantly greater capacity.The cost of a unit of capacity of many types of equipment, such as electric motors, centrifugal pumps, diesel and gasoline engines, decreases as size increases. Also, the efficiency increases with size.
Operating crew size for ships, airplanes, trains, etc., does not increase in direct proportion to capacity. (Operating crew consists of pilots, co-pilots, navigators, etc. and does not include passenger service personnel.) Many aircraft models were significantly lengthened or "stretched" to increase payload.Many manufacturing facilities, especially those making bulk materials like chemicals, refined petroleum products, cement and paper, have labor requirements that are not greatly influenced by changes in plant capacity. This is because labor requirements of automated processes tend to be based on the complexity of the operation rather than production rate, and many manufacturing facilities have nearly the same basic number of processing steps and pieces of equipment, regardless of production capacity.
Karl Marx noted that large scale manufacturing allowed economical use of products that would otherwise be waste. Marx cited the chemical industry as an example, which today along with petrochemicals, remains highly dependent on turning various residual reactant streams into salable products. In the pulp and paper industry it is economical to burn bark and fine wood particles to produce process steam and to recover the spent pulping chemicals for conversion back to a usable form.
Economies of scale is related to and can easily be confused with the theoretical economic notion of returns to scale. Where economies of scale refer to a firm's costs, returns to scale describe the relationship between inputs and outputs in a long-run (all inputs variable) production function. A production function has constant returns to scale if increasing all inputs by some proportion results in output increasing by that same proportion. Returns are decreasing if, say, doubling inputs results in less than double the output, and increasing if more than double the output. If a mathematical function is used to represent the production function, and if that production function is homogeneous, returns to scale are represented by the degree of homogeneity of the function. Homogeneous production functions with constant returns to scale are first degree homogeneous, increasing returns to scale are represented by degrees of homogeneity greater than one, and decreasing returns to scale by degrees of homogeneity less than one. If the firm is a perfect competitor in all input markets, and thus the per-unit prices of all its inputs are unaffected by how much of the inputs the firm purchases, then it can be shown that at a particular level of output, the firm has economies of scale if and only if it has increasing returns to scale, has diseconomies of scale if and only if it has decreasing returns to scale, and has neither economies nor diseconomies of scale if it has constant returns to scale. In this case, with perfect competition in the output market the long-run equilibrium will involve all firms operating at the minimum point of their long-run average cost curves (i.e., at the borderline between economies and diseconomies of scale). If, however, the firm is not a perfect competitor in the input markets, then the above conclusions are modified. For example, if there are increasing returns to scale in some range of output levels, but the firm is so big in one or more input markets that increasing its purchases of an input drives up the input's per-unit cost, then the firm could have diseconomies of scale in that range of output levels. Conversely, if the firm is able to get bulk discounts of an input, then it could have economies of scale in some range of output levels even if it has decreasing returns in production in that output range. In essence, returns to scale refer to the variation in the relationship between inputs and output. This relationship is therefore expressed in "physical" terms. But when talking about economies of scale, the relation taken into consideration is that between the average production cost and the dimension of scale. Economies of scale therefore are affected by variations in input prices. If input prices remain the same as their quantities purchased by the firm increase, the notions of increasing returns to scale and economies of scale can be considered equivalent. However, if input prices vary in relation to their quantities purchased by the company, it is necessary to distinguish between returns to scale and economies of scale. The concept of economies of scale is more general than that of returns to scale since it includes the possibility of changes in the price of inputs when the quantity purchased of inputs varies with changes in the scale of production.The literature assumed that due to the competitive nature of reverse auctions, and in order to compensate for lower prices and lower margins, suppliers seek higher volumes to maintain or increase the total revenue. Buyers, in turn, benefit from the lower transaction costs and economies of scale that result from larger volumes. In part as a result, numerous studies have indicated that the procurement volume must be sufficiently high to provide sufficient profits to attract enough suppliers, and provide buyers with enough savings to cover their additional costs.However, surprisingly enough, Shalev and Asbjornse found, in their research based on 139 reverse auctions conducted in the public sector by public sector buyers, that the higher auction volume, or economies of scale, did not lead to better success of the auction. They found that auction volume did not correlate with competition, nor with the number of bidders, suggesting that auction volume does not promote additional competition. They noted, however, that their data included a wide range of products, and the degree of competition in each market varied significantly, and offer that further research on this issue should be conducted to determine whether these findings remain the same when purchasing the same product for both small and high volumes. Keeping competitive factors constant, increasing auction volume may further increase competition.
The first systematic analysis of the advantages of the division of labour capable of generating economies of scale, both in a static and dynamic sense, was that contained in the famous First Book of Wealth of Nations (1776) by Adam Smith, generally considered the founder of political economy as an autonomous discipline. John Stuart Mill, in Chapter IX of the First Book of his Principles, referring to the work of Charles Babbage (On the economics of machines and manufactories), widely analyses the relationships between increasing returns and scale of production all inside the production unit.
In “Das Kapital” (1867), Karl Marx, referring to Charles Babbage, extensively analyses economies of scale and concludes that they are one of the factors underlying the ever-increasing concentration of capital. Marx observes that in the capitalist system the technical conditions of the work process are continuously revolutionized in order to increase the surplus by improving the productive force of work. According to Marx, with the cooperation of many workers brings about an economy in the use of the means of production and an increase in productivity due to the increase in the division of labour. Furthermore, the increase in the size of the machinery allows significant savings in construction, installation and operation costs. The tendency to exploit economies of scale entails a continuous increase in the volume of production which, in turn, requires a constant expansion of the size of the market. However, if the market does not expand at the same rate as production increases, overproduction crises can occur. According to Marx the capitalist system is therefore characterized by two tendencies, connected to economies of scale: towards a growing concentration and towards economic crises due to overproduction.In his 1844 Economic and Philosophic Manuscripts, Karl Marx observes that economies of scale have historically been associated with an increasing concentration of private wealth and have been used to justify such concentration. Marx points out that concentrated private ownership of large-scale economic enterprises is a historically contingent fact, and not essential to the nature of such enterprises. In the case of agriculture, for example, Marx calls attention to the sophistical nature of the arguments used to justify the system of concentrated ownership of land: As for large landed property, its defenders have always sophistically identified the economic advantages offered by large-scale agriculture with large-scale landed property, as if it were not precisely as a result of the abolition of property that this advantage, for one thing, received its greatest possible extension, and, for another, only then would be of social benefit.Instead of concentrated private ownership of land, Marx recommends that economies of scale should instead be realized by associations: Association, applied to land, shares the economic advantage of large-scale landed property, and first brings to realization the original tendency inherent in land-division, namely, equality. In the same way association re-establishes, now on a rational basis, no longer mediated by serfdom, overlordship and the silly mysticism of property, the intimate ties of man with the earth, for the earth ceases to be an object of huckstering, and through free labor and free enjoyment becomes once more a true personal property of man.
Alfred Marshall notes that "some, among whom Cournot himself", have considered "the internal economies [...] apparently without noticing that their premises lead inevitably to the conclusion that, whatever firm first gets a good start will obtain a monopoly of the whole business of its trade … ". Marshall believes that there are factors that limit this trend toward monopoly, and in particular: the death of the founder of the firm and the difficulty that the successors may have inherited his/her entrepreneurial skills; the difficulty of reaching new markets for one's goods; the growing difficulty of being able to adapt to changes in demand and to new techniques of production; The effects of external economies, that is the particular type of economies of scale connected not to the production scale of an individual production unit, but to that of an entire sector.
Piero Sraffa observes that Marshall, in order to justify the operation of the law of increasing returns without it coming into conflict with the hypothesis of free competition, tended to highlight the advantages of external economies linked to an increase in the production of an entire sector of activity. However, “those economies which are external from the point of view of the individual firm, but internal as regards the industry in its aggregate, constitute precisely the class which is most seldom to be met with”. “In any case - Sraffa notes – in so far as external economies of the kind in question exist, they are not linked to be called forth by small increases in production”, as required by the marginalist theory of price. Sraffa points out that, in the equilibrium theory of the individual industries, the presence of external economies cannot play an important role because this theory is based on marginal changes in the quantities produced. Sraffa concludes that, if the hypothesis of perfect competition is maintained, economies of scale should be excluded. He then suggests the possibility of abandoning the assumption of free competition to address the study of firms that have their own particular market. This stimulated a whole series of studies on the cases of imperfect competition in Cambridge. However, in the succeeding years Sraffa will follow a different path of research that will bring him to write and publish his main work Production of commodities by means of commodities (Sraffa, 1960). In this book Sraffa determines relative prices assuming no changes in output, so that no question arises as to the variation or constancy of returns.
It has been noted that in many industrial sectors there are numerous companies with different sizes and organizational structures, despite the presence of significant economies of scale. This contradiction, between the empirical evidence and the logical incompatibility between economies of scale and competition, has been called the ‘Cournot dilemma’. As Mario Morroni observes, Cournot's dilemma appears to be unsolvable if we only consider the effects of economies of scale on the dimension of scale. If, on the other hand, the analysis is expanded, including the aspects concerning the development of knowledge and the organization of transactions, it is possible to conclude that economies of scale do not always lead to monopoly. In fact, the competitive advantages deriving from the development of the firm's capabilities and from the management of transactions with suppliers and customers can counterbalance those provided by the scale, thus counteracting the tendency towards a monopoly inherent in economies of scale. In other words, the heterogeneity of the organizational forms and of the size of the companies operating in a sector of activity can be determined by factors regarding the quality of the products, the production flexibility, the contractual methods, the learning opportunities, the heterogeneity of preferences of customers who express a differentiated demand with respect to the quality of the product, and assistance before and after the sale. Very different organizational forms can therefore co-exist in the same sector of activity, even in the presence of economies of scale, such as, for example, flexible production on a large scale, small-scale flexible production, mass production, industrial production based on rigid technologies associated with flexible organizational systems and traditional artisan production. The considerations regarding economies of scale are therefore important, but not sufficient to explain the size of the company and the market structure. It is also necessary to take into account the factors linked to the development of capabilities and the management of transaction costs.
Economies of Scale Definition by The Linux Information Project (LINFO) Economies of Scale by Economics Online
The economy of Greenland can be characterized as small, mixed and vulnerable. Greenland's economy consists of a large public sector and comprehensive foreign trade. This has resulted in an economy with periods of strong growth, considerable inflation, unemployment problems and extreme dependence on capital inflow from the Kingdom Government.GDP per capita is close to the average for European economies, but the economy is critically dependent upon substantial support from the Danish government, which supplies about half the revenues of the Self-rule Government, which in turn employs 10,307 Greenlanders out of 25,620 currently in employment (2015). Unemployment nonetheless remains high, with the rest of the economy dependent upon demand for exports of shrimp and fish.
Except for an abortive royal colony established under Major Claus Paarss between 1728 and 1730, colonial Greenland was administered by companies under royal charter until 1908. Hans Egede's Hope Colony was organized under the auspices of the Bergen Greenland Company prior to its bankruptcy in 1727; it was succeeded by the merchant Jacob Severin (1733–1749), the General Trade Company (Det almindelige Handelskompagni; 1749–1774), and finally the Royal Greenland Trading Department (KGH; 1776–1908). Early hopes of mineral or agricultural wealth were dashed, and open trade proved a failure owing to other nations' better quality, lower priced goods and hostility. Kale, lettuce, and other vegetables were successfully introduced, but repeated attempts to cultivate wheat or clover failed throughout Greenland, limiting the ability to raise European livestock. After government-funded whaling failed, the KGH eventually settled on maintaining the native Greenlanders in their traditional pursuits of hunting and whaling and enforced a monopoly on trade between them and Europe. Repeated attempts to open trade were opposed on both commercial and humanitarian grounds, although minor reforms in the 1850s and 60s lowered the prices charged to the natives for "luxuries" like sugar and coffee; transferred more of the KGH's profits to local communities; and granted the important Ivigtut cryolite concession to a separate company.During the years before World War I, the KGH's independence was curtailed and the company folded into the Ministry of the Interior. Climate change, apparent since the 1920s, disrupted traditional Kalaallit life as the milder weather reduced the island's seal populations but filled the waters offshore with cod. After World War II, reforms were finally enacted by the Danish Greenland Commission composed of Greenland Provincial Council members and Danish economists. The report outlined a program to end the KGH model and establish a modern welfare state on the Danish model and supported by the Kingdom Government. The KGH monopolies were ended in 1950; Greenland was made an equal part of the Kingdom of Denmark in 1953 and Home Rule granted in 1979. The KGH had long opposed urbanization of the Kalaallit Greenlanders, but during the 1950s and 1960s the Danish government introduced an urbanization and modernization program aimed at consolidating existing settlements. The program was intended to reduce costs, improve access to education and health care, and provide workers for modernized cod fisheries, which were growing rapidly at the time. The program faced a number of problems including the collapse of the fisheries and the shoddy construction of many of the buildings, particularly the infamous Blok P, and produced a number of problems of its own, including continuing unemployment and alcoholism. Greenland left the European Economic Community in February 1985, principally due to EEC policies on fishing and sealskin. Most EU laws do not apply to Greenland; however, owing to its connection with Denmark, Greenland continues to enjoy preferential access to EU markets. In the same year, Greenland exercised its new control over the Royal Greenland Trading Company to reestablish it as KNI. Over the next few decades, divisions of the conglomerate were slowly spun off and competition within the Greenlandic economy somewhat increased. Following the closure of the Maarmorilik lead and zinc mine in 1990 and the collapse of the cod fisheries amid colder ocean currents, Greenland faced foreign trade deficits and a shrinking economy, but it has been growing since 1993.
The Greenland economy is extremely dependent on exports of fish and on support from the Danish Government, which supplies about half of government revenues. The public sector, including publicly owned enterprises and the municipalities, plays the dominant role in the economy.
The largest employers in Greenland are the various levels of administration, including the central Kingdom Government in Denmark, the Local Greenland Self-Rule Government, and the municipalities. Most of these positions are in the capital Nuuk. In addition to this direct employment, the government heavily subsidizes other major employers in other areas of the economy, including Great Greenland's sealskin purchases, Pilersuisoq's rural stores, and some of Air Greenland and Royal Arctic's regional routes.
The second-largest sector by employment is Greenland's fishing industry. The commercial fishing fleet consists of approximately 5,000 dinghies, 300 cutters, and 25 trawlers. While cod was formerly the main catch, today the industry centers on cold-water shrimp and Greenland halibut.The fish processing industry is almost entirely centered on Royal Greenland, the world's largest retailer of cold-water shrimp.
Whaling and seal hunting were once traditional mainstays of Greenland's economy. Greenlanders still kill an estimated 170,000 seals a year and 175 whales a year, ranking them second and third in the world respectively. Both whaling and sealing have become controversial, limiting the potential market for their products. As such, the only seal tannery in the country – Great Greenland in Qaqortoq – is heavily subsidized by the government to maintain the livelihood of smaller communities which are economically dependent on the hunt.Reindeer or caribou are found in the northwest of the island, while muskoxen are found in the northeast and at Kangerlussuaq. Because the muskoxen's natural range favors the protected Northeast Greenland National Park, it is a less common object of hunting than in the past. Polar bear and reindeer hunting in Greenland still occur but are regulated to avoid endangering the populations.
Approximately half of total sales are conducted by KNI, the state-owned successor to the Royal Greenland Trade Department; its rural sales division Pilersuisoq; or its daughter company – which has been purchased by the Danish Dagrofa – Pisiffik. The third major chain is the Brugsen association of cooperatives.
Ivigtut used to be the world's premier source of natural cryolite, an important mineral in aluminum extraction, but the commercially viable reserves were depleted in the 1980s. Similarly, deposits of coal, diamonds, and many metals – including silver, nickel, platinum, copper, molybdenum, iron, niobium, tantalum, uranium, and rare earths – are known to exist, but not yet in commercially viable deposits. Greenland's Bureau of Minerals and Petroleum is working to promote Greenland as an attractive destination for prospectors. Improvements in technology and increases in mineral prices have led to some mines being reopened, such as the lead and zinc mine at Maarmorilik and the gold mine at Nalunaq.Greenland is expected to be one of the world’s next great mining frontiers as global warming starts to uncover precious metals from the frozen surroundings. Substantial volumes of minerals are now within reach of geological land mapping technologies, according to research conducted by GlobalData, a natural resources business intelligence provider.
At 70%, Greenland has one of the highest shares of renewable energy in the world, mostly coming from hydropower.While the Greenland Home Rule Government has primary sovereignty over mineral deposits on the mainland, oil resources are within the domain of the Danish exclusive economic zone. Nonetheless, prospecting takes place under the auspices of NUNAOIL, a partnership between the two governments. Some geologists believe Greenland has some of the world's largest remaining oil resources: in 2001, the U.S. Geological Survey found that the waters off north-eastern Greenland (north and south of the Arctic Circle) could contain up to 110 billion barrels (17×10^9 m3) of oil, and in 2010 the British petrochemical company Cairns Oil reported "the first firm indications" of commercially viable oil deposits. Nonetheless, all six wells drilled since the 1970s have been dry. Greenland has offered eight license blocks for tender along its west coast by Baffin Bay. Seven of those blocks have been bid for by a combination of multinational oil companies and NUNAOIL. Companies that have participated successfully in the previous license rounds and have formed a partnership for the licenses with NUNAOIL are DONG Energy, Chevron, ExxonMobil, Husky Energy, and Cairn Energy. The area available known as the West Disko licensing round is of interest due to its relative accessibility compared to other Arctic basins, as the area remains largely free of ice and contains a number of promising geological leads and prospects from the Paleocene era. Coal used to be mined at Qullissat but this has been suspended. Electricity generation is controlled by the state-owned Nukissiorfiit. It is distributed at 220 V and 50 Hz and sockets of Danish type K are used. Electricity has historically been generated by oil or diesel power plants, even though there is a large surplus of potential hydropower. Because of rising oil prices, there is a program to build hydro power plants. Since the success of the 1993 Buksefjord dam, – whose distribution path to Nuuk includes the Ameralik Span – the long-term policy of the Greenland government is to produce the island's electricity from renewable domestic sources. A third turbine at Buksefjord brought its capacity up to 45 MW in 2008; in 2007, a second, 7.2 MW dam was constructed at Qorlortorsuaq; and in 2010, a third, 15 MW dam was constructed at Sisimiut. There is a plan for an Aluminium smelter plant, which requires multiple large (total 600-750 MW) hydropower plants. Domestic heating is provided by electricity at locations where there is a hydro power plant.
Tourism is limited by the short summers and high costs. Access is almost exclusively by air, mainly from Scandinavia and Iceland. Some tourists arrive by cruise ship (but they don't spend much locally, since the ship provides accommodation and meals). There have been tests with direct flights from the US East Coast from 2007 to 2008, but these were discontinued. The state-owned tourism agency Visit Greenland has the web address Greenland.com.
Agriculture is of little importance in the economy but due to climate change – in southern Greenland, the growing season averages about three weeks longer than a decade ago – which has enabled expanded production of existing crops. At present, local production accounts for 10% of potatoes consumption in Greenland, but that is projected to grow to 15% by 2020. Similarly, it has enabled new crops like apples, strawberries, broccoli, cauliflower, cabbage, and carrots to be grown and for the cultivated areas of the country to be extended although even now only about 1% of Greenland is considered arable. Expanded production is subsidized by the government through purchase guarantees by the state-owned Neqi A/S grocery store chain. The only forest in Greenland is in the Qinngua Valley near Nanortalik. It is protected and not used for timber production.
Animal husbandry consists mainly of sheep farming, with free-grazing flocks. Modern sheep farming methods were introduced in the early 20th century, with the first farm built in 1906. The farms provide meat for local consumption and wool mainly for export. Some 20,000 lambs are slaughtered annually in Narsaq by the state-owned Neqi A/S. The lack of private land ownership rights on Greenland forces farmers to jointly agree to terms of land usage. In the south, there is also a small cattle farm.Reindeer herding has been introduced to Greenland in waves since 1952. Supervision by Scandinavian Sami ended in 1978 and subsequent results were dismal. Repeated attempts in mid-west Greenland in the 1980s and the 1990s failed due to the immobility of the herds, which destroyed their forage. In 1998, the remaining herd was sold to the Nuuk municipality and removed through hunting. At that point, only one Greenlander was still a deerherd; the rest – about 20 people – were still hired Norwegian Sami. Although the conclusion was drawn that reindeer herding was incompatible with the local culture, the southern herds continue to prosper. In 2008, there was still a strong herd at the Isortoq Reindeer Station maintained by the Icelander Stefán Magnússon and Norwegian Ole Kristiansen.
Greenland krone Bank of Greenland
"Setting up a Business in Greenland", from the Greenland Home Rule Government
The economy of India is characterised as a developing market economy. It is the world's sixth-largest economy by nominal GDP and the third-largest by purchasing power parity (PPP). According to the IMF, on a per capita income basis, India ranked 142nd by GDP (nominal) and 124th by GDP (PPP) in 2020. From independence in 1947 until 1991, successive governments promoted protectionist economic policies with extensive state intervention and regulation which is characterised as Dirigism. The end of the Cold War and an acute balance of payments crisis in 1991 led to the adoption of a broad program of economic liberalisation. Since the start of the 21st century, annual average GDP growth has been 6% to 7%, and from 2014 to 2018, India was the world's fastest growing major economy, surpassing China. Historically, India was the largest economy in the world for most of the two millennia from the 1st until 19th century.The long-term growth perspective of the Indian economy remains positive due to its young population and corresponding low dependency ratio, healthy savings and investment rates, and is increasing integration into the global economy. The economy slowed in 2017, due to shocks of "demonetisation" in 2016 and introduction of Goods and Services Tax in 2017. Nearly 60% of India's GDP is driven by domestic private consumption and continues to remain the world's sixth-largest consumer market. Apart from private consumption, India's GDP is also fueled by government spending, investment, and exports. In 2018, India was the world's tenth-largest importer and the nineteenth-largest exporter. India has been a member of World Trade Organization since 1 January 1995. It ranks 63rd on Ease of doing business index and 68th on Global Competitiveness Report. With 520 million workers, the Indian labour force is the world's second-largest as of 2019. India has one of the world's highest number of billionaires and extreme income inequality. Since India has a vast informal economy, barely 2% of Indians pay income taxes. During the 2008 global financial crisis the economy faced mild slowdown, India undertook stimulus measures (both fiscal and monetary) to boost growth and generate demand; in subsequent years economic growth revived. According to 2017 PricewaterhouseCoopers (PwC) report, India's GDP at purchasing power parity could overtake that of the United States by 2050. According to World Bank, to achieve sustainable economic development India must focus on public sector reform, infrastructure, agricultural and rural development, removal of land and labour regulations, financial inclusion, spur private investment and exports, education and public health.In 2019, India's ten largest trading partners were USA, China, UAE, Saudi Arabia, Hong Kong, Iraq, Singapore, Germany, South Korea and Switzerland. In 2018–19, the foreign direct investment (FDI) in India was $64.4 billion with service sector, computer, and telecom industry remains leading sectors for FDI inflows. India has free trade agreements with several nations, including ASEAN, SAFTA, Mercosur, South Korea, Japan and few others which are in effect or under negotiating stage. The service sector makes up 55.6% of GDP and remains the fastest growing sector, while the industrial sector and the agricultural sector employs a majority of the labor force. The Bombay Stock Exchange and National Stock Exchange are one of the world's largest stock exchanges by market capitalization. India is the world's sixth-largest manufacturer, representing 3% of global manufacturing output and employs over 57 million people. Nearly 66% of India's population is rural whose primary source of livelihood is agriculture, and contributes about 50% of India's GDP. It has the world's fifth-largest foreign-exchange reserves worth ₹38,832.21 billion (US$540 billion). India has a high national debt with 68% of GDP, while its fiscal deficit remained at 3.4% of GDP. However, as per 2019 CAG report, the actual fiscal deficit is 5.85% of GDP. India's government-owned banks faced mounting bad debt, resulting in low credit growth, simultaneously the NBFC sector has been engulfed in a liquidity crisis. India faces high unemployment, rising income inequality, and major slump in aggregate demand. In recent years, independent economists and financial institutions have accused the government of fudging various economic data, especially GDP growth.India ranks second globally in food and agricultural production, while agricultural exports were $38.5 billion. The construction and real estate sector is the second largest employer after agriculture, and a vital sector to gauge economic activity. The Indian textiles industry is estimated at $150 billion and contributes 7% of industrial output and 2% of India's GDP while employs over 45 million people directly. The Indian IT industry is a major exporter of IT services with $180 billion in revenue and employs over four million people. India's telecommunication industry is the world's second largest by number of mobile phone, smartphone, and internet users. It is the world's tenth-largest oil producer and the third-largest oil consumer. The Indian automobile industry is the world's fourth largest by production. It has $672 billion worth of retail market which contributes over 10% of India's GDP and has one of world's fastest growing e-commerce markets. India has the world's fourth-largest natural resources, with mining sector contributes 11% of the country's industrial GDP and 2.5% of total GDP. It is also the world's second-largest coal producer, the second-largest cement producer, the second-largest steel producer, and the third-largest electricity producer.
For a continuous duration of nearly 1700 years from the year 1 AD, India was the top most economy constituting 35 to 40% of world GDP. The combination of protectionist, import-substitution, Fabian socialism, and social democratic-inspired policies governed India for sometime after the end of British rule. The economy was then characterised as Dirigism, It had extensive regulation, protectionism, public ownership of large monopolies, pervasive corruption and slow growth. Since 1991, continuing economic liberalisation has moved the country towards a market-based economy. By 2008, India had established itself as one of the world's faster-growing economies.
The citizens of the Indus Valley Civilisation, a permanent settlement that flourished between 2800 BC and 1800 BC, practised agriculture, domesticated animals, used uniform weights and measures, made tools and weapons, and traded with other cities. Evidence of well-planned streets, a drainage system and water supply reveals their knowledge of urban planning, which included the first-known urban sanitation systems and the existence of a form of municipal government.
Maritime trade was carried out extensively between South India and Southeast and West Asia from early times until around the fourteenth century AD. Both the Malabar and Coromandel Coasts were the sites of important trading centres from as early as the first century BC, used for import and export as well as transit points between the Mediterranean region and southeast Asia. Over time, traders organised themselves into associations which received state patronage. Historians Tapan Raychaudhuri and Irfan Habib claim this state patronage for overseas trade came to an end by the thirteenth century AD, when it was largely taken over by the local Parsi, Jewish, Syrian Christian and Muslim communities, initially on the Malabar and subsequently on the Coromandel coast.
Other scholars suggest trading from India to West Asia and Eastern Europe was active between the 14th and 18th centuries. During this period, Indian traders settled in Surakhani, a suburb of greater Baku, Azerbaijan. These traders built a Hindu temple, which suggests commerce was active and prosperous for Indians by the 17th century.Further north, the Saurashtra and Bengal coasts played an important role in maritime trade, and the Gangetic plains and the Indus valley housed several centres of river-borne commerce. Most overland trade was carried out via the Khyber Pass connecting the Punjab region with Afghanistan and onward to the Middle East and Central Asia. Although many kingdoms and rulers issued coins, barter was prevalent. Villages paid a portion of their agricultural produce as revenue to the rulers, while their craftsmen received a part of the crops at harvest time for their services.
The Indian economy was large and prosperous under the Mughal Empire, up until the 18th century. Sean Harkin estimates China and India may have accounted for 60 to 70 percent of world GDP in the 17th century. The Mughal economy functioned on an elaborate system of coined currency, land revenue and trade. Gold, silver and copper coins were issued by the royal mints which functioned on the basis of free coinage. The political stability and uniform revenue policy resulting from a centralised administration under the Mughals, coupled with a well-developed internal trade network, ensured that India–before the arrival of the British–was to a large extent economically unified, despite having a traditional agrarian economy characterised by a predominance of subsistence agriculture, with 64% of the workforce in the primary sector (including agriculture), but with 36% of the workforce also in the secondary and tertiary sectors, higher than in Europe, where 65–90% of its workforce were in agriculture in 1700 and 65–75% were in agriculture in 1750. Agricultural production increased under Mughal agrarian reforms, with Indian agriculture being advanced compared to Europe at the time, such as the widespread use of the seed drill among Indian peasants before its adoption in European agriculture, and higher per-capita agricultural output and standards of consumption. The Mughal Empire had a thriving industrial manufacturing economy, with India producing about 25% of the world's industrial output up until 1750, making it the most important manufacturing center in international trade. Manufactured goods and cash crops from the Mughal Empire were sold throughout the world. Key industries included textiles, shipbuilding, and steel, and processed exports included cotton textiles, yarns, thread, silk, jute products, metalware, and foods such as sugar, oils and butter. Cities and towns boomed under the Mughal Empire, which had a relatively high degree of urbanization for its time, with 15% of its population living in urban centres, higher than the percentage of the urban population in contemporary Europe at the time and higher than that of British India in the 19th century.In early modern Europe, there was significant demand for products from Mughal India, particularly cotton textiles, as well as goods such as spices, peppers, indigo, silks, and saltpeter (for use in munitions). European fashion, for example, became increasingly dependent on Mughal Indian textiles and silks. From the late 17th century to the early 18th century, Mughal India accounted for 95% of British imports from Asia, and the Bengal Subah province alone accounted for 40% of Dutch imports from Asia. In contrast, there was very little demand for European goods in Mughal India, which was largely self-sufficient. Indian goods, especially those from Bengal, were also exported in large quantities to other Asian markets, such as Indonesia and Japan. At the time, Mughal Bengal was the most important center of cotton textile production.In the early 18th century, the Mughal Empire declined, as it lost western, central and parts of south and north India to the Maratha Empire, which integrated and continued to administer those regions. The decline of the Mughal Empire led to decreased agricultural productivity, which in turn negatively affected the textile industry. The subcontinent's dominant economic power in the post-Mughal era was the Bengal Subah in the east., which continued to maintain thriving textile industries and relatively high real wages. However, the former was devastated by the Maratha invasions of Bengal and then British colonization in the mid-18th century. After the loss at the Third Battle of Panipat, the Maratha Empire disintegrated into several confederate states, and the resulting political instability and armed conflict severely affected economic life in several parts of the country – although this was mitigated by localised prosperity in the new provincial kingdoms. By the late eighteenth century, the British East India Company had entered the Indian political theatre and established its dominance over other European powers. This marked a determinative shift in India's trade, and a less-powerful impact on the rest of the economy.
There is no doubt that our grievances against the British Empire had a sound basis. As the painstaking statistical work of the Cambridge historian Angus Maddison has shown, India's share of world income collapsed from 22.6% in 1700, almost equal to Europe's share of 23.3% at that time, to as low as 3.8% in 1952. Indeed, at the beginning of the 20th century, "the brightest jewel in the British Crown" was the poorest country in the world in terms of per capita income. From the beginning of the 19th century, the British East India Company's gradual expansion and consolidation of power brought a major change in taxation and agricultural policies, which tended to promote commercialisation of agriculture with a focus on trade, resulting in decreased production of food crops, mass impoverishment and destitution of farmers, and in the short term, led to numerous famines. The economic policies of the British Raj caused a severe decline in the handicrafts and handloom sectors, due to reduced demand and dipping employment. After the removal of international restrictions by the Charter of 1813, Indian trade expanded substantially with steady growth. The result was a significant transfer of capital from India to England, which, due to the colonial policies of the British, led to a massive drain of revenue rather than any systematic effort at modernisation of the domestic economy. Under British rule, India's share of the world economy declined from 24.4% in 1700 down to 4.2% in 1950. India's GDP (PPP) per capita was stagnant during the Mughal Empire and began to decline prior to the onset of British rule. India's share of global industrial output declined from 25% in 1750 down to 2% in 1900. At the same time, the United Kingdom's share of the world economy rose from 2.9% in 1700 up to 9% in 1870. The British East India Company, following their conquest of Bengal in 1757, had forced open the large Indian market to British goods, which could be sold in India without tariffs or duties, compared to local Indian producers who were heavily taxed, while in Britain protectionist policies such as bans and high tariffs were implemented to restrict Indian textiles from being sold there, whereas raw cotton was imported from India without tariffs to British factories which manufactured textiles from Indian cotton and sold them back to the Indian market. British economic policies gave them a monopoly over India's large market and cotton resources. India served as both a significant supplier of raw goods to British manufacturers and a large captive market for British manufactured goods.British territorial expansion in India throughout the 19th century created an institutional environment that, on paper, guaranteed property rights among the colonisers, encouraged free trade, and created a single currency with fixed exchange rates, standardised weights and measures and capital markets within the company-held territories. It also established a system of railways and telegraphs, a civil service that aimed to be free from political interference, a common-law and an adversarial legal system. This coincided with major changes in the world economy – industrialisation, and significant growth in production and trade. However, at the end of colonial rule, India inherited an economy that was one of the poorest in the developing world, with industrial development stalled, agriculture unable to feed a rapidly growing population, a largely illiterate and unskilled labour force, and extremely inadequate infrastructure.The 1872 census revealed that 91.3% of the population of the region constituting present-day India resided in villages. This was a decline from the earlier Mughal era, when 85% of the population resided in villages and 15% in urban centers under Akbar's reign in 1600. Urbanisation generally remained sluggish in British India until the 1920s, due to the lack of industrialisation and absence of adequate transportation. Subsequently, the policy of discriminating protection (where certain important industries were given financial protection by the state), coupled with the Second World War, saw the development and dispersal of industries, encouraging rural–urban migration, and in particular the large port cities of Bombay, Calcutta and Madras grew rapidly. Despite this, only one-sixth of India's population lived in cities by 1951.The impact of British rule on India's economy is a controversial topic. Leaders of the Indian independence movement and economic historians have blamed colonial rule for the dismal state of India's economy in its aftermath and argued that financial strength required for industrial development in Britain was derived from the wealth taken from India. At the same time, right-wing historians have countered that India's low economic performance was due to various sectors being in a state of growth and decline due to changes brought in by colonialism and a world that was moving towards industrialisation and economic integration.Several economic historians have argued that real wage decline occurred in the early 19th century, or possibly beginning in the very late 18th century, largely as a result of British imperialism. Economic historian Prasannan Parthasarathi presented earnings data which showed real wages and living standards in 18th century Bengal and Mysore being higher than in Britain, which in turn had the highest living standards in Europe. Mysore's average per-capita income was five times higher than subsistence level, i.e. five times higher than $400 (1990 international dollars), or $2,000 per capita. In comparison, the highest national per-capita incomes in 1820 were $1,838 for the Netherlands and $1,706 for Britain. It has also been argued that India went through a period of deindustrialization in the latter half of the 18th century as an indirect outcome of the collapse of the Mughal Empire.
Indian economic policy after independence was influenced by the colonial experience, which was seen as exploitative by Indian leaders exposed to British social democracy and the planned economy of the Soviet Union. Domestic policy tended towards protectionism, with a strong emphasis on import substitution industrialisation, economic interventionism, a large government-run public sector, business regulation, and central planning, while trade and foreign investment policies were relatively liberal. Five-Year Plans of India resembled central planning in the Soviet Union. Steel, mining, machine tools, telecommunications, insurance, and power plants, among other industries, were effectively nationalised in the mid-1950s. The Indian economy of this period is characterised as Dirigism. Never talk to me about profit, Jeh, it is a dirty word. Jawaharlal Nehru, the first prime minister of India, along with the statistician Prasanta Chandra Mahalanobis, formulated and oversaw economic policy during the initial years of the country's independence. They expected favourable outcomes from their strategy, involving the rapid development of heavy industry by both public and private sectors, and based on direct and indirect state intervention, rather than the more extreme Soviet-style central command system. The policy of concentrating simultaneously on capital- and technology-intensive heavy industry and subsidising manual, low-skill cottage industries was criticised by economist Milton Friedman, who thought it would waste capital and labour, and retard the development of small manufacturers. I cannot decide how much to borrow, what shares to issue, at what price, what wages and bonus to pay, and what dividend to give. I even need the government's permission for the salary I pay to a senior executive. Since 1965, the use of high-yielding varieties of seeds, increased fertilisers and improved irrigation facilities collectively contributed to the Green Revolution in India, which improved the condition of agriculture by increasing crop productivity, improving crop patterns and strengthening forward and backward linkages between agriculture and industry. However, it has also been criticised as an unsustainable effort, resulting in the growth of capitalistic farming, ignoring institutional reforms and widening income disparities.In 1984, Rajiv Gandhi promised economic liberalization, he made V. P. Singh the finance minister, who tried to reduce tax-evasion and tax-receipts rose due to this crackdown although taxes were lowered. This process lost its momentum during later tenure of Mr. Gandhi as his government was marred by scandals.
The collapse of the Soviet Union, which was India's major trading partner, and the Gulf War, which caused a spike in oil prices, resulted in a major balance-of-payments crisis for India, which found itself facing the prospect of defaulting on its loans. India asked for a $1.8 billion bailout loan from the International Monetary Fund (IMF), which in return demanded de-regulation.In response, the Narasimha Rao government, including Finance Minister Manmohan Singh, initiated economic reforms in 1991. The reforms did away with the Licence Raj, reduced tariffs and interest rates and ended many public monopolies, allowing automatic approval of foreign direct investment in many sectors. Since then, the overall thrust of liberalisation has remained the same, although no government has tried to take on powerful lobbies such as trade unions and farmers, on contentious issues such as reforming labour laws and reducing agricultural subsidies. By the turn of the 21st century, India had progressed towards a free-market economy, with a substantial reduction in state control of the economy and increased financial liberalisation. This has been accompanied by increases in life expectancy, literacy rates, and food security, although urban residents have benefited more than rural residents. While the credit rating of India was hit by its nuclear weapons tests in 1998, it has since been raised to investment level in 2003 by Standard & Poor's (S&P) and Moody's. India experienced high growth rates, averaging 9% from 2003 to 2007. Growth then moderated in 2008 due to the global financial crisis. In 2003, Goldman Sachs predicted that India's GDP in current prices would overtake France and Italy by 2020, Germany, UK and Russia by 2025 and Japan by 2035, making it the third-largest economy of the world, behind the US and China. India is often seen by most economists as a rising economic superpower which will play a major role in the 21st-century global economy.Starting in 2012, India entered a period of reduced growth, which slowed to 5.6%. Other economic problems also became apparent: a plunging Indian rupee, a persistent high current account deficit and slow industrial growth. India started recovery in 2013–14 when the GDP growth rate accelerated to 6.4% from the previous year's 5.5%. The acceleration continued through 2014–15 and 2015–16 with growth rates of 7.5% and 8.0% respectively. For the first time since 1990, India grew faster than China which registered 6.9% growth in 2015. However the growth rate subsequently decelerated, to 7.1% and 6.6% in 2016–17 and 2017–18 respectively, partly because of the disruptive effects of 2016 Indian banknote demonetisation and the Goods and Services Tax (India).India is ranked 63rd out of 190 countries in the World Bank's 2020 ease of doing business index, up 14 points from the last year's 100 and up 37 points in just two years. In terms of dealing with construction permits and enforcing contracts, it is ranked among the 10 worst in the world, while it has a relatively favourable ranking when it comes to protecting minority investors or getting credit. The strong efforts taken by the Department of Industrial Policy and Promotion (DIPP) to boost ease of doing business rankings at the state level is said to impact the overall rankings of India.
During the COVID-19 pandemic, numerous rating agencies downgraded India's GDP predictions for FY21 to negative figures, signalling a recession in India, the most severe since 1979. According to a Dun & Bradstreet report, the country is likely to suffer a recession in the third quarter of FY2020 as a result of the over 2-month long nation-wide lockdown imposed to curb the spread of COVID-19.
The following table shows the main economic indicators in 1980–2018. Inflation under 5% is in green.
Historically, India has classified and tracked its economy and GDP in three sectors: agriculture, industry, and services. Agriculture includes crops, horticulture, milk and animal husbandry, aquaculture, fishing, sericulture, aviculture, forestry, and related activities. Industry includes various manufacturing sub-sectors. India's definition of services sector includes its construction, retail, software, IT, communications, hospitality, infrastructure operations, education, healthcare, banking and insurance, and many other economic activities.
Agriculture and allied sectors like forestry, logging and fishing accounted for 17% of the GDP, the sector employed 49% of its total workforce in 2014. Agriculture accounted for 23% of GDP, and employed 59% of the country's total workforce in 2016. As the Indian economy has diversified and grown, agriculture's contribution to GDP has steadily declined from 1951 to 2011, yet it is still the country's largest employment source and a significant piece of its overall socio-economic development. Crop-yield-per-unit-area of all crops has grown since 1950, due to the special emphasis placed on agriculture in the five-year plans and steady improvements in irrigation, technology, application of modern agricultural practices and provision of agricultural credit and subsidies since the Green Revolution in India. However, international comparisons reveal the average yield in India is generally 30% to 50% of the highest average yield in the world. The states of Uttar Pradesh, Punjab, Haryana, Madhya Pradesh, Andhra Pradesh, Telangana, Bihar, West Bengal, Gujarat and Maharashtra are key contributors to Indian agriculture. India receives an average annual rainfall of 1,208 millimetres (47.6 in) and a total annual precipitation of 4000 billion cubic metres, with the total utilisable water resources, including surface and groundwater, amounting to 1123 billion cubic metres. 546,820 square kilometres (211,130 sq mi) of the land area, or about 39% of the total cultivated area, is irrigated. India's inland water resources and marine resources provide employment to nearly six million people in the fisheries sector. In 2010, India had the world's sixth-largest fishing industry. India is the largest producer of milk, jute and pulses, and has the world's second-largest cattle population with 170 million animals in 2011. It is the second-largest producer of rice, wheat, sugarcane, cotton and groundnuts, as well as the second-largest fruit and vegetable producer, accounting for 10.9% and 8.6% of the world fruit and vegetable production, respectively. India is also the second-largest producer and the largest consumer of silk, producing 77,000 tons in 2005. India is the largest exporter of cashew kernels and cashew nut shell liquid (CNSL). Foreign exchange earned by the country through the export of cashew kernels during 2011–12 reached ₹43.9 billion (equivalent to ₹67 billion or US$930 million in 2019) based on statistics from the Cashew Export Promotion Council of India (CEPCI). 131,000 tonnes of kernels were exported during 2011–12. There are about 600 cashew processing units in Kollam, Kerala. India's foodgrain production remained stagnant at approximately 252 million tonnes (MT) during both the 2015–16 and 2014–15 crop years (July–June). India exports several agriculture products, such as Basmati rice, wheat, cereals, spices, fresh fruits, dry fruits, buffalo beef meat, cotton, tea, coffee and other cash crops particularly to the Middle East, Southeast and East Asian countries. About 10 percent of its export earnings come from this trade. At around 1,530,000 square kilometres (590,000 sq mi), India has the second-largest amount of arable land, after the US, with 52% of total land under cultivation. Although the total land area of the country is only slightly more than one-third of China or the US, India's arable land is marginally smaller than that of the US, and marginally larger than that of China. However, agricultural output lags far behind its potential. The low productivity in India is a result of several factors. According to the World Bank, India's large agricultural subsidies are distorting what farmers grow and hampering productivity-enhancing investment. Over-regulation of agriculture has increased costs, price risks and uncertainty, and governmental intervention in labour, land, and credit are hurting the market. Infrastructure such as rural roads, electricity, ports, food storage, retail markets and services remain inadequate. The average size of land holdings is very small, with 70% of holdings being less than one hectare (2.5 acres) in size. Irrigation facilities are inadequate, as revealed by the fact that only 46% of the total cultivable land was irrigated as of 2016, resulting in farmers still being dependent on rainfall, specifically the monsoon season, which is often inconsistent and unevenly distributed across the country. In an effort to bring an additional 20,000,000 hectares (49,000,000 acres) of land under irrigation, various schemes have been attempted, including the Accelerated Irrigation Benefit Programme (AIBP) which was provided ₹800 billion (equivalent to ₹930 billion or US$13 billion in 2019) in the union budget. Farming incomes are also hampered by lack of food storage and distribution infrastructure; a third of India's agricultural production is lost from spoilage.
Industry accounts for 26% of GDP and employs 22% of the total workforce. According to the World Bank, India's industrial manufacturing GDP output in 2015 was 6th largest in the world on current US dollar basis ($559 billion), and 9th largest on inflation-adjusted constant 2005 US dollar basis ($197.1 billion). The industrial sector underwent significant changes due to the 1991 economic reforms, which removed import restrictions, brought in foreign competition, led to the privatisation of certain government-owned public-sector industries, liberalised the foreign direct investment (FDI) regime, improved infrastructure and led to an expansion in the production of fast-moving consumer goods. Post-liberalisation, the Indian private sector was faced with increasing domestic and foreign competition, including the threat of cheaper Chinese imports. It has since handled the change by squeezing costs, revamping management, and relying on cheap labour and new technology. However, this has also reduced employment generation, even among smaller manufacturers who previously relied on labour-intensive processes.
With strength of over 1.3 million active personnel, India has the third-largest military force and the largest volunteer army. The total budget sanctioned for the Indian military for the financial year 2019–20 was ₹3.01 trillion (US$42 billion). Defence spending is expected to rise to US$62 billion by 2022.
Primary energy consumption of India is the third-largest after China and the US with 5.3% global share in the year 2015. Coal and crude oil together account for 85% of the primary energy consumption of India. India's oil reserves meet 25% of the country's domestic oil demand. As of April 2015, India's total proven crude oil reserves are 763.476 million metric tons, while gas reserves stood at 1,490 billion cubic metres (53 trillion cubic feet). Oil and natural gas fields are located offshore at Bombay High, Krishna Godavari Basin and the Cauvery Delta, and onshore mainly in the states of Assam, Gujarat and Rajasthan. India is the fourth-largest consumer of oil and net oil imports were nearly ₹8,200 billion (US$110 billion) in 2014–15, which had an adverse effect on the country's current account deficit. The petroleum industry in India mostly consists of public sector companies such as Oil and Natural Gas Corporation (ONGC), Hindustan Petroleum Corporation Limited (HPCL), Bharat Petroleum Corporation Limited (BPCL) and Indian Oil Corporation Limited (IOCL). There are some major private Indian companies in the oil sector such as Reliance Industries Limited (RIL) which operates the world's largest oil refining complex.India became the world's third-largest producer of electricity in 2013 with a 4.8% global share in electricity generation, surpassing Japan and Russia. By the end of calendar year 2015, India had an electricity surplus with many power stations idling for want of demand. The utility electricity sector had an installed capacity of 303 GW as of May 2016 of which thermal power contributed 69.8%, hydroelectricity 15.2%, other sources of renewable energy 13.0%, and nuclear power 2.1%. India meets most of its domestic electricity demand through its 106 billion tonnes of proven coal reserves. India is also rich in certain alternative sources of energy with significant future potential such as solar, wind and biofuels (jatropha, sugarcane). India's dwindling uranium reserves stagnated the growth of nuclear energy in the country for many years. Recent discoveries in the Tummalapalle belt may be among the top 20 natural uranium reserves worldwide, and an estimated reserve of 846,477 metric tons (933,081 short tons) of thorium – about 25% of world's reserves – are expected to fuel the country's ambitious nuclear energy program in the long-run. The Indo-US nuclear deal has also paved the way for India to import uranium from other countries.
Engineering is the largest sub-sector of India's industrial sector, by GDP, and the third-largest by exports. It includes transport equipment, machine tools, capital goods, transformers, switchgear, furnaces, and cast and forged parts for turbines, automobiles, and railways. The industry employs about four million workers. On a value-added basis, India's engineering subsector exported $67 billion worth of engineering goods in the 2013–14 fiscal year, and served part of the domestic demand for engineering goods.The engineering industry of India includes its growing car, motorcycle and scooters industry, and productivity machinery such as tractors. India manufactured and assembled about 18 million passenger and utility vehicles in 2011, of which 2.3 million were exported. India is the largest producer and the largest market for tractors, accounting for 29% of global tractor production in 2013. India is the 12th-largest producer and 7th-largest consumer of machine tools.The automotive manufacturing industry contributed $79 billion (4% of GDP) and employed 6.76 million people (2% of the workforce) in 2016.
India is one of the largest centres for polishing diamonds and gems and manufacturing jewellery; it is also one of the two largest consumers of gold. After crude oil and petroleum products, the export and import of gold, precious metals, precious stones, gems and jewellery accounts for the largest portion of India's global trade. The industry contributes about 7% of India's GDP, employs millions, and is a major source of its foreign-exchange earnings. The gems and jewellery industry created $60 billion in economic output on value-added basis in 2017, and is projected to grow to $110 billion by 2022.The gems and jewellery industry has been economically active in India for several thousand years. Until the 18th century, India was the only major reliable source of diamonds. Now, South Africa and Australia are the major sources of diamonds and precious metals, but along with Antwerp, New York City, and Ramat Gan, Indian cities such as Surat and Mumbai are the hubs of world's jewellery polishing, cutting, precision finishing, supply and trade. Unlike other centres, the gems and jewellery industry in India is primarily artisan-driven; the sector is manual, highly fragmented, and almost entirely served by family-owned operations. The particular strength of this sub-sector is in precision cutting, polishing and processing small diamonds (below one carat). India is also a hub for processing of larger diamonds, pearls, and other precious stones. Statistically, 11 out of 12 diamonds set in any jewellery in the world are cut and polished in India. It is also a major hub of gold and other precious-metal-based jewellery. Domestic demand for gold and jewellery products is another driver of India's GDP.
Petroleum products and chemicals are a major contributor to India's industrial GDP, and together they contribute over 34% of its export earnings. India hosts many oil refinery and petrochemical operations, including the world's largest refinery complex in Jamnagar that processes 1.24 million barrels of crude per day. By volume, the Indian chemical industry was the third-largest producer in Asia, and contributed 5% of the country's GDP. India is one of the five-largest producers of agrochemicals, polymers and plastics, dyes and various organic and inorganic chemicals. Despite being a large producer and exporter, India is a net importer of chemicals due to domestic demands.The chemical industry contributed $163 billion to the economy in FY18 and is expected to reach $300–400 billion by 2025. The industry employed 17.33 million people (4% of the workforce) in 2016.
The Indian pharmaceutical industry has grown in recent years to become a major manufacturer of health care products for the world. India holds a 20% market share in the global supply of generics by volume. The Indian pharmaceutical sector also supplies over 62% of the global demand for various vaccines. India's pharmaceutical exports stood at $17.27 billion in 2017–18 and are expected to reach $20 billion by 2020. The industry grew from $6 billion in 2005 to $36.7 billion in 2016, a compound annual growth rate (CAGR) of 17.46%. It is expected to grow at a CAGR of 15.92% to reach $55 billion in 2020. India is expected to become the sixth-largest pharmaceutical market in the world by 2020. It is one of the fastest-growing industrial sub-sectors and a significant contributor to India's export earnings. The state of Gujarat has become a hub for the manufacture and export of pharmaceuticals and active pharmaceutical ingredients (APIs).
The textile and apparel market in India was estimated to be $108.5 billion in 2015. It is expected to reach a size of $226 billion by 2023. The industry employees over 35 million people. By value, the textile industry accounts for 7% of India's industrial, 2% of GDP and 15% of the country's export earnings. India exported $39.2 billion worth of textiles in the 2017–18 fiscal year.India's textile industry has transformed in recent years from a declining sector to a rapidly developing one. After freeing the industry in 2004–2005 from a number of limitations, primarily financial, the government permitted massive investment inflows, both domestic and foreign. From 2004 to 2008, total investment into the textile sector increased by 27 billion dollars. Ludhiana produces 90% of woollens in India and is known as the Manchester of India. Tirupur has gained universal recognition as the leading source of hosiery, knitted garments, casual wear, and sportswear. Expanding textile centres such as Ichalkaranji enjoy one of the highest per-capita incomes in the country. India's cotton farms, fibre and textile industry provides employment to 45 million people in India, including some child labour (1%). The sector is estimated to employ around 400,000 children under the age of 18.
The services sector has the largest share of India's GDP, accounting for 57% in 2012, up from 15% in 1950. It is the seventh-largest services sector by nominal GDP, and third largest when purchasing power is taken into account. The services sector provides employment to 27% of the workforce. Information technology and business process outsourcing are among the fastest-growing sectors, having a cumulative growth rate of revenue 33.6% between fiscal years 1997–98 and 2002–03, and contributing to 25% of the country's total exports in 2007–08.
In March 1953, the Indian Parliament passed the Air Corporations Act to streamline and nationalise the then existing privately owned eight domestic airlines into Indian Airlines for domestic services and the Tata group-owned Air India for international services. The International Airports Authority of India (IAAI) was constituted in 1972 while the National Airports Authority was constituted in 1986. The Bureau of Civil Aviation Security was established in 1987 following the crash of Air India Flight 182.
The government de-regularised the civil aviation sector in 1991 when the government allowed private airlines to operate charter and non-scheduled services under the 'Air Taxi' Scheme until 1994, when the Air Corporation Act was repealed and private airlines could now operate scheduled services. Private airlines including Jet Airways, Air Sahara, Modiluft, Damania Airways and NEPC Airlines commenced domestic operations during this period.The aviation industry experienced a rapid transformation following deregulation. Several low-cost carriers entered the Indian market in 2004–05. Major new entrants included Air Deccan, Air Sahara, Kingfisher Airlines, SpiceJet, GoAir, Paramount Airways and IndiGo. Kingfisher Airlines became the first Indian air carrier on 15 June 2005 to order Airbus A380 aircraft worth US$3 billion. However, Indian aviation would struggle due to an economic slowdown and rising fuel and operation costs. This led to consolidation, buyouts and discontinuations. In 2007, Air Sahara and Air Deccan were acquired by Jet Airways and Kingfisher Airlines respectively. Paramount Airways ceased operations in 2010 and Kingfisher shut down in 2012. Etihad Airways agreed to acquire a 24% stake in Jet Airways in 2013. AirAsia India, a low-cost carrier operating as a joint venture between Air Asia and Tata Sons launched in 2014. As of 2013–14, only IndiGo and GoAir were generating profits. The average domestic passenger air fare dropped by 70% between 2005 and 2017, after adjusting for inflation.
The financial services industry contributed $809 billion (37% of GDP) and employed 14.17 million people (3% of the workforce) in 2016, and the banking sector contributed $407 billion (19% of GDP) and employed 5.5 million people (1% of the workforce) in 2016. The Indian money market is classified into the organised sector, comprising private, public and foreign-owned commercial banks and cooperative banks, together known as 'scheduled banks'; and the unorganised sector, which includes individual or family-owned indigenous bankers or money lenders and non-banking financial companies. The unorganised sector and microcredit are preferred over traditional banks in rural and sub-urban areas, especially for non-productive purposes such as short-term loans for ceremonies.Prime Minister Indira Gandhi nationalised 14 banks in 1969, followed by six others in 1980, and made it mandatory for banks to provide 40% of their net credit to priority sectors including agriculture, small-scale industry, retail trade and small business, to ensure that the banks fulfilled their social and developmental goals. Since then, the number of bank branches has increased from 8,260 in 1969 to 72,170 in 2007 and the population covered by a branch decreased from 63,800 to 15,000 during the same period. The total bank deposits increased from ₹59.1 billion (equivalent to ₹2.3 trillion or US$32 billion in 2019) in 1970–71 to ₹38,309 billion (equivalent to ₹78 trillion or US$1.1 trillion in 2019) in 2008–09. Despite an increase of rural branches – from 1,860 or 22% of the total in 1969 to 30,590 or 42% in 2007 – only 32,270 of 500,000 villages are served by a scheduled bank.India's gross domestic savings in 2006–07 as a percentage of GDP stood at a high 32.8%. More than half of personal savings are invested in physical assets such as land, houses, cattle, and gold. The government-owned public-sector banks hold over 75% of total assets of the banking industry, with the private and foreign banks holding 18.2% and 6.5% respectively. Since liberalisation, the government has approved significant banking reforms. While some of these relate to nationalised banks – such as reforms encouraging mergers, reducing government interference and increasing profitability and competitiveness – other reforms have opened the banking and insurance sectors to private and foreign companies.
According to the report of The National Association of Software and Services Companies (NASSCOM), India has a presence of around 400 companies in the fintech space, with an investment of about $420 million in 2015. The NASSCOM report also estimated the fintech software and services market to grow 1.7 times by 2020, making it worth $8 billion. The Indian fintech landscape is segmented as follows – 34% in payment processing, followed by 32% in banking and 12% in the trading, public and private markets.
The information technology (IT) industry in India consists of two major components: IT Services and business process outsourcing (BPO). The sector has increased its contribution to India's GDP from 1.2% in 1998 to 7.5% in 2012. According to NASSCOM, the sector aggregated revenues of US$147 billion in 2015, where export revenue stood at US$99 billion and domestic at US$48 billion, growing by over 13%.The growth in the IT sector is attributed to increased specialisation, and an availability of a large pool of low-cost, highly skilled, fluent English-speaking workers – matched by increased demand from foreign consumers interested in India's service exports, or looking to outsource their operations. The share of the Indian IT industry in the country's GDP increased from 4.8% in 2005–06 to 7% in 2008. In 2009, seven Indian firms were listed among the top 15 technology outsourcing companies in the world.The business process outsourcing services in the outsourcing industry in India caters mainly to Western operations of multinational corporations. As of 2012, around 2.8 million people work in the outsourcing sector. Annual revenues are around $11 billion, around 1% of GDP. Around 2.5 million people graduate in India every year. Wages are rising by 10–15 percent as a result of skill shortages.
India became the tenth-largest insurance market in the world in 2013, rising from 15th in 2011. At a total market size of US$66.4 billion in 2013, it remains small compared to world's major economies, and the Indian insurance market accounted for just 2% of the world's insurance business in 2017. India's life and non-life insurance industry collected ₹6.10 trillion (US$86 billion) in total gross insurance premiums in 2018. Life insurance accounts for 75.41% of the insurance market and the rest is general insurance. Of the 52 insurance companies in India, 24 are active in life-insurance business.Specialised insurers Export Credit Guarantee Corporation and Agriculture Insurance Company (AIC) offer credit guarantee and crop insurance. It has introduced several innovative products such as weather insurance and insurance related to specific crops. The premium underwritten by the non-life insurers during 2010–11 was ₹425 billion (equivalent to ₹700 billion or US$9.9 billion in 2019) against ₹346 billion (equivalent to ₹620 billion or US$8.8 billion in 2019) in 2009–10. The growth was satisfactory, particularly given across-the-broad cuts in the tariff rates. The private insurers underwrote premiums of ₹174 billion (equivalent to ₹290 billion or US$4.0 billion in 2019) against ₹140 billion (equivalent to ₹250 billion or US$3.5 billion in 2019) in 2009–10. The Indian insurance business had been under-developed with low levels of insurance penetration.
The retail industry, excluding wholesale, contributed $482 billion (22% of GDP) and employed 249.94 million people (57% of the workforce) in 2016. The industry is the second largest employer in India, after agriculture. The Indian retail market is estimated to be US$600 billion and one of the top-five retail markets in the world by economic value. India has one of the fastest-growing retail markets in the world, and is projected to reach $1.3 trillion by 2020. The e-commerce retail market in India was valued at $32.7 billion in 2018, and is expected to reach $71.9 billion by 2022.India's retail industry mostly consists of local mom-and-pop stores, owner-manned shops and street vendors. Retail supermarkets are expanding, with a market share of 4% in 2008. In 2012, the government permitted 51% FDI in multi-brand retail and 100% FDI in single-brand retail. However, a lack of back-end warehouse infrastructure and state-level permits and red tape continue to limit growth of organised retail. Compliance with over thirty regulations such as "signboard licences" and "anti-hoarding measures" must be made before a store can open for business. There are taxes for moving goods from state to state, and even within states. According to The Wall Street Journal, the lack of infrastructure and efficient retail networks cause a third of India's agriculture produce to be lost from spoilage.
The World Travel & Tourism Council calculated that tourism generated ₹15.24 trillion (US$210 billion) or 9.4% of the nation's GDP in 2017 and supported 41.622 million jobs, 8% of its total employment. The sector is predicted to grow at an annual rate of 6.9% to ₹32.05 trillion (US$450 billion) by 2028 (9.9% of GDP). Over 10 million foreign tourists arrived in India in 2017 compared to 8.89 million in 2016, recording a growth of 15.6%. India earned $21.07 billion in foreign exchange from tourism receipts in 2015. International tourism to India has seen a steady growth from 2.37 million arrivals in 1997 to 8.03 million arrivals in 2015. The United States is the largest source of international tourists to India, while European Union nations and Japan are other major sources of international tourists. Less than 10% of international tourists visit the Taj Mahal, with the majority visiting other cultural, thematic and holiday circuits. Over 12 million Indian citizens take international trips each year for tourism, while domestic tourism within India adds about 740 million Indian travellers.India has a fast-growing medical tourism sector of its health care economy, offering low-cost health services and long-term care. In October 2015, the medical tourism sector was estimated to be worth US$3 billion. It is projected to grow to $7–8 billion by 2020. In 2014, 184,298 foreign patients traveled to India to seek medical treatment.
An ASSOCHAM-PwC joint study projected that the Indian media and entertainment industry would grow from a size of $30.364 billion in 2017 to $52.683 billion by 2022, recording a CAGR of 11.7%. The study also predicted that television, cinema and over-the-top services would account for nearly half of the overall industry growth during the period.
India's healthcare sector is expected to grow at a CAGR of 29% between 2015 and 2020, to reach US$280 billion, buoyed by rising incomes, greater health awareness, increased precedence of lifestyle diseases, and improved access to health insurance.The ayurveda industry in India recorded a market size of $4.4 billion in 2018. The Confederation of Indian Industry estimates that the industry will grow at a CAGR 16% until 2025. Nearly 75% of the market comprises over-the-counter personal care and beauty products, while ayurvedic well-being or ayurvedic tourism services accounted for 15% of the market.
The telecommunication sector generated ₹2.20 trillion (US$31 billion) in revenue in 2014–15, accounting for 1.94% of total GDP. India is the second-largest market in the world by number of telephone users (both fixed and mobile phones) with 1.053 billion subscribers as of 31 August 2016. It has one of the lowest call-tariffs in the world, due to fierce competition among telecom operators. India has the world's third-largest Internet user-base. As of 31 March 2016, there were 342.65 million Internet subscribers in the country.Industry estimates indicate that there are over 554 million TV consumers in India as of 2012. India is the largest direct-to-home (DTH) television market in the world by number of subscribers. As of May 2016, there were 84.80 million DTH subscribers in the country.
Mining contributed $63 billion (3% of GDP) and employed 20.14 million people (5% of the workforce) in 2016. India's mining industry was the fourth-largest producer of minerals in the world by volume, and eighth-largest producer by value in 2009. In 2013, it mined and processed 89 minerals, of which four were fuel, three were atomic energy minerals, and 80 non-fuel. The government-owned public sector accounted for 68% of mineral production by volume in 2011–12.Nearly 50% of India's mining industry, by output value, is concentrated in eight states: Odisha, Rajasthan, Chhattisgarh, Andhra Pradesh, Telangana, Jharkhand, Madhya Pradesh and Karnataka. Another 25% of the output by value comes from offshore oil and gas resources. India operated about 3,000 mines in 2010, half of which were coal, limestone and iron ore. On output-value basis, India was one of the five largest producers of mica, chromite, coal, lignite, iron ore, bauxite, barite, zinc and manganese; while being one of the ten largest global producers of many other minerals. India was the fourth-largest producer of steel in 2013, and the seventh-largest producer of aluminium.India's mineral resources are vast. However, its mining industry has declined – contributing 2.3% of its GDP in 2010 compared to 3% in 2000, and employed 2.9 million people – a decreasing percentage of its total labour. India is a net importer of many minerals including coal. India's mining sector decline is because of complex permit, regulatory and administrative procedures, inadequate infrastructure, shortage of capital resources, and slow adoption of environmentally sustainable technologies.
Until the liberalisation of 1991, India was largely and intentionally isolated from world markets, to protect its economy and to achieve self-reliance. Foreign trade was subject to import tariffs, export taxes and quantitative restrictions, while foreign direct investment (FDI) was restricted by upper-limit equity participation, restrictions on technology transfer, export obligations and government approvals; these approvals were needed for nearly 60% of new FDI in the industrial sector. The restrictions ensured that FDI averaged only around $200 million annually between 1985 and 1991; a large percentage of the capital flows consisted of foreign aid, commercial borrowing and deposits of non-resident Indians. India's exports were stagnant for the first 15 years after independence, due to general neglect of trade policy by the government of that period; imports in the same period, with early industrialisation, consisted predominantly of machinery, raw materials and consumer goods. Since liberalisation, the value of India's international trade has increased sharply, with the contribution of total trade in goods and services to the GDP rising from 16% in 1990–91 to 47% in 2009–10. Foreign trade accounted for 48.8% of India's GDP in 2015. Globally, India accounts for 1.44% of exports and 2.12% of imports for merchandise trade and 3.34% of exports and 3.31% of imports for commercial services trade. India's major trading partners are the European Union, China, the United States and the United Arab Emirates. In 2006–07, major export commodities included engineering goods, petroleum products, chemicals and pharmaceuticals, gems and jewellery, textiles and garments, agricultural products, iron ore and other minerals. Major import commodities included crude oil and related products, machinery, electronic goods, gold and silver. In November 2010, exports increased 22.3% year-on-year to ₹851 billion (equivalent to ₹1.5 trillion or US$22 billion in 2019), while imports were up 7.5% at ₹1,251 billion (equivalent to ₹2.3 trillion or US$32 billion in 2019). The trade deficit for the same month dropped from ₹469 billion (equivalent to ₹950 billion or US$13 billion in 2019) in 2009 to ₹401 billion (equivalent to ₹720 billion or US$10 billion in 2019) in 2010.India is a founding-member of General Agreement on Tariffs and Trade (GATT) and its successor, the WTO. While participating actively in its general council meetings, India has been crucial in voicing the concerns of the developing world. For instance, India has continued its opposition to the inclusion of labour, environmental issues and other non-tariff barriers to trade in WTO policies.India secured 43rd place in competitiveness index.
Since independence, India's balance of payments on its current account has been negative. Since economic liberalisation in the 1990s, precipitated by a balance-of-payment crisis, India's exports rose consistently, covering 80.3% of its imports in 2002–03, up from 66.2% in 1990–91. However, the global economic slump followed by a general deceleration in world trade saw the exports as a percentage of imports drop to 61.4% in 2008–09. India's growing oil import bill is seen as the main driver behind the large current account deficit, which rose to $118.7 billion, or 11.11% of GDP, in 2008–09. Between January and October 2010, India imported $82.1 billion worth of crude oil. The Indian economy has run a trade deficit every year from 2002 to 2012, with a merchandise trade deficit of US$189 billion in 2011–12. Its trade with China has the largest deficit, about $31 billion in 2013.India's reliance on external assistance and concessional debt has decreased since liberalisation of the economy, and the debt service ratio decreased from 35.3% in 1990–91 to 4.4% in 2008–09. In India, external commercial borrowings (ECBs), or commercial loans from non-resident lenders, are being permitted by the government for providing an additional source of funds to Indian corporates. The Ministry of Finance monitors and regulates them through ECB policy guidelines issued by the Reserve Bank of India (RBI) under the Foreign Exchange Management Act of 1999. India's foreign exchange reserves have steadily risen from $5.8 billion in March 1991 to ₹38,832.21 billion (US$540 billion) in July 2020. In 2012, the United Kingdom announced an end to all financial aid to India, citing the growth and robustness of Indian economy.India's current account deficit reached an all-time high in 2013. India has historically funded its current account deficit through borrowings by companies in the overseas markets or remittances by non-resident Indians and portfolio inflows. From April 2016 to January 2017, RBI data showed that, for the first time since 1991, India was funding its deficit through foreign direct investment inflows. The Economic Times noted that the development was "a sign of rising confidence among long-term investors in Prime Minister Narendra Modi's ability to strengthen the country's economic foundation for sustained growth".
As the third-largest economy in the world in PPP terms, India has attracted foreign direct investment (FDI). During the year 2011, FDI inflow into India stood at $36.5 billion, 51.1% higher than the 2010 figure of $24.15 billion. India has strengths in telecommunication, information technology and other significant areas such as auto components, chemicals, apparels, pharmaceuticals, and jewellery. Despite a surge in foreign investments, rigid FDI policies were a significant hindrance. Over time, India has adopted a number of FDI reforms. India has a large pool of skilled managerial and technical expertise. The size of the middle-class population stands at 300 million and represents a growing consumer market.India liberalised its FDI policy in 2005, allowing up to a 100% FDI stake in ventures. Industrial policy reforms have substantially reduced industrial licensing requirements, removed restrictions on expansion and facilitated easy access to foreign technology and investment. The upward growth curve of the real-estate sector owes some credit to a booming economy and liberalised FDI regime. In March 2005, the government amended the rules to allow 100% FDI in the construction sector, including built-up infrastructure and construction development projects comprising housing, commercial premises, hospitals, educational institutions, recreational facilities, and city- and regional-level infrastructure. Between 2012 and 2014, India extended these reforms to defence, telecom, oil, retail, aviation, and other sectors.From 2000 to 2010, the country attracted $178 billion as FDI. The inordinately high investment from Mauritius is due to routing of international funds through the country given significant tax advantages – double taxation is avoided due to a tax treaty between India and Mauritius, and Mauritius is a capital gains tax haven, effectively creating a zero-taxation FDI channel. FDI accounted for 2.1% of India's GDP in 2015.As the government has eased 87 foreign investment direct rules across 21 sectors in the last three years, FDI inflows hit $60.1 billion between 2016 and 2017 in India.
Since 2000, Indian companies have expanded overseas, investing FDI and creating jobs outside India. From 2006 to 2010, FDI by Indian companies outside India amounted to 1.34 per cent of its GDP. Indian companies have deployed FDI and started operations in the United States, Europe and Africa. The Indian company Tata is the United Kingdom's largest manufacturer and private-sector employer.
The Indian rupee (₹) is the only legal tender in India, and is also accepted as legal tender in neighbouring Nepal and Bhutan, both of which peg their currency to that of the Indian rupee. The rupee is divided into 100 paise. The highest-denomination banknote is the ₹2,000 note; the lowest-denomination coin in circulation is the 50 paise coin. Since 30 June 2011, all denominations below 50 paise have ceased to be legal currency. India's monetary system is managed by the Reserve Bank of India (RBI), the country's central bank. Established on 1 April 1935 and nationalised in 1949, the RBI serves as the nation's monetary authority, regulator and supervisor of the monetary system, banker to the government, custodian of foreign exchange reserves, and as an issuer of currency. It is governed by a central board of directors, headed by a governor who is appointed by the Government of India. The benchmark interest rates are set by the Monetary Policy Committee. The rupee was linked to the British pound from 1927 to 1946, and then to the US dollar until 1975 through a fixed exchange rate. It was devalued in September 1975 and the system of fixed par rate was replaced with a basket of four major international currencies: the British pound, the US dollar, the Japanese yen and the Deutsche Mark. In 1991, after the collapse of its largest trading partner, the Soviet Union, India faced the major foreign exchange crisis and the rupee was devalued by around 19% in two stages on 1 and 2 July. In 1992, a Liberalized Exchange Rate Mechanism (LERMS) was introduced. Under LERMS, exporters had to surrender 40 percent of their foreign exchange earnings to the RBI at the RBI-determined exchange rate; the remaining 60% could be converted at the market-determined exchange rate. In 1994, the rupee was convertible on the current account, with some capital controls.After the sharp devaluation in 1991 and transition to current account convertibility in 1994, the value of the rupee has been largely determined by market forces. The rupee has been fairly stable during the decade 2000–2010. In October 2018, rupee touched an all-time low 74.90 to the US dollar.
In May 2014, the World Bank reviewed and proposed revisions to its poverty calculation methodology of 2005 and purchasing-power-parity basis for measuring poverty. According to the revised methodology, the world had 872.3 million people below the new poverty line, of which 179.6 million lived in India. With 17.5% of the total world's population, India had a 20.6% share of world's poorest in 2013. According to a 2005–2006 survey, India had about 61 million children under the age of 5 who were chronically malnourished. A 2011 UNICEF report stated that between 1990 and 2010, India achieved a 45 percent reduction in under age 5 mortality rates, and now ranks 46th of 188 countries on this metric.Since the early 1960s, successive governments have implemented various schemes to alleviate poverty, under central planning, that have met with partial success. In 2005, the government enacted the Mahatma Gandhi National Rural Employment Guarantee Act (MGNREGA), guaranteeing 100 days of minimum wage employment to every rural household in all the districts of India. In 2011, it was widely criticised and beset with controversy for corrupt officials, deficit financing as the source of funds, poor quality of infrastructure built under the programme, and unintended destructive effects. Other studies suggest that the programme has helped reduce rural poverty in some cases. Yet other studies report that India's economic growth has been the driver of sustainable employment and poverty reduction, though a sizeable population remains in poverty. India lifted 271 million people out of poverty between 2006 and 2016, recording the fastest reductions in the multidimensional poverty index values during the period with strong improvements in areas such as assets, cooking fuel, sanitation and nutrition.On the 2019 Global Hunger Index India ranked 102nd (out of 117 countries), being categorized as 'serious' in severity.
Agricultural and allied sectors accounted for about 52.1% of the total workforce in 2009–10. While agriculture employment has fallen over time in percentage of labour employed, services which includes construction and infrastructure have seen a steady growth accounting for 20.3% of employment in 2012–13. Of the total workforce, 7% is in the organised sector, two-thirds of which are in the government-controlled public sector. About 51.2% of the workforce in India is self-employed. According to a 2005–06 survey, there is a gender gap in employment and salaries. In rural areas, both men and women are primarily self-employed, mostly in agriculture. In urban areas, salaried work was the largest source of employment for both men and women in 2006.Unemployment in India is characterised by chronic (disguised) unemployment. Government schemes that target eradication of both poverty and unemployment – which in recent decades has sent millions of poor and unskilled people into urban areas in search of livelihoods – attempt to solve the problem by providing financial assistance for starting businesses, honing skills, setting up public sector enterprises, reservations in governments, etc. The decline in organised employment, due to the decreased role of the public sector after liberalisation, has further underlined the need for focusing on better education and created political pressure for further reforms. India's labour regulations are heavy, even by developing country standards, and analysts have urged the government to abolish or modify them to make the environment more conducive for employment generation. The 11th five-year plan has also identified the need for a congenial environment to be created for employment generation, by reducing the number of permissions and other bureaucratic clearances required. Inequalities and inadequacies in the education system have been identified as an obstacle, which prevents the benefits of increased employment opportunities from reaching all sectors of society.Child labour in India is a complex problem that is rooted in poverty. Since the 1990s, the government has implemented a variety of programs to eliminate child labour. These have included setting up schools, launching free school lunch programs, creating special investigation cells, etc. Author Sonalde Desai stated that recent studies on child labour in India have found some pockets of industries in which children are employed, but overall, relatively few Indian children are employed. Child labour below the age of 10 is now rare. In the 10–14 age group, the latest surveys find only 2% of children working for wage, while another 9% work within their home or rural farms assisting their parents in times of high work demand such as sowing and harvesting of crops.India has the largest diaspora around the world, an estimated 16 million people, many of whom work overseas and remit funds back to their families. The Middle East region is the largest source of employment of expat Indians. The crude oil production and infrastructure industry of Saudi Arabia employs over 2 million expat Indians. Cities such as Dubai and Abu Dhabi in United Arab Emirates have employed another 2 million Indians during the construction boom in recent decades. In 2009–10, remittances from Indian migrants overseas stood at ₹2,500 billion (equivalent to ₹4.5 trillion or US$63 billion in 2019), the highest in the world, but their share in FDI remained low at around 1%.
Corruption has been a pervasive problem in India. A 2005 study by Transparency International (TI) found that more than half of those surveyed had first-hand experience of paying a bribe or peddling influence to get a job done in a public office in the previous year. A follow-up study in 2008 found this rate to be 40 percent. In 2011, TI ranked India at 95th place amongst 183 countries in perceived levels of public sector corruption. By 2016, India saw a reduction in corruption and its ranking improved to 79th place.In 1996, red tape, bureaucracy and the Licence Raj were suggested as a cause for the institutionalised corruption and inefficiency. More recent reports suggest the causes of corruption include excessive regulations and approval requirements, mandated spending programs, monopoly of certain goods and service providers by government-controlled institutions, bureaucracy with discretionary powers, and lack of transparent laws and processes. Computerisation of services, various central and state vigilance commissions, and the 2005 Right to Information Act – which requires government officials to furnish information requested by citizens or face punitive action – have considerably reduced corruption and opened avenues to redress grievances.In 2011, the Indian government concluded that most spending fails to reach its intended recipients, as the large and inefficient bureaucracy consumes budgets. India's absence rates are among the worst in the world; one study found that 25% of public sector teachers and 40% of government-owned public-sector medical workers could not be found at the workplace. Similarly, there are many issues facing Indian scientists, with demands for transparency, a meritocratic system, and an overhaul of the bureaucratic agencies that oversee science and technology.India has an underground economy, with a 2006 report alleging that India topped the worldwide list for black money with almost $1,456 billion stashed in Swiss banks. This would amount to 13 times the country's total external debt. These allegations have been denied by the Swiss Banking Association. James Nason, the Head of International Communications for the Swiss Banking Association, suggested "The (black money) figures were rapidly picked up in the Indian media and in Indian opposition circles, and circulated as gospel truth. However, this story was a complete fabrication. The Swiss Bankers Association never published such a report. Anyone claiming to have such figures (for India) should be forced to identify their source and explain the methodology used to produce them." A recent step taken by Prime Minister Modi, on 8 November 2016, involved the demonetization of all 500 and 1000 rupee bank notes (replaced by new 500 and 2000 rupee notes) to return black money into the economy.
India has made progress increasing the primary education attendance rate and expanding literacy to approximately three-fourths of the population. India's literacy rate had grown from 52.2% in 1991 to 74.04% in 2011. The right to education at elementary level has been made one of the fundamental rights under the eighty-sixth Amendment of 2002, and legislation has been enacted to further the objective of providing free education to all children. However, the literacy rate of 74% is lower than the worldwide average and the country suffers from a high drop-out rate. Literacy rates and educational opportunities vary by region, gender, urban and rural areas, and among different social groups.
Poverty rates in India's poorest states are three to four times higher than those in the more advanced states. While India's average annual per capita income was $1,410 in 2011 – placing it among the poorest of the world's middle-income countries – it was just $436 in Uttar Pradesh (which has more people than Brazil) and only $294 in Bihar, one of India's poorest states. A critical problem facing India's economy is the sharp and growing regional variations among India's different states and territories in terms of poverty, availability of infrastructure and socio-economic development. Six low-income states – Assam, Chhattisgarh, Nagaland, Madhya Pradesh, Odisha and Uttar Pradesh – are home to more than one-third of India's population. Severe disparities exist among states in terms of income, literacy rates, life expectancy and living conditions.The five-year plans, especially in the pre-liberalisation era, attempted to reduce regional disparities by encouraging industrial development in the interior regions and distributing industries across states. The results have been discouraging as these measures increased inefficiency and hampered effective industrial growth. The more advanced states have been better placed to benefit from liberalisation, with well-developed infrastructure and an educated and skilled workforce, which attract the manufacturing and service sectors. Governments of less-advanced states have tried to reduce disparities by offering tax holidays and cheap land, and focused on sectors like tourism which can develop faster than other sectors. India's income Gini coefficient is 33.9, according to the United Nations Development Program (UNDP), indicating overall income distribution to be more uniform than East Asia, Latin America and Africa. The Global Wealth Migration Review 2019 report, published by New World Wealth, estimated that 48% of India's total wealth was held by high-net worth individuals.There is a continuing debate on whether India's economic expansion has been pro-poor or anti-poor. Studies suggest that economic growth has been pro-poor and has reduced poverty in India.
The development of Indian security markets began with the launch of the Bombay Stock Exchange (BSE) in July 1875 and Ahmedabad Stock exchange in 1894. Since then, 22 other exchanges have traded in Indian cities. In 2014, India's stock exchange market became the 10th largest in the world by market capitalisation, just above those of South Korea and Australia. India's two major stock exchanges, BSE and National Stock Exchange of India, had a market capitalisation of US$1.71 trillion and US$1.68 trillion as of February 2015, according to World Federation of Exchanges.The initial public offering (IPO) market in India has been small compared to NYSE and NASDAQ, raising US$300 million in 2013 and US$1.4 billion in 2012. Ernst & Young stated that the low IPO activity reflects market conditions, slow government approval processes and complex regulations. Before 2013, Indian companies were not allowed to list their securities internationally without first completing an IPO in India. In 2013, these security laws were reformed and Indian companies can now choose where they want to list first: overseas, domestically, or both concurrently. Further, security laws have been revised to ease overseas listings of already-listed companies, to increase liquidity for private equity and international investors in Indian companies.
Economic Advisory Council Economic development in India List of megaprojects in India Make in India – a government program to encourage manufacturing in India NITI Aayog Startup IndiaEvents: Late-2000s recession Oil price increases since 2003 Demonetization Economic impact of the COVID-19 pandemic in IndiaLists: List of companies of India List of largest companies in India List of the largest trading partners of India Trade unions in India Natural resources of India
Ministry of Finance Ministry of Commerce and Industry Ministry of Statistics and Programme Implementation India profile at the CIA World Factbook India profile at The World Bank India – OECD
The economy of Pakistan is the 23rd largest in the world in terms of purchasing power parity (PPP), and 42nd largest in terms of nominal gross domestic product. Pakistan has a population of over 220 million (the world's 5th-largest), giving it a nominal GDP per capita of $1,357 in 2019, which ranks 154th in the world and giving it a PPP GDP per capita of 5,839 in 2019, which ranks 132nd in the world for 2019. However, Pakistan's undocumented economy is estimated to be 36% of its overall economy, which is not taken into consideration when calculating per capita income. Pakistan is a developing country and is one of the Next Eleven countries identified by Jim O'Neill in a research paper as having a high potential of becoming, along with the BRICS countries, among the world's largest economies in the 21st century. The economy is semi-industrialized, with centres of growth along the Indus River. Primary export commodities include textiles, leather goods, sports goods, chemicals and carpets/rugs.Growth poles of Pakistan's economy are situated along the Indus River; the diversified economies of Karachi and major urban centers in the Punjab, coexisting with lesser developed areas in other parts of the country. The economy has suffered in the past from internal political disputes, a fast-growing population, mixed levels of foreign investment. Foreign exchange reserves are bolstered by steady worker remittances, but a growing current account deficit – driven by a widening trade gap as import growth outstrips export expansion – could draw down reserves and dampen GDP growth in the medium term. Pakistan is currently undergoing a process of economic liberalization, including privatization of all government corporations, aimed to attract foreign investment and decrease budget deficit. In October 2016, foreign currency reserves crossed $24.0 billion which has led to stable outlook on the long-term rating by Standard & Poor's. In 2016, BMI Research report named Pakistan as one of the ten emerging economies with a particular focus on its manufacturing hub.As of May 2019, IMF has predicted that future growth rates will be 2.9%, the lowest in South Asia. According to the World Bank, poverty in Pakistan fell from 64.3% in 2002 to 29.5% in 2014. The country's worsening macroeconomic position has led to Moody's downgrading Pakistan's debt outlook to "negative".In 2017, Pakistan's GDP in terms of purchasing power parity crossed $1 trillion. By May 2019, the Pakistani rupee had undergone a year-on-year depreciation of 30% vis-a-vis the US Dollar.
In 2016, the Atlantic Media Company (AMC) of the United States has ranked Pakistan as a relatively stronger economy in the South Asian markets and expected that it will grow rapidly during days ahead. AMC said that during the period January–July this year, Indian 100 point index was 6.67% while Karachi Stock Exchange (KSE) had achieved 100 point index of 17 percent.
the Pakistan's economy has been characterised as unstable and highly vulnerable to external and internal shocks. However, the economy proved to be unexpectedly resilient in the face of multiple adverse events concentrated into a four-year (1998–2002) period — the Asian financial crisis; economic sanctions – according to Colin Powell, Pakistan was "sanctioned to the eyeballs"; The global recession of 2001–2002; a severe drought – the worst in Pakistan's history, lasting about four years; the post-9/11 military action in neighbouring Afghanistan, with a massive influx of refugees from that country;
According to many sources, the Pakistani government has made substantial economic reforms since 2000, and medium-term prospects for job creation and poverty reduction are the best in nearly a decade. In 2005, the World Bank reported that "Pakistan was the top reformer in the region and the number 10 reformer globally – making it easier to start a business, reducing the cost to register property, increasing penalties for violating corporate governance rules, and replacing a requirement to license every shipment with two-year duration licences for traders."
The World Bank (WB) and International Finance Corporation's flagship report Ease of Doing Business Index 2020 ranked Pakistan 108 among 190 countries around the globe, indicating a continuous improvement and taking a jump from 136 last year. The top five countries were New Zealand, Singapore, Denmark, Hong Kong and South Korea.With improvement in ease of doing business ranking and giving an investment friendly road map from government, many new auto sector giants like France's Renault, South Korean's Hyundai and Kia, Chinese JW Forland and German auto giant Volkswagen are considering entry in Pakistan auto market through joint ventures with local manufacturers like Dewan Farooque Motors, Khalid Mushtaq Motors and United Motors.US oil and gas giant Exxon Mobil has again returned to Pakistan after nearly three decades gap and has acquired 25% shares in offshore drilling in May 2018, with initial survey showing a potential of huge hydrocarbon reserves discovery at offshore.With recent agreement from Saudi Arabia to invest more than US dollar 15 billion in establishing a mega oil refinery and petrochemical industry in Gwadar more commitments for investments are on its way to come in this sector especially from UAE, Qatar, Malaysia and Italy. To boost Pakistan's unstable foreign-exchange reserves, Qatar announced to invest $3 billion the form of deposits and direct investments in the country. By the end of June 2019, Qatar sent the first $500 million to Pakistan.
These are the economic indicators of Pakistan from Fiscal Year 2004 to 2020.
Data is from Pakistan Bureau of Statistics.
Data is from Ministry of Finance and PBS.
Data is from Pakistan Bureau of Statistics.
Data is from Ministry of Finance.
Data is taken from Ministry of Finance.
Total Public Debt = Gross Public Debt + External Liabilities + Private Sector External Debt + PSEs External Debt + PSEs Domestic Debt + Commodity Operations + Intercompany External Debt from Direct Investor abroad Gross Public Debt = Government (Federal+Provincial) Domestic Debt + Government (Federal+Provincial) External Debt + Debt from IMF Total Debt of Government / Net Public Debt = Gross Public Debt – Government Deposits in the Banking System. Public External Debt = Government External Debt + Debt from IMF (Foreign Exchange Liabilities are not included) Total External Debt = Public External Debt + Public Sector Enterprises + Banks + Private Sector + Debt Liabilities to Direct Investors Data is taken from state bank of Pakistan.
Data is taken from State bank of Pakistan.
Data is taken from SBP and Ministry of Finance.
In the first four years of the twenty-first century, Pakistan's KSE 100 Index was the best-performing stock market index in the world as declared by the international magazine "Business Week". The stock market capitalisation of listed companies in Pakistan was valued at $5,937 million in 2005 by the World Bank. But in 2008, after the General Elections, uncertain political environment, rising militancy along western borders of the country, and mounting inflation and current account deficits resulted in the steep decline of the Karachi Stock Exchange. As a result, the corporate sector of Pakistan has declined dramatically in recent times. However, the market bounced back strongly in 2009 and the trend continues in 2011. By 2014 the stock market burst into uncharted territories as the benchmark KSE 100 Index rose 907 points (3.1%) and shot past the 30,000-point barrier to close at a new record high, this came days after Moody's announced that it was upgrading the outlook of 5 major Pakistani banks from Negative to Stable, resulting in heavy buying in the banking sector. The rally was supported by heavy buying in the oil and gas and cement sectors. On 11 January 2016, aimed to help reduce market fragmentation and create a strong case for attracting strategic partnerships necessary for providing technological expertise all the three stock exchanges including Karachi Stock Exchange, Lahore Stock Exchange and Islamabad Stock Exchange were inducted into a unified Pakistan Stock Exchange. In May 2017 American provider of stock market indexes and analysis tools, MSCI has confirmed that the Pakistan Stock Exchange (PSX) has been reclassified from Frontier Markets to Emerging Markets in its semi-annual index review. Euphoria over the stock exchange's reclassification as an emerging market propelled the PSE-100 Index past another milestone when the Index recorded an increase of 636.96 points, or 1.23%, to end at 52,387.87. In the fiscal year 2018, the stock market showed a negative growth of 7.1% over the last fiscal year and stood at 47000 points at average.
As of 2017, according to Wall Street Journal, citing estimates largely based on income and the purchase of consumption goods, had suggested that as many as 42% of Pakistan's population may now belong to the upper and middle classes. If these numbers are correct, or even indicative in any broad sense, then 87 million Pakistanis belong to the middle and upper classes, a population size which is larger than that of Germany. Official figures also show that the proportion of households that own a motorcycle and washing machines has grown impressively over the past 15 years. Furthermore, the IBA-SBP Consumer Confidence Index recorded its highest-ever level of 174.9 points in January 2017, showing an increase of 17 points from July 2016. Separately, consumer financing posted an increase of Rs37.6 billion during first half of the current fiscal year of 2017. Auto finance continued to be the dominated segment, while personal loans showed a pickup as well. "The net credit off-take of Rs13.7 billion of personal loans witnessed in first half of the fiscal year 2017 is the highest half-year figure in about a decade," the report stated.
The high population growth in the past few decades has ensured that a very large number of young people are now entering the labor market. Even though it is among the six most populous Asian nations. In the past, excessive red tape made firing from jobs, and consequently hiring, difficult. Significant progress in taxation and business reforms has ensured that many firms now are not compelled to operate in the underground economy."In 2016 government took a remarkable initiative by announcing the Prime Minister's Youth Program to combat unemployment in the country. This program has a broad canvas of schemes enabling youth and poor segment of society to get better employment opportunities, economic empowerment, acquiring skills needed for gainful employment, access to IT and imparting on-the-job training for young graduates to improve the probability of getting a productive job. Prime Minister’s Youth Program includes six schemes which are Prime Minister’s Youth Business Loan Scheme, Prime Minister’s Interest Free Loan Scheme, Prime Minister’s Youth Skill Development Program, Prime Minister’s Program for Provision of Laptops to Talented Students, Prime Minister’s Fee Reimbursement Scheme,Prime Minister’s Youth Training Scheme". Government sector is also contributing in employment and according to estimate 4.5 million people are employed by federal, provincial and local governments in different sectors from Armed forces to education and health.
Tourism in Pakistan has been stated as being the tourism industry's "next big thing". Pakistan, with its diverse cultures, people and landscapes, has attracted 90 million tourists to the country, almost double to that of a decade ago. Due to threat of terrorism the number of foreigner tourists has gradually declined and the shock of 2013 Nanga Parbat tourist shooting has terribly adversely effected the tourism industry. As of 2016 tourism has begun to recover in Pakistan, albeit gradually.
Although the country is a Federation with constitutional division of taxation powers between the Federal Government and the four provinces, the revenue department of the Federal Government, the Federal board of Revenue, collects almost 86% of the entire national tax collection. The Federal Board of Revenue collected 3.842 trillion rupees in taxes against the revised target of 3.935 trillion rupees in the fiscal year 2017–2018. In FY 2013, FBR tax collection was Rs.1,946 billion. So in only 5 years it almost double its tax revenue which is a phenomenal achievement.
The Pakistani rupee depreciated against the US dollar until around the start of the 21st century, when Pakistan's large current-account surplus pushed the value of the rupee up versus the dollar. Pakistan's central bank then stabilised by lowering interest rates and buying dollars, in order to preserve the country's export competitiveness
Pakistan maintains foreign reserves with State Bank of Pakistan. The currency of the reserves was solely US dollar incurring speculated losses after the dollar prices fell during 2005, forcing the then Governor SBP Ishrat Hussain to step down. In the same year the SBP issued an official statement proclaiming diversification of reserves in currencies including Euro and Yen, withholding ratio of diversification. Following the international credit crisis and spikes in crude oil prices, Pakistan's economy could not withstand the pressure and on 11 October 2008, State Bank of Pakistan reported that the country's foreign exchange reserves had gone down by $571.9 million to $7749.7 million. The foreign exchange reserves had declined more by $10 billion to a level of $6.59 billion. in June 2013 Pakistan was on the brink of default on its financial commitments. Country's Forex reserves were at an historic low covering only two weeks' worth of imports. In January 2020, Pakistan's Foreign exchange reserves stood at US$11.503 b.
Agriculture accounted for about 53% of GDP in 1947. While per-capita agricultural output has grown since then, it has been outpaced by the growth of the non-agricultural sectors, and the share of agriculture has dropped to roughly one-fifth of Pakistan's economy. In recent years, the country has seen rapid growth in industries (such as apparel, textiles, and cement) and services (such as telecommunications, transportation, advertising, and finance).
The most important crops are wheat, sugarcane, cotton, and rice, which together account for more than 75% of the value of total crop output. Pakistan's largest food crop is wheat. In 2017, Pakistan produced 26,674,000 tonnes of wheat, almost equal to all of Africa (27.1 million tonnes) and more than all of South America (25.9 million tonnes), according to the FAOSTAT. In the previous market year of 2018/19 Pakistan exported a record 4.5 million tonnes of rice as compared to around 4 MMT during the corresponding period last year. Pakistan has also cut the use of dangerous pesticides dramatically.Pakistan is a net food exporter, except in occasional years when its harvest is adversely affected by droughts. Pakistan exports rice, cotton, fish, fruits (especially Oranges and Mangoes), and vegetables and imports vegetable oil, wheat, pulses and consumer foods. The country is Asia's largest camel market, second-largest apricot and ghee market and third-largest cotton, onion and milk market. The economic importance of agriculture has declined since independence, when its share of GDP was around 53%. Following the poor harvest of 1993, the government introduced agriculture assistance policies, including increased support prices for many agricultural commodities and expanded availability of agricultural credit. From 1993 to 1997, real growth in the agricultural sector averaged 5.7% but has since declined to about 4%. Agricultural reforms, including increased wheat and oil seed production, play a central role in the government's economic reform package. Majority of the population, directly or indirectly, dependent on this sector. It contributes about 18.5% percent of Gross Domestic Product (GDP) and accounts for 37.4% of employed labor force and is the largest source of foreign exchange earnings. During 2017–18, agriculture sector recorded a remarkable growth of 3.70 percent and surpassed its targeted growth of 3.5 percent and last year's growth of 2.18 percent. All the major crops showed a positive trend in their production except maize. Sugarcane and rice production surpassed their historic level with 82.1 and 7.4 million tons respectively. Pakistan Bureau of Statistics provisionally valued this sector at Rs. 7,764,218 million for the year 2018 thus registering the growth of 6.1% over the last year. Again in 2018–19, Agriculture sector did not hit its target growth and only grew by 0.85%. Major crops except wheat and maize fell below their previous year output. Pakistan's Top commodities productions in the last 3 fiscal years are : Pakistan's principal natural resources are arable land and water. About 25% of Pakistan's total land area is under cultivation and is watered by one of the largest irrigation systems in the world. Pakistan irrigates three times more acres than Russia. Pakistan agriculture also benefits from year round warmth. Agriculture accounts for about 18.9% of GDP and employs about 42.3% of the labour force. Zarai Taraqiati Bank Limited is the largest financial institution geared towards the development of agriculture sector through provision of financial services and technical expertise.
Pakistan is endowed with significant mineral resources and is emerging as a very promising area for prospecting/exploration for mineral deposits. Based on available information, the country's more than 6,00,000 km² of outcrops area demonstrates varied geological potential for metallic and non-metallic mineral deposits. In the wake of 18th amendment to the constitution all the provinces are free to exploit and explore the mineral resources which are in their jurisdiction. Mining and quarrying contributes 13.19% in industrial sector and its share in GDP is 2.8%. Pakistan mining and quarrying sector grew by 3.04% in 2018 against the negative growth of −0.38% last year. In the recent past, exploration by government agencies as well as by multinational mining companies presents ample evidence of the occurrences of sizeable minerals deposits. Recent discoveries of a thick oxidised zone underlain by sulphide zones in the shield area of the Punjab province, covered by thick alluvial cover have opened new vistas for metallic minerals exploration. Pakistan has a large base for industrial minerals. The discovery of coal deposits having over 175 billion tonnes of reserves at Thar in the Sindh province has given an impetus to develop it as an alternative source of energy. There is vast potential for precious and dimension stones. Extraction of principal minerals in the last 5 fiscal years is given in the table below :-
Pakistan's industrial sector accounts for approximately 18.17% of GDP. In 2018 it recorded a growth of 5.80% as compared to the growth of 5.43% in 2017. Manufacturing is the largest of Pakistan's industrial sectors, accounting for approximately 12.13% of GDP. Manufacturing sub-sector is further divided in three components including large-scale manufacturing (LSM) with the share of 79.6% percent in manufacturing sector, small scale manufacturing share is 13.8 percent in manufacturing sector, while slaughtering contributes 6.5 percent in the manufacturing. Major sectors in industries include cement, fertiliser, edible oil, sugar, steel, tobacco, chemicals, machinery, food processing and medical instruments, primarily surgical. Pakistan is one of the largest manufacturers and exporters of surgical instruments.The government is privatizing large-scale industrial units, and the public sector accounts for a shrinking proportion of industrial output, while growth in overall industrial output (including the private sector) has accelerated. Government policies aim to diversify the country's industrial base and bolster export industries. Large Scale Manufacturing is the fastest-growing sector in Pakistani economy. Major Industries include textiles, fertiliser, cement, oil refineries, dairy products, food processing, beverages, construction materials, clothing, paper products and shrimp. In Pakistan SMEs have a significant contribution in the total GDP of Pakistan, according to SMEDA and Economic survey reports, the share in the annual GDP is 40% likewise SMEs generating significant employment opportunities for skilled workers and entrepreneurs. Small and medium scale firms represent nearly 90% of all the enterprises in Pakistan and employ 80% of the non-agricultural labor force. These figures indicate the potential and further growth in this sector.Pakistan's largest corporations are mostly involved in utilities like oil, gas, power generation/distribution and telecommunication:
In 1947, Pakistan had inherited four cement plants with a total capacity of 0.5 million tons. Some expansion took place in 1956–66 but could not keep pace with the economic development and the country had to resort to imports of cement in 1976–77 and continued to do so until 1994–95. The cement sector consisting of 27 plants is contributing above Rs 30 billion to the national exchequer in the form of taxes. However, by 2013, Pakistan's cement is fast-growing mainly because of demand from Afghanistan and countries boosting real estate sector, In 2013 Pakistan exported 7,708,557 metric tons of cement. Pakistan has installed capacity of 44,768,250 metric tons of cement and 42,636,428 metric tons of clinker. In the 2012–2013 cement industry in Pakistan became the most profitable sector of economy.
The information communication technology (ICT) industry grossed over $4.8 billion in 2013. It is expected to exceed the $13 billion mark by 2018. A marked increase in software export figures are an indication of this booming industry's potential. The total number of IT companies increased to 1306 and the total estimated size of IT industry is $2.8 billion. In 2007, Pakistan was for the first time featured in the Global Services Location Index by A.T. Kearney and was rated as the 30th best location for offshoring. By 2009, Pakistan had improved its rank by ten places to reach 20th. According to Pakistan Startup report, there are about 1 million freelancers working from Pakistan mainly via Elance, Upwork, Fiverr, Guru, and freelancer – world's famous online marketplaces that count Pakistan among top 5 freelancing nations. An annual report that updated by State bank of Pakistan shows Pakistan cross 1 billion ($) IT Export which is a good achievement of Pakistan IT Industry. Also, an official said that Pakistan Freelance community generate 1 billion ($) revenue this year. Overall Pakistan makes 2 billion ($) IT export worldwide.
The defence industry of Pakistan, under the Ministry of Defence Production, was created in September 1951 to promote and coordinate the patchwork of military production facilities that have developed since independence. It is currently actively participating in many joint production projects such as Al Khalid 2, advance trainer aircraft, combat aircraft, navy ships and submarines. Pakistan is manufacturing and selling weapons to over 40 countries, bringing in $20 million annually. The country's arms imports increased by 119 percent between the 2004–2008 and 2009–13, with China providing 54pc and the USA 27pc of Pakistan's imports.
Most of the Textile Industry is established in Punjab. Before 1990, the situation was different; most of the industry was in Karachi. Textile industry in Pakistan is traditional and conservative, producing and exporting most of low cost raw articles e.g. Raw Cotton, Yarn, fabric etc. Share of finished goods and branded articles is nominal. Pakistan has a potential to quadruple its textile production and export, due to emerging Chinese markets and with its existing infrastructure. 10% of United States imports regarding clothing and other form of textiles is covered by Pakistan.The textile sector accounts for 70% of Pakistan's exports, but the working conditions of workers are deplorable. Small manufacturing workshops generally do not sign employment contracts, do not respect the minimum wage and sometimes employ children. Violations of labour law also occur among major subcontractors of international brands, where workers may be beaten, insulted by their superiors or paid below the minimum wage. Factories do not comply with safety standards, leading to accidents: in 2012, 255 workers died in a fire at a Karachi factory. With 547 labour inspectors in Pakistan supervising the country's 300,000 factories, the textile industry is out of control. Nor are workers protected by trade unions, which are prohibited in industrial export zones. Elsewhere, "workers involved in the creation of trade unions are victims of violence, intimidation, threats or dismissals".
Pakistan's service sector accounts for about 60.2% of GDP. Transport, storage, communications, finance, and insurance account for 24% of this sector, and wholesale and retail trade about 30%. Pakistan is trying to promote the information industry and other modern service industries through incentives such as long-term tax holidays.
After the deregulation of the telecommunication industry, the sector has seen an exponential growth. Pakistan Telecommunication Company Ltd has emerged as a successful Forbes 2000 conglomerate with over US$1 billion in sales in 2005. The mobile telephone market has exploded many-fold since 2003 to reach a subscriber base of 140 million users in July 2017, one of the highest mobile teledensities in the entire world. In addition, there are over 6 million landlines in the country with 100% fibre-optic network and coverage via WLL in even the remotest areas. As a result, Pakistan won the prestigious Government Leadership award of GSM Association in 2006.The World Bank estimates that it takes about 3 days to get a phone connection in Pakistan.In Pakistan, the following are the top mobile phone operators: Jazz Pakistan (Parent: VEON, Netherland) Ufone (Parent: PTCL (Etisalat), Pakistan/UAE) Telenor (Parent: Telenor, Norway) Zong (Parent: China Mobile, China) By March 2009, Pakistan had 91 million mobile subscribers – 25 million more subscribers than reported in the same period in 2008. In addition to the 3.1 million fixed lines, while as many as 2.4 million are using Wireless Local Loop connections. Sony Ericsson, Nokia and Motorola along with Samsung and LG remain the most popular brands among customers.Since liberalisation, over the past four years, the Pakistani telecom sector has attracted more than $9 billion in foreign investments. During 2007–08, the Pakistani communication sector alone received $1.62 billion in Foreign Direct Investment (FDI) – about 30% of the country's total foreign direct investment. Present growth of state-of-the-art infrastructures in the telecoms sector during the last four years has been the result of the PTA's vision and implementation of the deregulation policy. Paging and mobile (cellular) telephones were adopted early and freely. Cellular phones and the Internet were adopted through a rather laissez-faire policy with a proliferation of private service providers that led to the fast adoption. With a rapid increase in the number of Internet users and ISPs, and a large English-speaking population, Pakistani society has seen an unparalleled revolution in communications. According to the PC World, a total of 6.37 billion text messages were sent through Acision messaging systems across Asia Pacific over the 2008/2009 Christmas and New Year period. Pakistan was amongst the top five ranker with one of the highest SMS traffic with 763 million messages. On 14 August 2010, Pakistan became the first country in the world to experience EVDO's RevB 3G technology that offers maximum speeds of 9.3 Mbit/s. 3G and 4G was simultaneously launched in Pakistan on 23 April 2014 through a SMRA Auction. Three out of Five Companies got a 3G licence i.e. Ufone, Mobilink and Telenor while China Mobile's Zong got 3G as well as a 4G licence. Whereas fifth company, Warid Pakistan did not participate in the auction procedure, But they launched 4G LTE services on their existing 2G 1800 MHz spectrum due to Technology neutral terms and became world's first Telecom Company to transform directly from 2G to 4G. With that Pakistan joined the 3G and 4G world. In December 2017, 3G and 4G subscribers in Pakistan reached to 46 millions.Pakistan is ranked 4th in terms of broadband Internet growth in the world, as the subscriber base of broadband Internet has been increasing rapidly. The rankings are released by Point Topic Global broadband analysis, a global research centre. Pakistan has more than 20 million Internet users in 2009. The country is said to have a potential to absorb up to 50 million mobile phone Internet users in the next 5 years thus a potential of nearly 1 million connections per month. Almost all of the main government departments, organisations and institutions have their own websites. The use of search engines and instant messaging services is also booming. Pakistanis are some of the most ardent chatters on the Internet, communicating with users all over the world. Recent years have seen a huge increase in the use of online marriage services, for example, leading to a major re-alignment of the tradition of arranged marriages. Biometric reverification of SIMs in 2015 had an adverse impact on the cellular subscriber base when subscribers count dropped from 139.9 million to 114.6 million. However, the industry has survived through the tough period and continues to regain subscribers at a fast pace.According to the report released by PTA for the FY 2018–19 :- Total teledensity of Pakistan reached at 77.7%. Telecom revenues were reached to Rs. 551.9 billion. Total contribution of telecom sector to the national exchequer was RS. 95.8 billion. Investment came to the telecom sector was US$635.3 million.as of August 2020 there were four cell phone companies including PMCL (Jazz), Telenor, Ufone and Zong operating in the country with nearly 169 million mobile phone subscribers.Pakistan Telecommunication Authority released the figures in August 2020 that Broadband subscribers in the country reached to approximately 87 millions.
Pakistan International Airlines, the flagship airline of Pakistan's civil aviation industry, has turnover exceeding $25 billion in 2015. The government announced a new shipping policy in 2006 permitting banks and financial institutions to mortgage ships. Private sector airlines in Pakistan include Airblue, which serves the main cities within Pakistan in addition to destinations in the Persian Gulf and Manchester in the United Kingdom. A massive rehabilitation plan worth $1 billion over five years for Pakistan Railways has been announced by the government in 2005. A new rail link trial has been established from Islamabad to Istanbul, via the Iranian cities of Zahedan, Kerman and Tehran. It is expected to promote trade, tourism, especially for exports destined for Europe (as Turkey is part of Europe and Asia).
Pakistan has a large and diverse banking system. In 1974, a nationalization program led to the creation of six government-owned banks. A privatization program in the 1990s lead to the entry of foreign-owned and local banks into the industry. As of 2010, there were five public-owned commercial banks in Pakistan, as well as 25 domestic private banks, six multi-national banks and four specialized banks.Since 2000 Pakistani banks have begun aggressive marketing of consumer finance to the emerging middle class, allowing for a consumption boom (more than a 7-month waiting list for certain car models) as well as a construction bonanza. Pakistan's banking sector remained remarkably strong and resilient during the world financial crisis in 2008–09, a feature which has served to attract a substantial amount of FDI in the sector. Stress tests conducted in June 2008 data indicate that the large banks are relatively robust, with the medium and small-sized banks positioning themselves in niche markets. The Pakistan Bureau of Statistics provisionally valued this sector at Rs.807,807 million in 2012 thus registering over 510% growth since 2000.An article published in Journal of the Asia Pacific Economy by Mete Feridun of University of Greenwich in London with his Pakistani colleague Abdul Jalil presents strong econometric evidence that financial development fosters economic growth in Pakistan.
The property sector has expanded twenty-threefold since 2001, particularly in metropolises like Lahore. Nevertheless, the Karachi Chamber of Commerce and Industry estimated in late 2006 that the overall production of housing units in Pakistan has to be increased to 0.5 million units annually to address 6.1 million backlog of housing in Pakistan for meeting the housing shortfall in next 20 years. The report noted that the present housing stock is also rapidly aging and an estimate suggests that more than 50% of stock is over 50 years old. It is also estimated that 50% of the urban population now lives in slums and squatter settlements. The report said that meeting the backlog in housing, besides replacement of out-lived housing units, is beyond the financial resources of the government. This necessitates putting in place a framework to facilitate financing in the formal private sector and mobilise non-government resources for a market-based housing finance system.The Pakistan Bureau of Statistics provisionally valued this sector at Rs.459,829 million in 2012 thus registering over 149% growth since 2006.
The Pakistan Bureau of Statistics provisionally valued this sector at Rs.389,545 million in 2005 thus registering over 65% growth since 2000. The Pakistan Bureau of Statistics provisionally valued this sector at Rs.631,229 million in 2005 thus registering over 78% growth since 2000. The Pakistan Bureau of Statistics provisionally valued this sector at Rs.1,358,309 million in 2005 thus registering over 96% growth since 2000. The wholesale and retail trade is the largest sub-sector of the services. Its share in the overall services sector is estimated at 31.5 percent. The wholesale and retail trade sector is based on the margins taken by traders on the transaction of commodities traded. In 2012–13, this sector grew at 2.5 percent as compared to 1.7 percent in the previous year.
For years, the matter of balancing Pakistan's supply against the demand for electricity has remained a largely unresolved matter. Pakistan faces a significant challenge in revamping its network responsible for the supply of electricity. While the government claims credit for overseeing a turnaround in the economy through a comprehensive recovery, it has just failed to oversee a similar improvement in the quality of the network for electricity supply. Most cities in Pakistan receive substantial sunlight throughout the year, which would suggest good conditions for investment in solar energy. If the rich people in Pakistan are shifted to solar energy that they should be forced to purchase solar panels, the shortfall can be controlled. this will make the economy boost again as before 2007. According to an econometric analysis published in Quality & Quantity by Mete Feridun of University of Greenwich and his colleague Muhammad Shahbaz, economic growth in Pakistan leads to electricity consumption but not vice versa..
Foreign investment had significantly declined by 2010, dropping by 54.6% due to Pakistan's political instability and weak law and order, according to the Bank of Pakistan.Business regulations have been overhauled along liberal lines, especially since 1999. Most barriers to the flow of capital and international direct investment have been removed. Foreign investors do not face any restrictions on the inflow of capital, and investment of up to 100% of equity participation is allowed in most sectors. Unlimited remittance of profits, dividends, service fees or capital is now the rule. However, doing business has been becoming increasingly difficult over the past decade due to political instability, rising domestic insurgency and insecurity and vehement corruption. This can be confirmed by the World Bank's Ease of Doing Business Index report degrading its ratings for Pakistan each year since September 2009. Tariffs have been reduced to an average rate of 16%, with a maximum of 25% (except for the car industry). The privatization process, which started in the early 1990s, has gained momentum, with most of the banking system privately owned, and the oil sector targeted to be the next big privatization operation. The recent improvements in the economy and the business environment have been recognised by international rating agencies such as Moody's and Standard and Poor's (country risk upgrade at the end of 2003). 47.1% increase in Net FDI in 2014–2015 (July–October) as compared to 2013–14 (July–October).
With the rapid growth in Pakistan's economy, foreign investors are taking a keen interest in the corporate sector of Pakistan. In recent years, majority stakes in many corporations have been acquired by multinational groups. PICIC by Singapore-based Temasek Holdings for $339 million Union Bank by Standard Chartered Bank for $487 million Prime Commercial Bank by ABN Amro for $228 million PakTel by China Mobile for $460 million PTCL by Etisalat for $1.8 billion Additional 57.6% shares of Lakson Tobacco Company acquired by Philip Morris International for $382 million In 2016, Arçelik acquired Dawlance for $243 million. In 2016, FrieslandCampina acquired 51% stake in Engro Foods for $446.81 million. In 2016, The Abraaj Group sold its 66.4% stake in K-Electric to Shanghai Electric for $1.77 billion.The foreign exchange receipts from these sales are also helping cover the current account deficit.
Pakistan witnessed the highest export of US$25.1 billion in the FY 2013–14. However, in subsequent years exports have declined considerably. This declined started from financial year 2014–15 when an international commodity slump set in. This was compounded by structural supply side constraints including energy shortages, high input costs and an overvalued exchange rate. From financial year 2014 to 2016, exports declined by 12.4 percent. Exports growth trend over this period was similar to the world trade growth patterns. Pakistan's external sector continued facing stress during 2016–17. But still Pakistan's merchandise trade exports grew by 0.1 percent during the fiscal year 2016–17. The imports continued to grow at a much faster rate and grew by a large percentage of 18.0 during the FY 2017 as compared to the previous year. World imports had been stagnant between 2011 and 2014 but registered significant drop since early 2015 because of weak commodity and product prices and weak global economic activity. Economic growth was lacklustre in the OECD countries which contributed to the slowdown in China. Furthermore, the ratio between real growth in world imports and world real GDP growth substantially declined. This decline in the import content of economic activity triggered a shift in consumption worldwide from traded towards non-traded goods, import substitution, a slowdown in the pace of trade liberalization, and gave currency to protectionist measures. A bulk of Pakistan's exports are directed to the OECD region and China. Historical data suggest strong correlation between Pakistani exports to imports in OECD and China. As per FY 2016 data, more than half of country's exports are shipped to these two destinations i.e. OECD and China. A decline in Pakistan overall exports, thus occurred in this backdrop.Pakistan's imports are showing rising trend at a relatively faster rate due to the increased economic activity as part of China Pakistan Economic Corridor (CPEC), particularly in the Energy sector. The construction projects under CPEC require heavy machinery that has to be imported. It is also observed that the economy is currently being led both by investments as well as consumption, resulting in relatively higher levels of imports. During FY 2018 Pakistan's exports picked up and reached to US$24.8 billion showing a growth of 12.6 percent over previous year FY 2017. Imports on the other hand also increased by 16.2 percent and touched the highest figure of US$56.6 billion. As a result, the trade deficit widened to US$31.8 billion which was the highest since last ten years. Pakistan major exports commodities for the last five fiscal years are listed in the table below: Pakistan major imports commodities for the last five fiscal years are listed in the table below:-
During FY 2017, the increase in imports of capital equipment and fuel significantly put pressure on the external account. A reversal in global oil prices led to increase in POL imports, accompanied by falling exports, as a result the merchandised trade deficit grew by 39.4 percent to US$26.885 billion in FY 2017. While remittances and Coalition Support Fund inflows both declined slightly over the same period last year, however, the impact was offset by an improvement in the income account, mainly due to lower profit repatriations by oil and gas firms.Current account – The Current account deficit increased to US$12.4 billion in FY 2017, against US$3.2 billion in FY 2016.However, the impact of high current deficit on foreign exchange reserves was not severe, as financial inflows were available to the country to partially offset the gap; these inflows helped ensure stability in the exchange rate. Net FDI grew by 12.4 percent and reached US$1.6 billion in the nine-months period, whereas net FPI saw an inflow of US$631 million, against an outflow of US$393 million last year. Encouragingly for the country, the period saw the completion of multiple merger and acquisition deals between local and foreign companies. Moreover, multiple foreign automakers announced their intention to enter the Pakistani market, and some also entered into joint ventures with local conglomerates. This indicates that Pakistan is clearly on foreign investors' radar, and provides a positive outlook for FDI inflows going forward. government's successful issuance of a US$1.0 billion Sukuk in the international capital market, at an extremely low rate of 5.5 percent. Besides, Pakistan continued to enjoy support from international financial institutions (IFIs) like the World Bank and Asian Development Bank, and from bilateral partners like China, in the post-EFF period: net official loan inflows of US$1.1 billion were recorded during the period. As a result, the country's FX reserve amounted to US$20.8 billion by 4 May 2017 sufficient to finance around four month of import payments.
Pakistan receives economic aid from several sources as loans and grants. The International Monetary Fund (IMF), World Bank (WB), Asian Development Bank (ADB), etc. provide long-term loans to Pakistan. Pakistan also receives bilateral aid from developed and oil-rich countries. Foreign aid has been one of the main sources of money for the Pakistani Economy. Collection of Foreign aid has been one of the priorities of almost every Pakistani Government with the Prime Minister himself leading delegations on a regular basis to collect Foreign aid.The Asian Development Bank will provide close to $6 billion development assistance to Pakistan during 2006–9. The World Bank unveiled a lending programme of up to $6.5 billion for Pakistan under a new four-year, 2006–2009, aid strategy showing a significant increase in funding aimed largely at beefing up the country's infrastructure. Japan will provide $500 million annual economic aid to Pakistan. In November 2008, the International Monetary Fund (IMF) has approved a loan of 7.6 billion to Pakistan, to help stabilise and rebuild the country's economy. Between the 2008 and 2010 fiscal years, the IMF extended loans to Pakistan totalling 5.2 billion dollars. The government decided in 2011 to cut off ties with the IMF. However the government newly elected in 2013 re-established these ties, and a negotiated a three-year $6.6 billion package which would allow it to deal with on-going debt issues. In May 2019, Pakistan finalised a US$6 billion foreign aid with IMF. This is Pakistan's 22nd such bailout from the IMF.The China–Pakistan Economic Corridor is being developed with a contribution of mainly concessionary loans from China under the Belt and Road Initiative. Much like BRI, value of CPEC investments transcends any fiat currency and is only estimated vaguely as it spans over decades of past and future industrial development and global economic influence.
The remittances of Pakistanis living abroad has played important role in Pakistan's economy and foreign exchange reserves. The Pakistanis settled in Western Europe and North America are important sources of remittances to Pakistan. Since 1973 the Pakistani workers in the oil rich Arab states have been sources of billions of dollars of remittances. The 9 million-strong Pakistani diaspora, contributed US$19.3 billion to the economy in FY2017. The major source countries of remittances to Pakistan include UAE, US, Saudi Arabia, GCC countries (including Bahrain, Kuwait, Qatar and Oman), Australia, Canada, Japan, Norway, Switzerland, UK and EU countries. Remittances sent home by overseas Pakistani workers have seen a negative growth of 3.0% in the fiscal year 2017 compare to previous year when remittances reached at all-time high of 19.9 billion US dollars. This decline in remittances is mainly due to the adverse economic conditions of Arabian and gulf countries after the fall in oil prices in 2016. However, the recent development activities in the Qatar FIFA World Cup, Dubai Expo, Saudi Arabia's implementation of its Vision 2030 and particularly the recent visit of the P.M to Kuwait should all be helpful in opening new avenues for employment in these countries . Going forward one can expect improvements in the coming years. Remittances sent home by overseas Pakistanis in the fiscal year 2016/17 are as under:
Fiscal budget summary (FY2017/18) Fiscal year: 1 July – 30 June Budget outlay: Rs 5,013.8 billion rupees Revenues collection estimated: 4,713.7 billion rupees Expenditures estimated: 5,103.8 billion rupees Bank borrowing estimated: 390.1 billion rupees
Government expenditures were 4,383.6 billion rupees (FY 2016–2017 July to March). Total expenditures witnessed a downward trajectory without compromising the expenditures on development projects and social assistance. Particularly, expenditures under Public Sector Development Program (PSDP) have been raised adequately in order to meet the investment requirements. During FY 2017 the size of federal PSDP has increased to Rs 800 billion from Rs 348.3 billion during FY 2013, showing a cumulative increase of over 129 percent. During first nine months of current fiscal year, the fiscal deficit stood at 3.9 percent of GDP against 3.5 percent of GDP recorded in the same period of FY 2016 on account of higher development expenditures along with various tax incentives to promote investment and economic activity in the country and security related expenditures. On the basis of previous estimates of GDP at Rs 33,509 billion, the fiscal deficit was recorded at 3.7 percent during first nine months of current fiscal year against 3.4 percent registered in the comparable period of FY 2016. Total revenues grew at 6.2 percent to Rs 3,145.5 billion during July–March, FY 2017 against Rs 2,961.9 in the comparable period of FY 2016.
Since before the collapse of the USSR in 1991, progressive economic liberalization has been carried out by the government both at the provincial and the national level. Pakistan has achieved FDI of almost $8.4 billion in the financial fiscal year of 2006–07, surpassing the government target of $4 billion. Despite this milestone achievement, the Foreign investment had significantly declined by 2010, dropping by ~54.6% due to Pakistan's military operations, financial crises, law and order situation in Karachi, according to the Bank of Pakistan. From the 2006 estimate, the Government expenditures were ~$25 billion. Funding in science and education has been a primary policy of the Government of Pakistan, since 1947. Moreover, English is fast spreading in Pakistan, with 18 million Pakistanis (11% of the population) having a command over the English language, which makes it the 3rd Largest English Speaking Nation in the world and the 2nd largest in Asia. On top of that, Pakistan produces about 445,000 university graduates and 10,000 computer science graduates per year. Despite these statistics, Pakistan still has one of the highest illiteracy rates in the world and the second largest out of school population (5.1 million children) after Nigeria.
As per the CIA World Factbook, in 2010, Pakistan ranks 63rd in the world, with respect to the public external debt to various international monetary authorities (owning ~$55.98 billion in 2010), with a total of 60.1% of GDP.Since 2009, Pakistan has been trying to negotiate debt cancellation currently Pakistan spends $3 billion on debt servicing annually to largely western nations and the International Monetary Fund.
By province and administrative unit: Economy of Azad Kashmir Economy of Balochistan, Pakistan Economy of Gilgit Baltistan Economy of Khyber Pakhtunkhwa Economy of Punjab, Pakistan Economy of Sindh Economy of Karachi Economy of Islamabad Economy of Lahore Economy of Faisalabad Economy of RawalpindiOther Economic liberalisation in Pakistan Foreign trade of Pakistan Ministry of Finance (Pakistan) 2019–20 Pakistan federal budget Agriculture in Pakistan Economic effects of 2010 Pakistan floods Consensus Economics Economic forecasting Economic history of Pakistan Industry of Pakistan List of Pakistani Districts by Human Development Index List of Pakistani provinces by gross domestic product List of Pakistanis by net worth List of tariffs in Pakistan Ministry of Commerce (Pakistan) Pakistan Board of Investment Prize Bonds Science and technology in Pakistan Trade Development Authority of Pakistan Trading Corporation of Pakistan Economy of the OIC
Statistics Division, Government of Pakistan Ministry of Finance, Government of Pakistan Ministry of Commerce, Government of Pakistan World Bank Summary Trade Statistics Pakistan "Pakistan". The World Factbook. Central Intelligence Agency. Tariffs applied by Pakistan as provided by ITC's Market Access Map, an online database of customs tariffs and market requirements
The economy of the Philippines is a developing lower-middle income economy. As of the 2020 estimate by the International Monetary Fund, it is the world's 31st largest economy by nominal GDP and the 13th largest economy in Asia. The Philippines is one of the emerging markets and is the sixth highest in Southeast Asia by GDP per capita values, after the regional countries of Singapore, Brunei, Malaysia, Thailand and Indonesia. The Philippines is primarily considered a newly industrialized country, which has an economy in transition from one based on agriculture to one based more on services and manufacturing. As of 2020, GDP by purchasing power parity was estimated to be at $934 billion.Primary exports include semiconductors and electronic products, transport equipment, garments, copper products, petroleum products, coconut oil, and fruits. Major trading partners include Japan, China, the United States, Singapore, South Korea, the Netherlands, Hong Kong, Germany, Taiwan and Thailand. The Philippines has been named as one of the Tiger Cub Economies together with Indonesia, Malaysia, Vietnam, and Thailand. It is currently one of Asia's fastest growing economies. However, major problems remain, mainly having to do with alleviating the wide income and growth disparities between the country's different regions and socioeconomic classes, reducing corruption, and investing in the infrastructure necessary to ensure future growth. The Philippine economy is projected to be the 5th largest in Asia and 16th biggest in the world by 2050. According to the PricewaterhouseCoopers, it estimates that it will be the 12th to 14th richest economy in the world by 2060. While this opposes other reports from HSBC Holdings PLC, that by the year 2050, the Philippines will have been stated in 2050 maybe due to its yearly higher GDP growth rate of 6.5% (Second, after China). However, the economic statistics may still vary depending on the performance of the government every year, which are consistently plagued by corruption.
The economic history of the Philippine Islands had been traced back to the pre-colonial times. The country which was then composed of different kingdoms and thalassocracies oversaw the large number of merchants coming to the islands for trade. Indian, Arab, Chinese and Japanese merchants were welcomed by these kingdoms, which were mostly located by riverbanks, coastal ports and central plains. The merchants traded for goods such as gold, rice, pots and other products. The barter system was implemented at that time and the pre-colonial people enjoyed a life filled with imported goods which reflected their fashion and lifestyle. From the 12th century, a huge industry centred around the manufacture and trade of burnay clay pots, used for the storage of tea and other perishables, was set up in the northern Philippines with Japanese and Okinawan traders. These pots were known as 'Ruson-tsukuri' (Luzon-made) in Japanese, and were considered among the best storage vessels used for the purpose of keeping tea leaves and rice wine fresh. Hence, Ruson-Tsukuri pots became sought after in Northeast Asia. Each Philippine kiln had its own branding symbol, marked on the bottom of the Ruson-tsukuri by a single baybayin letter. The people also were great agriculturists and the islands especifically Luzon has great abundance of rice, fowls, wine as well as great numbers of carabaos, deer, wild boar and goats. In addition, there were also great quantities of cotton and colored clothes, wax, honey and date palms produced by the natives. The precolonial state of Caboloan in Pangasinan often exported deer-skins to Japan and Okinawa. The Nation of Ma-i produced beeswax, cotton, true pearls, tortoise shell, medicinal betel nuts and yuta cloth in their trade with East Asia. By the early sixteenth century, the two largest polities of the Pasig River delta, Maynila and Tondo, established a shared monopoly on the trade of Chinese goods throughout the rest of the Philippine archipelago.The Visayas islands which is home to the Kedatuan of Madja-as, the Kedatuan of Dapitan and the Rajahnate of Cebu on the other hand were abundant in rice, fish, cotton, swine, fowls, wax and honey. Leyte was said to produce two rice crops a year, and Pedro Chirino commented on the great rice and cotton harvests that were sufficient to feed and clothe the people. In Mindanao, the Rajahnate of Butuan specialized in the mining of gold and the manufacture of jewelry. The Sultanate of Maguindanao was known for the raising and harvesting of cinnamon. The Sultanate of Lanao had a fishing industry by lake Lanao and the Sultanate of Sulu had lively pearl-diving operations. The kingdoms of ancient Philippines were active in international trade, and they used the ocean as natural highways. Ancient peoples were engaged in long-range trading with their Asian neighbors as far as west as Maldives and as far as north as Japan. Some historians even proposed that they also had regular contacts with other Austronesian people in Western Micronesia because it was the only area in the Oceania that had rice crops, tuba (fermented coconut sap), and tradition of betel nut chewing when the first Europeans arrived there. The uncanny resemblance of complex body tattoos among the Visayans and those of Borneo also proved some interesting connection between Borneo and ancient Philippines. Magellan's chronicler, Antonio Pigafetta, mentioned that merchants and ambassadors from all surrounding areas came to pay tribute to the rajah of Sugbu (Cebu) for the purpose of trade. While Magellan's crew were with the rajah, a representative from Siam was paying tribute to the rajah. Miguel López de Legazpi also wrote how merchants from Luzon and Mindoro had come to Cebu for trade, and he also mentioned how the Chinese merchants regularly came to Luzon for the same purpose. The Visayan Islands had earlier encounters with Greek traders in 21 AD. Its people enjoyed extensive trade contacts with other cultures. Indians, Japanese, Arabs, Vietnamese, Cambodians, Thais, Malays and Indonesians as traders or immigrants.Aside from trade relations, the natives were also involved in aquaculture and fishing. The natives make use of the salambao, which is a type of raft that utilizes a large fishing net which is lowered into the water via a type of lever made of two criss-crossed poles. Night fishing was accomplished with the help of candles made from a particular type of resin similar to the copal of Mexico. Use of safe pens for incubation and protection of the small fry from predators was also observed, and this method astonished the Spaniards at that time. During fishing, large mesh nets were also used by the natives to protect the young and ensure future good catches. From the early 1500s to as late as the 1560s, people from Luzon, Philippines; were referred to in Portuguese Malacca as Luções, and they set up many overseas communities across Southeast Asia where they participated in trading ventures and military campaigns in Burma, Malacca and Eastern Timor as traders and mercenaries. One prominent Luções was Regimo de Raja, who was a spice magnate and a Temenggung (Jawi: تمڠݢوڠ) (Governor and Chief General) in Portuguese Malacca. He was also the head of an armada which traded and protected commerce between the Indian Ocean, the Strait of Malacca, the South China Sea, and the medieval maritime principalities of the Philippines.
The natives were slavered among them by other tribes like Lapu-Lapu which forced other islands to pay taxes. The arrival of the Spanish removed this slavering system. Miguel Lopez de Legazpi with Tlaxcaltecs from Mexico conquered and unified the islands. The conquered was possible thanks to the discovery of the trip back to Mexico coast by Agustino Urdaneta. The administration of Islas Filipinas was done through the Capitania General and depended on Mexico Capital which formed the New Spain Viceroyalty. The economy of Islas Filipinas grew further when the Spanish government inaugurated the Manila Galleon trade system. Trading ships, settlers and military reinforcements made voyages once or twice per year across the Pacific Ocean from the port of Acapulco in Mexico to Manila in the Philippines. Both cities were part of the then Province of New Spain. This trade made the city of Manila one of the major global cities in the world, improving the growth of the Philippine economy in the succeeding years. Trade also introduced foodstuffs such as maize, tomatoes, potatoes, chili peppers, chocolate and pineapples from Mexico and Peru. Tobacco, first domesticated in Latin-America, and then introduced to the Philippines, became an important cash crop for Filipinos. The Philippines also became the distribution center of silver mined in the Americas, which was in high demand in Asia, during the period. In exchange for this silver, Manila gathered Indonesian spices, Chinese silks and Indian gems to be exported to Mexico.The Manila Galleon system operated until 1815, when Mexico got its independence. Nevertheless, it didn't affect the economy of the islands. On March 10, 1785, King Charles III of Spain confirmed the establishment of the Royal Philippine Company with a 25-year charter. The Basque-based company was granted a monopoly on the importation of Chinese and Indian goods into the Philippines, as well as the shipping of the goods directly to Spain via the Cape of Good Hope.
After Spain lost Mexico as a territory, New Spain was dissolved making the Philippines and other Pacific islands to form the Spanish East Indies. This resulted in the Philippines being governed directly by the King of Spain and the Captaincy General of the Philippines while the Pacific islands of Northern Mariana Islands, Guam, Micronesia and Palau was governed by the Real Audiencia of Manila and was part of the Philippine territorial governance. It made the economy of the Philippines grow further as people saw the rise of opportunities. Agriculture remained the largest contributor to economy, being the largest producer of coffee in Asia as well as a large produce of tobacco. In Europe, the Industrial Revolution spread from Great Britain during the period known as the Victorian Age. The industrialization of Europe created great demands for raw materials from the colonies, bringing with it investment and wealth, although this was very unevenly distributed. Governor-General Basco had opened the Philippines to this trade. Previously, the Philippines was seen as a trading post for international trade but in the nineteenth century it was developed both as a source of raw materials and as a market for manufactured goods. The economy of the Philippines rose rapidly and its local industries developed to satisfy the rising demands of an industrializing Europe. A small flow of European immigrants came with the opening of the Suez Canal, which cut the travel time between Europe and the Philippines by half. New ideas about government and society, which the friars and colonial authorities found dangerous, quickly found their way into the Philippines, notably through the Freemasons, who along with others, spread the ideals of the American, French and other revolutions, including Spanish liberalism. In 1834 the Royal Company of the Philippines was abolished, and free trade was formally recognized. With its excellent harbor, Manila became an open port for Asian, European, and North American traders. European merchants alongside the Chinese immigrants opened stores selling goods from all parts of the world. The El Banco Español Filipino de Isabel II (now the Bank of the Philippine Islands) was the first bank opened in the Philippines in 1851. In 1873 additional ports were opened to foreign commerce, and by the late nineteenth century three crops—tobacco, abaca, and sugar—dominated Philippine exports.
The results of the economy under the Americans were mixed. An initial high growth phase occurred during the 1910s due to the recovery from the wars with Spain and the US, and investment in agriculture. The Philippines would at first briefly outpace its neighbors. This would not last as growth fell behind in the later years. Stagnation in the late 1920s and beyond took place as access to US markets became restricted by protectionist quotas and fiscal restraints forestalled any further development in agriculture.The growth period can be attributed to the results of a crash program in agricultural modernization undertaken in 1910–1920. This in turn was done in order to address the growing shortfall in the supply of rice. The Philippines once a net exporter became an importer of rice as a result of the wars with the Spanish and later the Americans and by the reallocation of labour to export crops.The 1930s would mark the end to this period of relative prosperity. The Sugar Act of 1934 capped Philippines sugar exports to the US at 921,000 tons per year. Expenditure on public infrastructure for agriculture was reduced as the Payne–Aldridge Act stripped the government of customs revenue. Manila hemp was now competing against the newly invented Nylon. Although the area of land cultivated for agriculture was still increasing, the rate was reduced to 1% per annum. The policy with the most far reaching consequences of this period was the peg between the peso and dollar. This was enforced by law until 1975. It provided monetary stability for foreign investment inflows, which lead to 40% of all capital invested in manufacturing and commercial enterprises to be owned by foreign entities by 1938. On the other hand, this overvaluation of the peso would have a negative impact with foreign trade with the rest of Asia. Economic policy leading to independence would have necessitated loosening trade links with the US. In order to achieve an internationally competitive exchange rate, the peso dollar link would have to be broken. The much belated move to a true floating exchange rate led to uncompetitive exports as such an import substitution strategy remained until significant currency devaluation opened up the opportunity for reorienting towards exports.
Due to the Japanese invasion establishing the unofficial Second Philippine Republic, the economic growth receded and food shortages occurred. Prioritizing the shortages of food, Jose Laurel, the appointed President, organized an agency to distribute rice, even though most of the rice was confiscated by Japanese soldiers. Manila was one of the many places in the country that suffered from severe shortages, due mainly to a typhoon that struck the country in November 1943. The people were forced to cultivate private plots which produced root crops like kangkong. The Japanese, in order to raise rice production in the country, brought a quick-maturing horai rice, which was first used in Taiwan. Horai rice was expected to make the Philippines self-sufficient in rice by 1943, but rains during 1942 prevented this. Also during World War II in the Philippines, the occupying Japanese government issued fiat currency in several denominations; this is known as the Japanese government-issued Philippine fiat peso. The first issue in 1942 consisted of denominations of 1, 5, 10 and 50 centavos and 1, 5, and 10 Pesos. The next year brought "replacement notes" of the 1, 5 and 10 Pesos while 1944 ushered in a 100 Peso note and soon after an inflationary 500 Pesos note. In 1945, the Japanese issued a 1,000 Pesos note. This set of new money, which was printed even before the war, became known in the Philippines as Mickey Mouse money due to its very low value caused by severe inflation. Anti-Japanese newspapers portrayed stories of going to the market laden with suitcases or "bayong" (native bags made of woven coconut or buri leaf strips) overflowing with the Japanese-issued bills. In 1944, a box of matches cost more than 100 Mickey Mouse pesos. In 1945, a kilogram of camote cost around 1000 Mickey Mouse pesos. Inflation plagued the country with the devaluation of the Japanese money, evidenced by a 60% inflation experienced in January 1944.
After the re-establishment of the Commonwealth in 1945, the country was left with a devastated city, food crisis and financial crisis. A year later in 1946, the Philippines got its independence in America, creating the Third Philippine Republic. In an effort to solve the massive socio-economic problems of the period, newly elected President Manuel Roxas reorganized the government, and proposed a wide-sweeping legislative program. Among the undertakings of the Third Republic's initial year were: The establishment of the Rehabilitation Finance Corporation (which would be reorganized in 1958 as the Development Bank of the Philippines); the creation of the Department of Foreign Affairs and the organization of the foreign service through Executive Order No. 18; the GI Bill of Rights for Filipino veterans; and the revision of taxation laws to increase government revenues.President Roxas moved to strengthen sovereignty by proposing a Central Bank for the Philippines to administer the Philippine banking system which was established by Republic Act No. 265. In leading a "cash-starved government" that needed to attend a battered nation, President Roxas campaigned for the parity amendment to the 1935 Constitution. This amendment, demanded by the Philippine Trade Relations Act or the Bell Trade Act, would give American citizens and industries the right to utilize the country's natural resources in return for rehabilitation support from the United States. The President, with the approval of Congress, proposed this move to the nation through a plebiscite. The Roxas administration also pioneered the foreign policy of the Republic. Vice President Elpidio Quirino was appointed Secretary of Foreign Affairs. General Carlos P. Romulo, as permanent representative of the Philippines to the United Nations, helped shape the country's international identity in the newly established stage for international diplomacy and relations. During the Roxas administration, the Philippines established diplomatic ties with foreign countries and gained membership to international entities, such as the United Nations General Assembly, the United Nations Educational, Scientific and Cultural Organization (UNESCO), the World Health Organization (WHO), the International Labor Organization (ILO), etc. When President Carlos P. Garcia won the elections, his administration promoted the "Filipino First" policy, whose focal point was to regain economic independence; a national effort by Filipinos to "obtain major and dominant participation in their economy." The administration campaigned for the citizens' support in patronizing Filipino products and services, and implemented import and currency controls favorable for Filipino industries. In connection with the government's goal of self-sufficiency was the "Austerity Program," which President Garcia described in his first State of the NatIon Address as "more work, more thrift, more productive investment, and more efficiency" that aimed to mobilize national savings. The Anti Graft and Corrupt Practices Act, through Republic Act No. 301, aimed to prevent corruption, and promote honesty and public trust. Another achievement of the Garcia administration was the Bohlen–Serrano Agreement of 1959, which shortened the term of lease of the US military bases in the country from the previous 99 to 25 years.President Diosdado Macapagal, during his inaugural address on December 30, 1961, emphasized the responsibilities and goals to be attained in the "new era" that was the Macapagal administration. He reiterated his resolve to eradicate corruption, and assured the public that honesty would prevail in his presidency. President Macapagal, too, aimed at self-sufficiency and the promotion of every citizen's welfare, through the partnership of the government and private sector, and to alleviate poverty by providing solutions for unemployment. Among the laws passed during the Macapagal administration were: Republic Act No. 3844 or the Agricultural Land Reform Code (an act that established the Land Bank of the Philippines); Republic Act No. 3466, which established the Emergency Employment Administration; Republic Act No. 3518, which established the Philippine Veterans Bank; Republic Act No. 3470, which established the National Cottage Industries Development Authority (NACIDA) to organize, revive, and promote the establishment of local cottage industries; and Republic Act No. 4156, which established the Philippine National Railways (PNR) to operate the national railroad and tramways. The administration lifted foreign exchange controls as part of the decontrol program in an attempt to promote national economic stability and growth.
President Ferdinand E. Marcos declared martial law in the midst of rising student movements and an increasing number communist and socialist groups lobbying for reforms in their respective sectors. Leftists held rallies to express their frustrations to the government, this restiveness culminating in the First Quarter Storm, where activists stormed Malacañang Palace only to be turned back by the Philippine Constabulary. This event in particular left four people dead and many injured after heavy exchanges of gunfire. There was further unrest, and in the middle of the disorder on September 21, 1972, Marcos issued Proclamation No. 1081, effectively installing martial law in the Philippines, a declaration that suspended civil rights and imposed military rule in the country. The GDP of the Philippines rose during the martial law, rising from P55 million to P193 million in about 8 years. This growth was spurred by massive lending from commercial banks, accounting for about 62% percent of external debt. As a developing country, the Philippines during the martial law was one of the heaviest borrowers. These aggressive moves were seen by critics as a means of legitimizing martial law by purportedly enhancing the chances of the country in the global market. Much of the money was spent on pump-priming to improve infrastructure and promote tourism. However, despite the aggressive borrowing and spending policies, the Philippines lagged behind its Southeast Asia counterparts in GDP growth rate per capita. The country, in 1970–1980, only registered an average 5.73 percent growth, while its counterparts like Thailand, Malaysia, Singapore, and Indonesia garnered a mean growth of 7.97 percent. This lag, which became very apparent at the end of the Marcos Regime, can be attributed to the failures of economic management that was brought upon by State-run monopolies, mismanaged exchange rates, imprudent monetary policy and debt management, all underpinned by rampant corruption and cronyism. As said by Emannuel de Dios “[…]main characteristics distinguishing the Marcos years from other periods of our history has been the trend towards the concentration of power in the hands of the government, and the use of governmental functions to dispense economic privileges to some small factions in the private sector.”There are few more palpable and glaring examples of the economic mismanagement of the time than the Bataan Nuclear Power Plant (BNPP) located in Morong, Bataan. Started in the 1970s, the BNPP was supposed to boost the country's competitiveness by providing affordable electricity to fuel industrialization and job creation in the country. Far from this, the US$2.3 billion nuclear plant suffered from cost over-runs and engineering and structural issues which eventually led to its mothballing—without generating a single watt of electricity. Income inequality grew during the era of martial law, as the poorest 60 percent of the nation were able to contribute only 22.5 percent of the income at 1980, down from 25.0 percent in 1970. The richest 10 percent, meanwhile, took a larger share of the income at 41.7 percent at 1980, up from 37.1 percent at 1970. These trends coincided with accusations of cronyism in the Marcos administration, as the administration faced questions of favoring certain companies that were close to the ruling family. According to the FIES (Family Income and Expenditure Survey) conducted from 1965 to 1985, poverty incidence in the Philippines rose from 41 percent in 1965 to 58.9 percent in 1985. This can be attributed to lower real agricultural wages and lesser real wages for unskilled and skilled laborers. Real agricultural wages fell about 25 percent from their 1962 level, while real wages for unskilled and skilled laborers decreased by about one-third of their 1962 level. It was observed that higher labor force participation and higher incomes of the rich helped cushion the blow of the mentioned problems.
The Aquino administration took over an economy that had gone through socio-political disasters during the People Power revolution, where there was financial and commodity collapse caused by an overall consumer cynicism, a result of the propaganda against cronies, social economic unrest resulting from numerous global shortages, massive protests, lack of government transparency, the opposition's speculations, and various assassination attempts and failed coups. At that point in time, the country's incurred debt from the Marcos Era's debt-driven development began crippling the country, which slowly made the Philippines the "Latin-American in East Asia" as it started to experience the worst recession since the post-war era. Most of the immediate efforts of the Aquino administration was directed in reforming the image of the country and paying off all debts, including those that some governments were ready to write-off, as possible. This resulted in budget cuts and further aggravated the plight of the lower class because the jobs offered to them by the government was now gone. Infrastructure projects, including repairs, were halted in secluded provinces turning concrete roads into asphalt. Privatization of many government corporations, most catering utilities, was the priority of the Aquino administration which led to massive lay-offs and inflation. The Aquino administration was persistent in its belief that the problems that arose from the removal of the previous administration can be solved by the decentralization of power. Growth gradually began in the next few years of the administration. Somehow, there was still a short-lived, patchy, and erratic recovery from 1987 to 1991 as the political situation stabilized a bit. With this, the peso became more competitive, confidence of investors was gradually regained, positive movements in terms of trade were realized, and regional growth gradually strengthened.
The Ramos administration basically served its role as the carrier of the momentum of reform and as an important vehicle in "hastening the pace of liberalization and openness in the country". The administration was a proponent of capital account liberalization, which made the country more open to foreign trade, investments, and relations. It was during the term of the administration when the Bangko Sentral ng Pilipinas was established, and the Philippines joined the World Trade Organization and other free trade associations such as the APEC. Also, debt reduction was considered and as such, the issuance of certain government bonds called Brady Bonds also came to fruition in 1992. Key negotiations with conflicting forces in Mindanao actually became more successful during the administration, with Jose Almonte as one of the key adviser of the administration. By the time Ramos succeeded Corazon Aquino in 1992, the Philippine economy was already burdened with a heavy budget deficit. This was largely the result of austerity measures imposed by a standard credit arrangement with the International Monetary Fund and the destruction caused by natural disasters such as the eruption of Mt. Pinatubo. Hence, according to Canlas, pump priming through government spending was immediately ruled out due to the deficit. Ramos therefore resorted to institutional changes through structural policy reforms, of which included privatization and deregulation. He sanctioned the formation of the Legislative-Executive Development Advisory Council (LEDAC), which served as a forum for consensus building, on the part of the Executive and the Legislative branches, on important bills on economic policy reform measures (4). The daily brownouts that plagued the economy were also addressed through the enactment of policies that placed guaranteed rates. The economy during the first year of Ramos administration suffered from severe power shortage, with frequent brownouts, each lasting from 8 to 12 hours. To resolve this problem, the Electric Power Crisis Act was made into law together with the Build-Operate-Transfer Law. Twenty power plants were built because of these, and in effect, the administration was able to eliminate the power shortage problems in December 1993 and sustained economic growth for some time.The economy seemed to be all set for long-run growth, as shown by sustainable and promising growth rates from 1994 to 1997. However, the Asian Crisis contagion which started from Thailand and Korea started affecting the Philippines. This prompted the Philippine economy to plunge into continuous devaluation and very risky ventures, resulting in property busts and a negative growth rate. The remarkable feat of the administration, however, was that it was able to withstand the contagion effect of the Asian Crisis better than anybody else in the neighboring countries. Most important in the administration was that it made clear the important tenets of reform, which included economic liberalization, stronger institutional foundations for development, redistribution, and political reform.Perhaps some of the most important policies and breakthroughs of the administration are the Capital Account Liberalization and the subsequent commitments to free trade associations such as APEC, AFTA, GATT, and WTO. The liberalization and opening of the capital opening culminated in full-peso convertibility in 1992. And then another breakthrough is again, the establishment of the Bangko Sentral ng Pilipinas, which also involved the reduction of debts in that the debts of the old central bank were taken off its books.
Although Estrada's administration had to endure the continued shocks of the Asian Crisis contagion, the administration was also characterized by the administration's economic mismanagement and "midnight cabinets." As if the pro-poor rhetoric, promises and drama were not really appalling enough, the administration also had "midnight cabinets composed of 'drinking buddies' influencing the decisions of the "daytime cabinet'". Cronyism and other big issues caused the country's image of economic stability to change towards the worse. And instead of adjustments happening, further deterioration of the economy occurred. Targeted revenues were not reached, implementation of policies became very slow, and fiscal adjustments were not efficiently conceptualized and implemented. All those disasters caused by numerous mistakes were made worse by the sudden entrance of the Jueteng controversy, which gave rise to the succeeding EDSA Revolutions. Despite all these controversies, the administration still had some meaningful and profound policies to applaud. The administration presents a reprise of the population policy, which involved the assisting of married couples to achieve their fertility goals, reduce unwanted fertility and match their unmet need for contraception. The administration also pushed for budget appropriations for family planning and contraceptives, an effort that was eventually stopped due to the fact that the church condemned it. The administration was also able to implement a piece of its overall Poverty Alleviation Plan, which involved the delivery of social services, basic needs, and assistance to the poor families. The Estrada administration also had limited contributions to Agrarian Reform, perhaps spurred by the acknowledgement that indeed, Agrarian Reform can also address poverty and inequitable control over resources. In that regard, the administration establishes the program "Sustainable Agrarian Reform Communities-Technical Support to Agrarian and Rural Development". As for regional development, however, the administration had no notable contributions or breakthroughs.
The Arroyo administration, in an economical standpoint, was a period of good growth rates simultaneous with the US, due perhaps to the emergence of the Overseas Filipino workers (OFW) and the Business Process Outsourcing (BPO). The emergence of the OFW and the BPO improved the contributions of OFW remittances and investments to growth. In 2004, however, fiscal deficits grew and grew as tax collections fell, perhaps due to rampant and wide scale tax avoidance and tax evasion incidences. Fearing that a doomsday prophecy featuring the [Argentina default] in 2002 might come to fruition, perhaps due to the same sort of fiscal crisis, the administration pushed for the enactment of the 12% VAT and the E-VAT to increase tax revenue and address the large fiscal deficits. This boosted fiscal policy confidence and brought the economy back on track once again. Soon afterwards, political instability afflicted the country and the economy anew with Abu Sayyaf terrors intensifying. The administration's Legitimacy Crisis also became a hot issue and threat to the authority of the Arroyo administration. Moreover, the Arroyo administration went through many raps and charges because of some controversial deals such as the NBN-ZTE Broadband Deal. Due however to the support of local leaders and the majority of the House of Representatives, political stability was restored and threats to the administration were quelled and subdued. Towards the end of the administration, high inflation rates for rice and oil in 2008 started to plague the country anew, and this led to another fiscal crisis, which actually came along with the major recession that the United States and the rest of the world were actually experiencing. The important policies of the Arroyo administration highlighted the importance of regional development, tourism, and foreign investments into the country. Therefore, apart from the enactment and establishment of the E-VAT policy to address the worsening fiscal deficits, the administration also pushed for regional development studies in order to address certain regional issues such as disparities in regional per capita income and the effects of commercial communities on rural growth. The administration also advocated for investments to improve tourism, especially in other unexplored regions that actually need development touches as well. To further improve tourism, the administration launched the policy touching on Holiday Economics, which involves the changing of days in which we would celebrate certain holidays. Indeed, through the Holiday Economics approach, investments and tourism really improved. As for investment, the Arroyo administration would frequently visit other countries to encourage foreign investment for the betterment of the Philippine economy and its development.
The Philippines consistently coined as one of the Newly Industrialized Countries has had a fair gain during the latter years under the Arroyo Presidency to the current administration. The government managed foreign debts falling from 58% in 2008 to 47% of total government borrowings. According to the 2012 World Wealth Report, the Philippines was the fastest growing economy in the world in 2010 with a GDP growth of 7.3% driven by the growing business process outsourcing and overseas remittances.The country markedly slipped to 3.6% in 2011 after the government placed less emphasis on exports, as well as spending less on infrastructure. In addition, the disruption of the flow of imports for raw materials as a result from floods in Thailand and the tsunami in Japan affected the manufacturing sector in the same year. "The Philippines contributed more than $125 million as of end-2011 to the pool of money disbursed by the International Monetary Fund to help address the financial crisis confronting economies in Europe. This was according to the Bangko Sentral ng Pilipinas, which reported Tuesday that the Philippines, which enjoys growing foreign exchange reserves, has made available about $251.5 million to the IMF to finance the assistance program—the Financial Transactions Plan (FTP)—for crisis-stricken countries."The economy saw continuous real GDP growth of at least 5% since 2012. The Philippine Stock Exchange index ended 2012 with 5,812.73 points a 32.95% growth from the 4,371.96-finish in 2011.
The Philippine economy has been growing steadily over decades and the International Monetary Fund in 2014 reported it as the 39th largest economy in the world. However its growth has been behind that of many of its Asian neighbors, the so-called Asian Tigers, and it is not a part of the Group of 20 nations. Instead it is grouped in a second tier for emerging markets or newly industrialized countries. Depending on the analyst, this second tier can go by the name the Next Eleven or the Tiger Cub Economies. In the years 2012 and 2013, the Philippines posted high GDP growth rates, reaching 6.8% in 2012 and 7.2% in 2013, the highest GDP growth rates in Asia for the first two quarters of 2013, followed by China and Indonesia.A chart of selected statistics showing trends in the gross domestic product of the Philippines using data taken from the International Monetary Fund.
As a newly industrialized country, the Philippines is still an economy with a large agricultural sector; however, services have come to dominate the economy. Much of the industrial sector is based on processing and assembly operations in the manufacturing of electronics and other high-tech components, usually from foreign multinational corporations. Filipinos who go abroad to work–-known as Overseas Filipino Workers or OFWs—are a significant contributor to the economy but are not reflected in the below sectoral discussion of the domestic economy. OFW remittances is also credited for the Philippines' recent economic growth resulting in investment status upgrades from credit ratings agencies such as the Fitch Group and Standard & Poor's. In 1994, more than $2 billion USD worth of remittance from Overseas Filipinos were sent to the Philippines. In 2012, Filipino Americans sent 43% of all remittances sent to the Philippines, totaling to US$10.6 billion.
Agriculture employs 30% of the Filipino workforce as of 2014. Agriculture accounts for 11% of Philippines GDP as of 2014. The type of activity ranges from small subsistence farming and fishing to large commercial ventures with significant export focus. The Philippines is the world's largest producer of coconuts producing 19,500,000 tons in 2009. Coconut production in the Philippines is generally concentrated in medium-sized farms. The Philippines is also the world's second largest producer of pineapples, producing 2,730,000 metric tons in 2018. Rice production in the Philippines is important to the food supply in the country and economy. The Philippines is the 8th largest rice producer in the world, accounting for 2.8% of global rice production. The Philippines was also the world's largest rice importer in 2010. Rice is the most important food crop, a staple food in most of the country. It is produced extensively in Luzon (especially Central Luzon), Western Visayas, Southern Mindanao and Central Mindanao. The Philippines is one of the largest producers of sugar in the world. At least 17 provinces located in eight regions of the nation have grown sugarcane crops, of which the Negros Island Region accounts for half of the country's total production. As of Crop Year 2012–2013, 29 mills are operational divided as follows: 13 mills in Negros, 6 mills in Luzon, 4 mills in Panay, 3 mills in Eastern Visayas and 3 mills in Mindanao. A range from 360,000 to 390,000 hectares are devoted to sugarcane production. The largest sugarcane areas are found in the Negros Island Region, which accounts for 51% of sugarcane areas planted. This is followed by Mindanao which accounts for 20%; Luzon by 17%; Panay by 07% and Eastern Visayas by 04%.
The Philippines is a major player in the global shipbuilding industry with shipyards in Subic, Cebu, General Santos City and Batangas. It became the fourth largest shipbuilding nation in 2010. Subic-made cargo vessels are now exported to countries where shipping operators are based. South Korea's Hanjin started production in Subic in 2007 of the 20 ships ordered by German and Greek shipping operators. The country's shipyards are now building ships like bulk carriers, container ships and big passenger ferries. General Santos' shipyard is mainly for ship repair and maintenance.Being surrounded by waters, the country has abundant natural deep-sea ports ideal for development as production, construction and repair sites. On top of the current operating shipyards, two additional shipyards in Misamis Oriental and Cagayan province are being expanded to support future locators. It has a vast manpower pool of 60,000 certified welders that comprise the bulk of workers in shipbuilding. In the ship repair sector, the Navotas complex in Metro Manila is expected to accommodate 96 vessels for repair.
The ABS used in Mercedes-Benz, BMW, and Volvo cars are made in the Philippines. Toyota, Mitsubishi, Nissan and Honda are the most prominent automakers manufacturing cars in the country. Kia and Suzuki produce small cars in the country. Isuzu also produces SUVs in the country. Honda and Suzuki produce motorcycles in the country. A 2003 Canadian market research report predicted that further investments in this sector were expected to grow in the following years. Toyota sells the most vehicles in the country. By 2011, China's Chery Automobile company is going to build their assembly plant in Laguna, that will serve and export cars to other countries in the region if monthly sales would reach 1,000 units. Automotive sales in the Philippines moved up from 165,056 units in 2011 to over 180,000 in 2012. Japan's automotive manufacturing giant Mitsubishi Motors has announced that it will be expanding its operations in the Philippines.
Aerospace products in the Philippines are mainly for the export market and include manufacturing parts for aircraft built by both Boeing and Airbus. Moog is the biggest aerospace manufacturer with base in Baguio in the Cordillera region. The company produces aircraft actuators in their manufacturing facility. In 2011, the total export output of aerospace products in the Philippines reached US$3 billion.
A Texas Instruments plant in Baguio has been operating for 20 years and is the largest producer of DSP chips in the world. Texas Instruments' Baguio plant produces all the chips used in Nokia cell phones and 80% of chips used in Ericsson cell phones in the world. Until 2005, Toshiba laptops were produced in Santa Rosa, Laguna. Presently the Philippine plant's focus is in the production of hard disk drives. Printer manufacturer Lexmark has a factory in Mactan in the Cebu region. Electronics and other light industries are concentrated in Laguna, Cavite, Batangas and other CALABARZON provinces with sizable numbers found in Southern Philippines that account for most of the country's export.
The country is rich in mineral and geothermal energy resources. In 2003, it produced 1931 MW of electricity from geothermal sources (27% of total electricity production), second only to the United States, and a recent discovery of natural gas reserves in the Malampaya oil fields off the island of Palawan is already being used to generate electricity in three gas-powered plants. Philippine gold, nickel, copper, palladium and chromite deposits are among the largest in the world. Other important minerals include silver, coal, gypsum, and sulphur. Significant deposits of clay, limestone, marble, silica, and phosphate exist. About 60% of total mining production are accounted for by non-metallic minerals, which contributed substantially to the industry's steady output growth between 1993 and 1998, with the value of production growing 58%. In 1999, however, mineral production declined 16% to $793 million. Mineral exports have generally slowed since 1996. Led by copper cathodes, Philippine mineral exports amounted to $650 million in 2000, barely up from 1999 levels. Low metal prices, high production costs, lack of investment in infrastructure, and a challenge to the new mining law have contributed to the mining industry's overall decline.The industry rebounded starting in late 2004 when the Supreme Court upheld the constitutionality of an important law permitting foreign ownership of Philippines mining companies. However, the DENR has yet to approve the revised Department Administrative Order (DAO) that will provide the Implementing Rules and Regulations of the Financial and Technical Assistance Agreement (FTAA), the specific part of the 1994 Mining Act that allows 100% foreign ownership of Philippines mines.
In 2008, the Philippines has surpassed India as the world leader in business process outsourcing. The majority of the top ten BPO firms of the United States operate in the Philippines. The industry generated 100,000 jobs, and total revenues were placed at $960 million for 2005. In 2012, BPO sector employment ballooned to over 700,000 people and is contributing to a growing middle class. BPO facilities are concentrated in IT parks and centers in Economic Zones across the Philippines. BPO facilities are located mainly in Metro Manila and Cebu City although other regional areas such as Baguio, Bacolod, Cagayan de Oro, Clark Freeport Zone, Dagupan, Davao City, Dumaguete, Lipa, Iloilo City, and Naga City, Camarines Sur are now being promoted and developed for BPO operations. Call centers began in the Philippines as plain providers of email response and managing services and is now a major source of employment. Call center services include customer relations, ranging from travel services, technical support, education, customer care, financial services, online business to customer support, and online business-to-business support. Business process outsourcing (BPO) is regarded as one of the fastest growing industries in the world. The Philippines is also considered as a location of choice due to its many outsourcing benefits such as less expensive operational and labor costs, the high proficiency in spoken English of a significant number of its people, and a highly educated labor pool. In 2011, the business process outsourcing industry in the Philippines generated 700 thousand jobs and some US$11 billion in revenue, 24 percent higher than 2010. By 2016, the industry is projected to reach US$27.4 billion in revenue with employment generation to almost double at 1.3 million workers.BPOs and the call center industry in general are also credited for the Philippines' recent economic growth resulting in investment status upgrades from credit ratings agencies such as Fitch and S&P.With the Philippines being the 34th largest economy in the world, the country continues to be a promising prospect for the BPO Industry. Just in August 2014, the Philippines hit an all-time high for employment in the BPO industry. From 101,000 workers in 2004, the labor force in the industry has grown to over 930,000 in just the first quarter of 2014.Growth in the BPO industry continues to show significant improvements with an average annual expansion rate of 20%. Figures have shown that from $1.3 Billion in 2004, export revenues from the BPO sector has increased to over $13.1 Billion in 2013. The IT and Business Process Association of the Philippines (IBPAP) also projects that the sector will have an expected total revenue of $25 Billion in 2016. IBPAP projects that the industry will employ 1.8 million workers and generate US$38.9 billion of revenue by 2022.This growth in the industry is further promoted by the Philippine government. The industry is highlighted by the Philippines Development Plan as among the 10 high potential and priority development areas. To further entice investors, government programs include different incentives such as tax holidays, tax exemptions, and simplified export and import procedures. Additionally, training is also available for BPO applicants.
Tourism is an important sector for the Philippine economy, contributing 7.8% to the Philippine gross domestic product (GDP) in 2014.The tourism industry employed 3.8 million Filipinos, or 10.2 per cent of national employment in 2011, according to data gathered by the National Statistical Coordination Board. In a greater thrust by the Aquino administration to pump billion to employ 7.4 million people by 2016, or about 18.8 per cent of the total workforce, contributing 8 per cent to 9 per cent to the nation's GDP.In 2014, the tourism sector contributed 1.4 trillion pesos to the country's economy.
According to PSA, Gross Regional Domestic Product (GRDP) is GDP measured at regional levels. Figures below are for the year 2018: Note: Green-colored cells indicate higher value or best performance in index, while yellow-colored cells indicate the opposite. Numbers may not add up to totals due to rounding.
The national government budget for 2016 has set the following budget allocations:
Bamboo network Economy of Asia Emerging markets List of companies of the Philippines Newly Industrialized countries Philippine Statistics Authority Tiger Cub Economies
Bangko Sentral ng Pilipinas (Central Bank of the Philippines) National Statistical Coordination Board Department of Trade and Industry Department of Finance Philippine Stock Exchange National Federation of Sugarcane Planters Department of Tourism Philippines Business Brokers Philippine Economic Zone AuthorityTradeWorld Bank Summary Trade Statistics Philippines 2012 Tariffs applied by the Philippines as provided by ITC's Market Access Map, an online database of customs tariffs and market requirements
An economy (from Greek οίκος – "household" and νέμoμαι – "manage") is an area of the production, distribution and trade, as well as consumption of goods and services by different agents. Understood in its broadest sense, 'The economy is defined as a social domain that emphasize the practices, discourses, and material expressions associated with the production, use, and management of resources'. A given economy is the result of a set of processes that involves its culture, values, education, technological evolution, history, social organization, political structure and legal systems, as well as its geography, natural resource endowment, and ecology, as main factors. These factors give context, content, and set the conditions and parameters in which an economy functions. In other words, the economic domain is a social domain of human practices and transactions. It does not stand alone. Economic agents can be individuals, businesses, organizations, or governments. Economic transactions occur when two groups or parties agree to the value or price of the transacted good or service, commonly expressed in a certain currency. However, monetary transactions only account for a small part of the economic domain. Economic activity is spurred by production which uses natural resources, labor and capital. It has changed over time due to technology (automation, accelerator of process, reduction of cost functions), innovation (new products, services, processes, expanding markets, diversification of markets, niche markets, increases revenue functions) such as, that which produces intellectual property and changes in industrial relations (most notably child labor being replaced in some parts of the world with universal access to education). A market-based economy is one where goods and services are produced and exchanged according to demand and supply between participants (economic agents) by barter or a medium of exchange with a credit or debit value accepted within the network, such as a unit of currency. A command-based economy is one where political agents directly control what is produced and how it is sold and distributed. A green economy is low-carbon, resource efficient and socially inclusive. In a green economy, growth in income and employment is driven by public and private investments that reduce carbon emissions and pollution, enhance energy and resource efficiency, and prevent the loss of biodiversity and ecosystem services. A gig economy is one in which short-term jobs are assigned or chosen via online platforms. New economy is a term that referred to the whole emerging ecosystem where new standards and practices were introduced, usually as a result of technological innovations. The global economy refers to humanity's economic system or systems overall.
Today the range of fields of study examining the economy revolves around the social science of economics, but may include sociology (economic sociology), history (economic history), anthropology (economic anthropology), and geography (economic geography). Practical fields directly related to the human activities involving production, distribution, exchange, and consumption of goods and services as a whole are engineering, management, business administration, applied science, and finance. All professions, occupations, economic agents or economic activities, contribute to the economy. Consumption, saving, and investment are variable components in the economy that determine macroeconomic equilibrium. There are three main sectors of economic activity: primary, secondary, and tertiary. Due to the growing importance of the economical sector in modern times, the term real economy is used by analysts as well as politicians to denote the part of the economy that is concerned with the actual production of goods and services, as ostensibly contrasted with the paper economy, or the financial side of the economy, which is concerned with buying and selling on the financial markets. Alternate and long-standing terminology distinguishes measures of an economy expressed in real values (adjusted for inflation), such as real GDP, or in nominal values (unadjusted for inflation).
As long as someone has been making, supplying and distributing goods or services, there has been some sort of economy; economies grew larger as societies grew and became more complex. Sumer developed a large-scale economy based on commodity money, while the Babylonians and their neighboring city states later developed the earliest system of economics as we think of, in terms of rules/laws on debt, legal contracts and law codes relating to business practices, and private property.The Babylonians and their city state neighbors developed forms of economics comparable to currently used civil society (law) concepts. They developed the first known codified legal and administrative systems, complete with courts, jails, and government records. The ancient economy was mainly based on subsistence farming. The Shekel referred to an ancient unit of weight and currency. The first usage of the term came from Mesopotamia circa 3000 BC. and referred to a specific mass of barley which related other values in a metric such as silver, bronze, copper etc. A barley/shekel was originally both a unit of currency and a unit of weight, just as the British Pound was originally a unit denominating a one-pound mass of silver. For most people, the exchange of goods occurred through social relationships. There were also traders who bartered in the marketplaces. In Ancient Greece, where the present English word 'economy' originated, many people were bond slaves of the freeholders. The economic discussion was driven by scarcity.
The European captures became branches of the European states, the so-called colonies. The rising nation-states Spain, Portugal, France, Great Britain and the Netherlands tried to control the trade through custom duties and (from mercator, lat.: merchant) was a first approach to intermediate between private wealth and public interest. The secularization in Europe allowed states to use the immense property of the church for the development of towns. The influence of the nobles decreased. The first Secretaries of State for economy started their work. Bankers like Amschel Mayer Rothschild (1773–1855) started to finance national projects such as wars and infrastructure. Economy from then on meant national economy as a topic for the economic activities of the citizens of a state.
The first economist in the true modern meaning of the word was the Scotsman Adam Smith (1723–1790) who was inspired partly by the ideas of physiocracy, a reaction to mercantilism and also later Economics student, Adam Mari. He defined the elements of a national economy: products are offered at a natural price generated by the use of competition - supply and demand - and the division of labor. He maintained that the basic motive for free trade is human self-interest. The so-called self-interest hypothesis became the anthropological basis for economics. Thomas Malthus (1766–1834) transferred the idea of supply and demand to the problem of overpopulation. The Industrial Revolution was a period from the 18th to the 19th century where major changes in agriculture, manufacturing, mining, and transport had a profound effect on the socioeconomic and cultural conditions starting in the United Kingdom, then subsequently spreading throughout Europe, North America, and eventually the world. The onset of the Industrial Revolution marked a major turning point in human history; almost every aspect of daily life was eventually influenced in some way. In Europe wild capitalism started to replace the system of mercantilism (today: protectionism) and led to economic growth. The period today is called industrial revolution because the system of Production, production and division of labor enabled the mass production of goods.
The contemporary concept of "the economy" wasn't popularly known until the American Great Depression in the 1930s.After the chaos of two World Wars and the devastating Great Depression, policymakers searched for new ways of controlling the course of the economy. This was explored and discussed by Friedrich August von Hayek (1899–1992) and Milton Friedman (1912–2006) who pleaded for a global free trade and are supposed to be the fathers of the so-called neoliberalism. However, the prevailing view was that held by John Maynard Keynes (1883–1946), who argued for a stronger control of the markets by the state. The theory that the state can alleviate economic problems and instigate economic growth through state manipulation of aggregate demand is called Keynesianism in his honor. In the late 1950s, the economic growth in America and Europe—often called Wirtschaftswunder (ger: economic miracle) —brought up a new form of economy: mass consumption economy. In 1958, John Kenneth Galbraith (1908–2006) was the first to speak of an affluent society. In most of the countries the economic system is called a social market economy.
With the fall of the Iron Curtain and the transition of the countries of the Eastern Bloc towards democratic government and market economies, the idea of the post-industrial society is brought into importance as its role is to mark together the significance that the service sector receives instead of industrialization. Some attribute the first use of this term to Daniel Bell's 1973 book, The Coming of Post-Industrial Society, while others attribute it to social philosopher Ivan Illich's book, Tools for Conviviality. The term is also applied in philosophy to designate the fading of postmodernism in the late 90s and especially in the beginning of the 21st century. With the spread of Internet as a mass media and communication medium especially after 2000-2001, the idea for the Internet and information economy is given place because of the growing importance of e-commerce and electronic businesses, also the term for a global information society as understanding of a new type of "all-connected" society is created. In the late 2000s, the new type of economies and economic expansions of countries like China, Brazil, and India bring attention and interest to different from the usually dominating Western type economies and economic models.
The economy may be considered as having developed through the following phases or degrees of precedence. The ancient economy was mainly based on subsistence farming. The industrial revolution phase lessened the role of subsistence farming, converting it to more extensive and mono-cultural forms of agriculture in the last three centuries. The economic growth took place mostly in mining, construction and manufacturing industries. Commerce became more significant due to the need for improved exchange and distribution of produce throughout the community. In the economies of modern consumer societies phase there is a growing part played by services, finance, and technology—the knowledge economy.In modern economies, these phase precedences are somewhat differently expressed by the three-sector theory. Primary stage/degree of the economy: Involves the extraction and production of raw materials, such as corn, coal, wood and iron. (A coal miner and a fisherman would be workers in the primary degree.) Secondary stage/degree of the economy: Involves the transformation of raw or intermediate materials into goods e.g. manufacturing steel into cars, or textiles into clothing. (A builder and a dressmaker would be workers in the secondary degree.) At this stage the associated industrial economy is also sub-divided into several economic sectors (also called industries). Their separate evolution during the Industrial Revolution phase is dealt with elsewhere. Tertiary stage/degree of the economy: Involves the provision of services to consumers and businesses, such as baby-sitting, cinema and banking. (A shopkeeper and an accountant would be workers in the tertiary degree.) Quaternary stage/degree of the economy: Involves the research and development needed to produce products from natural resources and their subsequent by-products. (A logging company might research ways to use partially burnt wood to be processed so that the undamaged portions of it can be made into pulp for paper.) Note that education is sometimes included in this sector.Other sectors of the developed community include : the public sector or state sector (which usually includes: parliament, law-courts and government centers, various emergency services, public health, shelters for impoverished and threatened people, transport facilities, air/sea ports, post-natal care, hospitals, schools, libraries, museums, preserved historical buildings, parks/gardens, nature-reserves, some universities, national sports grounds/stadiums, national arts/concert-halls or theaters and centers for various religions). the private sector or privately run businesses. the social sector or voluntary sector.
There are a number of concepts associated with the economy, such as these:
The GDP (gross domestic product) of a country is a measure of the size of its economy. The most conventional economic analysis of a country relies heavily on economic indicators like the GDP and GDP per capita. While often useful, GDP only includes economic activity for which money is exchanged.
An informal economy is economic activity that is neither taxed nor monitored by a government, contrasted with a formal economy. The informal economy is thus not included in that government's gross national product (GNP). Although the informal economy is often associated with developing countries, all economic systems contain an informal economy in some proportion. Informal economic activity is a dynamic process that includes many aspects of economic and social theory including exchange, regulation, and enforcement. By its nature, it is necessarily difficult to observe, study, define, and measure. No single source readily or authoritatively defines informal economy as a unit of study. The terms "underground", "under the table" and "off the books" typically refer to this type of economy. The term black market refers to a specific subset of the informal economy. The term "informal sector" was used in many earlier studies, and has been mostly replaced in more recent studies which use the newer term. The informal sector makes up a significant portion of the economies in developing countries but it is often stigmatized as troublesome and unmanageable. However, the informal sector provides critical economic opportunities for the poor and has been expanding rapidly since the 1960s. As such, integrating the informal economy into the formal sector is an important policy challenge.
Economic research is conducted in fields as different as economics, economic sociology, economic anthropology, and economic history.
Aristotle, Politics, Book I-IIX, translated by Benjamin Jowett, Classics.mit.edu Barnes, Peter, Capitalism 3.0, A Guide to Reclaiming the Commons, San Francisco 2006, Whatiseconomy.com Dill, Alexander, Reclaiming the Hidden Assets, Towards a Global Freeware Index, Global Freeware Research Paper 01-07, 2007, Whatiseconomy.com Fehr Ernst, Schmidt, Klaus M., The Economics Of Fairness, Reciprocity and Altruism - experimental Evidence and new Theories, 2005, Discussion PAPER 2005-20, Munich Economics, Whatiseconomy.com Marx, Karl, Engels, Friedrich, 1848, The Communist Manifesto, Marxists.org Stiglitz, Joseph E., Global public goods and global finance: does global governance ensure that the global public interest is served? In: Advancing Public Goods, Jean-Philippe Touffut, (ed.), Paris 2006, pp. 149/164, GSB.columbia.edu Where is the Wealth of Nations? Measuring Capital for the 21st Century. Wealth of Nations Report 2006, Ian Johnson and Francois Bourguignon, World Bank, Washington 2006, Whatiseconomy.com.
Several major groups of animals typically have readily distinguishable eggs.
The most common reproductive strategy for fish is known as oviparity, in which the female lays undeveloped eggs that are externally fertilized by a male. Typically large numbers of eggs are laid at one time (an adult female cod can produce 4–6 million eggs in one spawning) and the eggs are then left to develop without parental care. When the larvae hatch from the egg, they often carry the remains of the yolk in a yolk sac which continues to nourish the larvae for a few days as they learn how to swim. Once the yolk is consumed, there is a critical point after which they must learn how to hunt and feed or they will die. A few fish, notably the rays and most sharks use ovoviviparity in which the eggs are fertilized and develop internally. However, the larvae still grow inside the egg consuming the egg's yolk and without any direct nourishment from the mother. The mother then gives birth to relatively mature young. In certain instances, the physically most developed offspring will devour its smaller siblings for further nutrition while still within the mother's body. This is known as intrauterine cannibalism. In certain scenarios, some fish such as the hammerhead shark and reef shark are viviparous, with the egg being fertilized and developed internally, but with the mother also providing direct nourishment. The eggs of fish and amphibians are jellylike. Cartilaginous fish (sharks, skates, rays, chimaeras) eggs are fertilized internally and exhibit a wide variety of both internal and external embryonic development. Most fish species spawn eggs that are fertilized externally, typically with the male inseminating the eggs after the female lays them. These eggs do not have a shell and would dry out in the air. Even air-breathing amphibians lay their eggs in water, or in protective foam as with the Coast foam-nest treefrog, Chiromantis xerampelina.
The default color of vertebrate eggs is the white of the calcium carbonate from which the shells are made, but some birds, mainly passerines, produce colored eggs. The pigment biliverdin and its zinc chelate give a green or blue ground color, and protoporphyrin produces reds and browns as a ground color or as spotting. Non-passerines typically have white eggs, except in some ground-nesting groups such as the Charadriiformes, sandgrouse and nightjars, where camouflage is necessary, and some parasitic cuckoos which have to match the passerine host's egg. Most passerines, in contrast, lay colored eggs, even if there is no need of cryptic colors. However some have suggested that the protoporphyrin markings on passerine eggs actually act to reduce brittleness by acting as a solid-state lubricant. If there is insufficient calcium available in the local soil, the egg shell may be thin, especially in a circle around the broad end. Protoporphyrin speckling compensates for this, and increases inversely to the amount of calcium in the soil.For the same reason, later eggs in a clutch are more spotted than early ones as the female's store of calcium is depleted. The color of individual eggs is also genetically influenced, and appears to be inherited through the mother only, suggesting that the gene responsible for pigmentation is on the sex-determining W chromosome (female birds are WZ, males ZZ). It used to be thought that color was applied to the shell immediately before laying, but subsequent research shows that coloration is an integral part of the development of the shell, with the same protein responsible for depositing calcium carbonate, or protoporphyrins when there is a lack of that mineral. In species such as the common guillemot, which nest in large groups, each female's eggs have very different markings, making it easier for females to identify their own eggs on the crowded cliff ledges on which they breed.
Bird eggshells are diverse. For example: cormorant eggs are rough and chalky tinamou eggs are shiny duck eggs are oily and waterproof cassowary eggs are heavily pittedTiny pores in bird eggshells allow the embryo to breathe. The domestic hen's egg has around 7000 pores.Some bird eggshells have a coating of vaterite spherules, which is a rare polymorph of calcium carbonate. In Greater Ani Crotophaga major this vaterite coating is thought to act as a shock absorber, protecting the calcite shell from fracture during incubation, such as colliding with other eggs in the nest.
Most bird eggs have an oval shape, with one end rounded and the other more pointed. This shape results from the egg being forced through the oviduct. Muscles contract the oviduct behind the egg, pushing it forward. The egg's wall is still shapeable, and the pointed end develops at the back. Long, pointy eggs are an incidental consequence of having a streamlined body typical of birds with strong flying abilities; flight narrows the oviduct, which changes the type of egg a bird can lay. Cliff-nesting birds often have highly conical eggs. They are less likely to roll off, tending instead to roll around in a tight circle; this trait is likely to have arisen due to evolution via natural selection. In contrast, many hole-nesting birds have nearly spherical eggs.
Like amphibians, amniotes are air-breathing vertebrates, but they have complex eggs or embryos, including an amniotic membrane. Amniotes include reptiles (including dinosaurs and their descendants, birds) and mammals. Reptile eggs are often rubbery and are always initially white. They are able to survive in the air. Often the sex of the developing embryo is determined by the temperature of the surroundings, with cooler temperatures favouring males. Not all reptiles lay eggs; some are viviparous ("live birth"). Dinosaurs laid eggs, some of which have been preserved as petrified fossils. Among mammals, early extinct species laid eggs, as do platypuses and echidnas (spiny anteaters). Platypuses and two genera of echidna are Australian monotremes. Marsupial and placental mammals do not lay eggs, but their unborn young do have the complex tissues that identify amniotes.
The eggs of the egg-laying mammals (the platypus and the echidnas) are macrolecithal eggs very much like those of reptiles. The eggs of marsupials are likewise macrolecithal, but rather small, and develop inside the body of the female, but do not form a placenta. The young are born at a very early stage, and can be classified as a "larva" in the biological sense.In placental mammals, the egg itself is void of yolk, but develops an umbilical cord from structures that in reptiles would form the yolk sac. Receiving nutrients from the mother, the fetus completes the development while inside the uterus.
Eggs are common among invertebrates, including insects, spiders, mollusks, and crustaceans.
All sexually reproducing life, including both plants and animals, produces gametes. The male gamete cell, sperm, is usually motile whereas the female gamete cell, the ovum, is generally larger and sessile. The male and female gametes combine to produce the zygote cell. In multicellular organisms the zygote subsequently divides in an organised manner into smaller more specialised cells, so that this new individual develops into an embryo. In most animals the embryo is the sessile initial stage of the individual life cycle, and is followed by the emergence (that is, the hatching) of a motile stage. The zygote or the ovum itself or the sessile organic vessel containing the developing embryo may be called the egg. A recent proposal suggests that the phylotypic animal body plans originated in cell aggregates before the existence of an egg stage of development. Eggs, in this view, were later evolutionary innovations, selected for their role in ensuring genetic uniformity among the cells of incipient multicellular organisms.
Scientists often classify animal reproduction according to the degree of development that occurs before the new individuals are expelled from the adult body, and by the yolk which the egg provides to nourish the embryo.
Vertebrate eggs can be classified by the relative amount of yolk. Simple eggs with little yolk are called microlecithal, medium-sized eggs with some yolk are called mesolecithal, and large eggs with a large concentrated yolk are called macrolecithal. This classification of eggs is based on the eggs of chordates, though the basic principle extends to the whole animal kingdom.
Small eggs with little yolk are called microlecithal. The yolk is evenly distributed, so the cleavage of the egg cell cuts through and divides the egg into cells of fairly similar sizes. In sponges and cnidarians the dividing eggs develop directly into a simple larva, rather like a morula with cilia. In cnidarians, this stage is called the planula, and either develops directly into the adult animals or forms new adult individuals through a process of budding.Microlecithal eggs require minimal yolk mass. Such eggs are found in flatworms, roundworms, annelids, bivalves, echinoderms, the lancelet and in most marine arthropods. In anatomically simple animals, such as cnidarians and flatworms, the fetal development can be quite short, and even microlecithal eggs can undergo direct development. These small eggs can be produced in large numbers. In animals with high egg mortality, microlecithal eggs are the norm, as in bivalves and marine arthropods. However, the latter are more complex anatomically than e.g. flatworms, and the small microlecithal eggs do not allow full development. Instead, the eggs hatch into larvae, which may be markedly different from the adult animal. In placental mammals, where the embryo is nourished by the mother throughout the whole fetal period, the egg is reduced in size to essentially a naked egg cell.
Mesolecithal eggs have comparatively more yolk than the microlecithal eggs. The yolk is concentrated in one part of the egg (the vegetal pole), with the cell nucleus and most of the cytoplasm in the other (the animal pole). The cell cleavage is uneven, and mainly concentrated in the cytoplasma-rich animal pole.The larger yolk content of the mesolecithal eggs allows for a longer fetal development. Comparatively anatomically simple animals will be able to go through the full development and leave the egg in a form reminiscent of the adult animal. This is the situation found in hagfish and some snails. Animals with smaller size eggs or more advanced anatomy will still have a distinct larval stage, though the larva will be basically similar to the adult animal, as in lampreys, coelacanth and the salamanders.
Eggs with a large yolk are called macrolecithal. The eggs are usually few in number, and the embryos have enough food to go through full fetal development in most groups. Macrolecithal eggs are only found in selected representatives of two groups: Cephalopods and vertebrates.Macrolecithal eggs go through a different type of development than other eggs. Due to the large size of the yolk, the cell division can not split up the yolk mass. The fetus instead develops as a plate-like structure on top of the yolk mass, and only envelopes it at a later stage. A portion of the yolk mass is still present as an external or semi-external yolk sac at hatching in many groups. This form of fetal development is common in bony fish, even though their eggs can be quite small. Despite their macrolecithal structure, the small size of the eggs does not allow for direct development, and the eggs hatch to a larval stage ("fry"). In terrestrial animals with macrolecithal eggs, the large volume to surface ratio necessitates structures to aid in transport of oxygen and carbon dioxide, and for storage of waste products so that the embryo does not suffocate or get poisoned from its own waste while inside the egg, see amniote.In addition to bony fish and cephalopods, macrolecithal eggs are found in cartilaginous fish, reptiles, birds and monotreme mammals. The eggs of the coelacanths can reach a size of 9 cm (3.5 in) in diameter, and the young go through full development while in the uterus, living on the copious yolk.
Animals are commonly classified by their manner of reproduction, at the most general level distinguishing egg-laying (Latin. oviparous) from live-bearing (Latin. viviparous). These classifications are divided into more detail according to the development that occurs before the offspring are expelled from the adult's body. Traditionally: Ovuliparity means the female spawns unfertilized eggs (ova), which must then be externally fertilised. Ovuliparity is typical of bony fish, anurans, echinoderms, bivalves and cnidarians. Most aquatic organisms are ovuliparous. The term is derived from the diminutive meaning "little egg". Oviparity is where fertilisation occurs internally and so the eggs laid by the female are zygotes (or newly developing embryos), often with important outer tissues added (for example, in a chicken egg, no part outside of the yolk originates with the zygote). Oviparity is typical of birds, reptiles, some cartilaginous fish and most arthropods. Terrestrial organisms are typically oviparous, with egg-casings that resist evaporation of moisture. Ovo-viviparity is where the zygote is retained in the adult's body but there are no trophic (feeding) interactions. That is, the embryo still obtains all of its nutrients from inside the egg. Most live-bearing fish, amphibians or reptiles are actually ovoviviparous. Examples include the reptile Anguis fragilis, the sea horse (where zygotes are retained in the male's ventral "marsupium"), and the frogs Rhinoderma darwinii (where the eggs develop in the vocal sac) and Rheobatrachus (where the eggs develop in the stomach). Histotrophic viviparity means embryos develop in the female's oviducts but obtain nutrients by consuming other ova, zygotes or sibling embryos (oophagy or adelphophagy). This intra-uterine cannibalism occurs in some sharks and in the black salamander Salamandra atra. Marsupials excrete a "uterine milk" supplementing the nourishment from the yolk sac. Hemotrophic viviparity is where nutrients are provided from the female's blood through a designated organ. This most commonly occurs through a placenta, found in most mammals. Similar structures are found in some sharks and in the lizard Pseudomoia pagenstecheri. In some hylid frogs, the embryo is fed by the mother through specialized gills.The term hemotropic derives from the Latin for blood-feeding, contrasted with histotrophic for tissue-feeding.
Eggs laid by many different species, including birds, reptiles, amphibians, and fish, have probably been eaten by mankind for millennia. Popular choices for egg consumption are chicken, duck, roe, and caviar, but by a wide margin the egg most often humanly consumed is the chicken egg, typically unfertilized.
According to the Kashrut, that is the set of Jewish dietary laws, kosher food may be consumed according to halakha (Jewish law). Kosher meat and milk (or derivatives) cannot be mixed (Deuteronomy 14:21) or stored together. Eggs are considered pareve (neither meat nor dairy) despite being an animal product and can be mixed with either milk or kosher meat. Mayonnaise, for instance, is usually marked "pareve" despite by definition containing egg.
Many vaccines for infectious diseases are produced in fertile chicken eggs. The basis of this technology was the discovery in 1931 by Alice Miles Woodruff and Ernest William Goodpasture at Vanderbilt University that the rickettsia and viruses that cause a variety of diseases will grow in chicken embryos. This enabled the development of vaccines against influenza, chicken pox, smallpox, yellow fever, typhus, Rocky mountain spotted fever and other diseases.
The egg is a symbol of new life and rebirth in many cultures around the world. Christians view Easter eggs as symbolic of the resurrection of Jesus Christ. A popular Easter tradition in some parts of the world is the decoration of hard-boiled eggs (usually by dyeing, but often by hand-painting or spray-painting). Adults often hide the eggs for children to find, an activity known as an Easter egg hunt. A similar tradition of egg painting exists in areas of the world influenced by the culture of Persia. Before the spring equinox in the Persian New Year tradition (called Norouz), each family member decorates a hard-boiled egg and sets them together in a bowl. The tradition of a dancing egg is held during the feast of Corpus Christi in Barcelona and other Catalan cities since the 16th century. It consists of an emptied egg, positioned over the water jet from a fountain, which starts turning without falling.Although a food item, raw eggs are sometimes thrown at houses, cars, or people. This act, known commonly as "egging" in the various English-speaking countries, is a minor form of vandalism and, therefore, usually a criminal offense and is capable of damaging property (egg whites can degrade certain types of vehicle paint) as well as potentially causing serious eye injury. On Halloween, for example, trick or treaters have been known to throw eggs (and sometimes flour) at property or people from whom they received nothing. Eggs are also often thrown in protests, as they are inexpensive and nonlethal, yet very messy when broken.
Egg collecting was a popular hobby in some cultures, including among the first Australians. Traditionally, the embryo would be removed before a collector stored the egg shell.Collecting eggs of wild birds is now banned by many jurisdictions, as the practice can threaten rare species. In the United Kingdom, the practice is prohibited by the Protection of Birds Act 1954 and Wildlife and Countryside Act 1981. On the other hand, ongoing underground trading is becoming a serious issue.Since the protection of wild bird eggs was regulated, early collections have come to the museums as curiosities. For example, the Australian Museum hosts a collection of about 20,000 registered clutches of eggs, and the collection in Western Australia Museum has been archived in a gallery. Scientists regard egg collections as a good natural-history data, as the details recorded in the collectors' notes have helped them to understand birds' nesting behaviors.
Francis Bacon (28 October 1909 – 28 April 1992) was an Irish-born English figurative painter known for his raw, unsettling imagery. Focusing on the human form, his subjects included crucifixions, portraits of popes, self-portraits, and portraits of close friends, with abstracted figures sometimes isolated in geometrical structures. Rejecting various classifications of his work, Bacon claimed that he strove to render "the brutality of fact."Bacon said that he saw images "in series", and his work, which numbers c. 590 extant paintings along with many others he destroyed, typically focused on a single subject for sustained periods, often in triptych or diptych formats. His output can be broadly described as sequences or variations on single motifs; including the 1930s Picasso-influenced bio-morphs and Furies, the 1940s male heads isolated in rooms or geometric structures, the 1950s "screaming popes," the mid-to-late 1950s animals and lone figures, the early 1960s crucifixions, the mid-to-late 1960s portraits of friends, the 1970s self-portraits, and the cooler, more technical 1980s paintings. Bacon did not begin to paint until his late twenties, having drifted in the late 1920s and early 1930s as an interior decorator, bon vivant, and gambler. He said that his artistic career was delayed because he spent too long looking for subject matter that could sustain his interest. His breakthrough came with the 1944 triptych Three Studies for Figures at the Base of a Crucifixion, which sealed his reputation as a uniquely bleak chronicler of the human condition. From the mid-1960s he mainly produced portraits of friends and drinking companions, either as single, diptych or triptych panels. Following the suicide of his lover George Dyer in 1971 (memorialized in his Black Triptychs, and a number of posthumous portraits) his art became more sombre, inward-looking and preoccupied with the passage of time and death. The climax of his later period is marked the masterpieces Study for Self-Portrait (1982) and Study for a Self-Portrait—Triptych, 1985–86. Despite his existentialist and bleak outlook, Bacon was charismatic, articulate and well-read. A bon vivant, he spent his middle age eating, drinking and gambling in London's Soho with like-minded friends including Lucian Freud (although they fell out in the mid-1970s, for reasons neither ever explained), John Deakin, Muriel Belcher, Henrietta Moraes, Daniel Farson, Tom Baker and Jeffrey Bernard. After Dyer's suicide he largely distanced himself from this circle, and while still socially active and his passion for gambling and drinking continued, he settled into a platonic and somewhat fatherly relationship with his eventual heir, John Edwards. Since his death, Bacon's reputation has grown steadily, and his work is among the most acclaimed, expensive and sought-after on the art market. In the late 1990s a number of major works, previously assumed destroyed, including early 1950s popes and 1960s portraits, re-emerged to set record prices at auction.
Francis Bacon was born on 28 October 1909 in 63 Lower Baggot Street, Dublin. His father, Captain Anthony Edward Mortimer Bacon, known as Eddy, was born in Adelaide, South Australia, to an English father and an Australian mother. Eddie was a veteran of the Boer War, a racehorse trainer, and grandson of Anthony Bacon, who claimed descent from Sir Nicholas Bacon, elder half-brother of Sir Francis Bacon, the Elizabethan statesman, philosopher and essayist. Francis's mother, Christina Winifred Firth, known as Winnie, was heiress to a Sheffield steel business and coal mine. Bacon had an older brother, Harley, two younger sisters, Ianthe and Winifred, and a younger brother, Edward. Francis was raised by the family nanny, Jessie Lightfoot, from Cornwall, known as 'Nanny Lightfoot', a maternal figure who remained close to him until her death. During the early 1940s, he rented the ground floor of 7 Cromwell Place, South Kensington, John Everett Millais's old studio, and Nanny Lightfoot helped him install an illicit roulette wheel there, organised by Bacon and his friends, for their financial benefit. The family moved house often, moving between Ireland and England several times, leading to a sense of displacement which remained with Francis throughout his life. The family lived in Cannycourt House in County Kildare from 1911, later moving to Westbourne Terrace in London, close to where Bacon's father worked at the Territorial Force Records Office. They returned to Ireland after the First World War. Bacon lived with his maternal grandmother and step-grandfather, Winifred and Kerry Supple, at Farmleigh, Abbeyleix, County Laois, although the rest of the family again moved to Straffan Lodge near Naas, County Kildare. Bacon was shy as a child, and enjoyed dressing up. This, and his effeminate manner, angered his father. A story emerged in 1992 of his father having had Francis horsewhipped by their grooms. In 1924 his parents moved to Gloucestershire, first to Prescott House in Gotherington, then Linton Hall near the border with Herefordshire. At a fancy-dress party at the Firth family home, Cavendish Hall in Suffolk, Francis dressed as a flapper with an Eton crop, beaded dress, lipstick, high heels, and a long cigarette holder. In 1926, the family moved back to Straffan Lodge. His sister, Ianthe, twelve years his junior, recalled that Bacon made drawings of ladies with cloche hats and long cigarette holders. Later that year, Francis was thrown out of Straffan Lodge following an incident in which his father found him admiring himself in front of a large mirror wearing his mother's underwear.
The 1933 Crucifixion was his first painting to attract public attention, and was in part based on Pablo Picasso's The Three Dancers of 1925. It was not well received; disillusioned, he abandoned painting for nearly a decade, and suppressed his earlier works. He visited Paris in 1935 where he bought a secondhand book on anatomical diseases of the mouth containing high quality hand-coloured plates of both open mouths and oral interiors, which haunted and obsessed him for the remainder of his life. These and the scene with the nurse screaming on the Odessa steps from the Battleship Potemkin later became recurrent parts of Bacon's iconography, with the angularity of Eisenstein's images often combined with the thick red palette of his recently purchased medical tome. In the winter of 1935–36, Roland Penrose and Herbert Read, making a first selection for the International Surrealist Exhibition, visited his studio at 71 Royal Hospital Road, Chelsea saw "three or four large canvases including one with a grandfather clock", but found his work "insufficiently surreal to be included in the show". Bacon claimed Penrose told him "Mr. Bacon, don't you realise a lot has happened in painting since the Impressionists?" In 1936 or 1937 Bacon moved from 71 Royal Hospital Road to the top floor of 1 Glebe Place, Chelsea, which Eric Hall had rented. The following year, Patrick White moved to the top two floors of the building where De Maistre had his studio, on Eccleston Street and commissioned from Bacon, by now a friend, a writing desk (with wide drawers and a red linoleum top). Expressing one of his basic concerns from the late 1930s, Bacon said that his artistic career was delayed because he spent too long looking for subject matter that could sustain his interest.In January 1937, at Thomas Agnew and Sons, 43 Old Bond Street, London, Bacon exhibited in a group show, Young British Painters, which included Graham Sutherland and Roy De Maistre. Eric Hall organised the show. Four works by Bacon were shown: Figures in a Garden (1936), purchased by Diana Watson; Abstraction, and Abstraction from the Human Form, known from magazine photographs. They prefigure Three Studies for Figures at the Base of a Crucifixion (1944) in alternatively representing a tripod structure (Abstraction), bared teeth (Abstraction from the Human Form), and both being biomorphic in form. Seated Figure is lost. On 1 June 1940 Bacon's father died. Bacon was named sole Trustee/Executor of his father's will, which requested the funeral be as "private and simple as possible". Unfit for active wartime service, Francis volunteered for civil defence and worked full-time in the Air Raid Precautions (ARP) rescue service; the fine dust of bombed London worsened his asthma and he was discharged. At the height of the Blitz, Eric Hall rented a cottage for Bacon and himself at Bedales Lodge in Steep, near Petersfield, Hampshire. Figure Getting Out of a Car (ca. 1939/1940) was painted here but is known only from an early 1946 photograph taken by Peter Rose Pulham. The photograph was taken shortly before the canvas was painted over by Bacon and retitled Landscape with Car. An ancestor to the biomorphic form of the central panel of Three Studies for Figures at the Base of a Crucifixion (1944), the composition was suggested by a photograph of Hitler getting out of a car at one of the Nuremberg rallies. Bacon claims to have "copied the car and not much else".Bacon and Hall in 1943 took the ground floor of 7 Cromwell Place, South Kensington, formerly the house and studio of John Everett Millais. High-vaulted and north-lit, its roof was recently bombed – Bacon was able to adapt a large old billiard room at the back as his studio. Lightfoot, lacking an alternative location, slept on the kitchen table. They held illicit roulette parties, organised by Bacon with the assistance of Hall.
By 1944 Bacon had gained confidence. His Three Studies for Figures at the Base of a Crucifixion had summarised themes explored in his earlier paintings, including his examination of Picasso's biomorphs, his interpretations of the Crucifixion, and the Greek Furies. It is generally considered his first mature piece; he regarded his works before the triptych as irrelevant. The painting caused a sensation when exhibited in 1945 and established him as a foremost post-war painter. Remarking on the cultural significance of Three Studies, John Russell observed in 1971 that "there was painting in England before the Three Studies, and painting after them, and no one ... can confuse the two."Painting (1946) was shown in several group shows including in the British section of Exposition internationale d'art moderne (18 November – 28 December 1946) at the Musée National d'Art Moderne, for which Bacon travelled to Paris. Within a fortnight of the sale of Painting (1946) to the Hanover Gallery Bacon used the proceeds to decamp from London to Monte Carlo. After staying at a succession of hotels and flats, including the Hôtel de Ré, Bacon settled in a large villa, La Frontalière, in the hills above the town. Hall and Lightfoot would come to stay. Bacon spent much of the next few years in Monte Carlo apart from short visits to London. From Monte Carlo, Bacon wrote to Sutherland and Erica Brausen. His letters to Brausen show he painted there, but no paintings are known to survive. Bacon said he became "obsessed" with the Casino de Monte Carlo, where he would "spend whole days". Falling in debt from gambling here, he was unable to afford a new canvas. This compelled him to paint on the raw, unprimed side of his previous work, a practice he kept throughout his life.In 1948, Painting (1946) sold to Alfred Barr for the Museum of Modern Art (MoMA) in New York for £240. Bacon wrote to Sutherland asking that he apply fixative to the patches of pastel on Painting (1946) before it was shipped to New York. (The work is now too fragile to be moved from MoMA for exhibition elsewhere.) At least one visit to Paris in 1946 brought Bacon into more immediate contact with French postwar painting and with Left Bank ideas such as Existentialism. He had, by this time, embarked on his lifelong friendship with Isabel Rawsthorne, a painter closely involved with Giacometti and the Left Bank set. They shared many interests, including ethnography and classical literature.
In 1947, Sutherland introduced Bacon to Brausen, who represented Bacon for twelve years. Despite this, Bacon did not mount a one-man show in Brausen's Hanover Gallery until 1949. Bacon returned to London and Cromwell Place late in 1948. The following year Bacon exhibited his "Heads" series, most notable for Head VI, Bacon's first surviving engagement with Velázquez's Portrait of Pope Innocent X (three 'popes' were painted in Monte Carlo in 1946 but were destroyed). He kept an extensive inventory of images for source material, but preferred not to confront the major works in person; he viewed Portrait of Innocent X only once, much later in his life.
Bacon met George Dyer in 1963 at a pub, although a much-repeated myth claims their acquaintance started during the younger man's burglary into the artist's apartment. Dyer was about 30 years old, from London's East End. He came from a family steeped in crime, and had till then spent his life drifting between theft, detention and jail. Bacon's earlier relationships had been with older and tumultuous men. His first lover, Peter Lacy, tore up the artist's paintings, beat him in drunken rages, at times leaving him on streets half-conscious. Bacon was now the dominating personality; attracted to Dyer's vulnerability and trusting nature. Dyer was impressed by Bacon's self-confidence and success, and Bacon acted as a protector and father figure to the insecure younger man.Dyer was, like Bacon, a borderline alcoholic and similarly took obsessive care with his appearance. Pale-faced and a chain-smoker, Dyer typically confronted his daily hangovers by drinking again. His compact and athletic build belied a docile and inwardly tortured personality, although the art critic Michael Peppiatt describes him as having the air of a man who could "land a decisive punch". Their behaviours eventually overwhelmed their affair, and by 1970 Bacon was merely providing Dyer with enough money to stay more or less permanently drunk.As Bacon's work moved from the extreme subject matter of his early paintings to portraits of friends in the mid-1960s, Dyer became a dominating presence. Bacon's paintings emphasises Dyer's physicality, yet are uncharacteristically tender. More than any other of Bacon's close friends, Dyer came to feel inseparable from his portraits. The paintings gave him stature, a raison d'etre, and offered meaning to what Bacon described as Dyer's "brief interlude between life and death". Many critics have described Dyer's portraits as favourites, including Michel Leiris and Lawrence Gowing. Yet as Dyer's novelty diminished within Bacon's circle of sophisticated intellectuals, the younger man became increasingly bitter and ill at ease. Although Dyer welcomed the attention the paintings brought him, he did not pretend to understand or even like them. "All that money an' I fink they're reely 'orrible," he observed with choked pride.Dyer abandoned crime but descended into alcoholism. Bacon's money attracted hangers-on for benders around London's Soho. Withdrawn and reserved when sober, Dyer was highly animated and aggressive when drunk, and often attempted to "pull a Bacon" by buying large rounds and paying for expensive dinners for his wide circle. Dyer's erratic behaviour inevitably wore thin with his cronies, with Bacon, and with Bacon's friends. Most of Bacon's art world associates regarded Dyer as a nuisance – an intrusion into the world of high culture to which their Bacon belonged. Dyer reacted by becoming increasingly needy and dependent. By 1971, he was drinking alone and only in occasional contact with his former lover. In October 1971, Dyer joined Bacon in Paris for the opening of the artist's retrospective at the Grand Palais. The show was the high point of Bacon's career to date, and he was now described as Britain's "greatest living painter". Dyer was a desperate man, and although he was "allowed" to attend, he was well aware that he was slipping out of the picture. To draw Bacon's attention, he planted cannabis in his flat and phoned the police, and attempted suicide on a number of occasions. On the eve of the Paris exhibition, Bacon and Dyer shared a hotel room, but Bacon was forced to escape in disgust to the room of gallery employee Terry Danziger-Miles, as Dyer was entertaining an Arab rent boy with "smelly feet". When Bacon returned to his room the next morning, together with Danziger-Miles and Valerie Beston, they discovered Dyer in the bathroom dead, sat on the toilet. With the agreement of the hotel manager, the party agreed not to announce the death for two days.Bacon spent the following day surrounded by people eager to meet him. In mid-evening of the following day he was "informed" that Dyer had taken an overdose of barbiturates and was dead. Bacon continued with the retrospective and displayed powers of self-control "to which few of us could aspire", according to Russell. Bacon was deeply affected by the loss of Dyer, and had recently lost four other friends and his nanny. From this point, death haunted his life and work. Though outwardly stoic at the time, he was inwardly broken. He did not express his feelings to critics, but later admitted to friends that "daemons, disaster and loss" now stalked him as if his own version of the Eumenides (Greek for The Furies). Bacon spent the remainder of his stay in Paris attending to promotional activities and funeral arrangements. He returned to London later that week to comfort Dyer's family. During the funeral many of Dyer's friends, including hardened East-End criminals, broke down in tears. As the coffin was lowered into the grave one friend was overcome and screamed "you bloody fool!" Bacon remained stoic during the proceedings, but in the following months suffered an emotional and physical breakdown. Deeply affected, over the following two years he painted a number of single canvas portraits of Dyer, and the three highly regarded "Black Triptychs", each of which details moments immediately before and after Dyer's suicide.
While holidaying in Madrid in 1992, Bacon was admitted to the Handmaids of Maria, a private clinic, where he was cared for by Sister Mercedes. His chronic asthma, which had plagued him all his life, had developed into a more severe respiratory condition and he could not talk or breathe very well. He died of a heart attack on 28 April 1992. He bequeathed his estate (then valued at £11 million) to his heir and sole legatee John Edwards, and to Brian Clarke, a friend of Bacon and Edwards who was installed as sole executor of the estate by the High Court. In 1998 the director of the Hugh Lane Gallery in Dublin secured the donation of the contents of Bacon's studio at 7 Reece Mews, South Kensington. The contents of his studio were moved and reconstructed in the gallery.A collection of drawings, some little more than scribbles, given by Bacon to his driver and handyman Barry Joule, surfaced in 1998, when Joule handed them over to the Tate Gallery. According to Joule the items were given as a gift, although they were probably to be destroyed. Their artistic and commercial value proved negligible but they provided some insight into Bacon's imagination and his thinking, in the early stages of conceiving a finished work. Today most of the works are in the Hugh Lane Gallery in Dublin.
The imagery of the crucifixion weighs heavily in the work of Francis Bacon. Critic John Russell wrote that the crucifixion in Bacon's work is a "generic name for an environment in which bodily harm is done to one or more persons and one or more other persons gather to watch". Bacon admitted that he saw the scene as "a magnificent armature on which you can hang all types of feeling and sensation". He believed the imagery of the crucifixion allowed him to examine "certain areas of human behaviour" in a unique way, as the armature of the theme had been accumulated by so many old masters.Though he came to painting relatively late in life – he did not begin to paint seriously until his late 30s – crucifixion scenes can be found in his earliest works. In 1933, his patron Eric Hall commissioned a series of three paintings based on the subject. The early paintings were influenced by such old masters as Matthias Grünewald, Diego Velázquez and Rembrandt, but also by Picasso's late 1920s/early 1930s biomorphs and the early work of the Surrealists.
Bacon's series of Popes, largely quoting Velázquez's famous portrait Pope Innocent X (1650, Galeria Doria Pamphili, Rome) are striking images which further develop motifs already found in his earlier works, like the "Study for Three Figures at the Base of a Crucifixion", such as the screaming open mouth. The figures of the popes, pictorially isolated by partly curved parallel lines indicating psychological forces and symbolizing inner energy like strength of feeling, are alienated from their original representation and, stripped of their representation of power to allegories of suffering humanity.
Many of Bacon's paintings are "inhabited" by reclining figures. Single, or, as in triptychs, repeated with variations, they can be commented by symbolic indexes (like circular arrows as signs for rotation), turning painted images to blueprints for moving images of the type of contemporary GIFs. The composition of especially the nude figures is influenced by the sculptural work of Michelangelo. The multi-phasing of his rendition of the figures, which often is also applied to the sitters in the portraits, is also a reference to Eadweard Muybridge's chronophotography.
The inspiration for the recurring motif of screaming mouths in many Bacons of the late 1940s and early 1950s was drawn from a number of sources, including medical text books, the works of Matthias Grünewald and photographic stills of the nurse in the Odessa Steps scene in Eisenstein's 1925 silent film Battleship Potemkin. Bacon saw the film in 1935, and viewed it frequently thereafter. He kept in his studio a photographic still of the scene, showing a close-up of the nurse's head screaming in panic and terror and with broken pince-nez spectacles hanging from her blood-stained face. He referred to the image throughout his career, using it as a source of inspiration.Bacon described the screaming mouth as a catalyst for his work, and incorporated its shape when painting the chimera. His use of the motif can be seen in one of his first surviving works, Abstraction from the Human Form. By the early 1950s it became an obsessive concern, to the point, according to art critic and Bacon biographer Michael Peppiatt, "it would be no exaggeration to say that, if one could really explain the origins and implications of this scream, one would be far closer to understanding the whole art of Francis Bacon."
In 1999, England's High Court ruled that Marlborough Fine Art had to be replaced by a new independent representative for the Bacon estate. The estate moved its business to Faggionato Fine Arts in Europe and Tony Shafrazi in New York. That same year, the estate sued Marlborough UK and Marlborough International, Vaduz, charging them with wrongfully exploiting Bacon in a relationship that was manifestly disadvantageous to him until his death in 1992, and to his estate. The suit alleged Marlborough in London grossly underpaid Bacon for his works and resold them through its Liechtenstein branch at much higher prices. It contended that Marlborough never supplied a complete accounting of Bacon's works and sales and that Marlborough handled some works it has never accounted for. The suit was dropped in early 2002 when both sides agreed to pay their own costs and Marlborough released all its documents about Bacon. In 2003, the estate was handed to a four-person trust based in Jersey.
A first, incomplete catalogue raisonné was compiled by curator Ronald Alley in 1964. In 2016, a five-volume Francis Bacon: Catalogue Raisonné, documenting 584 paintings by Bacon, was released by Martin Harrison and others.Harrison's Catalogue Raisonné summarized the artist's motivations when he gifted the second version of his triptych of the Crucifixion to the Tate in 1991. "This reprise of the painting Bacon considered his 'Opus I' is both larger and more monumental than the original, the sumptuous, dusky crimson backgrounds and deep space evoking a subverted Baroque altarpiece. Conscious no doubt of his artistic legacy, Bacon intended a posthumous bequest of the painting to the Tate; he was persuaded to bring forward the donation and, on condition there should be no ceremony, gifted it to the gallery in 1991".
Official website Francis Bacon image licensing at Artimage 44 paintings by or after Francis Bacon at the Art UK site Francis Bacon in the Tate Collection Francis Bacon in the Guggenheim Collection Francis Bacon at the Museum of Modern Art Tate Channel Francis Bacon: A Brush with Violence (2017) BBC on YouTube
Francis Bacon, 1st Viscount St Alban, (; 22 January 1561 – 9 April 1626), also known as Lord Verulam, was an English philosopher and statesman who served as Attorney General and as Lord Chancellor of England. His works are credited with developing the scientific method and remained influential through the scientific revolution.Bacon has been called the father of empiricism. His works argued for the possibility of scientific knowledge based only upon inductive reasoning and careful observation of events in nature. Most importantly, he argued science could be achieved by use of a sceptical and methodical approach whereby scientists aim to avoid misleading themselves. Although his most specific proposals about such a method, the Baconian method, did not have a long-lasting influence, the general idea of the importance and possibility of a sceptical methodology makes Bacon the father of the scientific method. This method was a new rhetorical and theoretical framework for science, the practical details of which are still central in debates about science and methodology. Francis Bacon was a patron of libraries and developed a functional system for the cataloguing of books by dividing them into three categories—history, poetry, and philosophy—which could further be divided into more specific subjects and subheadings. Bacon was educated at Trinity College, Cambridge, where he rigorously followed the medieval curriculum, largely in Latin. Bacon was the first recipient of the Queen's counsel designation, which was conferred in 1597 when Elizabeth I of England reserved Bacon as her legal advisor. After the accession of James VI and I in 1603, Bacon was knighted. He was later created Baron Verulam in 1618 and Viscount St. Alban in 1621.Because he had no heirs, both titles became extinct upon his death in 1626, at 65 years. Bacon died of pneumonia, with one account by John Aubrey stating that he had contracted the condition while studying the effects of freezing on the preservation of meat. He is buried at St Michael's Church, St Albans, Hertfordshire.
Francis Bacon was born on 22 January 1561 at York House near the Strand in London, the son of Sir Nicholas Bacon (Lord Keeper of the Great Seal) by his second wife, Anne (Cooke) Bacon, the daughter of the noted Renaissance humanist Anthony Cooke. His mother's sister was married to William Cecil, 1st Baron Burghley, making Burghley Bacon's uncle.Biographers believe that Bacon was educated at home in his early years owing to poor health, which would plague him throughout his life. He received tuition from John Walsall, a graduate of Oxford with a strong leaning toward Puritanism. He went up to Trinity College at the University of Cambridge on 5 April 1573 at the age of 12, living for three years there, together with his older brother Anthony Bacon under the personal tutelage of Dr John Whitgift, future Archbishop of Canterbury. Bacon's education was conducted largely in Latin and followed the medieval curriculum. It was at Cambridge that Bacon first met Queen Elizabeth, who was impressed by his precocious intellect, and was accustomed to calling him "The young lord keeper".His studies brought him to the belief that the methods and results of science as then practised were erroneous. His reverence for Aristotle conflicted with his rejection of Aristotelian philosophy, which seemed to him barren, disputatious and wrong in its objectives. On 27 June 1576, he and Anthony entered de societate magistrorum at Gray's Inn. A few months later, Francis went abroad with Sir Amias Paulet, the English ambassador at Paris, while Anthony continued his studies at home. The state of government and society in France under Henry III afforded him valuable political instruction. For the next three years he visited Blois, Poitiers, Tours, Italy, and Spain. There is no evidence that he studied at the University of Poitiers. During his travels, Bacon studied language, statecraft, and civil law while performing routine diplomatic tasks. On at least one occasion he delivered diplomatic letters to England for Walsingham, Burghley, and Leicester, as well as for the queen.The sudden death of his father in February 1579 prompted Bacon to return to England. Sir Nicholas had laid up a considerable sum of money to purchase an estate for his youngest son, but he died before doing so, and Francis was left with only a fifth of that money. Having borrowed money, Bacon got into debt. To support himself, he took up his residence in law at Gray's Inn in 1579, his income being supplemented by a grant from his mother Lady Anne of the manor of Marks near Romford in Essex, which generated a rent of £46.
Bacon stated that he had three goals: to uncover truth, to serve his country, and to serve his church. He sought to further these ends by seeking a prestigious post. In 1580, through his uncle, Lord Burghley, he applied for a post at court that might enable him to pursue a life of learning, but his application failed. For two years he worked quietly at Gray's Inn, until he was admitted as an outer barrister in 1582.His parliamentary career began when he was elected MP for Bossiney, Cornwall, in a by-election in 1581. In 1584 he took his seat in Parliament for Melcombe in Dorset, and in 1586 for Taunton. At this time, he began to write on the condition of parties in the church, as well as on the topic of philosophical reform in the lost tract Temporis Partus Maximus. Yet he failed to gain a position that he thought would lead him to success. He showed signs of sympathy to Puritanism, attending the sermons of the Puritan chaplain of Gray's Inn and accompanying his mother to the Temple Church to hear Walter Travers. This led to the publication of his earliest surviving tract, which criticized the English church's suppression of the Puritan clergy. In the Parliament of 1586, he openly urged execution for the Catholic Mary, Queen of Scots. About this time, he again approached his powerful uncle for help; this move was followed by his rapid progress at the bar. He became a bencher in 1586 and was elected a Reader in 1587, delivering his first set of lectures in Lent the following year. In 1589, he received the valuable appointment of reversion to the Clerkship of the Star Chamber, although he did not formally take office until 1608; the post was worth £1,600 a year.In 1588 he became MP for Liverpool and then for Middlesex in 1593. He later sat three times for Ipswich (1597, 1601, 1604) and once for Cambridge University (1614).He became known as a liberal-minded reformer, eager to amend and simplify the law. Though a friend of the crown, he opposed feudal privileges and dictatorial powers. He spoke against religious persecution. He struck at the House of Lords in its usurpation of the Money Bills. He advocated for the union of England and Scotland, which made him a significant influence toward the consolidation of the United Kingdom; and he later would advocate for the integration of Ireland into the Union. Closer constitutional ties, he believed, would bring greater peace and strength to these countries.
Bacon soon became acquainted with Robert Devereux, 2nd Earl of Essex, Queen Elizabeth's favorite. By 1591 he acted as the earl's confidential adviser.In 1592 he was commissioned to write a tract in response to the Jesuit Robert Parson's anti-government polemic, which he titled Certain observations made upon a libel, identifying England with the ideals of democratic Athens against the belligerence of Spain.Bacon took his third parliamentary seat for Middlesex when in February 1593 Elizabeth summoned Parliament to investigate a Roman Catholic plot against her. Bacon's opposition to a bill that would levy triple subsidies in half the usual time offended the Queen: opponents accused him of seeking popularity, and for a time the Court excluded him from favour. When the office of Attorney General fell vacant in 1594, Lord Essex's influence was not enough to secure the position for Bacon and it was given to Sir Edward Coke. Likewise, Bacon failed to secure the lesser office of Solicitor General in 1595, the Queen pointedly snubbing him by appointing Sir Thomas Fleming instead. To console him for these disappointments, Essex presented him with a property at Twickenham, which Bacon subsequently sold for £1,800.In 1597 Bacon became the first Queen's Counsel designate, when Queen Elizabeth reserved him as her legal counsel. In 1597, he was also given a patent, giving him precedence at the Bar. Despite his designations, he was unable to gain the status and notoriety of others. In a plan to revive his position he unsuccessfully courted the wealthy young widow Lady Elizabeth Hatton. His courtship failed after she broke off their relationship upon accepting marriage to Sir Edward Coke, a further spark of enmity between the men. In 1598 Bacon was arrested for debt. Afterward, however, his standing in the Queen's eyes improved. Gradually, Bacon earned the standing of one of the learned counsels. His relationship with the Queen further improved when he severed ties with Essex—a shrewd move, as Essex would be executed for treason in 1601.With others, Bacon was appointed to investigate the charges against Essex. A number of Essex's followers confessed that Essex had planned a rebellion against the Queen. Bacon was subsequently a part of the legal team headed by the Attorney General Sir Edward Coke at Essex's treason trial. After the execution, the Queen ordered Bacon to write the official government account of the trial, which was later published as A DECLARATION of the Practices and Treasons attempted and committed by Robert late Earle of Essex and his Complices, against her Majestie and her Kingdoms ... after Bacon's first draft was heavily edited by the Queen and her ministers.According to his personal secretary and chaplain, William Rawley, as a judge Bacon was always tender-hearted, "looking upon the examples with the eye of severity, but upon the person with the eye of pity and compassion". And also that "he was free from malice", "no revenger of injuries", and "no defamer of any man".
The succession of James I brought Bacon into greater favour. He was knighted in 1603. In another shrewd move, Bacon wrote his Apologies in defense of his proceedings in the case of Essex, as Essex had favoured James to succeed to the throne. The following year, during the course of the uneventful first parliament session, Bacon married Alice Barnham. In June 1607 he was at last rewarded with the office of solicitor general. The following year, he began working as the Clerkship of the Star Chamber. Despite a generous income, old debts still could not be paid. He sought further promotion and wealth by supporting King James and his arbitrary policies. In 1610 the fourth session of James's first parliament met. Despite Bacon's advice to him, James and the Commons found themselves at odds over royal prerogatives and the king's embarrassing extravagance. The House was finally dissolved in February 1611. Throughout this period Bacon managed to stay in the favor of the king while retaining the confidence of the Commons. In 1613 Bacon was finally appointed attorney general, after advising the king to shuffle judicial appointments. As attorney general, Bacon, by his zealous efforts—which included torture—to obtain the conviction of Edmund Peacham for treason, raised legal controversies of high constitutional importance; and successfully prosecuted Robert Carr, 1st Earl of Somerset, and his wife, Frances Howard, Countess of Somerset, for murder in 1616. The so-called Prince's Parliament of April 1614 objected to Bacon's presence in the seat for Cambridge and to the various royal plans that Bacon had supported. Although he was allowed to stay, parliament passed a law that forbade the attorney general to sit in parliament. His influence over the king had evidently inspired resentment or apprehension in many of his peers. Bacon, however, continued to receive the King's favour, which led to his appointment in March 1617 as temporary Regent of England (for a period of a month), and in 1618 as Lord Chancellor. On 12 July 1618 the king created Bacon Baron Verulam, of Verulam, in the Peerage of England; he then became known as Francis, Lord Verulam.Bacon continued to use his influence with the king to mediate between the throne and Parliament, and in this capacity he was further elevated in the same peerage, as Viscount St Alban, on 27 January 1621.
Bacon was a devout Anglican. He believed that philosophy and the natural world must be studied inductively, but argued that we can only study arguments for the existence of God. Information on his attributes (such as nature, action, and purposes) can only come from special revelation. But Bacon also held that knowledge was cumulative, that study encompassed more than a simple preservation of the past. "Knowledge is the rich storehouse for the glory of the Creator and the relief of man's estate," he wrote. In his Essays, he affirms that "a little philosophy inclineth man's mind to atheism, but depth in philosophy bringeth men's minds about to religion."Bacon's idea of idols of the mind may have self-consciously represented an attempt to Christianize science at the same time as developing a new, reliable scientific method; Bacon gave worship of Neptune as an example of the idola tribus fallacy, hinting at the religious dimensions of his critique of the idols.
When he was 36, Bacon courted Elizabeth Hatton, a young widow of 20. Reportedly, she broke off their relationship upon accepting marriage to a wealthier man, Bacon's rival, Sir Edward Coke. Years later, Bacon still wrote of his regret that the marriage to Hatton had not taken place.At the age of 45, Bacon married Alice Barnham, the 14-year-old daughter of a well-connected London alderman and MP. Bacon wrote two sonnets proclaiming his love for Alice. The first was written during his courtship and the second on his wedding day, 10 May 1606. When Bacon was appointed lord chancellor, "by special Warrant of the King", Lady Bacon was given precedence over all other Court ladies. Bacon's personal secretary and chaplain, William Rawley, wrote in his biography of Bacon that his marriage was one of "much conjugal love and respect", mentioning a robe of honor that he gave to Alice and which "she wore unto her dying day, being twenty years and more after his death". However, an increasing number of reports circulated about friction in the marriage, with speculation that this may have been due to Alice's making do with less money than she had once been accustomed to. It was said that she was strongly interested in fame and fortune, and when household finances dwindled, she complained bitterly. Bunten wrote in her Life of Alice Barnham that, upon their descent into debt, she went on trips to ask for financial favours and assistance from their circle of friends. Bacon disinherited her upon discovering her secret romantic relationship with Sir John Underhill. He subsequently rewrote his will, which had previously been very generous—leaving her lands, goods, and income—and instead revoked it all.
Several authors believe that, despite his marriage, Bacon was primarily attracted to men. Forker, for example, has explored the "historically documentable sexual preferences" of both Francis Bacon and King James I and concluded they were both orientated to "masculine love", a contemporary term that "seems to have been used exclusively to refer to the sexual preference of men for members of their own gender."The well-connected antiquary John Aubrey noted in his Brief Lives concerning Bacon, "He was a Pederast. His Ganimeds and Favourites tooke Bribes". ("Pederast" in Renaissance diction meant generally "homosexual" rather than specifically a lover of minors; "ganimed" derives from the mythical prince abducted by Zeus to be his cup-bearer and bed warmer.) The Jacobean antiquarian, Sir Simonds D'Ewes (Bacon's fellow Member of Parliament) implied there had been a question of bringing him to trial for buggery, which his brother Anthony Bacon had also been charged with.In his Autobiography and Correspondence, in the diary entry for 3 May 1621, the date of Bacon's censure by Parliament, D'Ewes describes Bacon's love for his Welsh serving-men, in particular Godrick, a "very effeminate-faced youth" whom he calls "his catamite and bedfellow".This conclusion has been disputed by others, who point to lack of consistent evidence, and consider the sources to be more open to interpretation. Publicly, at least, Bacon distanced himself from the idea of homosexuality. In his New Atlantis, he described his utopian island as being "the chastest nation under heaven", and "as for masculine love, they have no touch of it".
Francis Bacon's philosophy is displayed in the vast and varied writings he left, which might be divided into three great branches: Scientific works – in which his ideas for a universal reform of knowledge into scientific methodology and the improvement of mankind's state using the Scientific method are presented. Religious and literary works – in which he presents his moral philosophy and theological meditations. Juridical works – in which his reforms in English Law are proposed.
Bacon's seminal work Novum Organum was influential in the 1630s and 1650s among scholars, in particular Sir Thomas Browne, who in his encyclopedia Pseudodoxia Epidemica (1646–72) frequently adheres to a Baconian approach to his scientific enquiries. This book entails the basis of the Scientific Method as a means of observation and induction. According to Francis Bacon, learning and knowledge all derive from the basis of inductive reasoning. Through his belief of experimental encounters, he theorized that all the knowledge that was necessary to fully understand a concept could be attainable because of induction. In order to get to the point of an inductive conclusion, one must consider the importance of observing the particulars (specific parts of nature). "Once these particulars have been gathered together, the interpretation of Nature proceeds by sorting them into a formal arrangement so that they may be presented to the understanding." Experimentation is essential to discovering the truths of Nature. When an experiment happens, parts of the tested hypothesis are started to be pieced together, forming a result and conclusion. Through this conclusion of particulars, an understanding of Nature can be formed. Now that an understanding of Nature has been arrived at, an inductive conclusion can be drawn. "For no one successfully investigates the nature of a thing in the thing itself; the inquiry must be enlarged to things that have more in common with it."Francis Bacon explains how we come to this understanding and knowledge because of this process in comprehending the complexities of nature. "Bacon sees nature as an extremely subtle complexity, which affords all the energy of the natural philosopher to disclose her secrets." Bacon described the evidence and proof revealed through taking a specific example from nature and expanding that example into a general, substantial claim of nature. Once we understand the particulars in nature, we can learn more about it and become surer of things occurring in nature, gaining knowledge and obtaining new information all the while. "It is nothing less than a revival of Bacon’s supremely confident belief that inductive methods can provide us with ultimate and infallible answers concerning the laws and nature of the universe." Bacon states that when we come to understand parts of nature, we can eventually understand nature better as a whole because of induction. Because of this, Bacon concludes that all learning and knowledge must be drawn from inductive reasoning. During the Restoration, Bacon was commonly invoked as a guiding spirit of the Royal Society founded under Charles II in 1660. During the 18th-century French Enlightenment, Bacon's non-metaphysical approach to science became more influential than the dualism of his French contemporary Descartes, and was associated with criticism of the Ancien Régime. In 1733 Voltaire introduced him to a French audience as the "father" of the scientific method, an understanding which had become widespread by the 1750s. In the 19th century his emphasis on induction was revived and developed by William Whewell, among others. He has been reputed as the "Father of Experimental Philosophy".He also wrote a long treatise on Medicine, History of Life and Death, with natural and experimental observations for the prolongation of life. One of his biographers, the historian William Hepworth Dixon, states: "Bacon's influence in the modern world is so great that every man who rides in a train, sends a telegram, follows a steam plough, sits in an easy chair, crosses the channel or the Atlantic, eats a good dinner, enjoys a beautiful garden, or undergoes a painless surgical operation, owes him something."In 1902 Hugo von Hofmannsthal published a fictional letter, known as The Lord Chandos Letter, addressed to Bacon and dated 1603, about a writer who is experiencing a crisis of language. Although Bacon’s works are extremely instrumental, his argument falls short because observation and the scientific method are not completely necessary for everything. Bacon takes the inductive method too far, as seen through one of his aphorisms which says, "Man, being the servant and interpreter of Nature, can do and understand so much only as he has observed in fact or in thought of the course of nature: Beyond this he neither knows anything nor can do anything." As humans, we are capable of more than pure observation and can use deduction to form theories. In fact, we must use deduction because Bacon’s pure inductive method is incomplete. Thus, it is not Bacon’s ideas alone that form the scientific method we use today. If that were the case, we would not be able to fully understand the observations we make and deduce new theories. Author Ernst Mayr states, "Inductivism had a great vogue in the eighteenth and early nineteenth centuries, but it is now clear that a purely inductive approach is quite sterile." Mayr points out that an inductive approach on its own just doesn’t work. One could observe an experiment multiple times, but still be unable to make generalizations and correctly understand the knowledge. Bacon’s inductive method is beneficial, but incomplete and leaves gaps. However, when combined with the ideas of Descartes, the gaps are filled in Bacon’s inductive method. The "anticipation of nature" as Bacon puts it, connects the information gained from observation, enabling hypotheses and theories to become more effective. Bacon’s inductive ideas now have more value. Jurgen Klein, who researched Bacon and analyzed his works, says, "The inductive method helps the human mind to find a way to ascertain truthful knowledge." Klein shows the value that Bacon’s method truly brings. It is not a value that stands on its own, for it has holes, but it is a value that supports and strengthens. The inductive method can be seen as a tool used alongside other ideas, such as deduction, which now creates a method which is most effective and used today: the scientific method. The inductive method is more prominent in the scientific method than other ideas, which leads to misconception, but the takeaway is that it has supporting ideas. Francis Bacon’s scientific method is extremely influential, but has been developed for its own good, as all great ideas are.
Bacon played a leading role in establishing the British colonies in North America, especially in Virginia, the Carolinas and Newfoundland in northeastern Canada. His government report on "The Virginia Colony" was submitted in 1609. In 1610 Bacon and his associates received a charter from the king to form the Tresurer and the Companye of Adventurers and planter of the Cittye of London and Bristoll for the Collonye or plantacon in Newfoundland, and sent John Guy to found a colony there. Thomas Jefferson, the third President of the United States, wrote: "Bacon, Locke and Newton. I consider them as the three greatest men that have ever lived, without any exception, and as having laid the foundation of those superstructures which have been raised in the Physical and Moral sciences".In 1910 Newfoundland issued a postage stamp to commemorate Bacon's role in establishing the colony. The stamp describes Bacon as "the guiding spirit in Colonization Schemes in 1610". Moreover, some scholars believe he was largely responsible for the drafting, in 1609 and 1612, of two charters of government for the Virginia Colony. William Hepworth Dixon considered that Bacon's name could be included in the list of Founders of the United States.
Although few of his proposals for law reform were adopted during his lifetime, Bacon's legal legacy was considered by the magazine New Scientist in 1961 as having influenced the drafting of the Napoleonic Code as well as the law reforms introduced by 19th-century British Prime Minister Sir Robert Peel. The historian William Hepworth Dixon referred to the Napoleonic Code as "the sole embodiment of Bacon's thought", saying that Bacon's legal work "has had more success abroad than it has found at home", and that in France "it has blossomed and come into fruit".Harvey Wheeler attributed to Bacon, in Francis Bacon's Verulamium—the Common Law Template of The Modern in English Science and Culture, the creation of these distinguishing features of the modern common law system: using cases as repositories of evidence about the "unwritten law"; determining the relevance of precedents by exclusionary principles of evidence and logic; treating opposing legal briefs as adversarial hypotheses about the application of the "unwritten law" to a new set of facts. As late as the 18th century some juries still declared the law rather than the facts, but already before the end of the 17th century Sir Matthew Hale explained modern common law adjudication procedure and acknowledged Bacon as the inventor of the process of discovering unwritten laws from the evidences of their applications. The method combined empiricism and inductivism in a new way that was to imprint its signature on many of the distinctive features of modern English society. Paul H. Kocher writes that Bacon is considered by some jurists to be the father of modern Jurisprudence.Bacon is commemorated with a statue in Gray's Inn, South Square in London where he received his legal training, and where he was elected Treasurer of the Inn in 1608.More recent scholarship on Bacon's jurisprudence has focused on his advocating torture as a legal recourse for the crown. Bacon himself was not a stranger to the torture chamber; in his various legal capacities in both Elizabeth I's and James I's reigns, Bacon was listed as a commissioner on five torture warrants. In 1613(?), in a letter addressed to King James I on the question of torture's place within English law, Bacon identifies the scope of torture as a means to further the investigation of threats to the state: "In the cases of treasons, torture is used for discovery, and not for evidence." For Bacon, torture was not a punitive measure, an intended form of state repression, but instead offered a modus operandi for the government agent tasked with uncovering acts of treason.
Francis Bacon developed the idea that a classification of knowledge must be universal while handling all possible resources. In his progressive view, humanity would be better if the access to educational resources were provided to the public, hence the need to organise it. His approach to learning reshaped the Western view of knowledge theory from an individual to a social interest. The original classification proposed by Bacon organised all types of knowledge in three general groups: history, poetry, and philosophy. He did that based on his understanding of how information is processed: memory, imagination, and reason, respectively. His methodical approach to the categorization of knowledge goes hand-in-hand with his principles of scientific methods. Bacon’s writings were the starting point for William Torrey Harris classification system for libraries in the United States by the second half of the 1800s.
The Baconian hypothesis of Shakespearean authorship, first proposed in the mid-19th century, contends that Francis Bacon wrote some or even all of the plays conventionally attributed to William Shakespeare.
Francis Bacon often gathered with the men at Gray's Inn to discuss politics and philosophy, and to try out various theatrical scenes that he admitted writing. Bacon's alleged connection to the Rosicrucians and the Freemasons has been widely discussed by authors and scholars in many books. However, others, including Daphne du Maurier in her biography of Bacon, have argued that there is no substantive evidence to support claims of involvement with the Rosicrucians. Frances Yates does not make the claim that Bacon was a Rosicrucian, but presents evidence that he was nevertheless involved in some of the more closed intellectual movements of his day. She argues that Bacon's movement for the advancement of learning was closely connected with the German Rosicrucian movement, while Bacon's New Atlantis portrays a land ruled by Rosicrucians. He apparently saw his own movement for the advancement of learning to be in conformity with Rosicrucian ideals. The link between Bacon's work and the Rosicrucians' ideals which Yates allegedly found was the conformity of the purposes expressed by the Rosicrucian Manifestos and Bacon's plan of a "Great Instauration", for the two were calling for a reformation of both "divine and human understanding", as well as both had in view the purpose of mankind's return to the "state before the Fall".Another major link is said to be the resemblance between Bacon's New Atlantis and the German Rosicrucian Johann Valentin Andreae's Description of the Republic of Christianopolis (1619). Andreae describes a utopic island in which Christian theosophy and applied science ruled, and in which the spiritual fulfilment and intellectual activity constituted the primary goals of each individual, the scientific pursuits being the highest intellectual calling—linked to the achievement of spiritual perfection. Andreae's island also depicts a great advancement in technology, with many industries separated in different zones which supplied the population's needs—which shows great resemblance to Bacon's scientific methods and purposes.While rejecting occult conspiracy theories surrounding Bacon and the claim Bacon personally identified as a Rosicrucian, intellectual historian Paolo Rossi has argued for an occult influence on Bacon's scientific and religious writing. He argues that Bacon was familiar with early modern alchemical texts and that Bacon's ideas about the application of science had roots in Renaissance magical ideas about science and magic facilitating humanity's domination of nature. Rossi further interprets Bacon's search for hidden meanings in myth and fables in such texts as The Wisdom of the Ancients as succeeding earlier occultist and Neoplatonic attempts to locate hidden wisdom in pre-Christian myths. As indicated by the title of his study, however, Rossi claims Bacon ultimately rejected the philosophical foundations of occultism as he came to develop a form of modern science.Rossi's analysis and claims have been extended by Jason Josephson-Storm in his study, The Myth of Disenchantment. Josephson-Storm also rejects conspiracy theories surrounding Bacon and does not make the claim that Bacon was an active Rosicrucian. However, he argues that Bacon's "rejection" of magic actually constituted an attempt to purify magic of Catholic, demonic, and esoteric influences and to establish magic as a field of study and application paralleling Bacon's vision of science. Furthermore, Josephson-Storm argues that Bacon drew on magical ideas when developing his experimental method. Josephson-Storm finds evidence that Bacon considered nature a living entity, populated by spirits, and argues Bacon's views on the human domination and application of nature actually depend on his spiritualism and personification of nature.The Rosicrucian organization AMORC claims that Bacon was the "Imperator" (leader) of the Rosicrucian Order in both England and the European continent, and would have directed it during his lifetime.Bacon's influence can also be seen on a variety of religious and spiritual authors, and on groups that have utilized his writings in their own belief systems.
Some of the more notable works by Bacon are: Essays 1st edition with 10 essays (1597) 2nd edition with 38 essays (1612) 3rd/final edition with 58 essays (1625) The Advancement and Proficience of Learning Divine and Human (1605) Instauratio magna (The Great Instauration) (1620): a multi-part work including Distributio operis (Plan of the Work); Novum Organum (New Engine); Parasceve ad historiam naturalem (Preparatory for Natural History) and Catalogus historiarum particularium (Catalogue of Particular Histories) De augmentis scientiarum (1623) – an enlargement of The Advancement of Learning translated into Latin New Atlantis (1626)
Cestui que (defence and comment on Chudleigh's Case) Romanticism and Bacon
Klein, Juergen. "Francis Bacon". In Zalta, Edward N. (ed.). Stanford Encyclopedia of Philosophy. "Francis Bacon". Internet Encyclopedia of Philosophy. Works by Francis Bacon at Project Gutenberg Works by or about Francis Bacon at Internet Archive Works by Francis Bacon at LibriVox (public domain audiobooks) "Archival material relating to Francis Bacon". UK National Archives. Contains the New Organon, slightly modified for easier reading Lord Macaulay's essay Lord Bacon (Edinburgh Review, 1837) [1] Francis Bacon of Verulam. Realistic Philosophy and its Age by Kuno Fischer, translated from the German by John Oxenford London 1857 Bacon by Thomas Fowler (1881) public domain at Internet Archive The Francis Bacon Society English translation of Hugo von Hofmannsthal's fictional The Lord Chandos Letter, addressed to Bacon The George Fabyan Collection at the Library of Congress is rich in the works of Francis Bacon. Francis Bacon Research Trust Sir Francis Bacon's New Advancement of Learning Montmorency, James E. G. (1913). "FRANCIS BACON". In Macdonell, John; Manson, Edward William Donoghue (eds.). Great Jurists of the World. London: John Murray. pp. 144–168. Retrieved 11 March 2019 – via Internet Archive. Letterbook and correspondence by Sir Francis Bacon at Columbia University. Rare Book & Manuscript Library.
Frank Ocean (born October 28, 1987) is an American singer, songwriter, record producer, photographer, and visual artist. Recognized for his idiosyncratic musical style, introspective and elliptical songwriting, unconventional production techniques, and wide vocal range, Ocean is among the most acclaimed artists of his generation. Music critics have credited him with revitalizing jazz and funk influenced R&B, as well as advancing the genre through his experimental approach. He is considered a representative artist of alternative R&B.Ocean began his musical career as a ghostwriter, prior to joining the hip hop collective Odd Future in 2010. In 2011, Ocean released his critically successful debut mixtape Nostalgia, Ultra and subsequently secured a recording contract with Def Jam Recordings. Drawing on electro-funk, pop-soul, jazz-funk, and psychedelic music, Ocean's debut studio album Channel Orange (2012) was one of the most acclaimed albums of 2012. It was nominated for Album of the Year and won Best Urban Contemporary Album at the 2013 Grammy Awards, while the album's hit single "Thinkin Bout You" garnered Ocean a nomination for Record of the Year. Following four years of recluse, Ocean released the visual project Endless, the day before releasing his highly anticipated second album Blonde (2016), in order to fulfill contractual obligations with Def Jam. Released independently, Blonde debuted at number one on the US Billboard 200 and was certified platinum by the Recording Industry Association of America (RIAA). Encompassing avant-garde, soul, and psychedelic rock, the album was acclaimed by critics and Ocean was praised for challenging the conventions of contemporary R&B and pop music.Among Ocean's awards are two Grammy Awards, a Brit Award for International Male Solo Artist in 2013 and an NME Award for Best International Male Artist in 2017. He was included in the 2013 edition of the Time's list of the 100 most influential people in the world and the 2017 edition of the Forbes 30 Under 30. Both Insider and The Wall Street Journal regarded Ocean as the most dominant artist of the 2010s decade. As a photographer, he worked with Vogue at the annual Met Gala and the British fashion magazine i-D. Premiered in 2017, he also has his own Beats 1 radio show, Blonded Radio, that often premiers his new singles.
Ocean was born on October 28, 1987, in Long Beach, California. When he was five years old, he and his family relocated to New Orleans. Ocean was first introduced to music through his mother, who would often play jazz music on her car stereo, as well as albums by Celine Dion and Anita Baker and the soundtrack to The Phantom of the Opera. He later frequented New Orleans jazz bars and parlors, which encouraged him to begin recording his own music. In order to raise funds for recording time, he performed several jobs as a teenager such as washing cars, mowing lawns, and walking his neighbors' dogs. After graduating from John Ehret High School in New Orleans in 2005, Ocean enrolled in the University of New Orleans to study English. However, Hurricane Katrina struck New Orleans in August 2005, destroying his home and personal recording facility and forcing him to transfer to the University of Louisiana at Lafayette. He stayed there for a brief time before dropping out to focus on his music career.
Ocean released the cover art for his debut studio album's lead single, titled "Thinkin Bout You", revealing the song would be released to digital retailers on April 10, 2012. However, a month earlier, a re-mastered version of the song had already leaked. About the prospective single he said: "It succinctly defines me as an artist for where I am right now and that was the aim," he said of the follow-up to his acclaimed Nostalgia, Ultra. "It's about the stories. If I write 14 stories that I love, then the next step is to get the environment of music around it to best envelop the story and all kinds of sonic goodness." In 2012, Ocean released his debut studio album Channel Orange to universal acclaim from critics, who later named it the best album of the year in the HMV's Poll of Polls. It also earned Ocean six Grammy Award nominations and was credited by some writers for moving the R&B genre in a different, more challenging direction. Considered as Ocean's first commercial release on a traditional record label, Channel Orange featured unconventional songs that were noted for their storytelling and social commentary, and a dense musical fusion that drew on jazz, soul, and R&B. Funk and electronic music also influenced his album. The songs about unrequited love in particular received the most attention, partly because of Ocean's announcement prior to the album's release, when he revealed that his first love was a man. The announcement made global headlines, and some critics compared its cultural impact to when David Bowie revealed that he was bisexual in 1972. Channel Orange debuted at number two on the Billboard 200 and sold 131,000 copies in its first week. The majority of its first-week sales were digital copies from iTunes, while approximately 3,000 of the sales were physical copies. On January 30, Channel Orange was certified gold by the Recording Industry Association of America (RIAA). By September 2014, it had sold 621,000 copies, according to Nielsen SoundScan. Ocean promoted the album with his 2012 Summer Tour, which featured final appearances at the Coachella and Lollapalooza festivals. At the 2013 Brit Awards, Ocean won the Brit Award for International Male Solo Artist. On May 28, 2013, Ocean announced the You're Not Dead ... 2013 Tour; a fourteen-date European and Canadian tour that began on June 16, 2013, in Munich. He had been scheduled to perform at the first night of OVO Fest on August 4, 2013; however he was forced to cancel his appearance due to a small vocal cord injury. The first night of the music festival was subsequently cancelled and James Blake was booked to appear during the second night as Ocean's replacement. Ocean appeared on John Mayer's album Paradise Valley, as a featured artist on a song called "Wildfire".
In February 2013, Ocean confirmed that he had started work on his second studio album, which he confirmed would be another concept album. He revealed that he was working with Tyler, the Creator, Pharrell Williams, and Danger Mouse on the record. He later stated that he was being influenced by The Beach Boys and The Beatles. He stated he was interested in collaborating with Tame Impala and King Krule and that he would record part of the album in Bora Bora.On March 10, 2014, the song "Hero" was made available for free download on SoundCloud. The song is a collaboration with Mick Jones, Paul Simonon and Diplo and is a part of Converse's Three Artists. One Song series.In April 2014, Ocean stated that his second album was nearly finished. In June, Billboard reported that the singer was working with a string of artists such as Happy Perez (whom he worked with on nostalgia, ULTRA), Charlie Gambetta and Kevin Ristro, while producers Hit-Boy, Rodney Jerkins and Danger Mouse were also said to be on board. On November 29, 2014, Ocean released a snippet of a new song supposedly from his upcoming follow-up to channel ORANGE called "Memrise" on his official Tumblr page. The Guardian described the song as: "...a song which affirms that despite reportedly changing labels and management, he has maintained both his experimentation and sense of melancholy in the intervening years". On April 6, 2015, Ocean announced that his follow-up to channel ORANGE would be released in July with "two versions", as well as a publication, although no further details were released. The album was ultimately not released in July, with no explanation given for its delay. The publication was rumoured to be called Boys Don't Cry, and the album was slated to feature the aforementioned "Memrise". In February 2016, Ocean was featured on Kanye West's album The Life of Pablo on the track "Wolves" along with Vic Mensa and Sia Furler. A month later, the song was re-edited by West, and Ocean's part was separated and listed on the track list as its own song titled "Frank's Track."In July 2016, he hinted at a possible second album with an image on his website pointing to a July release date. The image shows a library card labeled Boys Don't Cry with numerous stamps, implying various due dates. The dates begin with July 2, 2015 and conclude with July 2016. Ocean's brother, Ryan Breaux, further suggested this release with an Instagram caption of the same library card photo reading BOYS DON'T CRY #JULY2016.By August 1, 2016, at approximately 3 a.m., an endless live stream shot in negative lighting in what is allegedly a Brooklyn warehouse, sponsored by Apple Music began to surface on boysdontcry.co which appeared to show Ocean woodworking and sporadically playing instrumentals on loop. It later became clear that these instrumentals were from his upcoming visual album Endless; the full version is estimated to be 140 hours long. That same day, many news outlets reported that August 5, 2016 could be the release date for Boys Don't Cry. That date also turned out to be inaccurate, though in a Reddit AMA session, his collaborator Malay said that Ocean is a perfectionist, constantly tweaking things, and that his art cannot be rushed.On August 18 and 19, 2016, the live stream was accompanied with music and at midnight an Apple Music link was directed to a project called Endless. Endless would be Ocean's last album with Def Jam Recordings to fulfill his contract with the record label. Before the visual album's release on Apple Music, Ocean had already begun making efforts to part ways with Def Jam, who signed the artist in 2009. He describes his negotiations with the label as a "seven-year chess game", while adding that he had replaced many of his representatives (including his lawyer and manager) during the process, as well as having to buy back all of his master recordings that previously belonged to Def Jam.At midnight Pacific time on August 20, 2016, a music video for a song titled "Nikes" was uploaded to Ocean's Connect page on Apple Music and later to his own website. Also on August 20, Ocean announced pop-up shops in Los Angeles, New York City, Chicago, and London for his magazine Boys Don't Cry, and released his second studio album Blonde to widespread acclaim. Blonde debuted at number one in several countries, including the United States and the United Kingdom, and recorded sales of 232,000 copies (275,000 with album-equivalent units) in its first week. Rather than going on a typical promotional tour playing radio festivals and appearing on television shows, Ocean spent a month after the release of Blonde, traveling to countries such as China, Japan and France. He also chose not to submit Blonde for consideration at the Grammy Awards, stating "that institution certainly has nostalgic importance; it just doesn't seem to be representing very well for people who come from where I come from, and hold down what I hold down." Time ranked it as the best album of 2016 on its year-end list. Forbes estimated that Blonde earned Ocean nearly one million in profits after one week of availability, attributing this to him releasing the album independently and as a limited exclusive release on iTunes and Apple Music. On July 9, 2018, Blonde was certified platinum by the Recording Industry Association of America (RIAA).
Ocean's music has been characterized by music writers as idiosyncratic in style. His music generally includes the electronic keyboard, often performed by Ocean himself, and is backed by a subdued rhythm section in the production. His compositions are often midtempo, feature unconventional melodies, and occasionally have an experimental song structure. He has been characterised as both an "avant-garde R&B artist" and a "pop musician". In his songwriting, Jon Pareles of The New York Times observes "open echoes of self-guided, innovative R&B songwriters like Prince, Stevie Wonder, Marvin Gaye, Maxwell, Erykah Badu and particularly R. Kelly and his way of writing melodies that hover between speech and song, asymmetrical and syncopated." Jody Rosen of Rolling Stone calls him a torch singer due to "his feel for romantic tragedy, unfurling in slow-boiling ballads". Ocean's stage presence during live shows has been described by Chris Richards of the Washington Post as "low-key". While nostalgia, ULTRA featured both original music by Ocean and tracks relying on sampled melodies, channel ORANGE showcased Ocean as the primary musical composer, of which music journalist Robert Christgau opines, "when he's the sole composer Ocean resists making a show of himself—resists the dope hook, the smart tempo, the transcendent falsetto itself."Ocean's lyrics deal with themes of love, longing, misgiving, and nostalgia. His debut single "Novacane" juxtaposes the numbness and artificiality of a sexual relationship with that of mainstream radio, while "Voodoo" merges themes of spirituality and sexuality, and is an eccentric take on such subject matter common in R&B. The latter song was released by Ocean on his Tumblr account and references both the traditional spiritual "He's Got the Whole World in His Hands" and the female anatomy in its chorus: "she's got the whole wide world in her juicy fruit / he's got the whole wide world in his pants / he wrapped the whole wide world in a wedding band / then put the whole wide world on her hands / she's got the whole wide world in her hands / he's got the whole wide world in his hands." Certain songs on channel ORANGE allude to Ocean's experience with unrequited love.
Ocean is among the most acclaimed artists of his generation. Music critics have credited him with revitalizing pre-contemporary R&B, as well as approaching the genre differently to his contemporaries through his use of other genres, including avant-garde, electro, rock and psychedelic. His distinctive sound and style have influenced numerous artists of various music genres. Both Insider and The Wall Street Journal regarded Ocean as the most dominant artist of the 2010s decade. He was included in the 2013 edition of the Time's list of the 100 most influential people in the world and the 2017 edition of the Forbes 30 Under 30. Andy Kellman of AllMusic wrote,"Frank Ocean has been one of the more fascinating figures in contemporary music since his early-2010s arrival. A singer and songwriter whose artful output has defied rigid classification as R&B, he has nonetheless pushed that genre forward with seemingly offhanded yet imaginatively detailed narratives in which he has alternated between yearning romantic and easygoing braggart."Culture critic Nelson George asserts that, along with Miguel, Ocean has "staked out ground where [he is] not competing with those hit-driven [commercial R&B] acts" and is "cultivating a sound that balances adult concerns with a sense of young men trying to understand their own desires (an apt description of Ocean, particularly)." Writing for Insider, Callie Ahlgrim said that Ocean "changed our very understanding of modern music", and that he discusses themes like youth, innocence, lost love, loneliness, desire, and mortality in his music in a way that "feels fresh and extraordinary [and] makes the introspective sound universal and transcendent [which] is why he's one of the defining artists of our time." Jacob Shamsian of Business Insider said that Ocean "isn't just one of the most important artists in pop, he's one of the most important artists in all of music." In a GQ article titled 'Why Frank Ocean is a musical icon', Jon Savage described Ocean as "one of the pop elite", a "true pop star of today", and a "consummate contemporary artist in every sense who is immersed in new sonic possibilities, one who is deeply committed to artistic exploration in the most profound sense." Savage praised Ocean for taking R&B to a "new level [through] constructing startling sound pictures that fit his lyrics." Pitchfork regarded Ocean as a "master of confessional songwriting, earning a cult-icon status with his enigmatic persona and idiosyncratic approach to pop."
On August 20, 2016, Ocean released a 360-page magazine, Boys Don't Cry, alongside his long-awaited second album Blonde. The fashion and automobile-themed publication contains the photoprojects from Wolfgang Tillmans, Viviane Sassen, Tyrone Lebon, Ren Hang, Harley Weir, Michael Mayren and Ocean himself. Four months later, British magazine Print published another photowork from Frank Ocean.On May 1, 2017, Ocean attended annual Met Gala as a special photographer for Vogue. On October 23, 2017, he made two covers and a visual essay for British fashion magazine, i-D.
Ocean wrote an open letter, initially intended for the liner notes on Channel Orange, that preemptively addressed speculation about his attraction in the past to another man. Instead, on July 4, 2012, he published an open letter on his Tumblr blog recounting unrequited feelings he had for another young man when he was 19 years old, citing it as his first true love. He used the blog to thank the man for his influence, and also thanked his mother and other friends, saying, "I don't know what happens now, and that's alright. I don't have any secrets I need kept anymore... I feel like a free man." Numerous celebrities publicly voiced their support for Ocean following his announcement, including Beyoncé and Jay-Z. Members of the hip hop industry generally responded positively to the announcement. Tyler, the Creator and other members of OFWGKTA tweeted their support for Ocean. Russell Simmons wrote a congratulatory article in Global Grind in which he said, "Today is a big day for hip-hop. It is a day that will define who we really are. How compassionate will we be? How loving can we be? How inclusive are we? [...] Your decision to go public about your sexual orientation gives hope and light to so many young people still living in fear."In June 2016, following the Orlando nightclub shooting that killed 49 people, Ocean published an essay expressing his sadness and frustration. He mentioned that his first experience with homophobia and transphobia was with his father when he was six years old, and related how many people pass on their hateful ideals to the next generation and send thousands of people down suicidal paths. In 2017, Ocean's father subsequently sued him for defamation and requested $14.5 million. On October 17, 2017, after a hearing that saw Ocean and both of his parents taking the stand, the presiding judge ruled in favor of Ocean, stating that his father had not provided sufficient evidence of defamation.
In a 2011 interview, Ocean stated that he had attempted to change his name to Christopher Francis Ocean through a legal website on his 23rd birthday. The change was reportedly partly inspired by the 1960 film Ocean's 11. In March 2014, it was reported that he was legally changing his name to Frank Ocean. In November 2014, it was revealed that the name change had not been legalized due to multiple speeding offenses. It was finally legalized on April 23, 2015.
Ocean sampled the music from the Eagles' song "Hotel California" on the song "American Wedding" from Nostalgia, Ultra. When asked about it, Ocean stated that Eagles band member "Don Henley is apparently intimidated by my rendition of 'Hotel California'. He threatened to sue if I perform it again." In response to Ocean's comments, the Eagles' legal representative released a statement: "Frank Ocean did not merely 'sample' a portion of the Eagles' 'Hotel California,' he took the entire master track, plus the song's existing melody, and replaced the lyrics with his own; this is not creative, let alone 'intimidating.' It's illegal. For the record, Don Henley has not threatened or instituted any legal action against Frank Ocean, although the Eagles are now considering whether they should." Chris Richards of The Washington Post remarked that "certain boomers don't like Ocean as much" as "information-age babies" due to the controversy.On March 7, 2014, Chipotle Mexican Grill sued Ocean to receive the money back they paid him in advance for a commercial that he backed out of because he objected to material in the advertisement. The advertisement was to feature Ocean singing the song "Pure Imagination", and was to promote sustainable farming. Ocean backed out of the spot when Chipotle refused to remove their logo and name from the advertisement. The lawsuit was dropped on March 20 after Ocean paid the advance back in full. The commercial, titled The Scarecrow, was ultimately released with Fiona Apple performing the song.
Astronomy is the oldest of the natural sciences, dating back to antiquity, with its origins in the religious, mythological, cosmological, calendrical, and astrological beliefs and practices of prehistory: vestiges of these are still found in astrology, a discipline long interwoven with public and governmental astronomy. It was not completely separated in Europe (see astrology and astronomy) during the Copernican Revolution starting in 1543. In some cultures, astronomical data was used for astrological prognostication. The study of astronomy has received financial and social support from many institutions, especially the Church, which was its largest source of support between the 12th century to the Enlightenment.Ancient astronomers were able to differentiate between stars and planets, as stars remain relatively fixed over the centuries while planets will move an appreciable amount during a comparatively short time.
Early cultures identified celestial objects with gods and spirits. They related these objects (and their movements) to phenomena such as rain, drought, seasons, and tides. It is generally believed that the first astronomers were priests, and that they understood celestial objects and events to be manifestations of the divine, hence early astronomy's connection to what is now called astrology. A 32,500 year old carved ivory Mammoth tusk could contain the oldest known star chart (resembling the constellation Orion). It has also been suggested that drawing on the wall of the Lascaux caves in France dating from 33,000 to 10,000 years ago could be a graphical representation of the Pleiades, the Summer Triangle, and the Northern Crown. Ancient structures with possibly astronomical alignments (such as Stonehenge) probably fulfilled astronomical, religious, and social functions. Calendars of the world have often been set by observations of the Sun and Moon (marking the day, month and year), and were important to agricultural societies, in which the harvest depended on planting at the correct time of year, and for which the nearly full moon was the only lighting for night-time travel into city markets. The common modern calendar is based on the Roman calendar. Although originally a lunar calendar, it broke the traditional link of the month to the phases of the Moon and divided the year into twelve almost-equal months, that mostly alternated between thirty and thirty-one days. Julius Caesar instigated calendar reform in 46 BCE and introduced what is now called the Julian calendar, based upon the 365 ​1⁄4 day year length originally proposed by the 4th century BCE Greek astronomer Callippus.
The origins of Western astronomy can be found in Mesopotamia, the "land between the rivers" Tigris and Euphrates, where the ancient kingdoms of Sumer, Assyria, and Babylonia were located. A form of writing known as cuneiform emerged among the Sumerians around 3500–3000 BC. Our knowledge of Sumerian astronomy is indirect, via the earliest Babylonian star catalogues dating from about 1200 BC. The fact that many star names appear in Sumerian suggests a continuity reaching into the Early Bronze Age. Astral theology, which gave planetary gods an important role in Mesopotamian mythology and religion, began with the Sumerians. They also used a sexagesimal (base 60) place-value number system, which simplified the task of recording very large and very small numbers. The modern practice of dividing a circle into 360 degrees, or an hour into 60 minutes, began with the Sumerians. For more information, see the articles on Babylonian numerals and mathematics. Classical sources frequently use the term Chaldeans for the astronomers of Mesopotamia, who were, in reality, priest-scribes specializing in astrology and other forms of divination. The first evidence of recognition that astronomical phenomena are periodic and of the application of mathematics to their prediction is Babylonian. Tablets dating back to the Old Babylonian period document the application of mathematics to the variation in the length of daylight over a solar year. Centuries of Babylonian observations of celestial phenomena are recorded in the series of cuneiform tablets known as the Enūma Anu Enlil. The oldest significant astronomical text that we possess is Tablet 63 of the Enūma Anu Enlil, the Venus tablet of Ammi-saduqa, which lists the first and last visible risings of Venus over a period of about 21 years and is the earliest evidence that the phenomena of a planet were recognized as periodic. The MUL.APIN, contains catalogues of stars and constellations as well as schemes for predicting heliacal risings and the settings of the planets, lengths of daylight measured by a water clock, gnomon, shadows, and intercalations. The Babylonian GU text arranges stars in 'strings' that lie along declination circles and thus measure right-ascensions or time-intervals, and also employs the stars of the zenith, which are also separated by given right-ascensional differences.A significant increase in the quality and frequency of Babylonian observations appeared during the reign of Nabonassar (747–733 BC). The systematic records of ominous phenomena in Babylonian astronomical diaries that began at this time allowed for the discovery of a repeating 18-year cycle of lunar eclipses, for example. The Greek astronomer Ptolemy later used Nabonassar's reign to fix the beginning of an era, since he felt that the earliest usable observations began at this time. The last stages in the development of Babylonian astronomy took place during the time of the Seleucid Empire (323–60 BC). In the 3rd century BC, astronomers began to use "goal-year texts" to predict the motions of the planets. These texts compiled records of past observations to find repeating occurrences of ominous phenomena for each planet. About the same time, or shortly afterwards, astronomers created mathematical models that allowed them to predict these phenomena directly, without consulting past records. A notable Babylonian astronomer from this time was Seleucus of Seleucia, who was a supporter of the heliocentric model. Babylonian astronomy was the basis for much of what was done in Greek and Hellenistic astronomy, in classical Indian astronomy, in Sassanian Iran, in Byzantium, in Syria, in Islamic astronomy, in Central Asia, and in Western Europe.
Astronomy in the Indian subcontinent dates back to the period of Indus Valley Civilization during 3rd millennium BCE, when it was used to create calendars. As the Indus Valley civilization did not leave behind written documents, the oldest extant Indian astronomical text is the Vedanga Jyotisha, dating from the Vedic period. Vedanga Jyotisha describes rules for tracking the motions of the Sun and the Moon for the purposes of ritual. During the 6th century, astronomy was influenced by the Greek and Byzantine astronomical traditions.Aryabhata (476–550), in his magnum opus Aryabhatiya (499), propounded a computational system based on a planetary model in which the Earth was taken to be spinning on its axis and the periods of the planets were given with respect to the Sun. He accurately calculated many astronomical constants, such as the periods of the planets, times of the solar and lunar eclipses, and the instantaneous motion of the Moon. Early followers of Aryabhata's model included Varahamihira, Brahmagupta, and Bhaskara II. Astronomy was advanced during the Shunga Empire and many star catalogues were produced during this time. The Shunga period is known as the "Golden age of astronomy in India". It saw the development of calculations for the motions and places of various planets, their rising and setting, conjunctions, and the calculation of eclipses. Indian astronomers by the 6th century believed that comets were celestial bodies that re-appeared periodically. This was the view expressed in the 6th century by the astronomers Varahamihira and Bhadrabahu, and the 10th-century astronomer Bhattotpala listed the names and estimated periods of certain comets, but it is unfortunately not known how these figures were calculated or how accurate they were.Bhāskara II (1114–1185) was the head of the astronomical observatory at Ujjain, continuing the mathematical tradition of Brahmagupta. He wrote the Siddhantasiromani which consists of two parts: Goladhyaya (sphere) and Grahaganita (mathematics of the planets). He also calculated the time taken for the Earth to orbit the Sun to 9 decimal places. The Buddhist University of Nalanda at the time offered formal courses in astronomical studies. Other important astronomers from India include Madhava of Sangamagrama, Nilakantha Somayaji and Jyeshtadeva, who were members of the Kerala school of astronomy and mathematics from the 14th century to the 16th century. Nilakantha Somayaji, in his Aryabhatiyabhasya, a commentary on Aryabhata's Aryabhatiya, developed his own computational system for a partially heliocentric planetary model, in which Mercury, Venus, Mars, Jupiter and Saturn orbit the Sun, which in turn orbits the Earth, similar to the Tychonic system later proposed by Tycho Brahe in the late 16th century. Nilakantha's system, however, was mathematically more efficient than the Tychonic system, due to correctly taking into account the equation of the centre and latitudinal motion of Mercury and Venus. Most astronomers of the Kerala school of astronomy and mathematics who followed him accepted his planetary model.
The Ancient Greeks developed astronomy, which they treated as a branch of mathematics, to a highly sophisticated level. The first geometrical, three-dimensional models to explain the apparent motion of the planets were developed in the 4th century BC by Eudoxus of Cnidus and Callippus of Cyzicus. Their models were based on nested homocentric spheres centered upon the Earth. Their younger contemporary Heraclides Ponticus proposed that the Earth rotates around its axis. A different approach to celestial phenomena was taken by natural philosophers such as Plato and Aristotle. They were less concerned with developing mathematical predictive models than with developing an explanation of the reasons for the motions of the Cosmos. In his Timaeus, Plato described the universe as a spherical body divided into circles carrying the planets and governed according to harmonic intervals by a world soul. Aristotle, drawing on the mathematical model of Eudoxus, proposed that the universe was made of a complex system of concentric spheres, whose circular motions combined to carry the planets around the earth. This basic cosmological model prevailed, in various forms, until the 16th century. In the 3rd century BC Aristarchus of Samos was the first to suggest a heliocentric system, although only fragmentary descriptions of his idea survive. Eratosthenes estimated the circumference of the Earth with great accuracy.Greek geometrical astronomy developed away from the model of concentric spheres to employ more complex models in which an eccentric circle would carry around a smaller circle, called an epicycle which in turn carried around a planet. The first such model is attributed to Apollonius of Perga and further developments in it were carried out in the 2nd century BC by Hipparchus of Nicea. Hipparchus made a number of other contributions, including the first measurement of precession and the compilation of the first star catalog in which he proposed our modern system of apparent magnitudes. The Antikythera mechanism, an ancient Greek astronomical observational device for calculating the movements of the Sun and the Moon, possibly the planets, dates from about 150–100 BC, and was the first ancestor of an astronomical computer. It was discovered in an ancient shipwreck off the Greek island of Antikythera, between Kythera and Crete. The device became famous for its use of a differential gear, previously believed to have been invented in the 16th century, and the miniaturization and complexity of its parts, comparable to a clock made in the 18th century. The original mechanism is displayed in the Bronze collection of the National Archaeological Museum of Athens, accompanied by a replica. Depending on the historian's viewpoint, the acme or corruption of physical Greek astronomy is seen with Ptolemy of Alexandria, who wrote the classic comprehensive presentation of geocentric astronomy, the Megale Syntaxis (Great Synthesis), better known by its Arabic title Almagest, which had a lasting effect on astronomy up to the Renaissance. In his Planetary Hypotheses, Ptolemy ventured into the realm of cosmology, developing a physical model of his geometric system, in a universe many times smaller than the more realistic conception of Aristarchus of Samos four centuries earlier.
The precise orientation of the Egyptian pyramids affords a lasting demonstration of the high degree of technical skill in watching the heavens attained in the 3rd millennium BC. It has been shown the Pyramids were aligned towards the pole star, which, because of the precession of the equinoxes, was at that time Thuban, a faint star in the constellation of Draco. Evaluation of the site of the temple of Amun-Re at Karnak, taking into account the change over time of the obliquity of the ecliptic, has shown that the Great Temple was aligned on the rising of the midwinter Sun. The length of the corridor down which sunlight would travel would have limited illumination at other times of the year. The Egyptians also found the position of Sirius (the dog star) who they believed was Anubis their Jackal headed god moving through the heavens. Its position was critical to their civilisation as when it rose heliacal in the east before sunrise it foretold the flooding of the Nile. It is also where we get the phrase 'dog days of summer' from. Astronomy played a considerable part in religious matters for fixing the dates of festivals and determining the hours of the night. The titles of several temple books are preserved recording the movements and phases of the sun, moon and stars. The rising of Sirius (Egyptian: Sopdet, Greek: Sothis) at the beginning of the inundation was a particularly important point to fix in the yearly calendar. Writing in the Roman era, Clement of Alexandria gives some idea of the importance of astronomical observations to the sacred rites: And after the Singer advances the Astrologer (ὡροσκόπος), with a horologium (ὡρολόγιον) in his hand, and a palm (φοίνιξ), the symbols of astrology. He must know by heart the Hermetic astrological books, which are four in number. Of these, one is about the arrangement of the fixed stars that are visible; one on the positions of the Sun and Moon and five planets; one on the conjunctions and phases of the Sun and Moon; and one concerns their risings. The Astrologer's instruments (horologium and palm) are a plumb line and sighting instrument. They have been identified with two inscribed objects in the Berlin Museum; a short handle from which a plumb line was hung, and a palm branch with a sight-slit in the broader end. The latter was held close to the eye, the former in the other hand, perhaps at arm's length. The "Hermetic" books which Clement refers to are the Egyptian theological texts, which probably have nothing to do with Hellenistic Hermetism.From the tables of stars on the ceiling of the tombs of Rameses VI and Rameses IX it seems that for fixing the hours of the night a man seated on the ground faced the Astrologer in such a position that the line of observation of the pole star passed over the middle of his head. On the different days of the year each hour was determined by a fixed star culminating or nearly culminating in it, and the position of these stars at the time is given in the tables as in the centre, on the left eye, on the right shoulder, etc. According to the texts, in founding or rebuilding temples the north axis was determined by the same apparatus, and we may conclude that it was the usual one for astronomical observations. In careful hands it might give results of a high degree of accuracy.
The astronomy of East Asia began in China. Solar term was completed in Warring States period. The knowledge of Chinese astronomy was introduced into East Asia. Astronomy in China has a long history. Detailed records of astronomical observations were kept from about the 6th century BC, until the introduction of Western astronomy and the telescope in the 17th century. Chinese astronomers were able to precisely predict eclipses. Much of early Chinese astronomy was for the purpose of timekeeping. The Chinese used a lunisolar calendar, but because the cycles of the Sun and the Moon are different, astronomers often prepared new calendars and made observations for that purpose. Astrological divination was also an important part of astronomy. Astronomers took careful note of "guest stars"(Chinese: 客星; pinyin: kèxīng; lit.: 'guest star') which suddenly appeared among the fixed stars. They were the first to record a supernova, in the Astrological Annals of the Houhanshu in 185 AD. Also, the supernova that created the Crab Nebula in 1054 is an example of a "guest star" observed by Chinese astronomers, although it was not recorded by their European contemporaries. Ancient astronomical records of phenomena like supernovae and comets are sometimes used in modern astronomical studies. The world's first star catalogue was made by Gan De, a Chinese astronomer, in the 4th century BC.
Maya astronomical codices include detailed tables for calculating phases of the Moon, the recurrence of eclipses, and the appearance and disappearance of Venus as morning and evening star. The Maya based their calendrics in the carefully calculated cycles of the Pleiades, the Sun, the Moon, Venus, Jupiter, Saturn, Mars, and also they had a precise description of the eclipses as depicted in the Dresden Codex, as well as the ecliptic or zodiac, and the Milky Way was crucial in their Cosmology. A number of important Maya structures are believed to have been oriented toward the extreme risings and settings of Venus. To the ancient Maya, Venus was the patron of war and many recorded battles are believed to have been timed to the motions of this planet. Mars is also mentioned in preserved astronomical codices and early mythology.Although the Maya calendar was not tied to the Sun, John Teeple has proposed that the Maya calculated the solar year to somewhat greater accuracy than the Gregorian calendar. Both astronomy and an intricate numerological scheme for the measurement of time were vitally important components of Maya religion.
Since 1990 our understanding of prehistoric Europeans has been radically changed by discoveries of ancient astronomical artifacts throughout Europe. The artifacts demonstrate that Neolithic and Bronze Age Europeans had a sophisticated knowledge of mathematics and astronomy. Among the discoveries are: Paleolithic archaeologist Alexander Marshack put forward a theory in 1972 that bone sticks from locations like Africa and Europe from possibly as long ago as 35,000 BCE could be marked in ways that tracked the Moon's phases, an interpretation that has met with criticism. The Warren Field calendar in the Dee River valley of Scotland's Aberdeenshire. First excavated in 2004 but only in 2013 revealed as a find of huge significance, it is to date the world's oldest known calendar, created around 8000 BC and predating all other calendars by some 5,000 years. The calendar takes the form of an early Mesolithic monument containing a series of 12 pits which appear to help the observer track lunar months by mimicking the phases of the Moon. It also aligns to sunrise at the winter solstice, thus coordinating the solar year with the lunar cycles. The monument had been maintained and periodically reshaped, perhaps up to hundreds of times, in response to shifting solar/lunar cycles, over the course of 6,000 years, until the calendar fell out of use around 4,000 years ago. Goseck circle is located in Germany and belongs to the linear pottery culture. First discovered in 1991, its significance was only clear after results from archaeological digs became available in 2004. The site is one of hundreds of similar circular enclosures built in a region encompassing Austria, Germany, and the Czech Republic during a 200-year period starting shortly after 5000 BC. The Nebra sky disc is a Bronze Age bronze disc that was buried in Germany, not far from the Goseck circle, around 1600 BC. It measures about 30 cm diameter with a mass of 2.2 kg and displays a blue-green patina (from oxidization) inlaid with gold symbols. Found by archeological thieves in 1999 and recovered in Switzerland in 2002, it was soon recognized as a spectacular discovery, among the most important of the 20th century. Investigations revealed that the object had been in use around 400 years before burial (2000 BC), but that its use had been forgotten by the time of burial. The inlaid gold depicted the full moon, a crescent moon about 4 or 5 days old, and the Pleiades star cluster in a specific arrangement forming the earliest known depiction of celestial phenomena. Twelve lunar months pass in 354 days, requiring a calendar to insert a leap month every two or three years in order to keep synchronized with the solar year's seasons (making it lunisolar). The earliest known descriptions of this coordination were recorded by the Babylonians in 6th or 7th centuries BC, over one thousand years later. Those descriptions verified ancient knowledge of the Nebra sky disc's celestial depiction as the precise arrangement needed to judge when to insert the intercalary month into a lunisolar calendar, making it an astronomical clock for regulating such a calendar a thousand or more years before any other known method. The Kokino site, discovered in 2001, sits atop an extinct volcanic cone at an elevation of 1,013 metres (3,323 ft), occupying about 0.5 hectares overlooking the surrounding countryside in North Macedonia. A Bronze Age astronomical observatory was constructed there around 1900 BC and continuously served the nearby community that lived there until about 700 BC. The central space was used to observe the rising of the Sun and full moon. Three markings locate sunrise at the summer and winter solstices and at the two equinoxes. Four more give the minimum and maximum declinations of the full moon: in summer, and in winter. Two measure the lengths of lunar months. Together, they reconcile solar and lunar cycles in marking the 235 lunations that occur during 19 solar years, regulating a lunar calendar. On a platform separate from the central space, at lower elevation, four stone seats (thrones) were made in north-south alignment, together with a trench marker cut in the eastern wall. This marker allows the rising Sun's light to fall on only the second throne, at midsummer (about July 31). It was used for ritual ceremony linking the ruler to the local sun god, and also marked the end of the growing season and time for harvest. Golden hats of Germany, France and Switzerland dating from 1400–800 BC are associated with the Bronze Age Urnfield culture. The Golden hats are decorated with a spiral motif of the Sun and the Moon. They were probably a kind of calendar used to calibrate between the lunar and solar calendars. Modern scholarship has demonstrated that the ornamentation of the gold leaf cones of the Schifferstadt type, to which the Berlin Gold Hat example belongs, represent systematic sequences in terms of number and types of ornaments per band. A detailed study of the Berlin example, which is the only fully preserved one, showed that the symbols probably represent a lunisolar calendar. The object would have permitted the determination of dates or periods in both lunar and solar calendars.
The Arabic and the Persian world under Islam had become highly cultured, and many important works of knowledge from Greek astronomy and Indian astronomy and Persian astronomy were translated into Arabic, used and stored in libraries throughout the area. An important contribution by Islamic astronomers was their emphasis on observational astronomy. This led to the emergence of the first astronomical observatories in the Muslim world by the early 9th century. Zij star catalogues were produced at these observatories. In the 10th century, Abd al-Rahman al-Sufi (Azophi) carried out observations on the stars and described their positions, magnitudes, brightness, and colour and drawings for each constellation in his Book of Fixed Stars. He also gave the first descriptions and pictures of "A Little Cloud" now known as the Andromeda Galaxy. He mentions it as lying before the mouth of a Big Fish, an Arabic constellation. This "cloud" was apparently commonly known to the Isfahan astronomers, very probably before 905 AD. The first recorded mention of the Large Magellanic Cloud was also given by al-Sufi. In 1006, Ali ibn Ridwan observed SN 1006, the brightest supernova in recorded history, and left a detailed description of the temporary star. In the late 10th century, a huge observatory was built near Tehran, Iran, by the astronomer Abu-Mahmud al-Khujandi who observed a series of meridian transits of the Sun, which allowed him to calculate the tilt of the Earth's axis relative to the Sun. He noted that measurements by earlier (Indian, then Greek) astronomers had found higher values for this angle, possible evidence that the axial tilt is not constant but was in fact decreasing. In 11th-century Persia, Omar Khayyám compiled many tables and performed a reformation of the calendar that was more accurate than the Julian and came close to the Gregorian. Other Muslim advances in astronomy included the collection and correction of previous astronomical data, resolving significant problems in the Ptolemaic model, the development of the universal latitude-independent astrolabe by Arzachel, the invention of numerous other astronomical instruments, Ja'far Muhammad ibn Mūsā ibn Shākir's belief that the heavenly bodies and celestial spheres were subject to the same physical laws as Earth, the first elaborate experiments related to astronomical phenomena, the introduction of exacting empirical observations and experimental techniques, and the introduction of empirical testing by Ibn al-Shatir, who produced the first model of lunar motion which matched physical observations.Natural philosophy (particularly Aristotelian physics) was separated from astronomy by Ibn al-Haytham (Alhazen) in the 11th century, by Ibn al-Shatir in the 14th century, and Qushji in the 15th century, leading to the development of an astronomical physics.
After the significant contributions of Greek scholars to the development of astronomy, it entered a relatively static era in Western Europe from the Roman era through the 12th century. This lack of progress has led some astronomers to assert that nothing happened in Western European astronomy during the Middle Ages. Recent investigations, however, have revealed a more complex picture of the study and teaching of astronomy in the period from the 4th to the 16th centuries.Western Europe entered the Middle Ages with great difficulties that affected the continent's intellectual production. The advanced astronomical treatises of classical antiquity were written in Greek, and with the decline of knowledge of that language, only simplified summaries and practical texts were available for study. The most influential writers to pass on this ancient tradition in Latin were Macrobius, Pliny, Martianus Capella, and Calcidius. In the 6th century Bishop Gregory of Tours noted that he had learned his astronomy from reading Martianus Capella, and went on to employ this rudimentary astronomy to describe a method by which monks could determine the time of prayer at night by watching the stars.In the 7th century the English monk Bede of Jarrow published an influential text, On the Reckoning of Time, providing churchmen with the practical astronomical knowledge needed to compute the proper date of Easter using a procedure called the computus. This text remained an important element of the education of clergy from the 7th century until well after the rise of the Universities in the 12th century.The range of surviving ancient Roman writings on astronomy and the teachings of Bede and his followers began to be studied in earnest during the revival of learning sponsored by the emperor Charlemagne. By the 9th century rudimentary techniques for calculating the position of the planets were circulating in Western Europe; medieval scholars recognized their flaws, but texts describing these techniques continued to be copied, reflecting an interest in the motions of the planets and in their astrological significance.Building on this astronomical background, in the 10th century European scholars such as Gerbert of Aurillac began to travel to Spain and Sicily to seek out learning which they had heard existed in the Arabic-speaking world. There they first encountered various practical astronomical techniques concerning the calendar and timekeeping, most notably those dealing with the astrolabe. Soon scholars such as Hermann of Reichenau were writing texts in Latin on the uses and construction of the astrolabe and others, such as Walcher of Malvern, were using the astrolabe to observe the time of eclipses in order to test the validity of computistical tables.By the 12th century, scholars were traveling to Spain and Sicily to seek out more advanced astronomical and astrological texts, which they translated into Latin from Arabic and Greek to further enrich the astronomical knowledge of Western Europe. The arrival of these new texts coincided with the rise of the universities in medieval Europe, in which they soon found a home. Reflecting the introduction of astronomy into the universities, John of Sacrobosco wrote a series of influential introductory astronomy textbooks: the Sphere, a Computus, a text on the Quadrant, and another on Calculation.In the 14th century, Nicole Oresme, later bishop of Liseux, showed that neither the scriptural texts nor the physical arguments advanced against the movement of the Earth were demonstrative and adduced the argument of simplicity for the theory that the Earth moves, and not the heavens. However, he concluded "everyone maintains, and I think myself, that the heavens do move and not the earth: For God hath established the world which shall not be moved." In the 15th century, Cardinal Nicholas of Cusa suggested in some of his scientific writings that the Earth revolved around the Sun, and that each star is itself a distant sun.
During the renaissance period, astronomy began to undergo a revolution in thought known as the Copernican revolution, which gets the name from the astronomer Nicolaus Copernicus, who proposed a heliocentric system, in which the planets revolved around the Sun and not the Earth. His De Revolutionibus Orbium Coelestium was published in 1543. While in the long term this was a very controversial claim, in the very beginning it only brought minor controversy. The theory became the dominant view because many figures, most notably Galileo Galilei, Johannes Kepler and Isaac Newton championed and improved upon the work. Other figures also aided this new model despite not believing the overall theory, like Tycho Brahe, with his well-known observations.Brahe, a Danish noble, was an essential astronomer in this period. He came on the astronomical scene with the publication of De Nova Stella in which he disproved conventional wisdom on the supernova SN 1572. He also created the Tychonic System in which he blended the mathematical benefits of the Copernican system and the “physical benefits” of the Ptolemaic system. This was one of the systems people believed in when they did not accept heliocentrism, but could no longer accept the Ptolemaic system. He is most known for his highly accurate observations of the stars and the solar system. Later he moved to Prague and continued his work. In Prague he was at work on the Rudolphine Tables, that were not finished until after his death. The Rudolphine Tables was a star map designed to be more accurate than either the Alphonsine Tables, made in the 1300s and the Prutenic Tables which were inaccurate. He was assisted at this time by his assistant Johannes Kepler, who would later use his observations to finish Brahe's works and for his theories as well.After the death of Brahe, Kepler was deemed his successor and was given the job of completing Brahe's uncompleted works, like the Rudolphine Tables. He completed the Rudolphine Tables in 1624, although it was not published for several years. Like many other figures of this era, he was subject to religious and political troubles, like the Thirty Years War, which led to chaos that almost destroyed some of his works. Kepler was, however, the first to attempt to derive mathematical predictions of celestial motions from assumed physical causes. He discovered the three Kepler's Laws of Planetary Motion that now carry his name, those laws being as follows: The orbit of a planet is an ellipse with the Sun at one of the two foci. A line segment joining a planet and the Sun sweeps out equal areas during equal intervals of time. The square of the orbital period of a planet is proportional to the cube of the semi-major axis of its orbit. With these laws, he managed to improve upon the existing Heliocentric model. The first two were published in 1609. Kepler's contributions improved upon the overall system, giving it more credibility because it adequately explained events and could cause more reliable predictions. Before this the Copernican model was just as unreliable as the Ptolemaic model. This improvement came because Kepler realized the orbits were not perfect circles, but ellipses.Galileo Galilei was among the first to use a telescope to observe the sky, and after constructing a 20x refractor telescope. He discovered the four largest moons of Jupiter in 1610, which are now collectively known as the Galilean moons, in his honor. This discovery was the first known observation of satellites orbiting another planet. He also found that our Moon had craters and observed, and correctly explained, sunspots, and that Venus exhibited a full set of phases resembling lunar phases. Galileo argued that these facts demonstrated incompatibility with the Ptolemaic model, which could not explain the phenomenon and would even contradict it. With the moons it demonstrated that the Earth does not have to have everything orbiting it and that other parts of the Solar System could orbit another object, such as the Earth orbiting the Sun. In the Ptolemaic system the celestial bodies were supposed to be perfect so such objects should not have craters or sunspots. The phases of Venus could only happen in the event that Venus's orbit is insides Earth's orbit, which could not happen if the Earth was the center. He, as the most famous example, had to face challenges from church officials, more specifically the Roman Inquisition. They accused him of heresy because these beliefs went against the teachings of the Roman Catholic Church and were challenging the Catholic church's authority when it was at its weakest. While he was able to avoid punishment for a little while he was eventually tried and pled guilty to heresy in 1633. Although this came at some expense, his book was banned, and he was put under house arrest until he died in 1642.Sir Isaac Newton developed further ties between physics and astronomy through his law of universal gravitation. Realizing that the same force that attracts objects to the surface of the Earth held the Moon in orbit around the Earth, Newton was able to explain – in one theoretical framework – all known gravitational phenomena. In his Philosophiae Naturalis Principia Mathematica, he derived Kepler's laws from first principles. Those first principles are as follows: In an inertial frame of reference, an object either remains at rest or continues to move at constant velocity, unless acted upon by a force. In an inertial reference frame, the vector sum of the forces F on an object is equal to the mass m of that object multiplied by the acceleration a of the object: F = ma. (It is assumed here that the mass m is constant) When one body exerts a force on a second body, the second body simultaneously exerts a force equal in magnitude and opposite in direction on the first body.Thus while Kepler explained how the planets moved, Newton accurately managed to explain why the planets moved the way they do. Newton's theoretical developments laid many of the foundations of modern physics.
Outside of England, Newton's theory took some time to become established. Descartes' theory of vortices held sway in France, and Huygens, Leibniz and Cassini accepted only parts of Newton's system, preferring their own philosophies. Voltaire published a popular account in 1738. In 1748, the French Academy of Sciences offered a reward for solving the perturbations of Jupiter and Saturn which was eventually solved by Euler and Lagrange. Laplace completed the theory of the planets, publishing from 1798 to 1825. Edmund Halley succeeded Flamsteed as Astronomer Royal in England and succeeded in predicting the return in 1758 of the comet that bears his name. Sir William Herschel found the first new planet, Uranus, to be observed in modern times in 1781. The gap between the planets Mars and Jupiter disclosed by the Titius–Bode law was filled by the discovery of the asteroids Ceres and Pallas in 1801 and 1802 with many more following. At first, astronomical thought in America was based on Aristotelian philosophy, but interest in the new astronomy began to appear in Almanacs as early as 1659.
In the 19th century, Joseph von Fraunhofer discovered that when sunlight was dispersed, a multitude of spectral lines were observed (regions where there was less or no light). Experiments with hot gases showed that the same lines could be observed in the spectra of gases, with specific lines corresponding to unique elements. It was proved that the chemical elements found in the Sun (chiefly hydrogen and helium) were also found on Earth. During the 20th century spectroscopy (the study of these lines) advanced, especially because of the advent of quantum physics, which was necessary to understand the observations. Although in previous centuries noted astronomers were exclusively male, at the turn of the 20th century women began to play a role in the great discoveries. In this period prior to modern computers, women at the United States Naval Observatory (USNO), Harvard University, and other astronomy research institutions began to be hired as human "computers", who performed the tedious calculations while scientists performed research requiring more background knowledge. A number of discoveries in this period were originally noted by the women "computers" and reported to their supervisors. For example, at the Harvard Observatory Henrietta Swan Leavitt discovered the cepheid variable star period-luminosity relation which she further developed into a method of measuring distance outside of the Solar System. Annie Jump Cannon, also at Harvard, organized the stellar spectral types according to stellar temperature. In 1847, Maria Mitchell discovered a comet using a telescope. According to Lewis D. Eigen, Cannon alone, "in only 4 years discovered and catalogued more stars than all the men in history put together." Most of these women received little or no recognition during their lives due to their lower professional standing in the field of astronomy. Although their discoveries and methods are taught in classrooms around the world, few students of astronomy can attribute the works to their authors or have any idea that there were active female astronomers at the end of the 19th century.
Most of our current knowledge was gained during the 20th century. With the help of the use of photography, fainter objects were observed. The Sun was found to be part of a galaxy made up of more than 1010 stars (10 billion stars). The existence of other galaxies, one of the matters of the great debate, was settled by Edwin Hubble, who identified the Andromeda nebula as a different galaxy, and many others at large distances and receding, moving away from our galaxy. Physical cosmology, a discipline that has a large intersection with astronomy, made huge advances during the 20th century, with the model of the hot Big Bang heavily supported by the evidence provided by astronomy and physics, such as the redshifts of very distant galaxies and radio sources, the cosmic microwave background radiation, Hubble's law and cosmological abundances of elements.
In the 19th century, scientists began discovering forms of light which were invisible to the naked eye: X-Rays, gamma rays, radio waves, microwaves, ultraviolet radiation, and infrared radiation. This had a major impact on astronomy, spawning the fields of infrared astronomy, radio astronomy, x-ray astronomy and finally gamma-ray astronomy. With the advent of spectroscopy it was proven that other stars were similar to the Sun, but with a range of temperatures, masses and sizes. The existence of our galaxy, the Milky Way, as a separate group of stars was only proven in the 20th century, along with the existence of "external" galaxies, and soon after, the expansion of the universe seen in the recession of most galaxies from us.
DIO: The International Journal of Scientific History Journal for the History of Astronomy Journal of Astronomical History and Heritage
Paris Observatory books and manuscripts UNESCO-IAU Portal to the Heritage of Astronomy Astronomiae Historia / History of Astronomy at the Astronomical Institutes of Bonn University. Society for the History of Astronomy Mayan Astronomy Caelum Antiquum: Ancient Astronomy and Astrology at LacusCurtius Mesoamerican Archaeoastronomy "The Book of Instruction on Deviant Planes and Simple Planes" is a manuscript in Arabic that dates back to 1740 and talks about practical astronomy, with diagrams. More information on women astronomers Astronomy & Empire, BBC Radio 4 discussion with Simon Schaffer, Kristen Lippincott & Allan Chapman (In Our Time, May 4, 2006)
India (Hindi: Bhārat), officially the Republic of India (Hindi: Bhārat Gaṇarājya), is a country in South Asia. It is the second-most populous country, the seventh-largest country by land area, and the most populous democracy in the world. Bounded by the Indian Ocean on the south, the Arabian Sea on the southwest, and the Bay of Bengal on the southeast, it shares land borders with Pakistan to the west; China, Nepal, and Bhutan to the north; and Bangladesh and Myanmar to the east. In the Indian Ocean, India is in the vicinity of Sri Lanka and the Maldives; its Andaman and Nicobar Islands share a maritime border with Thailand and Indonesia. Modern humans arrived on the Indian subcontinent from Africa no later than 55,000 years ago. Their long occupation, initially in varying forms of isolation as hunter-gatherers, has made the region highly diverse, second only to Africa in human genetic diversity. Settled life emerged on the subcontinent in the western margins of the Indus river basin 9,000 years ago, evolving gradually into the Indus Valley Civilisation of the third millennium BCE. By 1200 BCE, an archaic form of Sanskrit, an Indo-European language, had diffused into India from the northwest, unfolding as the language of the Rigveda, and recording the dawning of Hinduism in India. The Dravidian languages of India were supplanted in the northern and western regions. By 400 BCE, stratification and exclusion by caste had emerged within Hinduism, and Buddhism and Jainism had arisen, proclaiming social orders unlinked to heredity. Early political consolidations gave rise to the loose-knit Maurya and Gupta Empires based in the Ganges Basin. Their collective era was suffused with wide-ranging creativity, but also marked by the declining status of women, and the incorporation of untouchability into an organised system of belief. In South India, the Middle kingdoms exported Dravidian-languages scripts and religious cultures to the kingdoms of Southeast Asia.In the early medieval era, Christianity, Islam, Judaism, and Zoroastrianism put down roots on India's southern and western coasts. Muslim armies from Central Asia intermittently overran India's northern plains, eventually establishing the Delhi Sultanate, and drawing northern India into the cosmopolitan networks of medieval Islam. In the 15th century, the Vijayanagara Empire created a long-lasting composite Hindu culture in south India. In the Punjab, Sikhism emerged, rejecting institutionalised religion. The Mughal Empire, in 1526, ushered in two centuries of relative peace, leaving a legacy of luminous architecture. Gradually expanding rule of the British East India Company followed, turning India into a colonial economy, but also consolidating its sovereignty. British Crown rule began in 1858. The rights promised to Indians were granted slowly, but technological changes were introduced, and ideas of education, modernity and the public life took root. A pioneering and influential nationalist movement emerged, which was noted for nonviolent resistance and became the major factor in ending British rule. In 1947 the British Indian Empire was partitioned into two independent dominions, a Hindu-majority Dominion of India and a Muslim-majority Dominion of Pakistan, amid large-scale loss of life and an unprecedented migration.India has been a secular federal republic since 1950, governed in a democratic parliamentary system. It is a pluralistic, multilingual and multi-ethnic society. India's population grew from 361 million in 1951 to 1,211 million in 2011. During the same time, its nominal per capita income increased from US$64 annually to US$1,498, and its literacy rate from 16.6% to 74%. From being a comparatively destitute country in 1951, India has become a fast-growing major economy, a hub for information technology services, with an expanding middle class. It has a space programme which includes several planned or completed extraterrestrial missions. Indian movies, music, and spiritual teachings play an increasing role in global culture. India has substantially reduced its rate of poverty, though at the cost of increasing economic inequality. India is a nuclear weapons state, which ranks high in military expenditure. It has disputes over Kashmir with its neighbours, Pakistan and China, unresolved since the mid-20th century. Among the socio-economic challenges India faces are gender inequality, child malnutrition, and rising levels of air pollution. India's land is megadiverse, with four biodiversity hotspots. Its forest cover comprises 21.4% of its area. India's wildlife, which has traditionally been viewed with tolerance in India's culture, is supported among these forests, and elsewhere, in protected habitats.
According to the Oxford English Dictionary (third edition 2009), the name "India" is derived from the Classical Latin India, a reference to South Asia and an uncertain region to its east; and in turn derived successively from: Hellenistic Greek India ( Ἰνδία); ancient Greek Indos ( Ἰνδός); Old Persian Hindush, an eastern province of the Achaemenid empire; and ultimately its cognate, the Sanskrit Sindhu, or "river," specifically the Indus river and, by implication, its well-settled southern basin. The ancient Greeks referred to the Indians as Indoi (Ἰνδοί), which translates as "The people of the Indus".The term Bharat (Bhārat; pronounced [ˈbʱaːɾət] (listen)), mentioned in both Indian epic poetry and the Constitution of India, is used in its variations by many Indian languages. A modern rendering of the historical name Bharatavarsha, which applied originally to a region of the Gangetic Valley, Bharat gained increased currency from the mid-19th century as a native name for India.Hindustan ([ɦɪndʊˈstaːn] (listen)) is a Middle Persian name for India, introduced during the Mughal Empire and used widely since. Its meaning has varied, referring to a region encompassing present-day northern India and Pakistan or to India in its near entirety.
By 55,000 years ago, the first modern humans, or Homo sapiens, had arrived on the Indian subcontinent from Africa, where they had earlier evolved. The earliest known modern human remains in South Asia date to about 30,000 years ago. After 6500 BCE, evidence for domestication of food crops and animals, construction of permanent structures, and storage of agricultural surplus appeared in Mehrgarh and other sites in what is now Balochistan. These gradually developed into the Indus Valley Civilisation, the first urban culture in South Asia, which flourished during 2500–1900 BCE in what is now Pakistan and western India. Centred around cities such as Mohenjo-daro, Harappa, Dholavira, and Kalibangan, and relying on varied forms of subsistence, the civilisation engaged robustly in crafts production and wide-ranging trade.During the period 2000–500 BCE, many regions of the subcontinent transitioned from the Chalcolithic cultures to the Iron Age ones. The Vedas, the oldest scriptures associated with Hinduism, were composed during this period, and historians have analysed these to posit a Vedic culture in the Punjab region and the upper Gangetic Plain. Most historians also consider this period to have encompassed several waves of Indo-Aryan migration into the subcontinent from the north-west. The caste system, which created a hierarchy of priests, warriors, and free peasants, but which excluded indigenous peoples by labelling their occupations impure, arose during this period. On the Deccan Plateau, archaeological evidence from this period suggests the existence of a chiefdom stage of political organisation. In South India, a progression to sedentary life is indicated by the large number of megalithic monuments dating from this period, as well as by nearby traces of agriculture, irrigation tanks, and craft traditions. In the late Vedic period, around the 6th century BCE, the small states and chiefdoms of the Ganges Plain and the north-western regions had consolidated into 16 major oligarchies and monarchies that were known as the mahajanapadas. The emerging urbanisation gave rise to non-Vedic religious movements, two of which became independent religions. Jainism came into prominence during the life of its exemplar, Mahavira. Buddhism, based on the teachings of Gautama Buddha, attracted followers from all social classes excepting the middle class; chronicling the life of the Buddha was central to the beginnings of recorded history in India. In an age of increasing urban wealth, both religions held up renunciation as an ideal, and both established long-lasting monastic traditions. Politically, by the 3rd century BCE, the kingdom of Magadha had annexed or reduced other states to emerge as the Mauryan Empire. The empire was once thought to have controlled most of the subcontinent except the far south, but its core regions are now thought to have been separated by large autonomous areas. The Mauryan kings are known as much for their empire-building and determined management of public life as for Ashoka's renunciation of militarism and far-flung advocacy of the Buddhist dhamma.The Sangam literature of the Tamil language reveals that, between 200 BCE and 200 CE, the southern peninsula was ruled by the Cheras, the Cholas, and the Pandyas, dynasties that traded extensively with the Roman Empire and with West and South-East Asia. In North India, Hinduism asserted patriarchal control within the family, leading to increased subordination of women. By the 4th and 5th centuries, the Gupta Empire had created a complex system of administration and taxation in the greater Ganges Plain; this system became a model for later Indian kingdoms. Under the Guptas, a renewed Hinduism based on devotion, rather than the management of ritual, began to assert itself. This renewal was reflected in a flowering of sculpture and architecture, which found patrons among an urban elite. Classical Sanskrit literature flowered as well, and Indian science, astronomy, medicine, and mathematics made significant advances.
The Indian early medieval age, 600 CE to 1200 CE, is defined by regional kingdoms and cultural diversity. When Harsha of Kannauj, who ruled much of the Indo-Gangetic Plain from 606 to 647 CE, attempted to expand southwards, he was defeated by the Chalukya ruler of the Deccan. When his successor attempted to expand eastwards, he was defeated by the Pala king of Bengal. When the Chalukyas attempted to expand southwards, they were defeated by the Pallavas from farther south, who in turn were opposed by the Pandyas and the Cholas from still farther south. No ruler of this period was able to create an empire and consistently control lands much beyond his core region. During this time, pastoral peoples, whose land had been cleared to make way for the growing agricultural economy, were accommodated within caste society, as were new non-traditional ruling classes. The caste system consequently began to show regional differences.In the 6th and 7th centuries, the first devotional hymns were created in the Tamil language. They were imitated all over India and led to both the resurgence of Hinduism and the development of all modern languages of the subcontinent. Indian royalty, big and small, and the temples they patronised drew citizens in great numbers to the capital cities, which became economic hubs as well. Temple towns of various sizes began to appear everywhere as India underwent another urbanisation. By the 8th and 9th centuries, the effects were felt in South-East Asia, as South Indian culture and political systems were exported to lands that became part of modern-day Myanmar, Thailand, Laos, Cambodia, Vietnam, Philippines, Malaysia, and Java. Indian merchants, scholars, and sometimes armies were involved in this transmission; South-East Asians took the initiative as well, with many sojourning in Indian seminaries and translating Buddhist and Hindu texts into their languages. After the 10th century, Muslim Central Asian nomadic clans, using swift-horse cavalry and raising vast armies united by ethnicity and religion, repeatedly overran South Asia's north-western plains, leading eventually to the establishment of the Islamic Delhi Sultanate in 1206. The sultanate was to control much of North India and to make many forays into South India. Although at first disruptive for the Indian elites, the sultanate largely left its vast non-Muslim subject population to its own laws and customs. By repeatedly repulsing Mongol raiders in the 13th century, the sultanate saved India from the devastation visited on West and Central Asia, setting the scene for centuries of migration of fleeing soldiers, learned men, mystics, traders, artists, and artisans from that region into the subcontinent, thereby creating a syncretic Indo-Islamic culture in the north. The sultanate's raiding and weakening of the regional kingdoms of South India paved the way for the indigenous Vijayanagara Empire. Embracing a strong Shaivite tradition and building upon the military technology of the sultanate, the empire came to control much of peninsular India, and was to influence South Indian society for long afterwards.
In the early 16th century, northern India, then under mainly Muslim rulers, fell again to the superior mobility and firepower of a new generation of Central Asian warriors. The resulting Mughal Empire did not stamp out the local societies it came to rule. Instead, it balanced and pacified them through new administrative practices and diverse and inclusive ruling elites, leading to more systematic, centralised, and uniform rule. Eschewing tribal bonds and Islamic identity, especially under Akbar, the Mughals united their far-flung realms through loyalty, expressed through a Persianised culture, to an emperor who had near-divine status. The Mughal state's economic policies, deriving most revenues from agriculture and mandating that taxes be paid in the well-regulated silver currency, caused peasants and artisans to enter larger markets. The relative peace maintained by the empire during much of the 17th century was a factor in India's economic expansion, resulting in greater patronage of painting, literary forms, textiles, and architecture. Newly coherent social groups in northern and western India, such as the Marathas, the Rajputs, and the Sikhs, gained military and governing ambitions during Mughal rule, which, through collaboration or adversity, gave them both recognition and military experience. Expanding commerce during Mughal rule gave rise to new Indian commercial and political elites along the coasts of southern and eastern India. As the empire disintegrated, many among these elites were able to seek and control their own affairs. By the early 18th century, with the lines between commercial and political dominance being increasingly blurred, a number of European trading companies, including the English East India Company, had established coastal outposts. The East India Company's control of the seas, greater resources, and more advanced military training and technology led it to increasingly flex its military muscle and caused it to become attractive to a portion of the Indian elite; these factors were crucial in allowing the company to gain control over the Bengal region by 1765 and sideline the other European companies. Its further access to the riches of Bengal and the subsequent increased strength and size of its army enabled it to annexe or subdue most of India by the 1820s. India was then no longer exporting manufactured goods as it long had, but was instead supplying the British Empire with raw materials. Many historians consider this to be the onset of India's colonial period. By this time, with its economic power severely curtailed by the British parliament and having effectively been made an arm of British administration, the company began more consciously to enter non-economic arenas like education, social reform, and culture.
Historians consider India's modern age to have begun sometime between 1848 and 1885. The appointment in 1848 of Lord Dalhousie as Governor General of the East India Company set the stage for changes essential to a modern state. These included the consolidation and demarcation of sovereignty, the surveillance of the population, and the education of citizens. Technological changes—among them, railways, canals, and the telegraph—were introduced not long after their introduction in Europe. However, disaffection with the company also grew during this time and set off the Indian Rebellion of 1857. Fed by diverse resentments and perceptions, including invasive British-style social reforms, harsh land taxes, and summary treatment of some rich landowners and princes, the rebellion rocked many regions of northern and central India and shook the foundations of Company rule. Although the rebellion was suppressed by 1858, it led to the dissolution of the East India Company and the direct administration of India by the British government. Proclaiming a unitary state and a gradual but limited British-style parliamentary system, the new rulers also protected princes and landed gentry as a feudal safeguard against future unrest. In the decades following, public life gradually emerged all over India, leading eventually to the founding of the Indian National Congress in 1885.The rush of technology and the commercialisation of agriculture in the second half of the 19th century was marked by economic setbacks and many small farmers became dependent on the whims of far-away markets. There was an increase in the number of large-scale famines, and, despite the risks of infrastructure development borne by Indian taxpayers, little industrial employment was generated for Indians. There were also salutary effects: commercial cropping, especially in the newly canalled Punjab, led to increased food production for internal consumption. The railway network provided critical famine relief, notably reduced the cost of moving goods, and helped nascent Indian-owned industry. After World War I, in which approximately one million Indians served, a new period began. It was marked by British reforms but also repressive legislation, by more strident Indian calls for self-rule, and by the beginnings of a nonviolent movement of non-co-operation, of which Mohandas Karamchand Gandhi would become the leader and enduring symbol. During the 1930s, slow legislative reform was enacted by the British; the Indian National Congress won victories in the resulting elections. The next decade was beset with crises: Indian participation in World War II, the Congress's final push for non-co-operation, and an upsurge of Muslim nationalism. All were capped by the advent of independence in 1947, but tempered by the partition of India into two states: India and Pakistan.Vital to India's self-image as an independent nation was its constitution, completed in 1950, which put in place a secular and democratic republic. It has remained a democracy with civil liberties, an active Supreme Court, and a largely independent press. Economic liberalisation, which began in the 1990s, has created a large urban middle class, transformed India into one of the world's fastest-growing economies, and increased its geopolitical clout. Indian movies, music, and spiritual teachings play an increasing role in global culture. Yet, India is also shaped by seemingly unyielding poverty, both rural and urban; by religious and caste-related violence; by Maoist-inspired Naxalite insurgencies; and by separatism in Jammu and Kashmir and in Northeast India. It has unresolved territorial disputes with China and with Pakistan. India's sustained democratic freedoms are unique among the world's newer nations; however, in spite of its recent economic successes, freedom from want for its disadvantaged population remains a goal yet to be achieved.
India accounts for the bulk of the Indian subcontinent, lying atop the Indian tectonic plate, a part of the Indo-Australian Plate. India's defining geological processes began 75 million years ago when the Indian Plate, then part of the southern supercontinent Gondwana, began a north-eastward drift caused by seafloor spreading to its south-west, and later, south and south-east. Simultaneously, the vast Tethyan oceanic crust, to its northeast, began to subduct under the Eurasian Plate. These dual processes, driven by convection in the Earth's mantle, both created the Indian Ocean and caused the Indian continental crust eventually to under-thrust Eurasia and to uplift the Himalayas. Immediately south of the emerging Himalayas, plate movement created a vast trough that rapidly filled with river-borne sediment and now constitutes the Indo-Gangetic Plain. Cut off from the plain by the ancient Aravalli Range lies the Thar Desert.The original Indian Plate survives as peninsular India, the oldest and geologically most stable part of India. It extends as far north as the Satpura and Vindhya ranges in central India. These parallel chains run from the Arabian Sea coast in Gujarat in the west to the coal-rich Chota Nagpur Plateau in Jharkhand in the east. To the south, the remaining peninsular landmass, the Deccan Plateau, is flanked on the west and east by coastal ranges known as the Western and Eastern Ghats; the plateau contains the country's oldest rock formations, some over one billion years old. Constituted in such fashion, India lies to the north of the equator between 6° 44′ and 35° 30′ north latitude and 68° 7′ and 97° 25′ east longitude.India's coastline measures 7,517 kilometres (4,700 mi) in length; of this distance, 5,423 kilometres (3,400 mi) belong to peninsular India and 2,094 kilometres (1,300 mi) to the Andaman, Nicobar, and Lakshadweep island chains. According to the Indian naval hydrographic charts, the mainland coastline consists of the following: 43% sandy beaches; 11% rocky shores, including cliffs; and 46% mudflats or marshy shores. Major Himalayan-origin rivers that substantially flow through India include the Ganges and the Brahmaputra, both of which drain into the Bay of Bengal. Important tributaries of the Ganges include the Yamuna and the Kosi; the latter's extremely low gradient, caused by long-term silt deposition, leads to severe floods and course changes. Major peninsular rivers, whose steeper gradients prevent their waters from flooding, include the Godavari, the Mahanadi, the Kaveri, and the Krishna, which also drain into the Bay of Bengal; and the Narmada and the Tapti, which drain into the Arabian Sea. Coastal features include the marshy Rann of Kutch of western India and the alluvial Sundarbans delta of eastern India; the latter is shared with Bangladesh. India has two archipelagos: the Lakshadweep, coral atolls off India's south-western coast; and the Andaman and Nicobar Islands, a volcanic chain in the Andaman Sea.The Indian climate is strongly influenced by the Himalayas and the Thar Desert, both of which drive the economically and culturally pivotal summer and winter monsoons. The Himalayas prevent cold Central Asian katabatic winds from blowing in, keeping the bulk of the Indian subcontinent warmer than most locations at similar latitudes. The Thar Desert plays a crucial role in attracting the moisture-laden south-west summer monsoon winds that, between June and October, provide the majority of India's rainfall. Four major climatic groupings predominate in India: tropical wet, tropical dry, subtropical humid, and montane.
India is a megadiverse country, a term employed for 17 countries which display high biological diversity and contain many species exclusively indigenous, or endemic, to them. India is a habitat for 8.6% of all mammal species, 13.7% of bird species, 7.9% of reptile species, 6% of amphibian species, 12.2% of fish species, and 6.0% of all flowering plant species. Fully a third of Indian plant species are endemic. India also contains four of the world's 34 biodiversity hotspots, or regions that display significant habitat loss in the presence of high endemism.India's forest cover is 701,673 km2 (270,917 sq mi), which is 21.35% of the country's total land area. It can be subdivided further into broad categories of canopy density, or the proportion of the area of a forest covered by its tree canopy. Very dense forest, whose canopy density is greater than 70%, occupies 2.61% of India's land area. It predominates in the tropical moist forest of the Andaman Islands, the Western Ghats, and Northeast India. Moderately dense forest, whose canopy density is between 40% and 70%, occupies 9.59% of India's land area. It predominates in the temperate coniferous forest of the Himalayas, the moist deciduous sal forest of eastern India, and the dry deciduous teak forest of central and southern India. Open forest, whose canopy density is between 10% and 40%, occupies 9.14% of India's land area, and predominates in the babul-dominated thorn forest of the central Deccan Plateau and the western Gangetic plain.Among the Indian subcontinent's notable indigenous trees are the astringent Azadirachta indica, or neem, which is widely used in rural Indian herbal medicine, and the luxuriant Ficus religiosa, or peepul, which is displayed on the ancient seals of Mohenjo-daro, and under which the Buddha is recorded in the Pali canon to have sought enlightenment,Many Indian species have descended from those of Gondwana, the southern supercontinent from which India separated more than 100 million years ago. India's subsequent collision with Eurasia set off a mass exchange of species. However, volcanism and climatic changes later caused the extinction of many endemic Indian forms. Still later, mammals entered India from Asia through two zoogeographical passes flanking the Himalayas. This had the effect of lowering endemism among India's mammals, which stands at 12.6%, contrasting with 45.8% among reptiles and 55.8% among amphibians. Notable endemics are the vulnerable hooded leaf monkey and the threatened Beddom's toad of the Western Ghats. India contains 172 IUCN-designated threatened animal species, or 2.9% of endangered forms. These include the endangered Bengal tiger and the Ganges river dolphin. Critically endangered species include: the gharial, a crocodilian; the great Indian bustard; and the Indian white-rumped vulture, which has become nearly extinct by having ingested the carrion of diclofenac-treated cattle. The pervasive and ecologically devastating human encroachment of recent decades has critically endangered Indian wildlife. In response, the system of national parks and protected areas, first established in 1935, was expanded substantially. In 1972, India enacted the Wildlife Protection Act and Project Tiger to safeguard crucial wilderness; the Forest Conservation Act was enacted in 1980 and amendments added in 1988. India hosts more than five hundred wildlife sanctuaries and thirteen biosphere reserves, four of which are part of the World Network of Biosphere Reserves; twenty-five wetlands are registered under the Ramsar Convention.
India is the world's most populous democracy. A parliamentary republic with a multi-party system, it has eight recognised national parties, including the Indian National Congress and the Bharatiya Janata Party (BJP), and more than 40 regional parties. The Congress is considered centre-left in Indian political culture, and the BJP right-wing. For most of the period between 1950—when India first became a republic—and the late 1980s, the Congress held a majority in the parliament. Since then, however, it has increasingly shared the political stage with the BJP, as well as with powerful regional parties which have often forced the creation of multi-party coalition governments at the centre.In the Republic of India's first three general elections, in 1951, 1957, and 1962, the Jawaharlal Nehru-led Congress won easy victories. On Nehru's death in 1964, Lal Bahadur Shastri briefly became prime minister; he was succeeded, after his own unexpected death in 1966, by Nehru's daughter Indira Gandhi, who went on to lead the Congress to election victories in 1967 and 1971. Following public discontent with the state of emergency she declared in 1975, the Congress was voted out of power in 1977; the then-new Janata Party, which had opposed the emergency, was voted in. Its government lasted just over two years. Voted back into power in 1980, the Congress saw a change in leadership in 1984, when Indira Gandhi was assassinated; she was succeeded by her son Rajiv Gandhi, who won an easy victory in the general elections later that year. The Congress was voted out again in 1989 when a National Front coalition, led by the newly formed Janata Dal in alliance with the Left Front, won the elections; that government too proved relatively short-lived, lasting just under two years. Elections were held again in 1991; no party won an absolute majority. The Congress, as the largest single party, was able to form a minority government led by P. V. Narasimha Rao. A two-year period of political turmoil followed the general election of 1996. Several short-lived alliances shared power at the centre. The BJP formed a government briefly in 1996; it was followed by two comparatively long-lasting United Front coalitions, which depended on external support. In 1998, the BJP was able to form a successful coalition, the National Democratic Alliance (NDA). Led by Atal Bihari Vajpayee, the NDA became the first non-Congress, coalition government to complete a five-year term. Again in the 2004 Indian general elections, no party won an absolute majority, but the Congress emerged as the largest single party, forming another successful coalition: the United Progressive Alliance (UPA). It had the support of left-leaning parties and MPs who opposed the BJP. The UPA returned to power in the 2009 general election with increased numbers, and it no longer required external support from India's communist parties. That year, Manmohan Singh became the first prime minister since Jawaharlal Nehru in 1957 and 1962 to be re-elected to a consecutive five-year term. In the 2014 general election, the BJP became the first political party since 1984 to win a majority and govern without the support of other parties. The incumbent prime minister is Narendra Modi, a former chief minister of Gujarat. On 20 July 2017, Ram Nath Kovind was elected India's 14th president and took the oath of office on 25 July 2017.
India is a federation with a parliamentary system governed under the Constitution of India—the country's supreme legal document. It is a constitutional republic and representative democracy, in which "majority rule is tempered by minority rights protected by law". Federalism in India defines the power distribution between the union and the states. The Constitution of India, which came into effect on 26 January 1950, originally stated India to be a "sovereign, democratic republic;" this characterisation was amended in 1971 to "a sovereign, socialist, secular, democratic republic". India's form of government, traditionally described as "quasi-federal" with a strong centre and weak states, has grown increasingly federal since the late 1990s as a result of political, economic, and social changes. The Government of India comprises three branches: Executive: The President of India is the ceremonial head of state, who is elected indirectly for a five-year term by an electoral college comprising members of national and state legislatures. The Prime Minister of India is the head of government and exercises most executive power. Appointed by the president, the prime minister is by convention supported by the party or political alliance having a majority of seats in the lower house of parliament. The executive of the Indian government consists of the president, the vice president, and the Union Council of Ministers—with the cabinet being its executive committee—headed by the prime minister. Any minister holding a portfolio must be a member of one of the houses of parliament. In the Indian parliamentary system, the executive is subordinate to the legislature; the prime minister and their council are directly responsible to the lower house of the parliament. Civil servants act as permanent executives and all decisions of the executive are implemented by them. Legislature: The legislature of India is the bicameral parliament. Operating under a Westminster-style parliamentary system, it comprises an upper house called the Rajya Sabha (Council of States) and a lower house called the Lok Sabha (House of the People). The Rajya Sabha is a permanent body of 245 members who serve staggered six-year terms. Most are elected indirectly by the state and union territorial legislatures in numbers proportional to their state's share of the national population. All but two of the Lok Sabha's 545 members are elected directly by popular vote; they represent single-member constituencies for five-year terms. The remaining two members are nominated by the president from among the Anglo-Indian community, in case the president decides they are not adequately represented. Judiciary: India has a three-tier unitary independent judiciary comprising the supreme court, headed by the Chief Justice of India, 25 high courts, and a large number of trial courts. The supreme court has original jurisdiction over cases involving fundamental rights and over disputes between states and the centre and has appellate jurisdiction over the high courts. It has the power to both strike down union or state laws which contravene the constitution, and invalidate any government action it deems unconstitutional.
India is a federal union comprising 28 states and 8 union territories (listed below as 1–28 and A–H, respectively). All states, as well as the union territories of Jammu and Kashmir, Puducherry and the National Capital Territory of Delhi, have elected legislatures and governments following the Westminster system of governance. The remaining five union territories are directly ruled by the central government through appointed administrators. In 1956, under the States Reorganisation Act, states were reorganised on a linguistic basis. There are over a quarter of a million local government bodies at city, town, block, district and village levels.
In the 1950s, India strongly supported decolonisation in Africa and Asia and played a leading role in the Non-Aligned Movement. After initially cordial relations with neighbouring China, India went to war with China in 1962, and was widely thought to have been humiliated. India has had tense relations with neighbouring Pakistan; the two nations have gone to war four times: in 1947, 1965, 1971, and 1999. Three of these wars were fought over the disputed territory of Kashmir, while the fourth, the 1971 war, followed from India's support for the independence of Bangladesh. In the late 1980s, the Indian military twice intervened abroad at the invitation of the host country: a peace-keeping operation in Sri Lanka between 1987 and 1990; and an armed intervention to prevent a 1988 coup d'état attempt in the Maldives. After the 1965 war with Pakistan, India began to pursue close military and economic ties with the Soviet Union; by the late 1960s, the Soviet Union was its largest arms supplier.Aside from ongoing its special relationship with Russia, India has wide-ranging defence relations with Israel and France. In recent years, it has played key roles in the South Asian Association for Regional Cooperation and the World Trade Organization. The nation has provided 100,000 military and police personnel to serve in 35 UN peacekeeping operations across four continents. It participates in the East Asia Summit, the G8+5, and other multilateral forums. India has close economic ties with South America, Asia, and Africa; it pursues a "Look East" policy that seeks to strengthen partnerships with the ASEAN nations, Japan, and South Korea that revolve around many issues, but especially those involving economic investment and regional security. China's nuclear test of 1964, as well as its repeated threats to intervene in support of Pakistan in the 1965 war, convinced India to develop nuclear weapons. India conducted its first nuclear weapons test in 1974 and carried out additional underground testing in 1998. Despite criticism and military sanctions, India has signed neither the Comprehensive Nuclear-Test-Ban Treaty nor the Nuclear Non-Proliferation Treaty, considering both to be flawed and discriminatory. India maintains a "no first use" nuclear policy and is developing a nuclear triad capability as a part of its "Minimum Credible Deterrence" doctrine. It is developing a ballistic missile defence shield and, a fifth-generation fighter jet. Other indigenous military projects involve the design and implementation of Vikrant-class aircraft carriers and Arihant-class nuclear submarines.Since the end of the Cold War, India has increased its economic, strategic, and military co-operation with the United States and the European Union. In 2008, a civilian nuclear agreement was signed between India and the United States. Although India possessed nuclear weapons at the time and was not a party to the Nuclear Non-Proliferation Treaty, it received waivers from the International Atomic Energy Agency and the Nuclear Suppliers Group, ending earlier restrictions on India's nuclear technology and commerce. As a consequence, India became the sixth de facto nuclear weapons state. India subsequently signed co-operation agreements involving civilian nuclear energy with Russia, France, the United Kingdom, and Canada. The President of India is the supreme commander of the nation's armed forces; with 1.395 million active troops, they compose the world's second-largest military. It comprises the Indian Army, the Indian Navy, the Indian Air Force, and the Indian Coast Guard. The official Indian defence budget for 2011 was US$36.03 billion, or 1.83% of GDP. For the fiscal year spanning 2012–2013, US$40.44 billion was budgeted. According to a 2008 Stockholm International Peace Research Institute (SIPRI) report, India's annual military expenditure in terms of purchasing power stood at US$72.7 billion. In 2011, the annual defence budget increased by 11.6%, although this does not include funds that reach the military through other branches of government. As of 2012, India is the world's largest arms importer; between 2007 and 2011, it accounted for 10% of funds spent on international arms purchases. Much of the military expenditure was focused on defence against Pakistan and countering growing Chinese influence in the Indian Ocean. In May 2017, the Indian Space Research Organisation launched the South Asia Satellite, a gift from India to its neighbouring SAARC countries. In October 2018, India signed a US$5.43 billion (over ₹400 billion) agreement with Russia to procure four S-400 Triumf surface-to-air missile defence systems, Russia's most advanced long-range missile defence system.
According to the International Monetary Fund (IMF), the Indian economy in 2019 was nominally worth $2.9 trillion; it is the fifth-largest economy by market exchange rates, and is around $11 trillion, the third-largest by purchasing power parity, or PPP. With its average annual GDP growth rate of 5.8% over the past two decades, and reaching 6.1% during 2011–2012, India is one of the world's fastest-growing economies. However, the country ranks 139th in the world in nominal GDP per capita and 118th in GDP per capita at PPP. Until 1991, all Indian governments followed protectionist policies that were influenced by socialist economics. Widespread state intervention and regulation largely walled the economy off from the outside world. An acute balance of payments crisis in 1991 forced the nation to liberalise its economy; since then it has moved slowly towards a free-market system by emphasising both foreign trade and direct investment inflows. India has been a member of WTO since 1 January 1995.The 513.7-million-worker Indian labour force is the world's second-largest, as of 2016. The service sector makes up 55.6% of GDP, the industrial sector 26.3% and the agricultural sector 18.1%. India's foreign exchange remittances of US$70 billion in 2014, the largest in the world, were contributed to its economy by 25 million Indians working in foreign countries. Major agricultural products include: rice, wheat, oilseed, cotton, jute, tea, sugarcane, and potatoes. Major industries include: textiles, telecommunications, chemicals, pharmaceuticals, biotechnology, food processing, steel, transport equipment, cement, mining, petroleum, machinery, and software. In 2006, the share of external trade in India's GDP stood at 24%, up from 6% in 1985. In 2008, India's share of world trade was 1.68%; In 2011, India was the world's tenth-largest importer and the nineteenth-largest exporter. Major exports include: petroleum products, textile goods, jewellery, software, engineering goods, chemicals, and manufactured leather goods. Major imports include: crude oil, machinery, gems, fertiliser, and chemicals. Between 2001 and 2011, the contribution of petrochemical and engineering goods to total exports grew from 14% to 42%. India was the world's second largest textile exporter after China in the 2013 calendar year.Averaging an economic growth rate of 7.5% for several years prior to 2007, India has more than doubled its hourly wage rates during the first decade of the 21st century. Some 431 million Indians have left poverty since 1985; India's middle classes are projected to number around 580 million by 2030. Though ranking 51st in global competitiveness, as of 2010, India ranks 17th in financial market sophistication, 24th in the banking sector, 44th in business sophistication, and 39th in innovation, ahead of several advanced economies. With seven of the world's top 15 information technology outsourcing companies based in India, as of 2009, the country is viewed as the second-most favourable outsourcing destination after the United States. India's consumer market, the world's eleventh-largest, is expected to become fifth-largest by 2030.Driven by growth, India's nominal GDP per capita increased steadily from US$329 in 1991, when economic liberalisation began, to US$1,265 in 2010, to an estimated US$1,723 in 2016. It is expected to grow to US$2,358 by 2020. However, it has remained lower than those of other Asian developing countries like Indonesia, Malaysia, Philippines, Sri Lanka, and Thailand, and is expected to remain so in the near future. Its GDP per capita is higher than Bangladesh, Pakistan, Nepal, Afghanistan and others. According to a 2011 PricewaterhouseCoopers (PwC) report, India's GDP at purchasing power parity could overtake that of the United States by 2045. During the next four decades, Indian GDP is expected to grow at an annualised average of 8%, making it potentially the world's fastest-growing major economy until 2050. The report highlights key growth factors: a young and rapidly growing working-age population; growth in the manufacturing sector because of rising education and engineering skill levels; and sustained growth of the consumer market driven by a rapidly growing middle-class. The World Bank cautions that, for India to achieve its economic potential, it must continue to focus on public sector reform, transport infrastructure, agricultural and rural development, removal of labour regulations, education, energy security, and public health and nutrition.According to the Worldwide Cost of Living Report 2017 released by the Economist Intelligence Unit (EIU) which was created by comparing more than 400 individual prices across 160 products and services, four of the cheapest cities were in India: Bangalore (3rd), Mumbai (5th), Chennai (5th) and New Delhi (8th).
India's telecommunication industry, the world's fastest-growing, added 227 million subscribers during the period 2010–2011, and after the third quarter of 2017, India surpassed the US to become the second largest smartphone market in the world after China.The Indian automotive industry, the world's second-fastest growing, increased domestic sales by 26% during 2009–2010, and exports by 36% during 2008–2009. India's capacity to generate electrical power is 300 gigawatts, of which 42 gigawatts is renewable. At the end of 2011, the Indian IT industry employed 2.8 million professionals, generated revenues close to US$100 billion equalling 7.5% of Indian GDP, and contributed 26% of India's merchandise exports.The pharmaceutical industry in India is among the significant emerging markets for the global pharmaceutical industry. The Indian pharmaceutical market is expected to reach $48.5 billion by 2020. India's R & D spending constitutes 60% of the biopharmaceutical industry. India is among the top 12 biotech destinations in the world. The Indian biotech industry grew by 15.1% in 2012–2013, increasing its revenues from ₹204.4 billion (Indian rupees) to ₹235.24 billion (US$3.94 billion at June 2013 exchange rates).
Despite economic growth during recent decades, India continues to face socio-economic challenges. In 2006, India contained the largest number of people living below the World Bank's international poverty line of US$1.25 per day. The proportion decreased from 60% in 1981 to 42% in 2005. Under the World Bank's later revised poverty line, it was 21% in 2011. 30.7% of India's children under the age of five are underweight. According to a Food and Agriculture Organization report in 2015, 15% of the population is undernourished. The Mid-Day Meal Scheme attempts to lower these rates.According to a 2016 Walk Free Foundation report there were an estimated 18.3 million people in India, or 1.4% of the population, living in the forms of modern slavery, such as bonded labour, child labour, human trafficking, and forced begging, among others. According to the 2011 census, there were 10.1 million child labourers in the country, a decline of 2.6 million from 12.6 million in 2001.Since 1991, economic inequality between India's states has consistently grown: the per-capita net state domestic product of the richest states in 2007 was 3.2 times that of the poorest. Corruption in India is perceived to have decreased. According to the Corruption Perceptions Index, India ranked 78th out of 180 countries in 2018 with a score of 41 out of 100, an improvement from 85th in 2014.
Indian cultural history spans more than 4,500 years. During the Vedic period (c. 1700 – c. 500 BCE), the foundations of Hindu philosophy, mythology, theology and literature were laid, and many beliefs and practices which still exist today, such as dhárma, kárma, yóga, and mokṣa, were established. India is notable for its religious diversity, with Hinduism, Buddhism, Sikhism, Islam, Christianity, and Jainism among the nation's major religions. The predominant religion, Hinduism, has been shaped by various historical schools of thought, including those of the Upanishads, the Yoga Sutras, the Bhakti movement, and by Buddhist philosophy.
Much of Indian architecture, including the Taj Mahal, other works of Mughal architecture, and South Indian architecture, blends ancient local traditions with imported styles. Vernacular architecture is also regional in its flavours. Vastu shastra, literally "science of construction" or "architecture" and ascribed to Mamuni Mayan, explores how the laws of nature affect human dwellings; it employs precise geometry and directional alignments to reflect perceived cosmic constructs. As applied in Hindu temple architecture, it is influenced by the Shilpa Shastras, a series of foundational texts whose basic mythological form is the Vastu-Purusha mandala, a square that embodied the "absolute". The Taj Mahal, built in Agra between 1631 and 1648 by orders of Emperor Shah Jahan in memory of his wife, has been described in the UNESCO World Heritage List as "the jewel of Muslim art in India and one of the universally admired masterpieces of the world's heritage". Indo-Saracenic Revival architecture, developed by the British in the late 19th century, drew on Indo-Islamic architecture.The earliest literature in India, composed between 1500 BCE and 1200 CE, was in the Sanskrit language. Major works of Sanskrit literature include the Rigveda (c. 1500 BCE – 1200 BCE), the epics: Mahābhārata (c. 400 BCE – 400 CE) and the Ramayana (c. 300 BCE and later); Abhijñānaśākuntalam (The Recognition of Śakuntalā, and other dramas of Kālidāsa (c. 5th century CE) and Mahākāvya poetry. In Tamil literature, the Sangam literature (c. 600 BCE – 300 BCE) consisting of 2,381 poems, composed by 473 poets, is the earliest work. From the 14th to the 18th centuries, India's literary traditions went through a period of drastic change because of the emergence of devotional poets like Kabīr, Tulsīdās, and Guru Nānak. This period was characterised by a varied and wide spectrum of thought and expression; as a consequence, medieval Indian literary works differed significantly from classical traditions. In the 19th century, Indian writers took a new interest in social questions and psychological descriptions. In the 20th century, Indian literature was influenced by the works of the Bengali poet and novelist Rabindranath Tagore, who was a recipient of the Nobel Prize in Literature.
Indian music ranges over various traditions and regional styles. Classical music encompasses two genres and their various folk offshoots: the northern Hindustani and southern Carnatic schools. Regionalised popular forms include filmi and folk music; the syncretic tradition of the bauls is a well-known form of the latter. Indian dance also features diverse folk and classical forms. Among the better-known folk dances are: the bhangra of Punjab, the bihu of Assam, the Jhumair and chhau of Jharkhand, Odisha and West Bengal, garba and dandiya of Gujarat, ghoomar of Rajasthan, and the lavani of Maharashtra. Eight dance forms, many with narrative forms and mythological elements, have been accorded classical dance status by India's National Academy of Music, Dance, and Drama. These are: bharatanatyam of the state of Tamil Nadu, kathak of Uttar Pradesh, kathakali and mohiniyattam of Kerala, kuchipudi of Andhra Pradesh, manipuri of Manipur, odissi of Odisha, and the sattriya of Assam. Theatre in India melds music, dance, and improvised or written dialogue. Often based on Hindu mythology, but also borrowing from medieval romances or social and political events, Indian theatre includes: the bhavai of Gujarat, the jatra of West Bengal, the nautanki and ramlila of North India, tamasha of Maharashtra, burrakatha of Andhra Pradesh, terukkuttu of Tamil Nadu, and the yakshagana of Karnataka. India has a theatre training institute the National School of Drama (NSD) that is situated at New Delhi It is an autonomous organisation under the Ministry of Culture, Government of India. The Indian film industry produces the world's most-watched cinema. Established regional cinematic traditions exist in the Assamese, Bengali, Bhojpuri, Hindi, Kannada, Malayalam, Punjabi, Gujarati, Marathi, Odia, Tamil, and Telugu languages. The Hindi language film industry (Bollywood) is the largest sector representing 43% of box office revenue, followed by the South Indian Telugu and Tamil film industries which represent 36% combined.Television broadcasting began in India in 1959 as a state-run medium of communication and expanded slowly for more than two decades. The state monopoly on television broadcast ended in the 1990s. Since then, satellite channels have increasingly shaped the popular culture of Indian society. Today, television is the most penetrative media in India; industry estimates indicate that as of 2012 there are over 554 million TV consumers, 462 million with satellite or cable connections compared to other forms of mass media such as the press (350 million), radio (156 million) or internet (37 million).
Traditional Indian society is sometimes defined by social hierarchy. The Indian caste system embodies much of the social stratification and many of the social restrictions found in the Indian subcontinent. Social classes are defined by thousands of endogamous hereditary groups, often termed as jātis, or "castes". India declared untouchability to be illegal in 1947 and has since enacted other anti-discriminatory laws and social welfare initiatives. At the workplace in urban India, and in international or leading Indian companies, caste-related identification has pretty much lost its importance.Family values are important in the Indian tradition, and multi-generational patriarchal joint families have been the norm in India, though nuclear families are becoming common in urban areas. An overwhelming majority of Indians, with their consent, have their marriages arranged by their parents or other family elders. Marriage is thought to be for life, and the divorce rate is extremely low, with less than one in a thousand marriages ending in divorce. Child marriages are common, especially in rural areas; many women wed before reaching 18, which is their legal marriageable age. Female infanticide in India, and lately female foeticide, have created skewed gender ratios; the number of missing women in the country quadrupled from 15 million to 63 million in the 50-year period ending in 2014, faster than the population growth during the same period, and constituting 20 percent of India's female electorate. Accord to an Indian government study, an additional 21 million girls are unwanted and do not receive adequate care. Despite a government ban on sex-selective foeticide, the practice remains commonplace in India, the result of a preference for boys in a patriarchal society. The payment of dowry, although illegal, remains widespread across class lines. Deaths resulting from dowry, mostly from bride burning, are on the rise, despite stringent anti-dowry laws.Many Indian festivals are religious in origin. The best known include: Diwali, Ganesh Chaturthi, Thai Pongal, Holi, Durga Puja, Eid ul-Fitr, Bakr-Id, Christmas, and Vaisakhi.
The most widely worn traditional dress in India, for both women and men, from ancient times until the advent of modern times, was draped. For women it eventually took the form of a sari, a single long piece of cloth, famously six yards long, and of width spanning the lower body. The sari is tied around the waist and knotted at one end, wrapped around the lower body, and then over the shoulder. In its more modern form, it has been used to cover the head, and sometimes the face, as a veil. It has been combined with an underskirt, or Indian petticoat, and tucked in the waist band for more secure fastening, It is also commonly worn with an Indian blouse, or choli, which serves as the primary upper-body garment, the sari's end, passing over the shoulder, now serving to obscure the upper body's contours, and to cover the midriff.For men, a similar but shorter length of cloth, the dhoti, has served as a lower-body garment. It too is tied around the waist and wrapped. In south India, it is usually wrapped around the lower body, the upper end tucked in the waistband, the lower left free. In addition, in northern India, it is also wrapped once around each leg before being brought up through the legs to be tucked in at the back. Other forms of traditional apparel that involve no stitching or tailoring are the chaddar (a shawl worn by both sexes to cover the upper body during colder weather, or a large veil worn by women for framing the head, or covering it) and the pagri (a turban or a scarf worn around the head as a part of a tradition, or to keep off the sun or the cold). Until the beginning of the first millennium CE, the ordinary dress of people in India was entirely unstitched. The arrival of the Kushans from Central Asia, circa 48 CE, popularised cut and sewn garments in the style of Central Asian favoured by the elite in northern India. However, it was not until Muslim rule was established, first with the Delhi sultanate and then the Mughal Empire, that the range of stitched clothes in India grew and their use became significantly more widespread. Among the various garments gradually establishing themselves in northern India during medieval and early-modern times and now commonly worn are: the shalwars and pyjamas both forms of trousers, as well as the tunics kurta and kameez. In southern India, however, the traditional draped garments were to see much longer continuous use.Shalwars are atypically wide at the waist but narrow to a cuffed bottom. They are held up by a drawstring or elastic belt, which causes them to become pleated around the waist. The pants can be wide and baggy, or they can be cut quite narrow, on the bias, in which case they are called churidars. The kameez is a long shirt or tunic. The side seams are left open below the waist-line,), which gives the wearer greater freedom of movement. The kameez is usually cut straight and flat; older kameez use traditional cuts; modern kameez are more likely to have European-inspired set-in sleeves. The kameez may have a European-style collar, a Mandarin-collar, or it may be collarless; in the latter case, its design as a women's garment is similar to a kurta. At first worn by Muslim women, the use of shalwar kameez gradually spread, making them a regional style, especially in the Punjab region.A kurta, which traces its roots to Central Asian nomadic tunics, has evolved stylistically in India as a garment for everyday wear as well as for formal occasions. It is traditionally made of cotton or silk; it is worn plain or with embroidered decoration, such as chikan; and it can be loose or tight in the torso, typically falling either just above or somewhere below the wearer's knees. The sleeves of a traditional kurta fall to the wrist without narrowing, the ends hemmed but not cuffed; the kurta can be worn by both men and women; it is traditionally collarless, though standing collars are increasingly popular; and it can be worn over ordinary pyjamas, loose shalwars, churidars, or less traditionally over jeans.In the last 50 years, fashions have changed a great deal in India. Increasingly, in urban settings in northern India, the sari is no longer the apparel of everyday wear, transformed instead into one for formal occasions. The traditional shalwar kameez is rarely worn by younger women, who favour churidars or jeans. The kurtas worn by young men usually fall to the shins and are seldom plain. In white-collar office settings, ubiquitous air conditioning allows men to wear sports jackets year-round. For weddings and formal occasions, men in the middle- and upper classes often wear bandgala, or short Nehru jackets, with pants, with the groom and his groomsmen sporting sherwanis and churidars. The dhoti, the once universal garment of Hindu India, the wearing of which in the homespun and handwoven form of khadi allowed Gandhi to bring Indian nationalism to the millions, is seldom seen in the cities, reduced now, with brocaded border, to the liturgical vestments of Hindu priests.
Indian cuisine consists of a wide variety of regional and traditional cuisines. Given the range of diversity in soil type, climate, culture, ethnic groups, and occupations, these cuisines vary substantially from each other, using locally available spices, herbs, vegetables, and fruit. Indian foodways have been influenced by religion, in particular Hindu cultural choices and traditions. They have been also shaped by Islamic rule, particularly that of the Mughals, by the arrival of the Portuguese on India's southwestern shores, and by British rule. These three influences are reflected, respectively, in the dishes of pilaf and biryani; the vindaloo; and the tiffin and the Railway mutton curry. Earlier, the Columbian exchange had brought the potato, the tomato, maize, peanuts, cashew nuts, pineapples, guavas, and most notably, chilli peppers, to India. Each became staples of use. In turn, the spice trade between India and Europe was a catalyst for Europe's Age of Discovery.The cereals grown in India, their choice, times, and regions of planting, correspond strongly to the timing of India's monsoons, and the variation across regions in their associated rainfall. In general, the broad division of cereal zones in India, as determined by their dependence on rain, was firmly in place before the arrival of artificial irrigation. Rice, which requires a lot of water, has been grown traditionally in regions of high rainfall in the northeast and the western coast, wheat in regions of moderate rainfall, like India's northern plains, and millet in regions of low rainfall, such as on the Deccan Plateau and in Rajasthan.The foundation of a typical Indian meal is a cereal cooked in plain fashion, and complemented with flavourful savoury dishes. The latter includes lentils, pulses and vegetables spiced commonly with ginger and garlic, but also more discerningly with a combination of spices that may include coriander, cumin, turmeric, cinnamon, cardamon and others as informed by culinary conventions. In an actual meal, this mental representation takes the form of a platter, or thali, with a central place for the cooked cereal, peripheral ones, often in small bowls, for the flavourful accompaniments, and the simultaneous, rather than piecemeal, ingestion of the two in each act of eating, whether by actual mixing—for example of rice and lentils—or in the folding of one—such as bread—around the other, such as cooked vegetables. A notable feature of Indian food is the existence of a number of distinctive vegetarian cuisines, each a feature of the geographical and cultural histories of its adherents. The appearance of ahimsa, or the avoidance of violence toward all forms of life in many religious orders early in Indian history, especially Upanishadic Hinduism, Buddhism and Jainism, is thought to have been a notable factor in the prevalence of vegetarianism among a segment of India's Hindu population, especially in southern India, Gujarat, and the Hindi-speaking belt of north-central India, as well as among Jains. Among these groups, strong discomfort is felt at thoughts of eating meat, and contributes to the low proportional consumption of meat to overall diet in India. Unlike China, which has increased its per capita meat consumption substantially in its years of increased economic growth, in India the strong dietary traditions have contributed to dairy, rather than meat, becoming the preferred form of animal protein consumption accompanying higher economic growth.In the last millennium, the most significant import of cooking techniques into India occurred during the Mughal Empire. The cultivation of rice had spread much earlier from India to Central and West Asia; however, it was during Mughal rule that dishes, such as the pilaf, developed in the interim during the Abbasid caliphate, and cooking techniques such as the marinating of meat in yogurt, spread into northern India from regions to its northwest. To the simple yogurt marinade of Persia, onions, garlic, almonds, and spices began to be added in India. Rice grown to the southwest of the Mughal capital, Agra, which had become famous in the Islamic world for its fine grain, was partially cooked and layered alternately with the sauteed meat, the pot sealed tightly, and slow cooked according to another Persian cooking technique, to produce what has today become the Indian biryani, a feature of festive dining in many parts of India. In food served in restaurants in urban north India, and internationally, the diversity of Indian food has been partially concealed by the dominance of Punjabi cuisine. This was caused in large part by an entrepreneurial response among people from the Punjab region who had been displaced by the 1947 partition of India, and had arrived in India as refugees. The identification of Indian cuisine with the tandoori chicken—cooked in the tandoor oven, which had traditionally been used for baking bread in the rural Punjab and the Delhi region, especially among Muslims, but which is originally from Central Asia—dates to this period.
In India, several traditional indigenous sports remain fairly popular, such as kabaddi, kho kho, pehlwani and gilli-danda. Some of the earliest forms of Asian martial arts, such as kalarippayattu, musti yuddha, silambam, and marma adi, originated in India. Chess, commonly held to have originated in India as chaturaṅga, is regaining widespread popularity with the rise in the number of Indian grandmasters. Pachisi, from which parcheesi derives, was played on a giant marble court by Akbar.The improved results garnered by the Indian Davis Cup team and other Indian tennis players in the early 2010s have made tennis increasingly popular in the country. India has a comparatively strong presence in shooting sports, and has won several medals at the Olympics, the World Shooting Championships, and the Commonwealth Games. Other sports in which Indians have succeeded internationally include badminton (Saina Nehwal and P V Sindhu are two of the top-ranked female badminton players in the world), boxing, and wrestling. Football is popular in West Bengal, Goa, Tamil Nadu, Kerala, and the north-eastern states. Cricket is the most popular sport in India. Major domestic competitions include the Indian Premier League, which is the most-watched cricket league in the world and ranks sixth among all sports leagues.India has hosted or co-hosted several international sporting events: the 1951 and 1982 Asian Games; the 1987, 1996, and 2011 Cricket World Cup tournaments; the 2003 Afro-Asian Games; the 2006 ICC Champions Trophy; the 2010 Hockey World Cup; the 2010 Commonwealth Games; and the 2017 FIFA U-17 World Cup. Major international sporting events held annually in India include the Chennai Open, the Mumbai Marathon, the Delhi Half Marathon, and the Indian Masters. The first Formula 1 Indian Grand Prix featured in late 2011 but has been discontinued from the F1 season calendar since 2014. India has traditionally been the dominant country at the South Asian Games. An example of this dominance is the basketball competition where the Indian team won three out of four tournaments to date.
Outline of India
Overview Etymology History Geography Biodiversity Politics Foreign relations and military Economy Demographics Culture
Government Official website of Government of India Government of India Web DirectoryGeneral information "India". The World Factbook. Central Intelligence Agency. India at Curlie India from UCB Libraries GovPubs India from the BBC News Indian State district block village website Wikimedia Atlas of India Geographic data related to India at OpenStreetMap Key Development Forecasts for India from International Futures
Marginal seas, gulfs, bays and straits of the Indian Ocean include:Along the east coast of Africa, the Mozambique Channel separates Madagascar from mainland Africa, while the Sea of Zanj is located north of Madagascar. On the northern coast of the Arabian Sea, Gulf of Aden is connected to the Red Sea by the strait of Bab-el-Mandeb. In the Gulf of Aden, the Gulf of Tadjoura is located in Djibouti and the Guardafui Channel separates Socotra island from the Horn of Africa. The northern end of the Red Sea terminates in the Gulf of Aqaba and Gulf of Suez. The Indian Ocean is artificially connected to the Mediterranean Sea through the Suez Canal, which is accessible via the Red Sea. The Arabian Sea is connected to the Persian Gulf by the Gulf of Oman and the Strait of Hormuz. In the Persian Gulf, the Gulf of Bahrain separates Qatar from the Arabic Peninsula. Along the west coast of India, the Gulf of Kutch and Gulf of Khambat are located in Gujarat in the northern end while the Laccadive Sea separates the Maldives from the southern tip of India. The Bay of Bengal is off the east coast of India. The Gulf of Mannar and the Palk Strait separates Sri Lanka from India, while the Adam's Bridge separates the two. The Andaman Sea is located between the Bay of Bengal and the Andaman Islands. In Indonesia, the so-called Indonesian Seaway is composed of the Malacca, Sunda and Torres Straits. The Gulf of Carpentaria of located on the Australian north coast while the Great Australian Bight constitutes a large part of its southern coast.
Several features make the Indian Ocean unique. It constitutes the core of the large-scale Tropical Warm Pool which, when interacting with the atmosphere, affects the climate both regionally and globally. Asia blocks heat export and prevents the ventilation of the Indian Ocean thermocline. That continent also drives the Indian Ocean monsoon, the strongest on Earth, which causes large-scale seasonal variations in ocean currents, including the reversal of the Somali Current and Indian Monsoon Current. Because of the Indian Ocean Walker circulation there are no continuous equatorial easterlies. Upwelling occurs near the Horn of Africa and the Arabian Peninsula in the Northern Hemisphere and north of the trade winds in the Southern Hemisphere. The Indonesian Throughflow is a unique Equatorial connection to the Pacific.The climate north of the equator is affected by a monsoon climate. Strong north-east winds blow from October until April; from May until October south and west winds prevail. In the Arabian Sea, the violent Monsoon brings rain to the Indian subcontinent. In the southern hemisphere, the winds are generally milder, but summer storms near Mauritius can be severe. When the monsoon winds change, cyclones sometimes strike the shores of the Arabian Sea and the Bay of Bengal. Some 80% of the total annual rainfall in India occurs during summer and the region is so dependent on this rainfall that many civilisations perished when the Monsoon failed in the past. The huge variability in the Indian Summer Monsoon has also occurred pre-historically, with a strong, wet phase 33,500–32,500 BP; a weak, dry phase 26,000–23,500 BC; and a very weak phase 17,000–15,000 BP, corresponding to a series of dramatic global events: Bølling-Allerød, Heinrich, and Younger Dryas. The Indian Ocean is the warmest ocean in the world. Long-term ocean temperature records show a rapid, continuous warming in the Indian Ocean, at about 1.2 °C (34.2 °F) (compared to 0.7 °C (33.3 °F) for the warm pool region) during 1901–2012. Research indicates that human induced greenhouse warming, and changes in the frequency and magnitude of El Niño (or the Indian Ocean Dipole), events are a trigger to this strong warming in the Indian Ocean.South of the Equator (20-5°S), the Indian Ocean is gaining heat from June to October, during the austral winter, while it is losing heat from November to March, during the austral summer.In 1999, the Indian Ocean Experiment showed that fossil fuel and biomass burning in South and Southeast Asia caused air pollution (also known as the Asian brown cloud) that reach as far as the Intertropical Convergence Zone at 60°S. This pollution has implications on both a local and global scale.
40% of the sediment of the Indian Ocean is found in the Indus and Ganges fans. The oceanic basins adjacent to the continental slopes mostly contain terrigenous sediments. The ocean south of the polar front (roughly 50° south latitude) is high in biologic productivity and dominated by non-stratified sediment composed mostly of siliceous oozes. Near the three major mid-ocean ridges the ocean floor is relatively young and therefore bare of sediment, except for the Southwest Indian Ridge due to its ultra-slow spreading rate.The ocean's currents are mainly controlled by the monsoon. Two large gyres, one in the northern hemisphere flowing clockwise and one south of the equator moving anticlockwise (including the Agulhas Current and Agulhas Return Current), constitute the dominant flow pattern. During the winter monsoon (November–February), however, circulation is reversed north of 30°S and winds are weakened during winter and the transitional periods between the monsoons.The Indian Ocean contains the largest submarine fans of the world, the Bengal Fan and Indus Fan, and the largest areas of slope terraces and rift valleys. The inflow of deep water into the Indian Ocean is 11 Sv, most of which comes from the Circumpolar Deep Water (CDW). The CDW enters the Indian Ocean through the Crozet and Madagascar basins and crosses the Southwest Indian Ridge at 30°S. In the Mascarene Basin the CDW becomes a deep western boundary current before it is met by a re-circulated branch of itself, the North Indian Deep Water. This mixed water partly flows north into the Somali Basin whilst most of it flows clockwise in the Mascarene Basin where an oscillating flow is produced by Rossby waves.Water circulation in the Indian Ocean is dominated by the Subtropical Anticyclonic Gyre, the eastern extension of which is blocked by the Southeast Indian Ridge and the 90°E Ridge. Madagascar and the Southwest Indian Ridge separates three cells south of Madagascar and off South Africa. North Atlantic Deep Water reaches into the Indian Ocean south of Africa at a depth of 2,000–3,000 m (6,600–9,800 ft) and flows north along the eastern continental slope of Africa. Deeper than NADW, Antarctic Bottom Water flows from Enderby Basin to Agulhas Basin across deep channels (<4,000 m (13,000 ft)) in the Southwest Indian Ridge, from where it continues into the Mozambique Channel and Prince Edward Fracture Zone.North of 20° south latitude the minimum surface temperature is 22 °C (72 °F), exceeding 28 °C (82 °F) to the east. Southward of 40° south latitude, temperatures drop quickly.The Bay of Bengal contributes more than half (2,950 km3 (710 cu mi)) of the runoff water to the Indian Ocean. Mainly in summer, this runoff flows into the Arabian Sea but also south across the Equator where it mixes with fresher seawater from the Indonesian Throughflow. This mixed freshwater joins the South Equatorial Current in the southern tropical Indian Ocean.Sea surface salinity is highest (more than 36 PSU) in the Arabian Sea because evaporation exceeds precipitation there. In the Southeast Arabian Sea salinity drops to less than 34 PSU. It is the lowest (c. 33 PSU) in the Bay of Bengal because of river runoff and precipitation. The Indonesian Throughflow and precipitation results in lower salinity (34 PSU) along the Sumatran west coast. Monsoonal variation results in eastward transportation of saltier water from the Arabian Sea to the Bay of Bengal from June to September and in westerly transport by the East India Coastal Current to the Arabian Sea from January to April.An Indian Ocean garbage patch was discovered in 2010 covering at least 5 million square kilometres (1.9 million square miles). Riding the southern Indian Ocean Gyre, this vortex of plastic garbage constantly circulates the ocean from Australia to Africa, down the Mozambique Channel, and back to Australia in a period of six years, except for debris that gets indefinitely stuck in the centre of the gyre. The garbage patch in the Indian Ocean will, according to a 2012 study, decrease in size after several decades to vanish completely over centuries. Over several millennia, however, the global system of garbage patches will accumulate in the North Pacific.There are two amphidromes of opposite rotation in the Indian Ocean, probably caused by Rossby wave propagation.Icebergs drift as far north as 55° south latitude, similar to the Pacific but less than in the Atlantic where icebergs reach up to 45°S. The volume of iceberg loss in the Indian Ocean between 2004 and 2012 was 24 Gt.Since the 1960s, anthropogenic warming of the global ocean combined with contributions of freshwater from retreating land ice causes a global rise in sea level. Sea level increases in the Indian Ocean too, except in the south tropical Indian Ocean where it decreases, a pattern most likely caused by rising levels of greenhouse gases.
Among the tropical oceans, the western Indian Ocean hosts one of the largest concentration of phytoplankton blooms in summer, due to the strong monsoon winds. The monsoonal wind forcing leads to a strong coastal and open ocean upwelling, which introduces nutrients into the upper zones where sufficient light is available for photosynthesis and phytoplankton production. These phytoplankton blooms support the marine ecosystem, as the base of the marine food web, and eventually the larger fish species. The Indian Ocean accounts for the second largest share of the most economically valuable tuna catch. It's fish are of great and growing importance to the bordering countries for domestic consumption and export. Fishing fleets from Russia, Japan, South Korea, and Taiwan also exploit the Indian Ocean, mainly for shrimp and tuna.Research indicates that increasing ocean temperatures are taking a toll on the marine ecosystem. A study on the phytoplankton changes in the Indian Ocean indicates a decline of up to 20% in the marine plankton in the Indian Ocean, during the past six decades. The tuna catch rates have also declined 50–90% during the past half century, mostly due to increased industrial fisheries, with the ocean warming adding further stress to the fish species.Endangered and vulnerable marine mammals and turtles: 80% of the Indian Ocean is open ocean and includes nine large marine ecosystems: the Agulhas Current, Somali Coastal Current, Red Sea, Arabian Sea, Bay of Bengal, Gulf of Thailand, West Central Australian Shelf, Northwest Australian Shelf, and Southwest Australian Shelf. Coral reefs cover c. 200,000 km2 (77,000 sq mi). The coasts of the Indian Ocean includes beaches and intertidal zones covering 3,000 km2 (1,200 sq mi) and 246 larger estuaries. Upwelling areas are small but important. The hypersaline salterns in India covers between 5,000–10,000 km2 (1,900–3,900 sq mi) and species adapted for this environment, such as Artemia salina and Dunaliella salina, are important to bird life. Coral reefs, sea grass beds, and mangrove forests are the most productive ecosystems of the Indian Ocean — coastal areas produce 20 tones per square kilometre of fish. These areas, however, are also being urbanised with populations often exceeding several thousand people per square kilometre and fishing techniques become more effective and often destructive beyond sustainable levels while increase in sea surface temperature spreads coral bleaching.Mangroves covers 80,984 km2 (31,268 sq mi) in the Indian Ocean region, or almost half of world's mangrove habitat, of which 42,500 km2 (16,400 sq mi) is located in Indonesia, or 50% of mangroves in the Indian Ocean. Mangroves originated in the Indian Ocean region and have adapted to a wide range of its habitats but it is also where it suffers its biggest loss of habitat.In 2016 six new animal species were identified at hydrothermal vents in the Southwest Indian Ridge: a "Hoff" crab, a "giant peltospirid" snail, a whelk-like snail, a limpet, a scaleworm and a polychaete worm.The West Indian Ocean coelacanth was discovered in the Indian Ocean off South Africa in the 1930s and in the late 1990s another species, the Indonesian coelacanth, was discovered off Sulawesi Island, Indonesia. Most extant coelacanths have been found in the Comoros. Although both species represent an order of lobe-finned fishes known from the Early Devonian (410 mya) and though extinct 66 mya, they are morphologically distinct from their Devonian ancestors. Over millions of years, coelacanths evolved to inhabit different environments — lungs adapted for shallow, brackish waters evolved into gills adapted for deep marine waters.
As the youngest of the major oceans, the Indian Ocean has active spreading ridges that are part of the worldwide system of mid-ocean ridges. In the Indian Ocean these spreading ridges meet at the Rodrigues Triple Point with the Central Indian Ridge, including the Carlsberg Ridge, separating the African Plate from the Indian Plate; the Southwest Indian Ridge separating the African Plate from the Antarctic Plate; and the Southeast Indian Ridge separating the Australian Plate from the Antarctic Plate. The Central Indian Ridge is intercepted by the Owen Fracture Zone. Since the late 1990s, however, it has become clear that this traditional definition of the Indo-Australian Plate cannot be correct; it consists of three plates — the Indian Plate, the Capricorn Plate, and Australian Plate — separated by diffuse boundary zones. Since 20 Ma the African Plate is being divided by the East African Rift System into the Nubian and Somalia plates.There are only two trenches in the Indian Ocean: the 6,000 km (3,700 mi)-long Java Trench between Java and the Sunda Trench and the 900 km (560 mi)-long Makran Trench south of Iran and Pakistan.A series of ridges and seamount chains produced by hotspots pass over the Indian Ocean. The Réunion hotspot (active 70–40 million years ago) connects Réunion and the Mascarene Plateau to the Chagos-Laccadive Ridge and the Deccan Traps in north-western India; the Kerguelen hotspot (100–35 million years ago) connects the Kerguelen Islands and Kerguelen Plateau to the Ninety East Ridge and the Rajmahal Traps in north-eastern India; the Marion hotspot (100–70 million years ago) possibly connects Prince Edward Islands to the Eighty Five East Ridge. These hotspot tracks have been broken by the still active spreading ridges mentioned above.There are fewer seamounts in the Indian Ocean than in the Atlantic and Pacific. These are typically deeper than 3,000 m (9,800 ft) and located north of 55°S and west of 80°E. Most originated at spreading ridges but some are now located in basins far away from these ridges. The ridges of the Indian Ocean form ranges of seamounts, sometimes very long, including the Carlsberg Ridge, Madagascar Ridge, Central Indian Ridge, Southwest Indian Ridge, Chagos-Laccadive Ridge, 85°E Ridge, 90°E Ridge, Southeast Indian Ridge, Broken Ridge, and East Indiaman Ridge. The Agulhas Plateau and Mascarene Plateau are the two major shallow areas.The opening of the Indian Ocean began c. 156 Ma when Africa separated from East Gondwana. The Indian Subcontinent began to separate from Australia-Antarctica 135–125 Ma and as the Tethys Ocean north of India began to close 118–84 Ma the Indian Ocean opened behind it.
The Indian Ocean, together with the Mediterranean, has connected people since ancient times, whereas the Atlantic and Pacific have had the roles of barriers or mare incognitum. The written history of the Indian Ocean, however, has been Eurocentric and largely dependent on the availability of written sources from the colonial era. This history is often divided into an ancient period followed by an Islamic period; the subsequent early modern and colonial/modern periods are often subdivided into Portuguese, Dutch, and British periods.A concept of an "Indian Ocean World" (IOW), similar to that of the "Atlantic World", exists but emerged much more recently and is not well established. The IOW is, nevertheless, sometimes referred to as the "first global economy" and was based on the monsoon which linked Asia, China, India, and Mesopotamia. It developed independently from the European global trade in the Mediterranean and Atlantic and remained largely independent from them until European 19th century colonial dominance.The diverse history of the Indian Ocean is a unique mix of cultures, ethnical groups, natural resources, and shipping routes. It grew in importance beginning in the 1960s and 1970s and after the Cold War it has undergone periods of political instability, most recently with the emergence of India and China as regional powers.
Pleistocene fossils of Homo erectus and other pre-H. sapiens homonin fossils, similar to H. heidelbergensis in Europe, have been found in India. According to the Toba catastrophe theory, a supereruption c. 74000 years ago at Lake Toba, Sumatra, covered India with volcanic ashes and wiped out one or more lineages of such archaic humans in India and Southeast Asia.The Out of Africa theory states that Homo sapiens spread from Africa into mainland Eurasia. The more recent Southern Dispersal or Coastal hypothesis instead advocates that modern humans spread along the coasts of the Arabic Peninsula and southern Asia. This hypothesis is supported by mtDNA research which reveals a rapid dispersal event during the Late Pleistocene (11,000 years ago). This coastal dispersal, however, began in East Africa 75,000 years ago and occurred intermittently from estuary to estuary along the northern perimetre of the Indian Ocean at rate of 0.7–4.0 km (0.43–2.49 mi) per year. It eventually resulted in modern humans migrating from Sunda over Wallacea to Sahul (Southeast Asia to Australia). Since then, waves of migration have resettled people and, clearly, the Indian Ocean littoral had been inhabited long before the first civilisations emerged. 5000–6000 years ago six distinct cultural centres had evolved around the Indian Ocean: East Africa, the Middle East, the Indian Subcontinent, South East Asia, the Malay World, and Australia; each interlinked to its neighbours.Food globalisation began on the Indian Ocean littoral c. 4.000 years ago. Five African crops — sorghum, pearl millet, finger millet, cowpea, and hyacinth bean — somehow found their way to Gujarat in India during the Late Harappan (2000–1700 BCE). Gujarati merchants evolved into the first explorers of the Indian Ocean as they traded African goods such as ivory, tortoise shells, and slaves. Broomcorn millet found its way from Central Asia to Africa, together with chicken and zebu cattle, although the exact timing is disputed. Around 2000 BCE black pepper and sesame, both native to Asia, appears in Egypt, albeit in small quantities. Around the same time the black rat and the house mouse emigrates from Asia to Egypt. Banana reached Africa around 3000 years ago.At least eleven prehistoric tsunamis have struck the Indian Ocean coast of Indonesia between 7400 and 2900 years ago. Analysing sand beds in caves in the Aceh region, scientists concluded that the intervals between these tsunamis have varied from series of minor tsunamis over a century to dormant periods of more than 2000 years preceding megathrusts in the Sunda Trench. Although the risk for future tsunamis is high, a major megathrust such as the one in 2004 is likely to be followed by a long dormant period.A group of scientists have argued that two large-scale impact events have occurred in the Indian Ocean: the Burckle Crater in the southern Indian Ocean in 2800 BCE and the Kanmare and Tabban craters in the Gulf of Carpentaria in northern Australia in 536 CE. Evidences for these impacts, the team argue, are micro-ejecta and Chevron dunes in southern Madagascar and in the Australian gulf. Geological evidences suggest the tsunamis caused by these impacts reached 205 m (673 ft) above sea level and 45 km (28 mi) inland. The impact events must have disrupted human settlements and perhaps even contributed to major climate changes.
The history of the Indian Ocean is marked by maritime trade; cultural and commercial exchange probably date back at least seven thousand years. Human culture spread early on the shores of the Indian Ocean and was always linked to the cultures of the Mediterranean and Persian Gulf. Before c. 2000 BCE, however, cultures on its shores were only loosely tied to each other; bronze, for example, was developed in Mesopotamia c. 3000 BCE but remained uncommon in Egypt before 1800 BCE. During this period, independent, short-distance oversea communications along its littoral margins evolved into an all-embracing network. The début of this network was not the achievement of a centralised or advanced civilisation but of local and regional exchange in the Persian Gulf, the Red Sea, and Arabian Sea. Sherds of Ubaid (2500–500 BCE) pottery have been found in the western Gulf at Dilmun, present-day Bahrain; traces of exchange between this trading centre and Mesopotamia. The Sumerians traded grain, pottery, and bitumen (used for reed boats) for copper, stone, timber, tin, dates, onions, and pearls. Coast-bound vessels transported goods between the Indus Valley Civilisation (2600–1900 BCE) in the Indian subcontinent (modern-day Pakistan and Northwest India) and the Persian Gulf and Egypt.The Red Sea, one of the main trade routes in Antiquity, was explored by Egyptians and Phoenicians during the last two millennia BCE. In the 6th century BCE Greek explorer Scylax of Caryanda made a journey to India, working for the Persian king Darius, and his now lost account put the Indian Ocean on the maps of Greek geographers. The Greeks began to explore the Indian Ocean following the conquests of Alexander the Great, who ordered a circumnavigation of the Arabian Peninsula in 323 BCE. During the two centuries that followed the reports of the explorers of Ptolemaic Egypt resulted in the best maps of the region until the Portuguese era many centuries later. The main interest in the region for the Ptolemies was not commercial but military; they explored Africa to hunt for war elephants.The Rub' al Khali desert isolates the southern parts of the Arabic Peninsula and the Indian Ocean from the Arabic world. This encouraged the development of maritime trade in the region linking the Red Sea and the Persian Gulf to East Africa and India. The monsoon (from mawsim, the Arabic word for season), however, was used by sailors long before being "discovered" by Hippalus in the 1st century. Indian wood have been found in Sumerian cities, there is evidence of Akkad coastal trade in the region, and contacts between India and the Red Sea dates back to the 2300 B.C.. The archipelagoes of the central Indian Ocean, the Laccadive and Maldive islands, were probably populated during the 2nd century B.C. from the Indian mainland. They appear in written history in the account of merchant Sulaiman al-Tajir in the 9th century but the treacherous reefs of the islands were most likely cursed by the sailors of Aden long before the islands were even settled.Periplus of the Erythraean Sea, an Alexandrian guide to the world beyond the Red Sea — including Africa and India — from the first century CE, not only gives insights into trade in the region but also shows that Roman and Greek sailors had already gained knowledge about the monsoon winds. The contemporaneous settlement of Madagascar by Austronesian sailors shows that the littoral margins of the Indian Ocean were being both well-populated and regularly traversed at least by this time. Albeit the monsoon must have been common knowledge in the Indian Ocean for centuries.The Indian Ocean's relatively calmer waters opened the areas bordering it to trade earlier than the Atlantic or Pacific oceans. The powerful monsoons also meant ships could easily sail west early in the season, then wait a few months and return eastwards. This allowed ancient Indonesian peoples to cross the Indian Ocean to settle in Madagascar around 1 CE.In the 2nd or 1st century BCE, Eudoxus of Cyzicus was the first Greek to cross the Indian Ocean. The probably fictitious sailor Hippalus is said to have learnt the direct route from Arabia to India around this time. During the 1st and 2nd centuries AD intensive trade relations developed between Roman Egypt and the Tamil kingdoms of the Cheras, Cholas and Pandyas in Southern India. Like the Indonesian people above, the western sailors used the monsoon to cross the ocean. The unknown author of the Periplus of the Erythraean Sea describes this route, as well as the commodities that were traded along various commercial ports on the coasts of the Horn of Africa and India circa 1 CE. Among these trading settlements were Mosylon and Opone on the Red Sea littoral.
Unlike the Pacific Ocean where the civilization of the Polynesians reached most of the far flung islands and atolls and populated them, almost all the islands, archipelagos and atolls of the Indian Ocean were uninhabited until colonial times. Although there were numerous ancient civilizations in the coastal states of Asia and parts of Africa, the Maldives were the only island group in the Central Indian Ocean region where an ancient civilization flourished. Maldivians, on their annual trade trip, took their oceangoing trade ships to Sri Lanka rather than mainland India, which is much closer, because their ships were dependent of the Indian Monsoon Current.Arabic missionaries and merchants began to spread Islam along the western shores of the Indian Ocean from the 8th century, if not earlier. A Swahili stone mosque dating to the 8th–15th centuries have been found in Shanga, Kenya. Trade across the Indian Ocean gradually introduced Arabic script and rice as a staple in Eastern Africa. Muslim merchants traded an estimated 1000 African slaves annually between 800 and 1700, a number that grew to c. 4000 during the 18th century, and 3700 during the period 1800–1870. Slave trade also occurred in the eastern Indian Ocean before the Dutch settled there around 1600 but the volume of this trade is unknown.From 1405 to 1433 admiral Zheng He said to have led large fleets of the Ming Dynasty on several treasure voyages through the Indian Ocean, ultimately reaching the coastal countries of East Africa.The Portuguese navigator Vasco da Gama rounded the Cape of Good Hope during his first voyage in 1497 and became the first European to sail to India. The Swahili people he encountered along the African eastcoast lived in a series of cities and had established trade routes to India and to China. Among them, the Portuguese kidnapped most of their pilots in coastal raids and onboard ships. A few of the pilots, however, were gifts by local Swahili rulers, including the sailor from Gujarat, a gift by a Malindi ruler in Kenya, who helped the Portuguese to reach India. In expeditions after 1500 the Portuguese attacked and colonised cities along the African coast. European slave trade in the Indian Ocean began when Portugal established Estado da Índia in the early 16th century. From then until the 1830s, c. 200 slaves were exported from Mozambique annually and similar figures has been estimated for slaves brought from Asia to the Philippines during the Iberian Union (1580–1640).The Ottoman Empire began its expansion into the Indian Ocean in 1517 with the conquest of Egypt under Sultan Selim I. Although the Ottomans shared the same religion as the trading communities in the Indian Ocean the region was unexplored by them. Maps that included the Indian Ocean had been produced by Muslim geographers centuries before the Ottoman conquests; Muslim scholars, such as Ibn Battuta in the 14th Century, had visited most parts of the known world; contemporarily with Vasco da Gama, Arab navigator Ahmad ibn Mājid had compiled a guide to navigation in the Indian Ocean; the Ottomans, nevertheless, began their own parallel era of discovery which rivaled the European expansion.The establishment of the Dutch East India Company in the early 17th century lead to a quick increase in trade volume; there were perhaps up to 500,000 slaves working in Dutch colonies during the 17th and 18th centuries mostly in the Indian Ocean. For example, some 4000 African slaves were used to build the Colombo fortress in Sri Lanka. Bali and neighbouring islands supplied regional networks with c. 100,000–150,000 slaves 1620–1830. Indian and Chinese traders supplied Dutch Indonesia with perhaps 250,000 slaves during 17th and 18th centuries.The British East India Company was established during the same period and in 1622 its ship first carried slaves from the Indian Coromandel Coast to Indonesia. The British mostly brought slaves from Africa and islands in the Indian Ocean to India and Indonesia but also exported slaves from India. The French colonised Réunion and Mauritius in 1721; by 1735 some 7200 slaves populated the Mascarene Islands, a number which had reached 133,000 in 1807. The British captured the islands in 1810, however, and because the British Parliament had prohibited slavery in 1807 a system of clandestine slave trade developed; resulting in 336,000–388,000 slaves exported to the Mascarane Islands 1670–1848.In all, Europeans traded 567,900–733,200 slaves within the Indian Ocean between 1500 and 1850 and almost that same amount were exported from the Indian Ocean to the Americas during the same period. Slave trade in the Indian Ocean was, nevertheless, very limited compared to c. 12,000,000 slaves exported across the Atlantic.
Scientifically, the Indian Ocean remained poorly explored before the International Indian Ocean Expedition in the early 1960s. However, the Challenger expedition 1872–1876 only reported from south of the polar front. The Valdivia expedition 1898–1899 made deep samples in the Indian Ocean. In the 1930s, the John Murray Expedition mainly studied shallow-water habitats. The Swedish Deep Sea Expedition 1947–1948 also sampled the Indian Ocean on its global tour and the Danish Galathea sampled deep-water fauna from Sri Lanka to South Africa on its second expedition 1950–1952. The Soviet research vessel Vityaz also did research in the Indian Ocean.The Suez Canal opened in 1869 when the Industrial Revolution dramatically changed global shipping – the sailing ship declined in importance as did the importance of European trade in favour of trade in East Asia and Australia. The construction of the canal introduced many non-indigenous species into the Mediterranean. For example, the goldband goatfish (Upeneus moluccensis) has replaced the red mullet (Mullus barbatus); since the 1980s huge swarms of scyphozoan jellyfish (Rhopilema nomadica) have affected tourism and fisheries along the Levantian coast and clogged power and desalination plants. Plans announced in 2014 to build a new, much larger Suez Canal parallel to the 19th century canal will most likely boost economy in the region but also cause ecological damage in a much wider area. Throughout the colonial era, islands such as Mauritius were important shipping nodes for the Dutch, French, and British. Mauritius, an inhabited island, became populated by slaves from Africa and indenture labour from India. The end of World War II marked the end of the colonial era. The British left Mauritius in 1974 and with 70% of the population of Indian descent, Mauritius became a close ally of India. In the 1980s, during the Cold War, the South African regime acted to destabilise several island nations in the Indian Ocean, including the Seychelles, Comoros, and Madagascar. India intervened in Mauritius to prevent a coup d'état, backed-up by the United States who feared the Soviet Union could gain access to Port Louis and threaten the U.S. base on Diego Garcia.Iranrud is an unrealised plan by Iran and the Soviet Union to build a canal between the Caspian Sea and Persian Gulf. Testimonies from the colonial era are stories of African slaves, Indian indentured labourers, and white settlers. But, while there was a clear racial line between free men and slaves in the Atlantic World, this delineation is less distinct in the Indian Ocean — there were Indian slaves and settlers as well as black indentured labourers. There were also a string of prison camps across the Indian Ocean, from Robben Island in South Africa to Cellular Jail in the Andamans, in which prisoners, exiles, POWs, forced labourers, merchants, and people of different faiths were forcefully united. On the islands of the Indian Ocean, therefore, a trend of creolisation emerged.On 26 December 2004 fourteen countries around the Indian Ocean were hit by a wave of tsunamis caused by the 2004 Indian Ocean earthquake. The waves radiated across the ocean at speeds exceeding 500 km/h (310 mph), reached up to 20 m (66 ft) in height, and resulted in an estimated 236,000 deaths.In the late 2000s the ocean evolved into a hub of pirate activity. By 2013, attacks off the Horn region's coast had steadily declined due to active private security and international navy patrols, especially by the Indian Navy.Malaysian Airlines Flight 370, a Boeing 777 airliner with 239 persons on board, disappeared on 8 March 2014 and is alleged to have crashed into the southeastern Indian Ocean about 2,000 km (1,200 mi) from the coast of southwest Western Australia. Despite an extensive search, the whereabouts of the remains of the aircraft are unknown.
The sea lanes in the Indian Ocean are considered among the most strategically important in the world with more than 80 percent of the world's seaborne trade in oil transits through the Indian Ocean and its vital chokepoints, with 40 percent passing through the Strait of Hormuz, 35 percent through the Strait of Malacca and 8 percent through the Bab el-Mandab Strait.The Indian Ocean provides major sea routes connecting the Middle East, Africa, and East Asia with Europe and the Americas. It carries a particularly heavy traffic of petroleum and petroleum products from the oil fields of the Persian Gulf and Indonesia. Large reserves of hydrocarbons are being tapped in the offshore areas of Saudi Arabia, Iran, India, and Western Australia. An estimated 40% of the world's offshore oil production comes from the Indian Ocean. Beach sands rich in heavy minerals, and offshore placer deposits are actively exploited by bordering countries, particularly India, Pakistan, South Africa, Indonesia, Sri Lanka, and Thailand. Chinese companies have made investments in several Indian Ocean ports, including Gwadar, Hambantota, Colombo and Sonadia. This has sparked a debate about the strategic implications of these investments. (See String of Pearls)
Indian Ocean in World War II Indian Ocean literature Indian Ocean Naval Symposium Indian Ocean Research Group List of islands in the Indian Ocean List of ports and harbours of the Indian Ocean List of sovereign states and dependent territories in the Indian Ocean Indian Ocean Rim Association Southern Ocean Antarctica Territorial claims in Antarctica Erythraean Sea
Interview with music video director Zach Merck
Bacon, the youngest of six children, was born and raised in a close-knit family in Philadelphia. His mother, Ruth Hilda (née Holmes; 1916–1991), taught at an elementary school and was a liberal activist, while his father, Edmund Norwood Bacon (1910–2005), was an architect who served for many years as executive director of the Philadelphia City Planning Commission.Bacon attended Julia R. Masterman High School for both middle and high school. At age 16, in 1975, Bacon won a full scholarship to and attended the Pennsylvania Governor's School for the Arts at Bucknell University, a state-funded five-week arts program at which he studied theater under Glory Van Scott. The experience solidified Bacon's passion for the arts.
In 1980, he appeared in the slasher film Friday the 13th. Some of his early stage work included Getting Out, performed at New York's Phoenix Theater, and Flux, at Second Stage Theatre during their 1981–1982 season.In 1982, he won an Obie Award for his role in Forty Deuce, and soon afterward he made his Broadway debut in Slab Boys, with then-unknowns Sean Penn and Val Kilmer. However, it was not until he portrayed Timothy Fenwick that same year in Barry Levinson's film Diner – costarring Steve Guttenberg, Daniel Stern, Mickey Rourke, Tim Daly, and Ellen Barkin – that he made an indelible impression on film critics and moviegoers alike.Bolstered by the attention garnered by his performance in Diner, Bacon starred in Footloose (1984). Richard Corliss of TIME likened Footloose to the James Dean classic Rebel Without a Cause and the old Mickey Rooney/Judy Garland musicals, commenting that the film includes "motifs on book burning, mid-life crisis, AWOL parents, fatal car crashes, drug enforcement, and Bible Belt vigilantism." To prepare for the role, Bacon enrolled at a high school as a transfer student named "Ren McCormick" and studied teenagers before leaving in the middle of the day. Bacon earned strong reviews for Footloose. Bacon's critical and box office success led to a period of typecasting in roles similar to the two he portrayed in Diner and Footloose, and he had difficulty shaking this on-screen image. For the next several years he chose films that cast him against either type and experienced, by his own estimation, a career slump. In 1988, he starred in John Hughes' comedy She's Having a Baby, and the following year he was in another comedy called The Big Picture.
In 2000, he appeared in Paul Verhoeven's Hollow Man. Bacon, Colin Firth and Rachel Blanchard depict a ménage à trois in their film, Where the Truth Lies. Bacon and director Atom Egoyan have condemned the MPAA ratings board decision to rate the film "NC-17" rather than the preferable "R". Bacon commented: "I don't get it, when I see films (that) are extremely violent, extremely objectionable sometimes in terms of the roles that women play, slide by with an R, no problem, because the people happen to have more of their clothes on."In 2003 he acted with Sean Penn and Tim Robbins in Clint Eastwood's movie Mystic River. Bacon was again acclaimed for a dark starring role playing an offending pedophile on parole in The Woodsman (2004), for which he was nominated for best actor and received the Independent Spirit Award. He appeared in the HBO Films production of Taking Chance, based on an eponymous story written by Lieutenant Colonel Michael Strobl, an American Desert Storm war veteran. The film premiered on HBO on February 21, 2009. Bacon won a Golden Globe Award and a Screen Actors Guild Award for Outstanding Performance by a Male Actor in a Miniseries or Television Movie for his role.
Beginning in 2012, Bacon has appeared in a major advertising campaign for the EE mobile network in the United Kingdom, based on the Six Degrees concept and his various film roles. In 2015, he became a commercial spokesperson for the U.S. egg industry.
Bacon is the subject of the trivia game titled "Six Degrees of Kevin Bacon," based on the idea that, due to his prolific screen career covering a diverse range of genres, any Hollywood actor can be linked to another in a handful of steps based on their association with Bacon. The name of the game derives from the idea of six degrees of separation. Although he was initially dismayed by the game, the meme stuck, and Bacon eventually embraced it, forming the charitable initiative SixDegrees.org, a social networking site intended to link people and charities to each other.The measure of proximity to Bacon has been mathematically formalized as the Bacon number and can be referenced at websites including Oracle of Bacon, which is in turn based upon Internet Movie Database data. In 2012, Google added a feature to their search engine, whereby searching for an actor's name followed by the words "Bacon Number" will show the ways in which that actor is connected to Kevin Bacon. This feature is no longer active. A similar measurement exists in the mathematics community, where one measures how far one is removed from co-writing a mathematical paper with the famous mathematician Paul Erdős. This is done by means of the Erdős number, which is 0 for Paul Erdős himself, 1 for someone who co-wrote an article with him, 2 for someone who co-wrote with someone who co-wrote with him, etc. People have combined the Bacon number and the Erdős number to form the Erdős–Bacon number, which is the sum of the two.
Kevin formed a band called The Bacon Brothers with his brother, Michael. The duo has released six albums.
2003, September 30: Inducted into Hollywood Walk of Fame with a star for his contribution to Motion Picture presented to him by the Chamber of Commerce. 2004: Received the John Cassavetes Award during the Denver International Film Festival. 2005: Received the Copper Wing Tribute Award during the Phoenix Film Festival. 2005: Received the American Rivera Award during the Santa Barbara International Film Festival. 2010: Honored with the Joel Siegel Award by the Broadcast Film Critics Association. 2015: Honored with the Career Achievement in Acting Award by the Seattle International Film Festival.
List of actors with Hollywood Walk of Fame motion picture stars
Kevin Bacon on IMDb Kevin Bacon at the Internet Broadway Database Kevin Bacon at the Internet Off-Broadway Database Kevin Bacon at AllMovie Oracle of Bacon
Keynesian economics ( KAYN-zee-ən; sometimes Keynesianism, named for the economist John Maynard Keynes) are various macroeconomic theories about how economic output is strongly influenced by aggregate demand (total spending in the economy). In the Keynesian view, aggregate demand does not necessarily equal the productive capacity of the economy. Instead, it is influenced by a host of factors. According to Keynes, the productive capacity of the economy sometimes behaves erratically, affecting production, employment, and inflation.Keynesian economics developed during and after the Great Depression from the ideas presented by Keynes in his 1936 book, The General Theory of Employment, Interest and Money. Keynes' approach was a stark contrast to the aggregate supply-focused classical economics that preceded his book. Interpreting Keynes's work is a contentious topic, and several schools of economic thought claim his legacy. Keynesian economics served as the standard economic model in the developed nations during the later part of the Great Depression, World War II, and the post-war economic expansion (1945–1973). It lost some influence following the oil shock and resulting stagflation of the 1970s. Keynesian economics was later redeveloped as New Keynesian economics, becoming part of the contemporary new neoclassical synthesis. The advent of the financial crisis of 2007–2008 caused a resurgence of popular interest in Keynesian thought.Keynesian economists generally argue that aggregate demand is volatile and unstable. They propose that a market economy often experiences inefficient macroeconomic outcomes in the form of economic recessions (when demand is low) and inflation (when demand is high), and that these can be mitigated by economic policy responses. In particular, monetary policy actions by the central bank and fiscal policy actions by the government can help stabilize output over the business cycle. Keynesian economists generally advocate a market economy – predominantly private sector, but with an active role for government intervention during recessions and depressions.
Macroeconomics is the study of the factors applying to an economy as a whole. Influential economic factors include the overall price level, the interest rate, and the level of employment (or equivalently, of income/output measured in real terms). The classical tradition of partial equilibrium theory had been to split the economy into separate markets, each of whose equilibrium conditions could be stated as a single equation determining a single variable. The theoretical apparatus of supply and demand curves developed by Fleeming Jenkin and Alfred Marshall provided a unified mathematical basis for this approach, which the Lausanne School generalized to general equilibrium theory. For macroeconomics, relevant partial theories included the Quantity theory of money determining the price level and the classical theory of the interest rate. In regards to employment, the condition referred to by Keynes as the "first postulate of classical economics" stated that the wage is equal to the marginal product, which is a direct application of the marginalist principles developed during the nineteenth century (see The General Theory). Keynes sought to supplant all three aspects of the classical theory.
Although Keynes's work was crystallized and given impetus by the advent of the Great Depression, it was part of a long-running debate within economics over the existence and nature of general gluts. A number of the policies Keynes advocated to address the Great Depression (notably government deficit spending at times of low private investment or consumption), and many of the theoretical ideas he proposed (effective demand, the multiplier, the paradox of thrift), had been advanced by various authors in the 19th and early 20th centuries. Keynes's unique contribution was to provide a general theory of these, which proved acceptable to the economic establishment. An intellectual precursor of Keynesian economics was underconsumption theories associated with John Law, Thomas Malthus, the Birmingham School of Thomas Attwood, and the American economists William Trufant Foster and Waddill Catchings, who were influential in the 1920s and 1930s. Underconsumptionists were, like Keynes after them, concerned with failure of aggregate demand to attain potential output, calling this "underconsumption" (focusing on the demand side), rather than "overproduction" (which would focus on the supply side), and advocating economic interventionism. Keynes specifically discussed underconsumption (which he wrote "under-consumption") in the General Theory, in Chapter 22, Section IV and Chapter 23, Section VII. Numerous concepts were developed earlier and independently of Keynes by the Stockholm school during the 1930s; these accomplishments were described in a 1937 article, published in response to the 1936 General Theory, sharing the Swedish discoveries.The paradox of thrift was stated in 1892 by John M. Robertson in his The Fallacy of Saving, in earlier forms by mercantilist economists since the 16th century, and similar sentiments date to antiquity.
In 1923 Keynes published his first contribution to economic theory, A Tract on Monetary Reform, whose point of view is classical but incorporates ideas that later played a part in the General Theory. In particular, looking at the hyperinflation in European economies, he drew attention to the opportunity cost of holding money (identified with inflation rather than interest) and its influence on the velocity of circulation.In 1930 he published A Treatise on Money, intended as a comprehensive treatment of its subject "which would confirm his stature as a serious academic scholar, rather than just as the author of stinging polemics", and marks a large step in the direction of his later views. In it, he attributes unemployment to wage stickiness and treats saving and investment as governed by independent decisions: the former varying positively with the interest rate, the latter negatively. The velocity of circulation is expressed as a function of the rate of interest. He interpreted his treatment of liquidity as implying a purely monetary theory of interest.Keynes's younger colleagues of the Cambridge Circus and Ralph Hawtrey believed that his arguments implicitly assumed full employment, and this influenced the direction of his subsequent work. During 1933, he wrote essays on various economic topics "all of which are cast in terms of movement of output as a whole".
At the time that Keynes's wrote the General Theory, it had been a tenet of mainstream economic thought that the economy would automatically revert to a state of general equilibrium: it had been assumed that, because the needs of consumers are always greater than the capacity of the producers to satisfy those needs, everything that is produced would eventually be consumed once the appropriate price was found for it. This perception is reflected in Say's law and in the writing of David Ricardo, which states that individuals produce so that they can either consume what they have manufactured or sell their output so that they can buy someone else's output. This argument rests upon the assumption that if a surplus of goods or services exists, they would naturally drop in price to the point where they would be consumed. Given the backdrop of high and persistent unemployment during the Great Depression, Keynes argued that there was no guarantee that the goods that individuals produce would be met with adequate effective demand, and periods of high unemployment could be expected, especially when the economy was contracting in size. He saw the economy as unable to maintain itself at full employment automatically, and believed that it was necessary for the government to step in and put purchasing power into the hands of the working population through government spending. Thus, according to Keynesian theory, some individually rational microeconomic-level actions such as not investing savings in the goods and services produced by the economy, if taken collectively by a large proportion of individuals and firms, can lead to outcomes wherein the economy operates below its potential output and growth rate. Prior to Keynes, a situation in which aggregate demand for goods and services did not meet supply was referred to by classical economists as a general glut, although there was disagreement among them as to whether a general glut was possible. Keynes argued that when a glut occurred, it was the over-reaction of producers and the laying off of workers that led to a fall in demand and perpetuated the problem. Keynesians therefore advocate an active stabilization policy to reduce the amplitude of the business cycle, which they rank among the most serious of economic problems. According to the theory, government spending can be used to increase aggregate demand, thus increasing economic activity, reducing unemployment and deflation.
As the 1929 election approached "Keynes was becoming a strong public advocate of capital development" as a public measure to alleviate unemployment. Winston Churchill, the Conservative Chancellor, took the opposite view: It is the orthodox Treasury dogma, steadfastly held ... [that] very little additional employment and no permanent additional employment can, in fact, be created by State borrowing and State expenditure. Keynes pounced on a chink in the Treasury view. Cross-examining Sir Richard Hopkins, a Second Secretary in the Treasury, before the Macmillan Committee on Finance and Industry in 1930 he referred to the "first proposition" that "schemes of capital development are of no use for reducing unemployment" and asked whether "it would be a misunderstanding of the Treasury view to say that they hold to the first proposition". Hopkins responded that "The first proposition goes much too far. The first proposition would ascribe to us an absolute and rigid dogma, would it not?"Later the same year, speaking in a newly created Committee of Economists, Keynes tried to use Kahn's emerging multiplier theory to argue for public works, "but Pigou's and Henderson's objections ensured that there was no sign of this in the final product". In 1933 he gave wider publicity to his support for Kahn's multiplier in a series of articles titled "The road to prosperity" in The Times newspaper.A. C. Pigou was at the time the sole economics professor at Cambridge. He had a continuing interest in the subject of unemployment, having expressed the view in his popular Unemployment (1913) that it was caused by "maladjustment between wage-rates and demand" – a view Keynes may have shared prior to the years of the General Theory. Nor were his practical recommendations very different: "on many occasions in the thirties" Pigou "gave public support ... to State action designed to stimulate employment." Where the two men differed is in the link between theory and practice. Keynes was seeking to build theoretical foundations to support his recommendations for public works while Pigou showed no disposition to move away from classical doctrine. Referring to him and Dennis Robertson, Keynes asked rhetorically: "Why do they insist on maintaining theories from which their own practical conclusions cannot possibly follow?"
John Maynard Keynes (1883–1946) set forward the ideas that became the basis for Keynesian economics in his main work, The General Theory of Employment, Interest and Money (1936). It was written during the Great Depression, when unemployment rose to 25% in the United States and as high as 33% in some countries. It is almost wholly theoretical, enlivened by occasional passages of satire and social commentary. The book had a profound impact on economic thought, and ever since it was published there has been debate over its meaning.
Keynes begins the General Theory with a summary of the classical theory of employment, which he encapsulates in his formulation of Say's Law as the dictum "Supply creates its own demand". Under the classical theory, the wage rate is determined by the marginal productivity of labour, and as many people are employed as are willing to work at that rate. Unemployment may arise through friction or may be "voluntary," in the sense that it arises from a refusal to accept employment owing to "legislation or social practices ... or mere human obstinacy", but "...the classical postulates do not admit of the possibility of the third category," which Keynes defines as involuntary unemployment.Keynes raises two objections to the classical theory's assumption that "wage bargains ... determine the real wage". The first lies in the fact that "labour stipulates (within limits) for a money-wage rather than a real wage". The second is that classical theory assumes that, "The real wages of labour depend on the wage bargains which labour makes with the entrepreneurs," whereas, "If money wages change, one would have expected the classical school to argue that prices would change in almost the same proportion, leaving the real wage and the level of unemployment practically the same as before." Keynes considers his second objection the more fundamental, but most commentators concentrate on his first one: it has been argued that the quantity theory of money protects the classical school from the conclusion Keynes expected from it.
Saving is that part of income not devoted to consumption, and consumption is that part of expenditure not allocated to investment, i.e., to durable goods. Hence saving encompasses hoarding (the accumulation of income as cash) and the purchase of durable goods. The existence of net hoarding, or of a demand to hoard, is not admitted by the simplified liquidity preference model of the General Theory. Once he rejects the classical theory that unemployment is due to excessive wages, Keynes proposes an alternative based on the relationship between saving and investment. In his view, unemployment arises whenever entrepreneurs' incentive to invest fails to keep pace with society's propensity to save (propensity is one of Keynes's synonyms for "demand"). The levels of saving and investment are necessarily equal, and income is therefore held down to a level where the desire to save is no greater than the incentive to invest. The incentive to invest arises from the interplay between the physical circumstances of production and psychological anticipations of future profitability; but once these things are given the incentive is independent of income and depends solely on the rate of interest r. Keynes designates its value as a function of r as the "schedule of the marginal efficiency of capital".The propensity to save behaves quite differently. Saving is simply that part of income not devoted to consumption, and: ... the prevailing psychological law seems to be that when aggregate income increases, consumption expenditure will also increase but to a somewhat lesser extent. Keynes adds that "this psychological law was of the utmost importance in the development of my own thought".
Keynes viewed the money supply as one of the main determinants of the state of the real economy. The significance he attributed to it is one of the innovative features of his work, and was influential on the politically hostile monetarist school. Money supply comes into play through the liquidity preference function, which is the demand function that corresponds to money supply. It specifies the amount of money people will seek to hold according to the state of the economy. In Keynes's first (and simplest) account – that of Chapter 13 – liquidity preference is determined solely by the interest rate r—which is seen as the earnings forgone by holding wealth in liquid form: hence liquidity preference can be written L(r ) and in equilibrium must equal the externally fixed money supply M̂.
Money supply, saving and investment combine to determine the level of income as illustrated in the diagram, where the top graph shows money supply (on the vertical axis) against interest rate. M̂ determines the ruling interest rate r̂ through the liquidity preference function. The rate of interest determines the level of investment Î through the schedule of the marginal efficiency of capital, shown as a blue curve in the lower graph. The red curves in the same diagram show what the propensities to save are for different incomes Y ; and the income Ŷ corresponding to the equilibrium state of the economy must be the one for which the implied level of saving at the established interest rate is equal to Î. In Keynes's more complicated liquidity preference theory (presented in Chapter 15) the demand for money depends on income as well as on the interest rate and the analysis becomes more complicated. Keynes never fully integrated his second liquidity preference doctrine with the rest of his theory, leaving that to John Hicks: see the IS-LM model below.
Keynes rejects the classical explanation of unemployment based on wage rigidity, but it is not clear what effect the wage rate has on unemployment in his system. He treats wages of all workers as proportional to a single rate set by collective bargaining, and chooses his units so that this rate never appears separately in his discussion. It is present implicitly in those quantities he expresses in wage units, while being absent from those he expresses in money terms. It is therefore difficult to see whether, and in what way, his results differ for a different wage rate, nor is it clear what he thought about the matter.
An increase in the money supply, according to Keynes's theory, leads to a drop in the interest rate and an increase in the amount of investment that can be undertaken profitably, bringing with it an increase in total income.
The liquidity trap is a phenomenon that may impede the effectiveness of monetary policies in reducing unemployment. Economists generally think the rate of interest will not fall below a certain limit, often seen as zero or a slightly negative number. Keynes suggested that the limit might be appreciably greater than zero but did not attach much practical significance to it. The term "liquidity trap" was coined by Dennis Robertson in his comments on the General Theory, but it was John Hicks in "Mr. Keynes and the Classics" who recognised the significance of a slightly different concept. If the economy is in a position such that the liquidity preference curve is almost vertical, as must happen as the lower limit on r is approached, then a change in the money supply M̂ makes almost no difference to the equilibrium rate of interest r̂ or, unless there is compensating steepness in the other curves, to the resulting income Ŷ. As Hicks put it, "Monetary means will not force down the rate of interest any further." Paul Krugman has worked extensively on the liquidity trap, claiming that it was the problem confronting the Japanese economy around the turn of the millennium. In his later words: Short-term interest rates were close to zero, long-term rates were at historical lows, yet private investment spending remained insufficient to bring the economy out of deflation. In that environment, monetary policy was just as ineffective as Keynes described. Attempts by the Bank of Japan to increase the money supply simply added to already ample bank reserves and public holdings of cash...
Keynes argued that the solution to the Great Depression was to stimulate the country ("incentive to invest") through some combination of two approaches: A reduction in interest rates (monetary policy), and Government investment in infrastructure (fiscal policy).If the interest rate at which businesses and consumers can borrow decreases, investments that were previously uneconomic become profitable, and large consumer sales normally financed through debt (such as houses, automobiles, and, historically, even appliances like refrigerators) become more affordable. A principal function of central banks in countries that have them is to influence this interest rate through a variety of mechanisms collectively called monetary policy. This is how monetary policy that reduces interest rates is thought to stimulate economic activity, i.e., "grow the economy"—and why it is called expansionary monetary policy. Expansionary fiscal policy consists of increasing net public spending, which the government can effect by a) taxing less, b) spending more, or c) both. Investment and consumption by government raises demand for businesses' products and for employment, reversing the effects of the aforementioned imbalance. If desired spending exceeds revenue, the government finances the difference by borrowing from capital markets by issuing government bonds. This is called deficit spending. Two points are important to note at this point. First, deficits are not required for expansionary fiscal policy, and second, it is only change in net spending that can stimulate or depress the economy. For example, if a government ran a deficit of 10% both last year and this year, this would represent neutral fiscal policy. In fact, if it ran a deficit of 10% last year and 5% this year, this would actually be contractionary. On the other hand, if the government ran a surplus of 10% of GDP last year and 5% this year, that would be expansionary fiscal policy, despite never running a deficit at all. But – contrary to some critical characterizations of it – Keynesianism does not consist solely of deficit spending, since it recommends adjusting fiscal policies according to cyclical circumstances. An example of a counter-cyclical policy is raising taxes to cool the economy and to prevent inflation when there is abundant demand-side growth, and engaging in deficit spending on labour-intensive infrastructure projects to stimulate employment and stabilize wages during economic downturns. Keynes's ideas influenced Franklin D. Roosevelt's view that insufficient buying-power caused the Depression. During his presidency, Roosevelt adopted some aspects of Keynesian economics, especially after 1937, when, in the depths of the Depression, the United States suffered from recession yet again following fiscal contraction. But to many the true success of Keynesian policy can be seen at the onset of World War II, which provided a kick to the world economy, removed uncertainty, and forced the rebuilding of destroyed capital. Keynesian ideas became almost official in social-democratic Europe after the war and in the U.S. in the 1960s. The Keynesian advocacy of deficit spending contrasted with the classical and neoclassical economic analysis of fiscal policy. They admitted that fiscal stimulus could actuate production. But, to these schools, there was no reason to believe that this stimulation would outrun the side-effects that "crowd out" private investment: first, it would increase the demand for labour and raise wages, hurting profitability; Second, a government deficit increases the stock of government bonds, reducing their market price and encouraging high interest rates, making it more expensive for business to finance fixed investment. Thus, efforts to stimulate the economy would be self-defeating. The Keynesian response is that such fiscal policy is appropriate only when unemployment is persistently high, above the non-accelerating inflation rate of unemployment (NAIRU). In that case, crowding out is minimal. Further, private investment can be "crowded in": Fiscal stimulus raises the market for business output, raising cash flow and profitability, spurring business optimism. To Keynes, this accelerator effect meant that government and business could be complements rather than substitutes in this situation. Second, as the stimulus occurs, gross domestic product rises—raising the amount of saving, helping to finance the increase in fixed investment. Finally, government outlays need not always be wasteful: government investment in public goods that is not provided by profit-seekers encourages the private sector's growth. That is, government spending on such things as basic research, public health, education, and infrastructure could help the long-term growth of potential output. In Keynes's theory, there must be significant slack in the labour market before fiscal expansion is justified. Keynesian economists believe that adding to profits and incomes during boom cycles through tax cuts, and removing income and profits from the economy through cuts in spending during downturns, tends to exacerbate the negative effects of the business cycle. This effect is especially pronounced when the government controls a large fraction of the economy, as increased tax revenue may aid investment in state enterprises in downturns, and decreased state revenue and investment harm those enterprises.
In the last few years of his life, John Maynard Keynes was much preoccupied with the question of balance in international trade. He was the leader of the British delegation to the United Nations Monetary and Financial Conference in 1944 that established the Bretton Woods system of international currency management. He was the principal author of a proposal – the so-called Keynes Plan – for an International Clearing Union. The two governing principles of the plan were that the problem of settling outstanding balances should be solved by 'creating' additional 'international money', and that debtor and creditor should be treated almost alike as disturbers of equilibrium. In the event, though, the plans were rejected, in part because "American opinion was naturally reluctant to accept the principle of equality of treatment so novel in debtor-creditor relationships".The new system is not founded on free trade (liberalisation of foreign trade) but rather on regulating international trade to eliminate trade imbalances. Nations with a surplus would have a powerful incentive to get rid of it, which would automatically clear other nations' deficits. Keynes proposed a global bank that would issue its own currency—the bancor—which was exchangeable with national currencies at fixed rates of exchange and would become the unit of account between nations, which means it would be used to measure a country's trade deficit or trade surplus. Every country would have an overdraft facility in its bancor account at the International Clearing Union. He pointed out that surpluses lead to weak global aggregate demand – countries running surpluses exert a "negative externality" on trading partners, and posed far more than those in deficit, a threat to global prosperity. Keynes thought that surplus countries should be taxed to avoid trade imbalances. In "National Self-Sufficiency" The Yale Review, Vol. 22, no. 4 (June 1933), he already highlighted the problems created by free trade. His view, supported by many economists and commentators at the time, was that creditor nations may be just as responsible as debtor nations for disequilibrium in exchanges and that both should be under an obligation to bring trade back into a state of balance. Failure for them to do so could have serious consequences. In the words of Geoffrey Crowther, then editor of The Economist, "If the economic relationships between nations are not, by one means or another, brought fairly close to balance, then there is no set of financial arrangements that can rescue the world from the impoverishing results of chaos."These ideas were informed by events prior to the Great Depression when – in the opinion of Keynes and others – international lending, primarily by the U.S., exceeded the capacity of sound investment and so got diverted into non-productive and speculative uses, which in turn invited default and a sudden stop to the process of lending.Influenced by Keynes, economic texts in the immediate post-war period put a significant emphasis on balance in trade. For example, the second edition of the popular introductory textbook, An Outline of Money, devoted the last three of its ten chapters to questions of foreign exchange management and in particular the 'problem of balance'. However, in more recent years, since the end of the Bretton Woods system in 1971, with the increasing influence of Monetarist schools of thought in the 1980s, and particularly in the face of large sustained trade imbalances, these concerns – and particularly concerns about the destabilising effects of large trade surpluses – have largely disappeared from mainstream economics discourse and Keynes' insights have slipped from view. They are receiving some attention again in the wake of the financial crisis of 2007–08.
Keynes's ideas became widely accepted after World War II, and until the early 1970s, Keynesian economics provided the main inspiration for economic policy makers in Western industrialized countries. Governments prepared high quality economic statistics on an ongoing basis and tried to base their policies on the Keynesian theory that had become the norm. In the early era of social liberalism and social democracy, most western capitalist countries enjoyed low, stable unemployment and modest inflation, an era called the Golden Age of Capitalism. In terms of policy, the twin tools of post-war Keynesian economics were fiscal policy and monetary policy. While these are credited to Keynes, others, such as economic historian David Colander, argue that they are, rather, due to the interpretation of Keynes by Abba Lerner in his theory of functional finance, and should instead be called "Lernerian" rather than "Keynesian".Through the 1950s, moderate degrees of government demand leading industrial development, and use of fiscal and monetary counter-cyclical policies continued, and reached a peak in the "go go" 1960s, where it seemed to many Keynesians that prosperity was now permanent. In 1971, Republican US President Richard Nixon even proclaimed "I am now a Keynesian in economics."Beginning in the late 1960s, a new classical macroeconomics movement arose, critical of Keynesian assumptions (see sticky prices), and seemed, especially in the 1970s, to explain certain phenomena better. It was characterized by explicit and rigorous adherence to microfoundations, as well as use of increasingly sophisticated mathematical modelling. With the oil shock of 1973, and the economic problems of the 1970s, Keynesian economics began to fall out of favour. During this time, many economies experienced high and rising unemployment, coupled with high and rising inflation, contradicting the Phillips curve's prediction. This stagflation meant that the simultaneous application of expansionary (anti-recession) and contractionary (anti-inflation) policies appeared necessary. This dilemma led to the end of the Keynesian near-consensus of the 1960s, and the rise throughout the 1970s of ideas based upon more classical analysis, including monetarism, supply-side economics, and new classical economics. However, by the late 1980s, certain failures of the new classical models, both theoretical (see Real business cycle theory) and empirical (see the "Volcker recession") hastened the emergence of New Keynesian economics, a school that sought to unite the most realistic aspects of Keynesian and neo-classical assumptions and place them on more rigorous theoretical foundation than ever before. One line of thinking, utilized also as a critique of the notably high unemployment and potentially disappointing GNP growth rates associated with the new classical models by the mid-1980s, was to emphasize low unemployment and maximal economic growth at the cost of somewhat higher inflation (its consequences kept in check by indexing and other methods, and its overall rate kept lower and steadier by such potential policies as Martin Weitzman's share economy).
Multiple schools of economic thought that trace their legacy to Keynes currently exist, the notable ones being neo-Keynesian economics, New Keynesian economics, post-Keynesian economics, and the new neoclassical synthesis. Keynes's biographer Robert Skidelsky writes that the post-Keynesian school has remained closest to the spirit of Keynes's work in following his monetary theory and rejecting the neutrality of money. Today these ideas, regardless of provenance, are referred to in academia under the rubric of "Keynesian economics", due to Keynes's role in consolidating, elaborating, and popularizing them. In the postwar era, Keynesian analysis was combined with neoclassical economics to produce what is generally termed the "neoclassical synthesis", yielding neo-Keynesian economics, which dominated mainstream macroeconomic thought. Though it was widely held that there was no strong automatic tendency to full employment, many believed that if government policy were used to ensure it, the economy would behave as neoclassical theory predicted. This post-war domination by neo-Keynesian economics was broken during the stagflation of the 1970s. There was a lack of consensus among macroeconomists in the 1980s, and during this period New Keynesian economics was developed, ultimately becoming- along with new classical macroeconomics- a part of the current consensus, known as the new neoclassical synthesis.Post-Keynesian economists, on the other hand, reject the neoclassical synthesis and, in general, neoclassical economics applied to the macroeconomy. Post-Keynesian economics is a heterodox school that holds that both neo-Keynesian economics and New Keynesian economics are incorrect, and a misinterpretation of Keynes's ideas. The post-Keynesian school encompasses a variety of perspectives, but has been far less influential than the other more mainstream Keynesian schools.Interpretations of Keynes have emphasized his stress on the international coordination of Keynesian policies, the need for international economic institutions, and the ways in which economic forces could lead to war or could promote peace.
In a 2014 paper, economist Alan Blinder argues that, "for not very good reasons," public opinion in the United States has associated Keynesianism with liberalism, and he states that such is incorrect. For example, both Presidents Ronald Reagan (1981-89) and George W. Bush (2001-09) supported policies that were, in fact, Keynesian, even though both men were conservative leaders. And tax cuts can provide highly helpful fiscal stimulus during a recession, just as much as infrastructure spending can. Blinder concludes, "If you are not teaching your students that 'Keynesianism' is neither conservative nor liberal, you should be."
The Keynesian schools of economics are situated alongside a number of other schools that have the same perspectives on what the economic issues are, but differ on what causes them and how best to resolve them. Today, most of these schools of thought have been subsumed into modern macroeconomic theory.
The Stockholm school rose to prominence at about the same time that Keynes published his General Theory and shared a common concern in business cycles and unemployment. The second generation of Swedish economists also advocated government intervention through spending during economic downturns although opinions are divided over whether they conceived the essence of Keynes's theory before he did.
There was debate between monetarists and Keynesians in the 1960s over the role of government in stabilizing the economy. Both monetarists and Keynesians agree that issues such as business cycles, unemployment, and deflation are caused by inadequate demand. However, they had fundamentally different perspectives on the capacity of the economy to find its own equilibrium, and the degree of government intervention that would be appropriate. Keynesians emphasized the use of discretionary fiscal policy and monetary policy, while monetarists argued the primacy of monetary policy, and that it should be rules-based.The debate was largely resolved in the 1980s. Since then, economists have largely agreed that central banks should bear the primary responsibility for stabilizing the economy, and that monetary policy should largely follow the Taylor rule – which many economists credit with the Great Moderation. The financial crisis of 2007–08, however, has convinced many economists and governments of the need for fiscal interventions and highlighted the difficulty in stimulating economies through monetary policy alone during a liquidity trap.
Some Marxist economists criticized Keynesian economics. For example, in his 1946 appraisal Paul Sweezy—while admitting that there was much in the General Theory's analysis of effective demand that Marxists could draw on—described Keynes as a prisoner of his neoclassical upbringing. Sweezy argued that Keynes had never been able to view the capitalist system as a totality. He argued that Keynes regarded the class struggle carelessly, and overlooked the class role of the capitalist state, which he treated as a deus ex machina, and some other points. While Michał Kalecki was generally enthusiastic about the Keynesian revolution, he predicted that it would not endure, in his article "Political Aspects of Full Employment". In the article Kalecki predicted that the full employment delivered by Keynesian policy would eventually lead to a more assertive working class and weakening of the social position of business leaders, causing the elite to use their political power to force the displacement of the Keynesian policy even though profits would be higher than under a laissez faire system: The erosion of social prestige and political power would be unacceptable to the elites despite higher profits.
James M. Buchanan criticized Keynesian economics on the grounds that governments would in practice be unlikely to implement theoretically optimal policies. The implicit assumption underlying the Keynesian fiscal revolution, according to Buchanan, was that economic policy would be made by wise men, acting without regard to political pressures or opportunities, and guided by disinterested economic technocrats. He argued that this was an unrealistic assumption about political, bureaucratic and electoral behaviour. Buchanan blamed Keynesian economics for what he considered a decline in America's fiscal discipline. Buchanan argued that deficit spending would evolve into a permanent disconnect between spending and revenue, precisely because it brings short-term gains, so, ending up institutionalizing irresponsibility in the federal government, the largest and most central institution in our society.Martin Feldstein argues that the legacy of Keynesian economics–the misdiagnosis of unemployment, the fear of saving, and the unjustified government intervention–affected the fundamental ideas of policy makers.Milton Friedman thought that Keynes's political bequest was harmful for two reasons. First, he thought whatever the economic analysis, benevolent dictatorship is likely sooner or later to lead to a totalitarian society. Second, he thought Keynes's economic theories appealed to a group far broader than economists primarily because of their link to his political approach.Alex Tabarrok argues that Keynesian politics–as distinct from Keynesian policies–has failed pretty much whenever it's been tried, at least in liberal democracies.In response to this argument, John Quiggin, wrote about these theories' implication for a liberal democratic order. He thought that if it is generally accepted that democratic politics is nothing more than a battleground for competing interest groups, then reality will come to resemble the model. Paul Krugman wrote "I don’t think we need to take that as an immutable fact of life; but still, what are the alternatives?" Daniel Kuehn, criticized James M. Buchanan. He argued, "if you have a problem with politicians - criticize politicians," not Keynes. He also argued that empirical evidence makes it pretty clear that Buchanan was wrong.James Tobin argued, if advising government officials, politicians, voters, it's not for economists to play games with them. Keynes implicitly rejected this argument, in "soon or late it is ideas not vested interests which are dangerous for good or evil."Brad DeLong has argued that politics is the main motivator behind objections to the view that government should try to serve a stabilizing macroeconomic role. Paul Krugman argued that a regime that by and large lets markets work, but in which the government is ready both to rein in excesses and fight slumps is inherently unstable, due to intellectual instability, political instability, and financial instability.
Another influential school of thought was based on the Lucas critique of Keynesian economics. This called for greater consistency with microeconomic theory and rationality, and in particular emphasized the idea of rational expectations. Lucas and others argued that Keynesian economics required remarkably foolish and short-sighted behaviour from people, which totally contradicted the economic understanding of their behaviour at a micro level. New classical economics introduced a set of macroeconomic theories that were based on optimizing microeconomic behaviour. These models have been developed into the real business-cycle theory, which argues that business cycle fluctuations can to a large extent be accounted for by real (in contrast to nominal) shocks. Beginning in the late 1950s new classical macroeconomists began to disagree with the methodology employed by Keynes and his successors. Keynesians emphasized the dependence of consumption on disposable income and, also, of investment on current profits and current cash flow. In addition, Keynesians posited a Phillips curve that tied nominal wage inflation to unemployment rate. To support these theories, Keynesians typically traced the logical foundations of their model (using introspection) and supported their assumptions with statistical evidence. New classical theorists demanded that macroeconomics be grounded on the same foundations as microeconomic theory, profit-maximizing firms and rational, utility-maximizing consumers.The result of this shift in methodology produced several important divergences from Keynesian macroeconomics: Independence of consumption and current income (life-cycle permanent income hypothesis) Irrelevance of current profits to investment (Modigliani–Miller theorem) Long run independence of inflation and unemployment (natural rate of unemployment) The inability of monetary policy to stabilize output (rational expectations) Irrelevance of taxes and budget deficits to consumption (Ricardian equivalence)
Adam Smith Economic theories Game theory Invisible hand Job guarantee Pareto principle
Works by John Maynard Keynes at Project Gutenberg "We are all Keynesians now" – Historic article from Time magazine, 1965
Labour economics seeks to understand the functioning and dynamics of the markets for wage labour. Labour is a commodity that supplied by labourers in exchange for a wage paid by demanding firms.Labour markets or job markets function through the interaction of workers and employers. Labour economics looks at the suppliers of labour services (workers) and the demanders of labour services (employers), and attempts to understand the resulting pattern of wages, employment, and income. Labour markets are normally geographically bounded, but the rise of the internet has brought about a 'planetary labour market' in some sectors.Labour is a measure of the work done by human beings. It is conventionally contrasted with such other factors of production as land and capital. Some theories focus on human capital (referring to the skills that workers possess, not necessarily their actual work). Labour is unique to study because it is a special type of good that cannot be separated from the owner (i.e. the work cannot be separated from the person who does it). A labour market is also different from other markets in that workers are the suppliers and firms are the demanders.
There are two sides to labour economics. Labour economics can generally be seen as the application of microeconomic or macroeconomic techniques to the labour market. Microeconomic techniques study the role of individuals and individual firms in the labour market. Macroeconomic techniques look at the interrelations between the labour market, the goods market, the money market, and the foreign trade market. It looks at how these interactions influence macro variables such as employment levels, participation rates, aggregate income and gross domestic product.
The Labour force (LF) is defined as the number of people of working age, who are either employed or actively looking for work (unemployed). The labour force participation rate (LFPR) is the number of people in the labour force divided by the size of the adult civilian non-institutional population (or by the population of working age that is not institutionalized), LFPR = LF/Population.The non-labour force includes those who are not looking for work, those who are institutionalized (such as in prisons or psychiatric wards), stay-at-home spouses, children not of working age, and those serving in the military. The unemployment level is defined as the labour force minus the number of people currently employed. The unemployment rate is defined as the level of unemployment divided by the labour force. The employment rate is defined as the number of people currently employed divided by the adult population (or by the population of working age). In these statistics, self-employed people are counted as employed.The skills required in a labour force can vary from individual to individual, as well as from firm to firm. Some firms have specific skills they are interested in, limiting the labour force to certain criteria. A firm requiring specific skills will help determine the size of the market.Variables like employment level, unemployment level, labour force, and unfilled vacancies are called stock variables because they measure a quantity at a point in time. They can be contrasted with flow variables which measure a quantity over a duration of time. Changes in the labour force are due to flow variables such as natural population growth, net immigration, new entrants, and retirements. Changes in unemployment depend on inflows (non-employed people starting to look for jobs and employed people who lose their jobs that are looking for new ones) and outflows (people who find new employment and people who stop looking for employment). When looking at the overall macroeconomy, several types of unemployment have been identified, which can be separated into two categories of natural and unnatural unemployment.Natural Unemployment Frictional unemployment – This reflects the fact that it takes time for people to find and settle into new jobs that they feel are appropriate for them and their skill set. Technological advancement often reduces frictional unemployment; for example, internet search engines have reduced the cost and time associated with locating employment or personnel selection. Structural unemployment – the number of jobs available in an industry are not sufficient enough to provide jobs to all persons who are interested in working or qualified to work in that industry. This can be due to the changes in industries prevalent in a country or because wages for the industry are too high, causing people to want to supply their labour to that industry. Natural rate of unemployment (also known as full employment) – This is the summation of frictional and structural unemployment, that excludes cyclical contributions of unemployment (e.g. recessions) and seasonal unemployment. It is the lowest rate of unemployment that a stable economy can expect to achieve, given that some frictional and structural unemployment is inevitable. Economists do not agree on the level of the natural rate, with estimates ranging from 1% to 5%, or on its meaning – some associate it with "non-accelerating inflation". The estimated rate varies between countries and across time.Unnatural Unemployment Demand deficient unemployment (also known as cyclical unemployment) – In Keynesian economics, any level of unemployment beyond the natural rate is probably due to insufficient goods demand in the overall economy. During a recession, aggregate expenditure is deficient causing the underutilisation of inputs (including labour). Aggregate expenditure (AE) can be increased, according to Keynes, by increasing consumption spending (C), increasing investment spending (I), increasing government spending (G), or increasing the net of exports minus imports (X−M), since AE = C + I + G + (X−M). Seasonal unemployment -- unemployment due to seasonal fluctuations of demand for workers across industries, such as in the retail industry after holidays that involve a lot of shopping are over.
Neoclassical economists view the labour market as similar to other markets in that the forces of supply and demand jointly determine the price (in this case the wage rate) and quantity (in this case the number of people employed). However, the labour market differs from other markets (like the markets for goods or the financial market) in several ways. In particular, the labour market may act as a non-clearing market. While according to neoclassical theory most markets quickly attain a point of equilibrium without excess supply or demand, this may not be true of the labour market: it may have a persistent level of unemployment. Contrasting the labour market to other markets also reveals persistent compensating differentials among similar workers. Models that assume perfect competition in the labour market, as discussed below, conclude that workers earn their marginal product of labour.
A firm's labour demand is based on its marginal physical product of labour (MPPL). This is defined as the additional output (or physical product) that results from an increase of one unit of labour (or from an infinitesimal increase in labour). (See also Production theory basics.) Labour demand is a derived demand; that is, hiring labour is not desired for its own sake but rather because it aids in producing output, which contributes to an employer's revenue and hence profits. The demand for an additional amount of labour depends on the Marginal Revenue Product (MRP) and the marginal cost (MC) of the worker. With a perfectly competitive goods market, the MRP is calculated by multiplying the price of the end product or service by the Marginal Physical Product of the worker. If the MRP is greater than a firm's Marginal Cost, then the firm will employ the worker since doing so will increase profit. The firm only employs however up to the point where MRP=MC, and not beyond, in neoclassical economic theory.The MRP of the worker is affected by other inputs to production with which the worker can work (e.g. machinery), often aggregated under the term "capital". It is typical in economic models for greater availability of capital for a firm to increase the MRP of the worker, all else equal. Education and training are counted as "human capital". Since the amount of physical capital affects MRP, and since financial capital flows can affect the amount of physical capital available, MRP and thus wages can be affected by financial capital flows within and between countries, and the degree of capital mobility within and between countries.According to neoclassical theory, over the relevant range of outputs, the marginal physical product of labour is declining (law of diminishing returns). That is, as more and more units of labour are employed, their additional output begins to decline. Additionally, although the MRP is a good way of expressing an employer's demand, other factors such as social group formation can the demand, as well as the labour supply. This constantly restructures exactly what a labour market is, and leads way to cause problems for theories of inflation.
The marginal revenue product of labour can be used as the demand for labour curve for this firm in the short run. In competitive markets, a firm faces a perfectly elastic supply of labour which corresponds with the wage rate and the marginal resource cost of labour (W = SL = MFCL). In imperfect markets, the diagram would have to be adjusted because MFCL would then be equal to the wage rate divided by marginal costs. Because optimum resource allocation requires that marginal factor costs equal marginal revenue product, this firm would demand L units of labour as shown in the diagram. The demand for labour of this firm can be summed with the demand for labour of all other firms in the economy to obtain the aggregate demand for labour. Likewise, the supply curves of all the individual workers (mentioned above) can be summed to obtain the aggregate supply of labour. These supply and demand curves can be analysed in the same way as any other industry demand and supply curves to determine equilibrium wage and employment levels. Wage differences exist, particularly in mixed and fully/partly flexible labour markets. For example, the wages of a doctor and a port cleaner, both employed by the NHS, differ greatly. There are various factors concerning this phenomenon. This includes the MRP of the worker. A doctor's MRP is far greater than that of the port cleaner. In addition, the barriers to becoming a doctor are far greater than that of becoming a port cleaner. To become a doctor takes a lot of education and training which is costly, and only those who excel in academia can succeed in becoming doctors. The port cleaner, however, requires relatively less training. The supply of doctors is therefore significantly less elastic than that of port cleaners. Demand is also inelastic as there is a high demand for doctors and medical care is a necessity, so the NHS will pay higher wage rates to attract the profession.
Some labour markets have a single employer and thus do not satisfy the perfect competition assumption of the neoclassical model above. The model of a monopsonistic labour market gives a lower quantity of employment and a lower equilibrium wage rate than does the competitive model.
In many real-life situations, the assumption of perfect information is unrealistic. An employer does not necessarily know how hard workers are working or how productive they are. This provides an incentive for workers to shirk from providing their full effort, called moral hazard. Since it is difficult for the employer to identify the hard-working and the shirking employees, there is no incentive to work hard and productivity falls overall, leading to the hiring of more workers and a lower unemployment rate. One solution that is used to avoid a moral hazard is stock options that grant employees the chance to benefit directly from a firm's success. However, this solution has attracted criticism as executives with large stock-option packages have been suspected of acting to over-inflate share values to the detriment of the long-run welfare of the firm. Another solution, foreshadowed by the rise of temporary workers in Japan and the firing of many of these workers in response to the financial crisis of 2008, is more flexible job- contracts and -terms that encourage employees to work less than full-time by partially compensating for the loss of hours, relying on workers to adapt their working time in response to job requirements and economic conditions instead of the employer trying to determine how much work is needed to complete a given task and overestimating.[citation needed]Another aspect of uncertainty results from the firm's imperfect knowledge about worker ability. If a firm is unsure about a worker's ability, it pays a wage assuming that the worker's ability is the average of similar workers. This wage under compensates high-ability workers which may drive them away from the labour market as well as at the same time attracting low-ability workers. Such a phenomenon, called adverse selection, can sometimes lead to market collapse.One way to combat adverse selection, firms will try to use signalling, pioneered by Michael Spence, whereby employers could use various characteristics of applicants differentiate between high-ability or low-ability workers. One common signal used is education, whereby employers assume that high-ability workers will have higher levels of education. Employers can then compensate high-ability workers with higher wages. However, signalling does not always work, and it may appear to an external observer that education has raised the marginal product of labour, without this necessarily being true.
One of the major research achievements of the 1990-2010 period was the development of a framework with dynamic search, matching, and bargaining.
At the micro level, one sub-discipline eliciting increased attention in recent decades is analysis of internal labour markets, that is, within firms (or other organisations), studied in personnel economics from the perspective of personnel management. By contrast, external labour markets "imply that workers move somewhat fluidly between firms and wages are determined by some aggregate process where firms do not have significant discretion over wage setting." The focus is on "how firms establish, maintain, and end employment relationships and on how firms provide incentives to employees," including models and empirical work on incentive systems and as constrained by economic efficiency and risk/incentive tradeoffs relating to personnel compensation.
Inequality and discrimination in the workplace can have many effects on workers. In the context of labour economics, inequality is usually referring to the unequal distribution of earning between households. Inequality is commonly measured by economists using the Gini coefficient. This coefficient does not have a concrete meaning but is more used as a way to compare inequality across regions. The higher the Gini coefficient is calculated to be the larger inequality exists in a region. Over time, inequality has, on average, been increasing. This is due to numerous factors including labour supply and demand shifts as well as institutional changes in the labour market. On the shifts in labour supply and demand, factors include demand for skilled workers going up more than the supply of skilled workers and relative to unskilled workers as well as technological changes that increase productivity; all of these things cause wages to go up for skilled labour while unskilled worker wages stay the same or decline. As for the institutional changes, a decrease in union power and a declining real minimum wage, which both reduce unskilled workers wages, and tax cuts for the wealthy all increase the inequality gap between groups of earners. As for discrimination, it is the difference in pay that can be attributed to the demographic differences between people, such as gender, race, ethnicity, religion, sexual orientation, etc, even though these factors do not affect the productivity of the worker. Many regions and countries have enacted government policies to combat discrimination, including discrimination in the workplace. Discrimination can be modelled and measured in numerous ways. The oaxaca decomposition is a common method used to calculate the amount of discrimination that exists when wages differ between groups of people. This decomposition aims to calculate the difference in wages that occurs because of differences in skills versus the returns to those skills. A way of modelling discrimination in the workplace when dealing with wages are Gary Becker's taste models. Using taste models, employer discrimination can be thought of as the employer not hiring the minority worker because of their perceived cost of hiring that worker is higher than that of the cost of hiring a non-minority worker, which causes less hiring of the minority. Another taste model is for employee discrimination, which does not cause a decline in the hiring of minorities, but instead causes a more segregated workforce because the prejudiced worker feels that they should be paid more to work next to the worker they are prejudiced against or that they are not paid an equal amount as the worker they are prejudiced against. One more taste model involves customer discrimination, whereby the employers themselves are not prejudiced but believe that their customers might be, so therefore the employer is less likely to hire the minority worker if they are going to interact with customers that are prejudiced. There are many other taste models other than these that Gary Becker has made to explain discrimination that causes differences in hiring in wages in the labour market.
Many sociologists, political economists, and heterodox economists claim that labour economics tends to lose sight of the complexity of individual employment decisions. These decisions, particularly on the supply side, are often loaded with considerable emotional baggage and a purely numerical analysis can miss important dimensions of the process, such as social benefits of a high income or wage rate regardless of the marginal utility from increased consumption or specific economic goals. From the perspective of mainstream economics, neoclassical models are not meant to serve as a full description of the psychological and subjective factors that go into a given individual's employment relations, but as a useful approximation of human behaviour in the aggregate, which can be fleshed out further by the use of concepts such as information asymmetry, transaction costs, contract theory etc. Also missing from most labour market analyses is the role of unpaid labour such as unpaid internships where workers with little or no experience are allowed to work a job without pay so that they can gain experience in a particular profession. Even though this type of labour is unpaid it can nevertheless play an important part in society if not abused by employers. The most dramatic example is child raising. However, over the past 25 years an increasing literature, usually designated as the economics of the family, has sought to study within household decision making, including joint labour supply, fertility, child-raising, as well as other areas of what is generally referred to as home production.
The labour market, as institutionalised under today's market economic systems, has been criticised, especially by both mainstream socialists and anarcho-syndicalists, who utilise the term wage slavery as a pejorative for wage labour. Socialists draw parallels between the trade of labour as a commodity and slavery. Cicero is also known to have suggested such parallels.According to Noam Chomsky, analysis of the psychological implications of wage slavery goes back to the Enlightenment era. In his 1791 book On the Limits of State Action, classical liberal thinker Wilhelm von Humboldt explained how "whatever does not spring from a man's free choice, or is only the result of instruction and guidance, does not enter into his very nature; he does not perform it with truly human energies, but merely with mechanical exactness" and so when the labourer works under external control, "we may admire what he does, but we despise what he is." Both the Milgram and Stanford experiments have been found useful in the psychological study of wage-based workplace relations.The American philosopher John Dewey posited that until "industrial feudalism" is replaced by "industrial democracy," politics will be "the shadow cast on society by big business". Thomas Ferguson has postulated in his investment theory of party competition that the undemocratic nature of economic institutions under capitalism causes elections to become occasions when blocs of investors coalesce and compete to control the state.As per anthropologist David Graeber, the earliest wage labour contracts we know about were in fact contracts for the rental of chattel slaves (usually the owner would receive a share of the money, and the slave, another, with which to maintain his or her living expenses.) Such arrangements, according to Graeber, were quite common in New World slavery as well, whether in the United States or Brazil. C. L. R. James argued that most of the techniques of human organisation employed on factory workers during the industrial revolution were first developed on slave plantations.Additionally, Marxists posit that labour-as-commodity, which is how they regard wage labour, provides an absolutely fundamental point of attack against capitalism. "It can be persuasively argued," noted one concerned philosopher, "that the conception of the worker's labour as a commodity confirms Marx's stigmatisation of the wage system of private capitalism as 'wage-slavery;' that is, as an instrument of the capitalist's for reducing the worker's condition to that of a slave, if not below it."
Ageing workers EU-OSHA The Labour Economics Gateway – Collection of Internet sites that are of interest to labour economists Labour & Worklife Program at Harvard Law School, Changing Labour Markets Project W.E. Upjohn Institute for Employment Research ILO: Key Indicators of the Labour Market (KILM). 5. ed. Sept. 2007 LabourFair Resources – Link to Fair Labour Practices Labour Research Network – Labour research programme treating various fields Labour Research Department – Independent labour economics research organisation
Languages spoken in India belong to several language families, the major ones being the Indo-Aryan languages spoken by 78.05% of Indians and the Dravidian languages spoken by 19.64% of Indians. Languages spoken by the remaining 2.31% of the population belong to the Austroasiatic, Sino-Tibetan, Tai-Kadai and a few other minor language families and isolates. India has the world's fourth highest number of languages (427), after Nigeria (524), Indonesia (710) and Papua New Guinea (840).Article 343 of the Indian constitution stated that the official language of the Union is Hindi in Devanagari script instead of the extant English. Later, a constitutional amendment, The Official Languages Act, 1963, allowed for the continuation of English alongside Hindi in the Indian government indefinitely until legislation decides to change it. The form of numerals to be used for the official purposes of the Union are "the international form of Indian numerals", which are referred to as Arabic numerals in most English-speaking countries. Despite the misconceptions, Hindi is not the national language of India; the Constitution of India does not give any language the status of national language.The Eighth Schedule of the Indian Constitution lists 22 languages, which have been referred to as scheduled languages and given recognition, status and official encouragement. In addition, the Government of India has awarded the distinction of classical language to Kannada, Malayalam, Odia, Sanskrit, Tamil and Telugu. Classical language status is given to languages which have a rich heritage and independent nature. According to the Census of India of 2001, India has 122 major languages and 1599 other languages. However, figures from other sources vary, primarily due to differences in definition of the terms "language" and "dialect". The 2001 Census recorded 30 languages which were spoken by more than a million native speakers and 122 which were spoken by more than 10,000 people. Two contact languages have played an important role in the history of India: Persian and English. Persian was the court language during the Mughal period in India. It reigned as an administrative language for several centuries until the era of British colonisation. English continues to be an important language in India. It is used in higher education and in some areas of the Indian government. Hindi, the most commonly spoken language in India today, serves as the lingua franca across much of North and Central India. Bengali is the second most spoken and understood language in the country with a significant amount of speakers in Eastern and North- eastern regions. Marathi is the third most spoken and understood language in the country with a significant amount of speakers in South-Western regions. However, there have been concerns raised with Hindi being imposed in South India, most notably in the state of Tamil Nadu and Karnataka. Maharashtra, West Bengal, Assam, Punjab and other non-Hindi regions have also started to voice concerns about Hindi.
The Southern Indian languages are from the Dravidian family. The Dravidian languages are indigenous to the Indian subcontinent. Proto-Dravidian languages were spoken in India in the 4th millennium BCE and started disintegrating into various branches around 3rd millennium BCE. The Dravidian languages are classified in four groups: North, Central (Kolami–Parji), South-Central (Telugu–Kui), and South Dravidian (Tamil-Kannada).The Northern Indian languages from the Indo-Aryan branch of the Indo-European family evolved from Old Indic by way of the Middle Indic Prakrit languages and Apabhraṃśa of the Middle Ages. The Indo-Aryan languages developed and emerged in three stages — Old Indo-Aryan (1500 BCE to 600 BCE), Middle Indo-Aryan stage (600 BCE and 1000 CE) and New Indo-Aryan (between 1000 CE and 1300 CE). The modern north Indian Indo-Aryan languages all evolved into distinct, recognisable languages in the New Indo-Aryan Age.Persian, or Farsi, was brought into India by the Ghaznavids and other Turko-Afghan dynasties as the court language. Culturally Persianized, they, in combination with the later Mughal dynasty (of Turco-Mongol origin), influenced the art, history and literature of the region for more than 500 years, resulting in the Persianisation of many Indian tongues, mainly lexically. In 1837, the British replaced Persian with English and Hindustani in Perso-Arabic script for administrative purposes and the Hindi movement of the 19th Century replaced Persianised vocabulary with Sanskrit derivations and replaced or supplemented the use of Perso-Arabic script for administrative purposes with Devanagari.Each of the northern Indian languages had different influences. For example, Hindustani was strongly influenced by Sanskrit, Arabic and Persian, leading to the emergence of Modern Standard Hindi and Modern Standard Urdu as registers of the Hindustani language. Bangla on the other hand has retained its Sanskritic roots while heavily expanding its vocabulary with words from Persian, English, French and other foreign languages.
The first official survey of language diversity in the Indian subcontinent was carried out by Sir George Abraham Grierson from 1898 to 1928. Titled the Linguistic Survey of India, it reported a total of 179 languages and 544 dialects. However, the results were skewed due to ambiguities in distinguishing between "dialect" and "language", use of untrained personnel and under-reporting of data from South India, as the former provinces of Burma and Madras, as well as the princely states of Cochin, Hyderabad, Mysore and Travancore were not included in the survey.Different sources give widely differing figures, primarily based on how the terms "language" and "dialect" are defined and grouped. Ethnologue, produced by the Christian evangelist organisation SIL International, lists 461 tongues for India (out of 6,912 worldwide), 447 of which are living, while 14 are extinct. The 447 living languages are further subclassified in Ethnologue as follows:- Institutional – 63 Developing – 130 Vigorous – 187 In trouble – 54 Dying – 13The People's Linguistic Survey of India, a privately owned research institution in India, has recorded over 66 different scripts and more than 780 languages in India during its nationwide survey, which the organisation claims to be the biggest linguistic survey in India.The People of India (POI) project of Anthropological Survey of India reported 325 languages which are used for in-group communication by 5,633 Indian communities.
The Census of India records and publishes data with respect to the number of speakers for languages and dialects, but uses its own unique terminology, distinguishing between language and mother tongue. The mother tongues are grouped within each language. Many of the mother tongues so defined could be considered a language rather than a dialect by linguistic standards. This is especially so for many mother tongues with tens of millions of speakers that are officially grouped under the language Hindi. 1951 CensusSeparate figures for Hindi, Urdu, and Punjabi were not issued, due to the fact the returns were intentionally recorded incorrectly in states such as East Punjab, Himachal Pradesh, Delhi, PEPSU, and Bilaspur. 1961 CensusThe 1961 census recognised 1,652 mother tongues spoken by 438,936,918 people, counting all declarations made by any individual at the time when the census was conducted. However, the declaring individuals often mixed names of languages with those of dialects, subdialects and dialect clusters or even castes, professions, religions, localities, regions, countries and nationalities. The list therefore includes languages with barely a few individual speakers as well as 530 unclassified mother tongues and more than 100 idioms that are non-native to India, including linguistically unspecific demonyms such as "African", "Canadian" or "Belgian". 1991 CensusThe 1991 census recognises 1,576 classified mother tongues. According to the 1991 census, 22 languages had more than a million native speakers, 50 had more than 100,000 and 114 had more than 10,000 native speakers. The remaining accounted for a total of 566,000 native speakers (out of a total of 838 million Indians in 1991). 2001 CensusAs per the census of 2001, there are 1635 rationalised mother tongues, 234 identifiable mother tongues and 22 major languages. Of these, 29 languages have more than a million native speakers, 60 have more than 100,000 and 122 have more than 10,000 native speakers. There are a few languages like Kodava that do not have a script but have a group of native speakers in Coorg (Kodagu). 2011 CensusAccording to the most recent census of 2011, after thorough linguistic scrutiny, edit and rationalization on 19,569 raw linguistic affiliation, the census recognizes 1369 rationalized mother tongues and 1474 names which were treated as ‘unclassified’ and relegated to ‘other’ mother tongue category. Among, the 1369 rationalized mother tongues which are spoken by 10,000 or more speakers, are further grouped into appropriate set that resulted into total 121 languages. In these 121 languages, 22 are already part of the Eighth Schedule to the Constitution of India and other 99 are termed as "Total of other languages" which is one short as of the other languages recognized in 2001 census.
The following list consist of Indian subcontinent languages' total speakers worldwide in the 2019 edition of Ethnologue, a language reference published by SIL International, which is based in the United States.
Ethnolinguistically, the languages of South Asia, echoing the complex history and geography of the region, form a complex patchwork of language families, language phyla and isolates. Languages spoken in India belong to several language families, the major ones being the Indo-Aryan languages spoken by 78.05% of Indians and the Dravidian languages spoken by 19.64% of Indians. The languages of India belong to several language families, the most important of which are:
The largest of the language families represented in India, in terms of speakers, is the Indo-Aryan language family, a branch of the Indo-Iranian family, itself the easternmost, extant subfamily of the Indo-European language family. This language family predominates, accounting for some 1035 million speakers, or over 76.5 of the population, as per 2018 estimate. The most widely spoken languages of this group are Hindi, Bengali, Marathi, Urdu, Gujarati, Punjabi, Kashmiri, Rajasthani, Sindhi, Assamese (Asamiya), Maithili and Odia. Aside from the Indo-Aryan languages, other Indo-European languages are also spoken in India, the most prominent of which is English, as a lingua franca.
The second largest language family is the Dravidian language family, accounting for some 277 million speakers, or approximately 20.5% as per 2018 estimate The Dravidian languages are spoken mainly in southern India and parts of eastern and central India as well as in parts of northeastern Sri Lanka, Pakistan, Nepal and Bangladesh. The Dravidian languages with the most speakers are Telugu, Tamil, Kannada and Malayalam. Besides the mainstream population, Dravidian languages are also spoken by small scheduled tribe communities, such as the Oraon and Gond tribes. Only two Dravidian languages are exclusively spoken outside India, Brahui in Pakistan and Dhangar, a dialect of Kurukh, in Nepal.
Families with smaller numbers of speakers are Austroasiatic and numerous small Sino-Tibetan languages, with some 10 and 6 million speakers, respectively, together 3% of the population.The Austroasiatic language family (austro meaning South) is the autochthonous language in Southeast Asia, arrived by migration. Austroasiatic languages of mainland India are the Khasi and Munda languages, including Santali. The languages of the Nicobar islands also form part of this language family. With the exceptions of Khasi and Santali, all Austroasiatic languages on Indian territory are endangered.
The Tibeto-Burman language family are well represented in India. However, their interrelationships are not discernible, and the family has been described as "a patch of leaves on the forest floor" rather than with the conventional metaphor of a "family tree".Tibeto-Burman languages are spoken across the Himalayas in the regions of Ladakh, Himachal Pradesh, Nepal, Sikkim, Bhutan, Arunachal Pradesh, and also in the Indian states of West Bengal, Assam (hills and autonomous councils), Meghalaya, Nagaland, Manipur, Tripura and Mizoram. Sino-Tibetan languages spoken in India include the scheduled languages Meitei and Bodo, the non-scheduled languages of Karbi, Lepcha, and many varieties of several related Tibetic, West Himalayish, Tani, Brahmaputran, Angami–Pochuri, Tangkhul, Zeme, Kukish language groups, amongst many others.
Ahom language, a Southwestern Tai language, had been once the dominant language of the Ahom Kingdom in modern-day Assam, but was later replaced by the Assamese language (known as Kamrupi in ancient era which is the pre-form of the Kamrupi dialect of today). Nowadays, small Tai communities and their languages remain in Assam and Arunachal Pradesh together with Sino-Tibetans, e.g. Tai Phake, Tai Aiton and Tai Khamti, which are similar to the Shan language of Shan State, Myanmar; the Dai language of Yunnan, China; the Lao language of Laos; the Thai language of Thailand; and the Zhuang language in Guangxi, China.
The languages of the Andaman Islands form another group: the Great Andamanese languages, comprising a number of extinct, and one highly endangered language the Ongan family of the southern Andaman Islands, comprising two extant languages, Önge and Jarawa, and one extinct language, Jangil.In addition, Sentinelese, is thought likely to be related to the above languages.
The only language found in the Indian mainland that is considered a language isolate is Nihali. The status of Nihali is ambiguous, having been considered as a distinct Austroasiatic language, as a dialect of Korku and also as being a "thieves' argot" rather than a legitimate language.The other language isolates found in the rest of South Asia include Burushaski, a language spoken in Gilgit–Baltistan (administered by Pakistan), Kusunda (in western Nepal) and Vedda (in Sri Lanka). The validity of the Great Andamanese language group as a language family has been questioned and it has been considered a language isolate by some authorities.In addition, a Bantu language, Sidi, was spoken until the mid-20th century in Gujarat by the Siddi.
Prior to Independence, in British India, English was the sole language used for administrative purposes as well as for higher education purposes.In 1946, the issue of national language was a bitterly contested subject in the proceedings of the Constituent Assembly of India, specifically what should be the language in which the Constitution of India is written and the language spoken during the proceedings of Parliament and thus deserving of the epithet "national". Members belonging to the northern parts of India insisted that the Constitution be drafted in Hindi with the unofficial translation in English. This was not agreed to by the drafting Committee on the grounds that English was much better to craft the nuanced prose on constitutional subjects. The efforts to make Hindi the pre-eminent language were bitterly resisted by the members from those parts of India where Hindi was not spoken natively. Eventually, a compromise was reached not to include any mention to a national language. Instead, Hindi in Devanagari script was declared to be the official language of the union, but for "fifteen years from the commencement of the Constitution, the English Language shall continue to be used for all the official purposes of the Union for which it was being used immediately before such commencement."Article 343 (1) of the Constitution of India states "The Official Language of the Union government shall be Hindi in Devanagari script." Unless Parliament decided otherwise, the use of English for official purposes was to cease 15 years after the constitution came into effect, i.e. on 26 January 1965. As the date for changeover approached, however, there was much alarm in the non Hindi-speaking areas of India, especially in Kerala, Gujarat, Maharashtra, Tamil Nadu, Punjab, West Bengal, Karnataka, Puducherry and Andhra Pradesh. Accordingly, Jawaharlal Nehru ensured the enactment of the Official Languages Act, 1963, which provided that English "may" still be used with Hindi for official purposes, even after 1965. The wording of the text proved unfortunate in that while Nehru understood that "may" meant shall, politicians championing the cause of Hindi thought it implied exactly the opposite.In the event, as 1965 approached, India's new Prime Minister Lal Bahadur Shastri prepared to make Hindi paramount with effect from 26 January 1965. This led to widespread agitation, riots, self-immolations and suicides in Tamil Nadu. The split of Congress politicians from the South from their party stance, the resignation of two Union ministers from the South and the increasing threat to the country's unity forced Shastri to concede.As a result, the proposal was dropped, and the Act itself was amended in 1967 to provide that the use of English would not be ended until a resolution to that effect was passed by the legislature of every state that had not adopted Hindi as its official language, and by each house of the Indian Parliament.The Constitution of India does not give any language the status of national language.
Hindi, written in Devanagari script, is the most prominent language spoken in the country. In the 2001 census, 422 million (422,048,642) people in India reported Hindi to be their native language. This figure not only included Hindi speakers of Hindustani, but also people who identify as native speakers of related languages who consider their speech to be a dialect of Hindi, the Hindi belt. Hindi (or Hindustani) is the native language of most people living in Delhi, Uttar Pradesh, Uttarakhand, Chhattisgarh, Himachal Pradesh, Chandigarh, Bihar, Jharkhand, Madhya Pradesh, Haryana, and Rajasthan."Modern Standard Hindi", a standardised language is one of the official languages of the Union of India. In addition, it is one of only two languages used for business in Parliament however the Rajya Sabha now allows all 22 official languages on the Eighth Schedule to be spoken.Hindustani, evolved from khari boli (खड़ी बोली), a prominent tongue of Mughal times, which itself evolved from Apabhraṃśa, an intermediary transition stage from Prakrit, from which the major North Indian Indo-Aryan languages have evolved.Varieties of Hindi spoken in India include Rajasthani, Braj Bhasha, Haryanvi, Bundeli, Kannauji, Hindustani, Awadhi, Bagheli, Bhojpuri, Magahi, Nagpuri and Chhattisgarhi. By virtue of its being a lingua franca, Hindi has also developed regional dialects such as Bambaiya Hindi in Mumbai. In addition, a trade language, Andaman Creole Hindi has also developed in the Andaman Islands.In addition, by use in popular culture such as songs and films, Hindi also serves as a lingua franca across both North and Central IndiaHindi is widely taught both as a primary language and language of instruction, and as a second tongue in most states.
British colonial legacy has resulted in English being a language for government, business and education. English, along with Hindi, is one of the two languages permitted in the Constitution of India for business in Parliament. Despite the fact that Hindi has official Government patronage and serves as a lingua franca over large parts of India, there was considerable opposition to the use of Hindi in the southern states of India, and English has emerged as a de facto lingua franca over much of India. Journalist Manu Joseph, in a 2011 article in The New York Times, wrote that due to the prominence and usage of the language and the desire for English-language education, "English is the de facto national language of India. It is a bitter truth."
Until the Twenty-first Amendment of the Constitution of India in 1967, the country recognised 14 official regional languages. The Eighth Schedule and the Seventy-First Amendment provided for the inclusion of Sindhi, Konkani, Meitei and Nepali, thereby increasing the number of official regional languages of India to 18. The Eighth Schedule of the Constitution of India, as of 1 December 2007, lists 22 languages, which are given in the table below together with the regions where they are used.The individual states, the borders of most of which are or were drawn on socio-linguistic lines, can legislate their own official languages, depending on their linguistic demographics. The official languages chosen reflect the predominant as well as politically significant languages spoken in that state. Certain states having a linguistically defined territory may have only the predominant language in that state as its official language, examples being Karnataka and Gujarat, which have Kannada and Gujarati as their sole official language respectively. Telangana, with a sizeable Urdu-speaking Muslim population, has two languages, Telugu and Urdu, as its official languages. Some states buck the trend by using minority languages as official languages. Jammu and Kashmir uses Urdu, which is spoken by fewer than 1% of the population. Meghalaya uses English spoken by 0.01% of the population. This phenomenon has turned majority languages into "minority languages" in a functional sense. Lists of Official Languages of States and Union Territories of IndiaIn addition to states and union territories, India has autonomous administrative regions which may be permitted to select their own official language – a case in point being the Bodoland Territorial Council in Assam which has declared the Bodo language as official for the region, in addition to Assamese and English already in use. and Bengali in the Barak Valley, as its official languages.
In British India, English was the sole language used for administrative purposes as well as for higher education purposes. When India became independent in 1947, the Indian legislators had the challenge of choosing a language for official communication as well as for communication between different linguistic regions across India. The choices available were: Making "Hindi", which a plurality of the people (41%) identified as their native language, the official language. Making English, as preferred by non-Hindi speakers, particularly Kannadigas and Tamils, and those from Mizoram and Nagaland, the official language. See also Anti-Hindi agitations. Declare both Hindi and English as official languages and each state is given freedom to choose the official language of the state.The Indian constitution, in 1950, declared Hindi in Devanagari script to be the official language of the union. Unless Parliament decided otherwise, the use of English for official purposes was to cease 15 years after the constitution came into effect, i.e. on 26 January 1965. The prospect of the changeover, however, led to much alarm in the non Hindi-speaking areas of India, especially in South India whose native tongues are not related to Hindi. As a result, Parliament enacted the Official Languages Act in 1963, which provided for the continued use of English for official purposes along with Hindi, even after 1965.
Native to the Bengal region, comprising the nation of Bangladesh and the states of West Bengal, Tripura and Barak Valley region of Assam. Bengali (also spelt as Bangla: বাংলা) is the sixth most spoken language in the world. After the partition of India (1947), refugees from East Pakistan were settled in Tripura, and Jharkhand and the union territory of Andaman and Nicobar Islands. There is also a large number of Bengali-speaking people in Maharashtra and Gujarat where they work as artisans in jewellery industries. Bengali developed from Abahatta, a derivative of Apabhramsha, itself derived from Magadhi Prakrit. The modern Bengali vocabulary contains the vocabulary base from Magadhi Prakrit and Pali, also borrowings from Sanskrit and other major borrowings from Persian, Arabic, Austroasiatic languages and other languages in contact with. Like most Indian languages, Bengali has a number of dialects. It exhibits diglossia, with the literary and standard form differing greatly from the colloquial speech of the regions that identify with the language. Bengali language has developed a rich cultural base spanning art, music, literature and religion. Bengali has some of the oldest literature of all modern Indo-Aryan languages, dating from about 10th to 12th century ('Chargapada' buddhist songs). There have been many movements in defence of this language and in 1999 UNESCO declared 21 Feb as the International Mother Language Day in commemoration of the Bengali Language Movement in 1952.
Marathi is an Indo-Aryan language. It is the official language and co-official language in Maharashtra and Goa states of Western India respectively, and is one of the official languages of India. There were 83 million speakers of the language in 2011. Marathi has the third largest number of native speakers in India and ranks 10th in the list of most spoken languages in the world. Marathi has some of the oldest literature of all modern Indo-Aryan languages; Oldest stone inscriptions from 8th century & literature dating from about 1100 AD (Mukundraj's Vivek Sindhu dates to the 12th century). The major dialects of Marathi are Standard Marathi and the Varhadi dialect. There are other related languages such as Khandeshi, Dangi, Vadvali, Samavedi. Malvani Konkani has been heavily influenced by Marathi varieties. Marathi is one of several languages that descend from Maharashtri Prakrit. Further change led to the Apabhraṃśa languages like Old Marathi. Marathi Language Day (मराठी दिन/मराठी दिवस (transl. Marathi Dina/Marathi Diwasa) is celebrated on 27 February every year across the Indian states of Maharashtra and Goa. This day is regulated by the State Government. It is celebrated on the Birthday of eminent Marathi Poet Vi. Va. Shirwadkar, popularly known as Kusumagraj. Marathi is the official language of Maharashtra and co-official language in the union territories of Daman and Diu and Dadra and Nagar Haveli. In Goa, Konkani is the sole official language; however, Marathi may also be used for all official purposes. Over a period of many centuries the Marathi language and people came into contact with many other languages and dialects. The primary influence of Prakrit, Maharashtri, Dravidian languages, Apabhraṃśa and Sanskrit is understandable. Marathi has influenced by the Austroasiatic, Dravidian and foreign languages such as Persian, Arabic. Marathi contains loanwords from Persian, Arabic, English and a little from French & Portuguese languages.
Telugu is the most widely spoken Dravidian language in India and around the world. Telugu is an official language in Andhra Pradesh, Telangana and Yanam, making it one of the few languages (along with Hindi, Bengali, and Urdu) with official status in more than one state. It is also spoken by a significant number of people in the Andaman and Nicobar Islands, Chhattisgarh, Karnataka, Maharashtra, Odisha, Tamil Nadu, Gujarat and by the Sri Lankan Gypsy people. It is one of six languages with classical status in India. Telugu ranks fourth by the number of native speakers in India (81 million in the 2011 Census), fifteenth in the Ethnologue list of most-spoken languages worldwide and is the most widely spoken Dravidian language.
Tamil (also spelt as Thamizh: தமிழ்) is a Dravidian language predominantly spoken in Tamil Nadu, Puducherry and many parts of Sri Lanka. It is also spoken by large minorities in the Andaman and Nicobar Islands, Kerala, Karnataka, Andhra Pradesh, Malaysia, Singapore, Mauritius and throughout the world. Tamil ranks fifth by the number of native speakers in India (61 million in the 2001 Census) and ranks 20th in the list of most spoken languages. It is one of the 22 scheduled languages of India and was the first Indian language to be declared a classical language by the Government of India in 2004. Tamil is one of the longest surviving classical languages in the world. It has been described as "the only language of contemporary India which is recognisably continuous with a classical past". The two earliest manuscripts from India, acknowledged and registered by UNESCO Memory of the World register in 1997 and 2005, are in Tamil. Tamil is an official language of Tamil Nadu, Puducherry, Andaman and Nicobar Islands, Sri Lanka and Singapore. It is also recognized as a minority language in Canada, Malaysia, Mauritius and South Africa.
After independence, Modern Standard Urdu, the Persianised register of Hindustani became the national language of Pakistan. During British colonial times, a knowledge of Hindustani or Urdu was a must for officials. Hindustani was made the second language of British Indian Empire after English and considered as the language of administration. The British introduced the use of Roman script for Hindustani as well as other languages. Urdu had 70 million speakers in India (as per the Census of 2001), and, along with Hindi, is one of the 22 officially recognised regional languages of India and also an official language in the Indian states of Jammu and Kashmir, Delhi, Uttar Pradesh, Bihar and Telangana that have significant Muslim populations.
Kannada language is a Dravidian language which branched off from Kannada-Tamil sub group around 500 B.C.E according to the Dravidian scholar Zvelebil. According to the Dravidian scholars Steever and Krishnamurthy, the study of Kannada language is usually divided into three linguistic phases: Old (450–1200 CE), Middle (1200–1700 CE) and Modern (1700–present). The earliest written records are from the 5th century, and the earliest available literature in rich manuscript (Kavirajamarga) is from c. 850. Kannada language has the second oldest written tradition of all languages of India. Current estimates of the total number of epigraph present in Karnataka range from 25,000 by the scholar Sheldon Pollock to over 30,000 by the Sahitya Akademi, making Karnataka state "one of the most densely inscribed pieces of real estate in the world". According to Garg and Shipely, more than a thousand notable writers have contributed to the wealth of the language.
Malayalam (; [ maləjaːɭəm]) has official language status in the state of Kerala and in the union territories of Lakshadweep and Puducherry. It belongs to the Dravidian family of languages and is spoken by some 38 million people. Malayalam is also spoken in the neighboring states of Tamil Nadu and Karnataka; with some speakers in the Nilgiris, Kanyakumari and Coimbatore districts of Tamil Nadu, and the Dakshina Kannada and the Kodagu district of Karnataka. Malayalam originated from Middle Tamil (Sen-Tamil) in the 7th century. As Malayalam began to freely borrow words as well as the rules of grammar from Sanskrit, the Grantha alphabet was adopted for writing and came to be known as Arya Eluttu. This developed into the modern Malayalam script.
Odia (formerly spelled Oriya) is the only modern language officially recognized as a classical language from the Indo-Aryan group. Odia is primarily spoken in the Indian state of Odisha and has over 40 million speakers. It was declared as a classical language of India in 2014. Native speakers comprise 91.85% of the population in Odisha. Odia originated from Odra Prakrit which developed from Magadhi Prakrit, a language spoken in eastern India over 2,500 years ago. The history of Odia language can be divided to Old Odia (3rd century BC −1200 century AD), Early Middle Odia (1200–1400), Middle Odia (1400–1700), Late Middle Odia (1700–1870) and Modern Odia (1870 till present day). The National Manuscripts Mission of India have found around 213,000 unearthed and preserved manuscripts written in Odia.
Punjabi, written in the Gurmukhi script in India, is one of the prominent languages of India with about 32 million speakers. In Pakistan it is spoken by over 80 million people and is written in the Shahmukhi alphabet. It is mainly spoken in Punjab but also in neighboring areas. It is an official language of Delhi and Punjab.
Asamiya or Assamese language is most popular in the state of Assam. It's an Eastern Indo-Aryan language having more than 15 million speakers as per world estimates by Encarta.
Maithili (; Maithilī) is an Indo-Aryan language native to India and Nepal. In India, it is widely spoken in the Bihar and Jharkhand states. Native speakers are also found in other states and union territories of India, most notably in Uttar Pradesh and the National Capital Territory of Delhi. In the 2011 census of India, It was reported by 1,35,83,464 people as their mother tongue comprising about 1.12% of the total population of India. In Nepal, it is spoken in the eastern Terai, and is the second most prevalent language of Nepal. Tirhuta was formerly the primary script for written Maithili. Less commonly, it was also written in the local variant of Kaithi. Today it is written in the Devanagari script.In 2003, Maithili was included in the Eighth Schedule of the Indian Constitution as a recognised regional language of India, which allows it to be used in education, government, and other official contexts.
In 2004, the Government of India declared that languages that met certain requirements could be accorded the status of a "Classical Language" of India. Over the next few years, several languages were granted the Classical status, and demands have been made for other languages, including Bengali and Marathi.Languages thus far declared to be Classical: Tamil (in 2004), Sanskrit (in 2005), Kannada (in 2008), Telugu (in 2008), Malayalam (in 2013), Odia (in 2014).In a 2006 press release, Minister of Tourism and Culture Ambika Soni told the Rajya Sabha the following criteria were laid down to determine the eligibility of languages to be considered for classification as a "Classical Language", High antiquity of its early texts/recorded history over a period of 1500–2000 years; a body of ancient literature/texts, which is considered a valuable heritage by generations of speakers; the literary tradition be original and not borrowed from another speech community; the classical language and literature being distinct from modern, there may also be a discontinuity between the classical language and its later forms or its offshoots.
As per Government of India's Resolution No. 2-16/2004-US(Akademies) dated 1 November 2004, the benefits that will accrue to a language declared as a "Classical Language" are: Two major international awards for scholars of eminence in Classical Indian Languages are awarded annually. A Centre of Excellence for Studies in Classical Languages is set up. The University Grants Commission will be requested to create, to start with at least in the Central Universities, a certain number of Professional Chairs for Classical Languages for scholars of eminence in Classical Indian Languages.
The 2001 census identified the following native languages having more than one million speakers. Most of them are dialects/variants grouped under Hindi.
India has several languages in use; choosing any single language as an official language presents problems to all those whose "mother tongue" is different. However, all the boards of education across India recognise the need for training people to one common language. There are complaints that in North India, non-Hindi speakers have language trouble. Similarly, there are complaints that North Indians have to undergo difficulties on account of language when travelling to South India. It is common to hear of incidents that result due to friction between those who strongly believe in the chosen official language, and those who follow the thought that the chosen language(s) do not take into account everyone's preferences. Local official language commissions have been established and various steps are being taken in a direction to reduce tensions and friction.
There are conflicts over linguistic rights in India. The first major linguistic conflict, known as the Anti-Hindi agitations of Tamil Nadu, took place in Tamil Nadu against the implementation of Hindi as the official language of India. Political analysts consider this as a major factor in bringing DMK to power and leading to the ousting and nearly total elimination of the Congress party in Tamil Nadu. Strong cultural pride based on language is also found in other Indian states such as Assam, Odisha, Karnataka, West Bengal, Punjab and Maharashtra. To express disapproval of the imposition of Hindi on its states' people as a result of the central government, the government of Maharashtra made the state language Marathi mandatory in educational institutions of CBSE and ICSE through Class/Grade 10.The Government of India attempts to assuage these conflicts with various campaigns, coordinated by the Central Institute of Indian Languages, Mysore, a branch of the Department of Higher Education, Language Bureau, and the Ministry of Human Resource Development.
Most languages in India are written in scripts derived from Brahmi..These include Devanagari, Tamil, Telugu, Kannada, Meitei Mayek, Odia, Eastern Nagari – Assamese/Bengali, Gurumukhi and other.Urdu is written in a script derived from Arabic.A few minor languages such as Santali use independent scripts.Various Indian languages have their own scripts. Hindi, Marathi, Maithili and Angika are languages written using the Devanagari script. Most major languages are written using a script specific to them, such as Assamese (Asamiya) with Asamiya, Bengali with Bengali, Punjabi with Gurmukhi, Meitei with Meitei Mayek, Odia with Odia script, Gujarati with Gujarati, etc. Urdu and sometimes Kashmiri, Saraiki and Sindhi are written in modified versions of the Perso-Arabic script. With this one exception, the scripts of Indian languages are native to India. Languages like Kodava that didn't have a script whereas Tulu which had a script adopted Kannada due to its readily available printing settings; these languages have taken up the scripts of the local official languages as their own and are written in the Kannada script.
List of endangered languages in India List of languages by number of native speakers in India National Translation Mission Romanization of Sindhi Languages of Pakistan Languages of Bangladesh Languages of Sri Lanka
Linguistic map of India with a detailed map of the Seven Sister States (India) at Muturzikin.com Languages and Scripts of India Diversity of Languages in India A comprehensive federal government site that offers complete info on Indian Languages Technology Development for Indian Languages, Government of India Languages Spoken in Himachal Pradesh - Himachal Pariksha
Latin America is a group of countries and dependencies in the Western Hemisphere where Romance languages such as Spanish, French or Portuguese are predominantly spoken. Some territories such as Quebec, where French is spoken, or areas of the United States where Spanish is predominantly spoken are not included due to the nation being a part of Anglo America. The term is more ambiguous than categories such as Hispanic America or Ibero-America which specifically refer to Spanish and Portuguese-speaking countries respectively. The term is also more recent in origin. The term "Latin America" was first used in an 1856 conference with the title "Initiative of America. Idea for a Federal Congress of the Republics" (Iniciativa de la América. Idea de un Congreso Federal de las Repúblicas), by the Chilean politician Francisco Bilbao. The term was further popularised by French emperor Napoleon III's government in the 1860s as Amérique latine to justify France's military involvement in Mexico and try to include French-speaking territories in the Americas such as French Canada, French Louisiana, or French Guiana, in the larger group of countries where Spanish and Portuguese languages prevailed.. Including French-speaking territories, Latin America would consist of 20 countries and 14 dependent territories that cover an area that stretches from Mexico to Tierra del Fuego and includes much of the Caribbean. It has an area of approximately 19,197,000 km2 (7,412,000 sq mi), almost 13% of the Earth's land surface area. As of March 2, 2020, population of Latin America and the Caribbean was estimated at more than 652 million, and in 2019, Latin America had a combined nominal GDP of US$5,188,250 million and a GDP PPP of 10,284,588 million USD.
There is no universal agreement on the origin of the term Latin America. Some historians believe that the term was created by geographers in the 16th century to refer to the parts of the New World colonized by Spain and Portugal, whose Romance languages derive from Latin. Others argue that the term arose in 1860s France during the reign of Napoleon III, as part of the attempt to create a French empire in the Americas. The idea that a part of the Americas has a linguistic affinity with the Romance cultures as a whole can be traced back to the 1830s, in the writing of the French Saint-Simonian Michel Chevalier, who postulated that this part of the Americas was inhabited by people of a "Latin race", and that it could, therefore, ally itself with "Latin Europe", ultimately overlapping the Latin Church, in a struggle with "Teutonic Europe", "Anglo-Saxon America" and "Slavic Europe".Historian John Leddy Phelan located the origins of the term Latin America in the French occupation of Mexico. His argument is that French imperialists used the concept of "Latin" America as a way to counter British imperialism, as well as to challenge the German threat to France. The idea of a "Latin race" was then taken up by Latin American intellectuals and political leaders of the mid- and late-nineteenth century, who no longer looked to Spain or Portugal as cultural models, but rather to France. French ruler Napoleon III had a strong interest in extending French commercial and political power in the region he and his business promoter Felix Belly called "Latin America" to emphasize the shared Latin background of France with the former Viceroyalties of Spain and colonies of Portugal. This led to Napoleon's failed attempt to take military control of Mexico in the 1860s.However, though Phelan thesis is still frequently mentioned in the U.S. academy, two Latin American historians, the Uruguayan Arturo Ardao and the Chilean Miguel Rojas Mix proved decades ago that the term "Latin America" was used earlier than Phelan claimed, and the first use of the term was completely opposite to support imperialist projects in the Americas. Ardao wrote about this subject in his book Génesis de la idea y el nombre de América latina (Genesis of the Idea and the Name of Latin America, 1980), and Miguel Rojas Mix in his article "Bilbao y el hallazgo de América latina: Unión continental, socialista y libertaria" (Bilbao and the Finding of Latin America: a Continental, Socialist and Libertarian Union, 1986). As Michel Gobat reminds in his article "The Invention of Latin America: A Transnational History of Anti-Imperialism, Democracy, and Race", "Arturo Ardao, Miguel Rojas Mix, and Aims McGuinness have revealed [that] the term 'Latin America' had already been used in 1856 by Central Americans and South Americans protesting U.S. expansion into the Southern Hemisphere". Edward Shawcross summarizes Ardao's and Rojas Mix's findings in the following way: "Ardao identified the term in a poem by a Colombian diplomat and intellectual resident in France, José María Torres Caicedo, published on 15 February 1857 in a French based Spanish-language newspaper, while Rojas Mix located it in a speech delivered in France by the radical liberal Chilean politician Francisco Bilbao in June 1856".Now under the administration of the United States, by the late 1850s, the term was being used in local California newspapers such as El Clamor Público by Californios writing about América latina and latinoamérica, and identifying as latinos as the abbreviated term for their "hemispheric membership in la raza latina".So, regarding when the words "Latin" and "America" were combined for the first time in a printed work, the term "Latin America" was first used in 1856 in a conference by the Chilean politician Francisco Bilbao in Paris. The conference had the title "Initiative of the America. Idea for a Federal Congress of Republics." The following year the Colombian writer José María Torres Caicedo also used the term in his poem "The Two Americas". Two events related with the U.S. played a central role in both works. The first event happened less than a decade before the publication of Bilbao's and Torres Caicedo works: the Invasion of Mexico or, in USA the Mexican–American War, after which Mexico lost a third of its territory. The second event, the Walker affair, happened the same year both works were written: the decision by U.S. president Franklin Pierce to recognize the regime recently established in Nicaragua by American William Walker and his band of filibusters who ruled Nicaragua for nearly a year (1856–57) and attempted to reinstate slavery there, where it had been already abolished for three decades In both Bilbao's and Torres Caicedo's works, the Mexican-American War and Walker's expedition to Nicaragua are explicitly mentioned as examples of dangers for the region. For Bilbao, "Latin America" was not a geographical concept, since he excluded Brazil, Paraguay and Mexico. Both authors also ask for the union of all Latin American countries as the only way to defend their territories against further foreign U.S. interventions. Both rejected also European imperialism, claiming that the return of European countries to non-democratic forms of government was another danger for Latin American countries, and used the same word to describe the state of European politics at the time: "despotism." Several years later, during the French invasion of Mexico, Bilbao wrote another work, "Emancipation of the Spirit in America," where he asked all Latin American countries to support the Mexican cause against France, and rejected French imperialism in Asia, Africa, Europe and the Americas. He asked Latin American intellectuals to search for their "intellectual emancipation" by abandoning all French ideas, claiming that France was: "Hypocrite, because she [France] calls herself protector of the Latin race just to subject it to her exploitation regime; treacherous, because she speaks of freedom and nationality, when, unable to conquer freedom for herself, she enslaves others instead!" Therefore, as Michel Gobat puts it, the term Latin America itself had an "anti-imperial genesis," and their creators were far from supporting any form of imperialism in the region, or in any other place of the globe. However, in France the term Latin America was used with the opposite intention. It was employed by the French Empire of Napoleon III during the French invasion of Mexico as a way to include France among countries with influence in the Americas and to exclude Anglophone countries. It played a role in his campaign to imply cultural kinship of the region with France, transform France into a cultural and political leader of the area, and install Maximilian of Habsburg as emperor of the Second Mexican Empire. This term was also used in 1861 by French scholars in La revue des races Latines, a magazine dedicated to the Pan-Latinism movement.
Latin America is often used synonymously with Ibero-America ("Iberian America"), excluding the predominantly French- and English-speaking territories. Thus the countries of Haiti, Belize, Guyana and Suriname, and several French overseas departments, are excluded. Latin America generally refers to territories in the Americas where the Spanish, Portuguese or French languages prevail, including: Mexico, most of Central and South America, and in the Caribbean, Cuba, the Dominican Republic, Haiti, and Puerto Rico. Latin America is, therefore, defined as all those parts of the Americas that were once part of the Spanish, Portuguese and French Empires. The term is sometimes used more broadly to refer to all of the Americas south of the United States, thus including the Guianas (French Guiana, Guyana, and Suriname), the Anglophone Caribbean (and Belize); the Francophone Caribbean; and the Dutch Caribbean. This definition emphasizes a similar socioeconomic history of the region, which was characterized by formal or informal colonialism, rather than cultural aspects (see, for example, dependency theory). As such, some sources avoid this oversimplification by using the phrase "Latin America and the Caribbean" instead, as in the United Nations geoscheme for the Americas. In a more literal definition, which is close to the semantic origin, Latin America designates countries in the Americas where a Romance language (a language derived from Latin) predominates: Spanish, Portuguese, French, and the creole languages based upon these.The distinction between Latin America and Anglo-America is a convention based on the predominant languages in the Americas by which Romance-language and English-speaking cultures are distinguished. Neither area is culturally or linguistically homogeneous; in substantial portions of Latin America (e.g., highland Peru, Bolivia, Mexico, Guatemala), Native American cultures and, to a lesser extent, Amerindian languages, are predominant, and in other areas, the influence of African cultures is strong (e.g., the Caribbean basin – including parts of Colombia and Venezuela). The term is not without controversy. Historian Mauricio Tenorio-Trillo explores at length the "allure and power" of the idea of Latin America. He remarks at the outset, "The idea of 'Latin America' ought to have vanished with the obsolescence of racial theory... But it is not easy to declare something dead when it can hardly be said to have existed," going on to say, "The term is here to stay, and it is important." Following in the tradition of Chilean writer Francisco Bilbao, who excluded Brazil, Argentina and Paraguay from his early conceptualization of Latin America, Chilean historian Jaime Eyzaguirre has criticized the term Latin America for "disguising" and "diluting" the Spanish character of a region (i.e. Hispanic America) with the inclusion of nations that according to him do not share the same pattern of conquest and colonization.
Latin America can be subdivided into several subregions based on geography, politics, demographics and culture. If defined as all of the Americas south of the United States, the basic geographical subregions are North America, Central America, the Caribbean and South America; the latter contains further politico-geographical subdivisions such as the Southern Cone, the Guianas and the Andean states. It may be subdivided on linguistic grounds into Hispanic America, Portuguese America and French America. *: Not a sovereign state
The earliest known settlement was identified at Monte Verde, near Puerto Montt in Southern Chile. Its occupation dates to some 14,000 years ago and there is some disputed evidence of even earlier occupation. Over the course of millennia, people spread to all parts of the continents. By the first millennium CE, South America's vast rainforests, mountains, plains and coasts were the home of tens of millions of people. The earliest settlements in the Americas are of the Las Vegas Culture from about 8000 BCE and 4600 BCE, a sedentary group from the coast of Ecuador, the forefathers of the more known Valdivia culture, of the same era. Some groups formed more permanent settlements such as the Chibcha (or "Muisca" or "Muysca") and the Tairona groups. These groups are in the circum Caribbean region. The Chibchas of Colombia, the Quechuas and Aymaras of Bolivia were the three indigenous groups that settled most permanently. The region was home to many indigenous peoples and advanced civilizations, including the Aztecs, Toltecs, Maya, and Inca. The golden age of the Maya began about 250, with the last two great civilizations, the Aztecs and Incas, emerging into prominence later on in the early fourteenth century and mid-fifteenth centuries, respectively. The Aztec empire was ultimately the most powerful civilization known throughout the Americas, until its downfall in part by the Spanish invasion.
With the arrival of the Spaniards and Portuguese, the indigenous elites, such as the Incas and Aztecs, were deposed and/or co-opted.. Hernándo Cortés seized the Aztec elite's power in alliance with peoples who had been subjugated by this polity. Francisco Pizarro eliminated the Incan rule in Peru. Both Spain and Portugal colonized and settled the Americas, which along with the rest of the uncolonized world, was divided among them by the line of demarcation in 1494. This treaty wh gave Spain all areas to the west, and Portugal all areas to the east (the Portuguese lands in South America subsequently becoming Brazil). By the end of the sixteenth century Spain and Portugal controlled territory extending from Alaska to the southern tips of the Patagonia. Iberian culture, customs and government were introduced with the settlers who widely intermarried with local populations. The Catholic Religion was the only official religion in all territories under Spanish and Portuguese rule. Epidemics of diseases which came with the Spaniards, such as smallpox and measles, wiped out a large portion of the indigenous population. Historians cannot determine the number of natives who died due to European diseases, but some put the figures as high as 85% and as low as 25%. Due to the lack of written records, specific numbers are hard to verify. Many of the survivors were forced to work in European plantations and mines until indigenous slavery was outlawed with the New Laws of 1542. Unlike in English colonies, Intermixing between the indigenous peoples and Iberian colonists was very common and, by the end of the colonial period, people of mixed ancestry (mestizos) formed majorities in several colonies.
Indigenous peoples of the Americas in various colonies were forced to work in plantations and mines; along with African slaves who were also introduced in the proceeding centuries. The Mita of Colonial Latin America was a system of forced labor imposed on the natives. First established by Viceroy Francisco de Toledo (1569–1581), the Mita was upheld by laws that designated how large draft levies were and how much money the workers would receive that was based on how many shifts each individual worker performed. Toledo established Mitas at Potosi and Huancavelica, where the Mitayos—the workers—would be reduced in number to a fraction of how many were originally assigned before the 1700s. While several villages managed to resist the Mita, others offered payment to colonial administrators as a way out. In exchange, free labor became available through volunteers, though the Mita was kept in place as workers like miners, for example, were paid low wages. The Spanish Crown had not made any ruling on the Mita or approved of it when Toledo first established it in spite of the uncertainty of the practice since the Crown could have gained benefits from it. However, the cortes of Spain later abolished it in 1812 once complaints of the Mita violating humanitarian rights were made. Yet complaints also came from: governors; landowners; native leaders known as Kurakas; and even priests, each of whom preferred other methods of economic exploitation. Despite its fall, the Mita made it to the 1800s.Another important group of slaves to mention were the slaves brought over from Africa. The first slaves came over with Christopher Columbus from the very beginning on his earliest voyages. However in the few hundred years, the Atlantic Slave trade would begin delivering slaves, imported by Spain and other colonizers, by the millions. Many of the large scale productions were run by forced slave labor. They were a part of sugar and coffee production, farming (beans, rice, corn, fruit, etc.), Mining, whale oil and multiple other jobs. Slaves were also house workers, servants, military soldiers, and much more. To say the least these people were property and treated as such. Though indigenous slaves existed, they were no match in quantity and lack of quality jobs when compared to the African slave. The slave population was massive compared to the better known slave ownership in the United States. After 1860 Brazil alone had imported over 4 million slaves, which only represented about 35% of the Atlantic slave trade. Despite the large number of slaves in Latin America, there was not as much reproduction of slaves amongst the population. Because most of the slaves then were African-born, they were more subject to rebellion. The United States involvement in the slave trade is well known amongst North America, however it hides a larger and in some ways crueler operation in the south which had a much longer history.
In 1804, Haiti became the first Latin American nation to gain independence, following a violent slave revolt led by Toussaint L'ouverture on the French colony of Saint-Domingue. The victors abolished slavery. Haitian independence inspired independence movements in Spanish America. By the end of the eighteenth century, Spanish and Portuguese power waned on the global scene as other European powers took their place, notably Britain and France. Resentment grew among the majority of the population in Latin America over the restrictions imposed by the Spanish government, as well as the dominance of native Spaniards (Iberian-born Peninsulares) in the major social and political institutions. Napoleon's invasion of Spain in 1808 marked a turning point, compelling Criollo elites to form juntas that advocated independence. Also, the newly independent Haiti, the second oldest nation in the New World after the United States, further fueled the independence movement by inspiring the leaders of the movement, such as Miguel Hidalgo y Costilla of Mexico, Simón Bolívar of Venezuela and José de San Martín of Argentina, and by providing them with considerable munitions and troops. Fighting soon broke out between juntas and the Spanish colonial authorities, with initial victories for the advocates of independence. Eventually, these early movements were crushed by the royalist troops by 1810, including those of Miguel Hidalgo y Costilla in Mexico in the year 1810. Later on Francisco de Miranda in Venezuela by 1812. Under the leadership of a new generation of leaders, such as Simón Bolívar "The Liberator", José de San Martín of Argentina, and other Libertadores in South America, the independence movement regained strength, and by 1825, all Spanish America, except for Puerto Rico and Cuba, had gained independence from Spain. In the same year in Mexico, a military officer, Agustín de Iturbide, led a coalition of conservatives and liberals who created a constitutional monarchy, with Iturbide as emperor. This First Mexican Empire was short-lived, and was followed by the creation of a republic in 1823.
The Brazilian War of Independence, which had already begun along other independent movements around the region, spread through northern, northeastern regions and in Cisplatina province. With the last Portuguese soldiers surrendering on March 8, 1824, Portugal officially recognized Brazil on August 29, 1825. On April 7, 1831, worn down by years of administrative turmoil and political dissensions with both liberal and conservative sides of politics, including an attempt of republican secession, as well as unreconciled with the way that absolutists in Portugal had given to the succession of King John VI, Pedro I went to Portugal to reclaim his daughter's crown, abdicating the Brazilian throne in favor of his five-year-old son and heir (who thus became the Empire's second monarch, with the regnal title of Dom Pedro II).As the new Emperor could not exert his constitutional powers until he became of age, a regency was set up by the National Assembly. In the absence of a charismatic figure who could represent a moderate face of power, during this period a series of localized rebellions took place, as the Cabanagem, the Malê Revolt, the Balaiada, the Sabinada, and the Ragamuffin War, which emerged from the dissatisfaction of the provinces with the central power, coupled with old and latent social tensions peculiar of a vast, slaveholding and newly independent nation state. This period of internal political and social upheaval, which included the Praieira revolt, was overcome only at the end of the 1840s, years after the end of the regency, which occurred with the premature coronation of Pedro II in 1841.During the last phase of the monarchy, an internal political debate was centered on the issue of slavery. The Atlantic slave trade was abandoned in 1850, as a result of the British' Aberdeen Act, but only in May 1888 after a long process of internal mobilization and debate for an ethical and legal dismantling of slavery in the country, was the institution formally abolished.On November 15, 1889, worn out by years of economic stagnation, in attrition with the majority of Army officers, as well as with rural and financial elites (for different reasons), the monarchy was overthrown by a military coup.
After the independence of many Latin American countries, there was a conflict between the people and the government, much of which can be reduced to the contrasting ideologies between liberalism and conservatism. Conservatism was the dominant system of government prior to the revolutions and it was founded on having social classes, including governing by kings. Liberalists wanted to see a change in the ruling systems, and to move away from monarchs and social classes to promote equality. When liberal Guadalupe Victoria became the first president of Mexico in 1824, conservatists relied on their belief that the state had been better off before the new government came into power, so, by comparison, the old government was better in the eyes of the Conservatives. Following this sentiment, the conservatives pushed to take control of the government, and they succeeded. General Santa Anna was elected president in 1833. The following decade, the Mexican–American War (1846–48) caused Mexico to lose a significant amount of territory to the United States. This loss led to a rebellion by the enraged liberal forces against the conservative government. In 1837, conservative Rafael Carrera conquered Guatemala and separated from the Central American Union. The instability that followed the disintegration of the union led to the independence of the other Central American countries. In Brazil, rural aristocrats were in conflict with the urban conservatives. Portuguese control over Brazilian ports continued after Brazil's independence. Following the conservative idea that the old government was better, urbanites tended to support conservatism because more opportunities were available to them as a result of the Portuguese presence. Simón Bolívar became president of Gran Colombia in 1819 after the region gained independence from Spain. He led a military-controlled state. Citizens did not like the government's position under Bolívar: The people in the military were unhappy with their roles, and the civilians were of the opinion that the military had too much power. After the dissolution of Gran Colombia, New Grenada continued to have conflicts between conservatives and liberals. These conflicts were each concentrated in particular regions, with conservatives particularly in the southern mountains and the Valley of Cauca. In the mid-1840s some leaders in Caracas organized a liberal opposition. Antonio Leocadio Guzman was an active participant and journalist in this movement and gained much popularity among the people of Caracas.In Argentina, the conflict manifested itself as a prolonged civil war between unitarianas (i.e. centralists) and federalists, which were in some aspects respectively analogous to liberals and conservatives in other countries. Between 1832 and 1852, the country existed as a confederation, without a head of state, although the federalist governor of Buenos Aires province, Juan Manuel de Rosas, was given the powers of debt payment and international relations and exerted a growing hegemony over the country. A national constitution was only enacted in 1853, reformed in 1860, and the country reorganized as a federal republic led by a liberal-conservative elite. After Uruguay achieved its independence, in 1828, a similar polarization crystallized between blancos and colorados, where the agrarian conservative interests were pitted against the liberal commercial interests based in Montevideo, and which eventually resulted in the Guerra Grande civil war (1839–1851).
Losing most of its North American colonies at the end of the 18th century left Great Britain in need of new markets to supply resources in the early 19th century. In order to solve this problem, Great Britain turned to the Spanish colonies in South America for resources and markets. In 1806 a small British force surprise attacked the capitol of the viceroyalty in Río de la Plata. As a result, the local garrison protecting the capitol was destroyed in an attempt to defend against the British conquest. The British were able to capture large amounts of precious metals, before a French naval force intervened on behalf of the Spanish King and took down the invading force. However, this caused much turmoil in the area as militia took control of the area from the viceroy. The next year the British attacked once again with a much larger force attempting to reach and conquer Montevideo. They failed to reach Montevideo but succeeded in establishing an alliance with the locals. As a result, the British were able to take control of the Indian markets. This newly gained British dominance hindered the development of Latin American industries and strengthened the dependence on the world trade network. Britain now replaced Spain as the region's largest trading partner. Great Britain invested significant capital in Latin America to develop the area as a market for processed goods. From the early 1820s to 1850, the post-independence economies of Latin American countries were lagging and stagnant. Eventually, enhanced trade among Britain and Latin America led to state development such as infrastructure improvements. These improvements included roads and railroads which grew the trades between countries and outside nations such as Great Britain. By 1870, exports dramatically increased, attracting capital from abroad (including Europe and USA).
Between 1821 and 1910, Mexico battled through various civil wars between the established Conservative government and the Liberal reformists ("Mexico Timeline- Page 2)". On May 8, 1827 Baron Damas, the French Minister of Foreign Affairs, and Sebastián Camacho, a Mexican diplomat, signed an agreement called "The Declarations" which contained provisions regarding commerce and navigation between France and Mexico. At this time the French government did not recognize Mexico as an independent entity. It was not until 1861 that the liberalist rebels, led by Benito Juárez, took control of Mexico City, consolidating liberal rule. However, the constant state of warfare left Mexico with a tremendous amount of debt owed to Spain, England, and France, all of whom funded the Mexican war effort (Neeno). As newly appointed president, Benito Juárez suspended payment of debts for next two years, to focus on a rebuilding and stabilization initiative in Mexico under the new government. On December 8, 1861, Spain, England and France landed in Veracruz to seize unpaid debts from Mexico. However, Napoleon III, with intentions of establishing a French client state to further push his economic interests, pressured the other two powers to withdraw in 1862 (Greenspan; "French Intervention in Mexico…"). France under Napoleon III remained and established Maximilian of Habsburg, Archduke of Austria, as Emperor of Mexico. The march by the French to Mexico City enticed heavy resistance by the Mexican government, it resulted in open warfare. The Battle of Puebla in 1862 in particular presented an important turning point in which Ignacio Zaragoza led the Mexican army to victory as they pushed back the French offensive ("Timeline of the Mexican Revolution"). The victory came to symbolize Mexico's power and national resolve against foreign occupancy and as a result delayed France's later attack on Mexico City for an entire year (Cinco de Mayo (Mexican History)). With heavy resistance by Mexican rebels and the fear of United States intervention against France, forced Napoleon III to withdraw from Mexico, leaving Maximilian to surrender, where he would be later executed by Mexican troops under the rule of Porfirio Díaz. Napoleon III's desire to expand France's economic empire influenced the decision to seize territorial domain over the Central American region. The port city of Veracruz, Mexico and France's desire to construct a new canal were of particular interest. Bridging both New World and East Asian trade routes to the Atlantic were key to Napoleon III's economic goals to the mining of precious rocks and the expansion of France's textile industry. Napoleon's fear of the United States' economic influence over the Pacific trade region, and in turn all New World economic activity, pushed France to intervene in Mexico under the pretense of collecting on Mexico's debt. Eventually France began plans to build the Panama Canal in 1881 until 1904 when the United States took over and proceeded with its construction and implementation ("Read Our Story").
The Monroe Doctrine was included in President James Monroe's 1823 annual message to Congress. The doctrine warns European nations that the United States will no longer tolerate any new colonization of Latin American countries. It was originally drafted to meet the present major concerns, but eventually became the precept of U.S. foreign policy in the Western Hemisphere. The doctrine was put into effect in 1865 when the U.S. government supported Mexican president, Benito Juárez, diplomatically and militarily. Some Latin American countries viewed the U.S. interventions, allowed by the Monroe Doctrine when the U.S. deems necessary, with suspicion.Another important aspect of United States involvement in Latin America is the case of the filibuster William Walker. In 1855, he traveled to Nicaragua hoping to overthrow the government and take the land for the United States. With only the aid of 56 followers, he was able to take over the city of Granada, declaring himself commander of the army and installing Patricio Rivas as a puppet president. However, Rivas's presidency ended when he fled Nicaragua; Walker rigged the following election to ensure that he became the next president. His presidency did not last long, however, as he was met with much opposition from political groups in Nicaragua and neighbouring countries. On May 1, 1857, Walker was forced by a coalition of Central American armies to surrender himself to a United States Navy officer who repatriated him and his followers. When Walker subsequently returned to Central America in 1860, he was apprehended by the Honduran authorities and executed.
The Mexican–American War, another instance of U.S. involvement in Latin America, was a war between the United States and Mexico that started in April 1846 and lasted until February 1848. The main cause of the war was the United States' annexation of Texas in 1845 and a dispute afterwards about whether the border between Mexico and the United States ended where Mexico claimed, at the Nueces River, or ended where the United States claimed, at the Rio Grande. Peace was negotiated between the United States and Mexico with the Treaty of Guadalupe Hidalgo, which stated that Mexico was to cede land which would later become part of California and New Mexico as well as give up all claims to Texas, for which the United States would pay $15,000,000. However, tensions between the two countries were still high and over the next six years things only got worse with raids along the border and attacks by Native Americans against Mexican citizens. To defuse the situation, the United States agreed to purchase 29,670 squares miles of land from Mexico for $10,000,000 so a southern railroad could be built to connect the Pacific and Atlantic coasts. This would become known as the Gadsden Purchase. A critical component of U.S. intervention in Latin American affairs took form in the Spanish–American War, which drastically affected the futures of Cuba and Puerto Rico in the Americas, as well as Guam and the Philippines, by acquiring the majority of the last remaining Spanish colonial possessions.
In the late 19th century and early 20th century, the U.S. banana importing companies United Fruit Company, Cuyamel Fruit Company (both ancestors of Chiquita), and Standard Fruit Company (now Dole), acquired large amounts of land in Central American countries like Guatemala, Honduras, and Costa Rica. The companies gained leverage over the governments and a ruling elite in these countries by dominating their economies and paying kickbacks, and exploited local workers. These countries came to be called banana republics. Cubans, with the aid of Dominicans, launched a war for independence in 1868 and, over the next 30 years, suffered 279,000 losses in a brutal war against Spain that culminated in U.S. intervention. The 1898 Spanish–American War resulted in the end of Spanish colonial presence in the Americas. A period of frequent U.S. intervention in Latin America followed, with the acquisition of the Panama Canal Zone in 1903, the so-called Banana Wars in Cuba, Haiti, Dominican Republic, Nicaragua, and Honduras; the Caco Wars in Haiti; and the so-called Border War with Mexico. Some 3,000 Latin Americans were killed between 1914 and 1933. The U.S. press described the occupation of the Dominican Republic as an 'Anglo-Saxon crusade', carried out to keep the Latin Americans 'harmless against the ultimate consequences of their own misbehavior'.After World War I, U.S. interventionism diminished, culminating in President Franklin D. Roosevelt's Good Neighbor policy in 1933.
The Zimmermann Telegram was a 1917 diplomatic proposal from the German Empire for Mexico to join an alliance with Germany in the event of the United States entering World War I against Germany. The proposal was intercepted and decoded by British intelligence. The revelation of the contents outraged the American public and swayed public opinion. President Woodrow Wilson moved to arm American merchant ships to defend themselves against German submarines, which had started to attack them. The news helped generate support for the United States declaration of war on Germany in April of that year.The message came as a coded telegram dispatched by the Foreign Secretary of the German Empire, Arthur Zimmermann, on January 16, 1917. The message was sent to the German ambassador of Mexico, Heinrich von Eckardt. Zimmermann sent the telegram in anticipation of the resumption of unrestricted submarine warfare by Germany on February 1, an act which Germany presumed would lead to war. The telegram instructed Ambassador Eckardt that if the U.S. appeared certain to enter the war, he was to approach the Mexican Government with a proposal for a military alliance, with funding from Germany. As part of the alliance, Germany would assist Mexico in reconquering Texas and the Southwest. Eckardt was instructed to urge Mexico to help broker an alliance between Germany and Japan. Mexico, in the middle of the Mexican Revolution, far weaker militarily, economically and politically than the U.S., ignored the proposal; after the U.S. entered the war, it officially rejected it.
After World War I, in which Brazil was an ally of the United States, Great Britain, and France, the country realized it needed a more capable army but did not have the technology to create it. In 1919, the French Military Mission was established by the French Commission in Brazil. Their main goal was to contain the inner rebellions in Brazil. They tried to assist the army by bringing them up to the European military standard but constant civil missions did not prepare them for World War II. Brazil's President, Getúlio Vargas, wanted to industrialize Brazil, allowing it to be more competitive with other countries. He reached out to Germany, Italy, France, and the United States to act as trade allies. Many Italian and German people immigrated to Brazil many years before World War II began thus creating a Nazi influence. The immigrants held high positions in government and the armed forces. Brazil continued to try to remain neutral to the United States and Germany because it was trying to make sure it could continue to be a place of interest for both opposing countries. Brazil attended continental meetings in Buenos Aires, Argentina (1936); Lima, Peru (1938); and Havana, Cuba (1940) that obligated them to agree to defend any part of the Americas if they were to be attacked. Eventually, Brazil decided to stop trading with Germany once Germany started attacking offshore trading ships resulting in Germany declaring a blockade against the Americas in the Atlantic Ocean. Furthermore, Germany also ensured that they would be attacking the Americas soon. Once the German submarines attacked unarmed Brazilian trading ships, President Vargas met with the United States President Franklin D. Roosevelt to discuss how they could retaliate. On January 22, 1942, Brazil officially ended all relations with Germany, Japan, and Italy, becoming a part of the Allies. The Brazilian Expeditionary Force was sent to Naples, Italy to fight for democracy. Brazil was the only Latin American country to send troops to Europe. Initially, Brazil wanted to only provide resources and shelter for the war to have a chance of gaining a high postwar status but ended up sending 25,000 men to fight.However, it was not a secret that Vargas had an admiration for Hitler's Nazi Germany and its Führer. He even let German Luftwaffe build secret air forces around Brazil. This alliance with Germany became Brazil's second best trade alliance behind the United States. It was recently found that 9,000 war criminals escaped to South America, including Croats, Ukrainians, Russians and other western Europeans who aided the Nazi war machine. Most, perhaps as many as 5,000, went to Argentina; between 1,500 and 2,000 are thought to have made it to Brazil; around 500 to 1,000 to Chile; and the rest to Paraguay and Uruguay.After World War II, the United States and Latin America continued to have a close relationship. For example, USAID created family planning programs in Latin America combining the NGOs already in place, providing the women in largely Catholic areas access to contraception.
Mexico entered World War II in response to German attacks on Mexican ships. The Potrero del Llano, originally an Italian tanker, had been seized in port by the Mexican government in April 1941 and renamed in honor of a region in Veracruz. It was attacked and crippled by the German submarine U-564 on May 13, 1942. The attack killed 13 of 35 crewmen. On May 20, 1942, a second tanker, Faja de Oro, also a seized Italian ship, was attacked and sunk by the German submarine U-160, killing 10 of 37 crewmen. In response, President Manuel Ávila Camacho and the Mexican government declared war on the Axis powers on May 22, 1942. A large part of Mexico's contribution to the war came through an agreement January 1942 that allowed Mexican nationals living in the United States to join the American armed forces. As many as 250,000 Mexicans served in this way. In the final year of the war, Mexico sent one air squadron to serve under the Mexican flag: the Mexican Air Force's Escuadrón Aéreo de Pelea 201 (201st Fighter Squadron), which saw combat in the Philippines in the war against Imperial Japan. Mexico was the only Latin-American country to send troops to the Asia-Pacific theatre of the war. In addition to those in the armed forces, tens of thousands of Mexican men were hired as farm workers in the United States during the war years through the Bracero program, which continued and expanded in the decades after the war.World War II helped spark an era of rapid industrialization known as the Mexican Miracle. Mexico supplied the United States with more strategic raw materials than any other country, and American aid spurred the growth of industry. President Ávila was able to use the increased revenue to improve the country's credit, invest in infrastructure, subsidize food, and raise wages.
President Federico Laredo Brú led Cuba when war broke out in Europe, though real power belonged to Fulgencio Batista as Chief of Staff of the army. In 1940, Laredo Brú infamously denied entry to 900 Jewish refugees who arrived in Havana aboard the MS St. Louis. After both the United States and Canada likewise refused to accept the refugees, they returned to Europe, where many were eventually murdered in the Holocaust. Batista became president in his own right following the election of 1940. He cooperated with the United States as it moved closer to war against the Axis. Cuba declared war on Japan on December 8, 1941, and on Germany and Italy on December 11.Cuba was an important participant in the Battle of the Caribbean and its navy gained a reputation for skill and efficiency. The navy escorted hundreds of Allied ships through hostile waters, flew thousands of hours on convoy and patrol duty, and rescued over 200 victims of German U-Boat attacks from the sea. Six Cuban merchant ships were sunk by U-boats, taking the lives of around eighty sailors. On May 15, 1943, a squadron of Cuban submarine chasers sank the German submarine U-176 near Cayo Blanquizal. Cuba received millions of dollars in American military aid through the Lend-Lease program, which included air bases, aircraft, weapons, and training. The United States naval station at Guantanamo Bay also served as a base for convoys passing between the mainland United States and the Panama Canal or other points in the Caribbean.The Dominican Republic declared war on Germany and Japan following the attack on Pearl Harbor and the Nazi declaration of war on the US. It did not directly contribute with troops, aircraft, or ships, however 112 Dominicans were integrated into the US military and fought in the war. On May 3, 1942, German submarine U-125 sank Dominican ship San Rafael with 1 torpedo and 32 rounds from the deck gun 50 miles west off Jamaica; 1 was killed, 37 survived. On May 21, 1942, German submarine U-156 sank Dominican ship Presidente Trujillo off Fort-de-France, Martinique; 24 were killed, 15 survived. Rumors of pro-Nazi Dominicans supplying German U-boats with food, water and fuel abounded during the war.
There was a Nazi influence in certain parts of the region, but Jewish migration from Europe during the war continued. Only a few people recognized or knew about the Holocaust. Furthermore, numerous military bases were built during the war by the United States, but some also by the Germans. Even now, unexploded bombs from the second world war that need to be made safe still remain.The only international conflicts since World War II have been the Football War between El Salvador and Honduras (1969), the Cenepa War between Ecuador and Peru (1995), along with Argentina's war with the United Kingdom for control of the Falkland Islands (1982). The Falklands War left 649 Argentines (including 143 conscripted privates) dead and 1,188 wounded, while the UK lost 255 (88 Royal Navy, 27 Royal Marines, 16 Royal Fleet Auxiliary, 123 British Army, and 1 Royal Air Force) dead.
The Great Depression caused Latin America to grow at a slow rate, separating it from leading industrial democracies. The two world wars and U.S. Depression also made Latin American countries favor internal economic development, leading Latin America to adopt the policy of import substitution industrialization. Countries also renewed emphasis on exports. Brazil began selling automobiles to other countries, and some Latin American countries set up plants to assemble imported parts, letting other countries take advantage of Latin America's low labor costs. Colombia began to export flowers, emeralds and coffee grains and gold, becoming the world's second-leading flower exporter. Economic integration was called for, to attain economies that could compete with the economies of the United States or Europe. Starting in the 1960s with the Latin American Free Trade Association and Central American Common Market, Latin American countries worked toward economic integration. In efforts to help regain global economic strength, the U.S. began to heavily assist countries involved in World War II at the expense of Latin America. Markets that were previously unopposed as a result of the war in Latin America grew stagnant as the rest of the world no longer needed their goods.
Large countries like Argentina called for reforms to lessen the disparity of wealth between the rich and the poor, which has been a long problem in Latin America that stunted economic growth.Advances in public health caused an explosion of population growth, making it difficult to provide social services. Education expanded, and social security systems introduced, but benefits usually went to the middle class, not the poor. As a result, the disparity of wealth increased. Increasing inflation and other factors caused countries to be unwilling to fund social development programs to help the poor.
Bureaucratic authoritarianism was practised in Brazil after 1964, in Argentina, and in Chile under Augusto Pinochet, in a response to harsh economic conditions. It rested on the conviction that no democracy could take the harsh measures to curb inflation, reassure investors, and quicken economic growth quickly and effectively. Though inflation fell sharply, industrial production dropped with the decline of official protection.
After World War II and the beginning of a Cold War between the United States and the Soviet Union, US diplomats became interested in Asia, Africa, and Latin America, and frequently waged proxy wars against the Soviet Union in these countries. The US sought to stop the spread of communism. Latin American countries generally sided with the US in the Cold War period, even though they were neglected since the US's concern with communism were focused in Europe and Asia, not Latin America. Between 1946 and 1959 Latin America received only 2% of the United States foreign aid despite having poor conditions similar to the main recipients of The Marshall Plan. Some Latin American governments also complained of the US support in the overthrow of some nationalist governments, and intervention through the CIA. In 1947, the US Congress passed the National Security Act, which created the National Security Council in response to the United States's growing obsession with anti-communism. In 1954, when Jacobo Arbenz of Guatemala accepted the support of communists and attacked holdings of the United Fruit Company, the US decided to assist Guatemalan counter-revolutionaries in overthrowing Arbenz. These interventionist tactics featured the use of the CIA rather than the military, which was used in Latin America for the majority of the Cold War in events including the overthrow of Salvador Allende. Latin America was more concerned with issues of economic development, while the United States focused on fighting communism, even though the presence of communism was small in Latin America.Dominican dictator Rafael Leónidas Trujillo (r. 1930–61) achieved support from the US by becoming Latin America's leading anti-communist. Trujillo extended his tyranny to the USA, and his regime committed multiple murders in New York City. American officials had long recognized that the Dominican Republic's conduct under Trujillo was "below the level of recognized civilian nations, certainly not much above that of the communists." But after Castro's seizure of power in 1959, President Dwight D. Eisenhower concluded that Trujillo had become a Cold War liability. In 1960, Trujillo threatened to align with the Communist world in response to US and Latin American rejection of his regime. La Voz Dominicana and Radio Caribe began attacking the US in Marxian terms, and the Dominican Communist party was legalized. Trujillo also unsuccessfully attempted to establish contacts and relations with the Soviet Bloc. In 1961, Trujillo was murdered with weapons supplied by the CIA. Ramfis Trujillo, the dictator's son, remained in de facto control of the government for the next six months through his position as commander of the armed forces. Trujillo's brothers, Hector Bienvenido and Jose Arismendi Trujillo, returned to the country and began immediately to plot against President Balaguer. On 18 November 1961, as a planned coup became more evident, US Secretary of State Dean Rusk issued a warning that the United States would not "remain idle" if the Trujillos attempted to "reassert dictatorial domination" over the Dominican Republic. Following this warning, and the arrival of a fourteen-vessel US naval task force within sight of Santo Domingo, Ramfis and his uncles fled the country on 19 November with $200 million from the Dominican treasury.
By 1959, Cuba was afflicted with a corrupt dictatorship under Batista, and Fidel Castro ousted Batista that year and set up the first communist state in the hemisphere. The United States imposed a trade embargo on Cuba, and combined with Castro's expropriation of private enterprises, this was detrimental to the Cuban economy. Around Latin America, rural guerrilla conflict and urban terrorism increased, inspired by the Cuban example. The United States put down these rebellions by supporting Latin American countries in their counter-guerrilla operations through the Alliance for Progress launched by President John F. Kennedy. This thrust appeared to be successful. A Marxist, Salvador Allende, became president of Chile in 1970, but was overthrown three years later in a military coup backed by the United States. Despite civil war, high crime and political instability, most Latin American countries eventually adopted bourgeois liberal democracies while Cuba maintained its socialist system.
Encouraged by the success of Guatemala in the 1954 Guatemalan coup d'état, in 1960, the U.S. decided to support an attack on Cuba by anti-Castro rebels. The Bay of Pigs invasion was an unsuccessful invasion of Cuba in 1961, financed by the U.S. through the CIA, to overthrow Fidel Castro. The incident proved to be very embarrassing for the new Kennedy administration.The failure of the invasion led to a Soviet-Cuban alliance.
President John F. Kennedy initiated the Alliance for Progress in 1961, to establish economic cooperation between the U.S. and Latin America. The Alliance would provide $20 billion for reform in Latin America, and counterinsurgency measures. Instead, the reform failed because of the simplistic theory that guided it and the lack of experienced American experts who could understand Latin American customs.
Armed Cuban intervention overseas began on June 14, 1959 with an invasion of the Dominican Republic by a group of fifty-six men, who landed a C-56 transport aircraft at the military airport of the town of Constanza. Upon their landing, the fifteen-man Dominican garrison began an ongoing gun battle with the invaders, until the survivors disappeared into the surrounding mountains. Immediately after, the Dominican Air Force bombed the area around Constanza with British made Vampire jets in an unsuccessful attempt to kill the invaders, which instead killed civilians. The invaders either died at the hands of machete-swinging peasants, or the military captured, tortured, and imprisoned them. A week later, two yachts offloaded 186 invaders onto Chris-Craft launches for a landing on the north coast. Dominican Air Force pilots fired rockets from their Vampire jets into the approaching launches, killing most of the invaders. The survivors were brutally tortured and murdered. From 1966 until the late 1980s, the Soviet government upgraded Cuba's military capabilities, and Castro saw to it that Cuba assisted with the independence struggles of several countries across the world, most notably Angola and Mozambique in southern Africa, and the anti-imperialist struggles of countries such as Syria, Algeria, Venezuela, Bolivia, and Vietnam.South Africa developed nuclear weapons due to the threat to its security posed by the presence of large numbers of Cuban troops in Angola and Mozambique. In November 1975, Cuba poured more than 65,000 troops into Angola in one of the fastest military mobilizations in history. On November 10, 1975, Cuban forces defeated the National Liberation Front of Angola (FNLA) in the Battle of Quifangondo. On November 25, 1975, as the South African Defence Force (SADF) tried to cross a bridge, Cubans hidden along the banks of the river attacked, destroying seven armored cars and killing upwards of 90 enemy soldiers. On March 27, 1976, the last South African troops withdrew from Angola. In September 1977, 12 MiG-21s conducted strafing flights over Puerto Plata in Dominican Republic to warn then president Joaquín Balaguer against intercepting Cuban warships headed to or returning from Angola. In 1988, Cuba returned to Angola with a vengeance. The crisis began in 1987 with an assault by Soviet-equipped national army troops against the pro-Western rebel movement UNITA in southeastern Angola. Soon, the SADF invaded to support the beleaguered US-backed faction and the Angolan offensive stalled. Cuba reinforced its African ally with 55,000 troops, tanks, artillery and MiG-23s, prompting Pretoria to call up 140,000 reservists. In June 1988, SADF armor and artillery engaged FAPLA-Cuban forces at Techipa, killing 290 Angolans and 10 Cubans. In retaliation, Cuban warplanes hammered South African troops. However, both sides quickly pulled back to avoid an escalation of hostilities. The Battle of Cuito Cuanavale stalemated, and a peace treaty was signed in September 1988. Within two years, the Cold War was over and Cuba's foreign policy shifted away from military intervention.
Following the American occupation of Nicaragua in 1912, as part of the Banana Wars, the Somoza family political dynasty came to power, and would rule Nicaragua until their ouster in 1979 during the Nicaraguan Revolution. The era of Somoza family rule was characterized by strong U.S. support for the government and its military as well as a heavy reliance on U.S.-based multi-national corporations. The Nicaraguan Revolution (Spanish: Revolución Nicaragüense or Revolución Popular Sandinista) encompassed the rising opposition to the Somoza dictatorship in the 1960s and 1970s, the campaign led by the Sandinista National Liberation Front (FSLN) to violently oust the dictatorship in 1978–79, the subsequent efforts of the FSLN to govern Nicaragua from 1979 until 1990 and the Contra War which was waged between the FSLN and the Contras from 1981 to 1990. The Revolution marked a significant period in Nicaraguan history and revealed the country as one of the major proxy war battlegrounds of the Cold War with the events in the country rising to international attention. Although the initial overthrow of the Somoza regime in 1978–79 was a bloody affair, the Contra War of the 1980s took the lives of tens of thousands of Nicaraguans and was the subject of fierce international debate. During the 1980s both the FSLN (a leftist collection of political parties) and the Contras (a rightist collection of counter-revolutionary groups) received large amounts of aid from the Cold War super-powers (respectively, the Soviet Union and the United States).
The set of specific economic policy prescriptions that were considered the "standard" reform package were promoted for crisis-wracked developing countries by Washington, D.C.-based institutions such as the International Monetary Fund (IMF), World Bank, and the US Department of the Treasury during the 1980s and 1990s. In recent years, several Latin American countries led by socialist or other left wing governments – including Argentina and Venezuela – have campaigned for (and to some degree adopted) policies contrary to the Washington Consensus set of policies. (Other Latin countries with governments of the left, including Brazil, Mexico, Chile and Peru, have in practice adopted the bulk of the policies.) Also critical of the policies as actually promoted by the International Monetary Fund have been some US economists, such as Joseph Stiglitz and Dani Rodrik, who have challenged what are sometimes described as the "fundamentalist" policies of the International Monetary Fund and the US Treasury for what Stiglitz calls a "one size fits all" treatment of individual economies. The term has become associated with neoliberal policies in general and drawn into the broader debate over the expanding role of the free market, constraints upon the state, and US influence on other countries' national sovereignty. This politico-economical initiative was institutionalized in North America by 1994 NAFTA, and elsewhere in the Americas through a series of like agreements. The comprehensive Free Trade Area of the Americas project, however, was rejected by most South American countries at the 2005 4th Summit of the Americas.
In most countries, since the 2000s left-wing political parties have risen to power. The presidencies of Hugo Chávez in Venezuela, Ricardo Lagos and Michelle Bachelet in Chile, Lula da Silva and Dilma Rousseff in Brazil, Néstor Kirchner and his wife Cristina Fernández in Argentina, Tabaré Vázquez and José Mujica in Uruguay, Evo Morales in Bolivia, Daniel Ortega in Nicaragua, Rafael Correa in Ecuador, Fernando Lugo in Paraguay, Manuel Zelaya in Honduras (removed from power by a coup d'état), Mauricio Funes and Salvador Sánchez Cerén in El Salvador are all part of this wave of left-wing politicians who often declare themselves socialists, Latin Americanists, or anti-imperialists (often implying opposition to US policies towards the region). A development of this has been the creation of the eight-member ALBA alliance, or "The Bolivarian Alliance for the Peoples of Our America" (Spanish: Alianza Bolivariana para los Pueblos de Nuestra América) by some of the countries already mentioned. By June 2014, Honduras (Juan Orlando Hernández), Guatemala (Otto Pérez Molina), and Panama (Ricardo Martinelli) had right-wing governments.
In 1982, Mexico announced that it could not meet its foreign debt payment obligations, inaugurating a debt crisis that would "discredit" Latin American economies throughout the decade. This debt crisis would lead to neoliberal reforms that would instigate many social movements in the region. A "reversal of development" reigned over Latin America, seen through negative economic growth, declines in industrial production, and thus, falling living standards for the middle and lower classes. Governments made financial security their primary policy goal over social security, enacting new neoliberal economic policies that implemented privatization of previously national industries and informalization of labor. In an effort to bring more investors to these industries, these governments also embraced globalization through more open interactions with the international economy. Significantly, as democracy spread across much of Latin America, the realm of government became more inclusive (a trend that proved conducive to social movements), the economic ventures remained exclusive to a few elite groups within society. Neoliberal restructuring consistently redistributed income upward while denying political responsibility to provide social welfare rights, and though development projects took place throughout the region, both inequality and poverty increased. Feeling excluded from these new projects, the lower classes took ownership of their own democracy through a revitalization of social movements in Latin America. Both urban and rural populations had serious grievances as a result of the above economic and global trends and have voiced them in mass demonstrations. Some of the largest and most violent of these have been protests against cuts in urban services, such as the Caracazo in Venezuela and the Argentinazo in Argentina. Rural movements have made diverse demands related to unequal land distribution, displacement at the hands of development projects and dams, environmental and indigenous concerns, neoliberal agricultural restructuring, and insufficient means of livelihood. These movements have benefited considerably from transnational support from conservationists and INGOs. The Movement of Rural Landless Workers (MST) is perhaps the largest contemporary Latin American social movement. As indigenous populations are primarily rural, indigenous movements account for a large portion of rural social movements, including the Zapatista rebellion in Mexico, the Confederation of Indigenous Nationalities of Ecuador (CONAIE), indigenous organizations in the Amazon region of Ecuador and Bolivia, pan-Mayan communities in Guatemala, and mobilization by the indigenous groups of Yanomami peoples in the Amazon, Kuna peoples in Panama, and Altiplano Aymara and Quechua peoples in Bolivia. Other significant types of social movements include labor struggles and strikes, such as recovered factories in Argentina, as well as gender-based movements such as the Mothers of the Plaza de Mayo in Argentina and protests against maquila production, which is largely a women's issue because of how it draws on women for cheap labor.
The 2000s commodities boom caused positive effects for many Latin American economies. Another trend is the rapidly increasing importance of the relations with China.With the end of the commodity boom in the 2010s, economic stagnation or recession resulted in some countries. As a result, the left-wing governments of the Pink Tide lost support. The worst-hit was Venezuela, which is facing severe social and economic upheaval. The corruption scandal of Odebrecht, a Brazilian conglomerate, has raised allegations of corruption across the region's governments (see Operation Car Wash). The bribery ring has become the largest corruption scandal in Latin American history. As of July 2017, the highest ranking politicians charged were former Brazilian President Luiz Inácio Lula da Silva (arrested) and former Peruvian presidents Ollanta Humala (arrested) and Alejandro Toledo (fugitive, fled to the US).
The following is a list of the ten largest metropolitan areas in Latin America.
The inhabitants of Latin America are of a variety of ancestries, ethnic groups, and races, making the region one of the most diverse in the world. The specific composition varies from country to country: some have a predominance of European-Amerindian or more commonly referred to as Mestizo or Castizo depending on the admixture, population; in others, Amerindians are a majority; some are dominated by inhabitants of European ancestry; and some countries' populations are primarily Mulatto. Various black, Asian and Zambo (mixed black and Amerindian) minorities are also identified regularly. People with European ancestry are the largest single group, and along with people of part-European ancestry, they combine to make up approximately 80% of the population, or even more.According to Jon Aske: Before Hispanics became such a 'noticeable' group in the U.S., the distinction between black and white was the major racial division and according to the one-drop rule adhered to by the culture at large, one drop of African ancestry usually meant that the person was Black. ... The notion of racial continuum and a separation of race (or skin color) and ethnicity, on the other hand, is the norm in most of Latin America. In the Spanish and Portuguese empires, racial mixing or miscegenation was the norm and something that the Spanish and Portuguese had grown rather accustomed to during the hundreds of years of contact with Arabs and North Africans in the Iberian peninsula. But, demographics may have made this inevitable as well. Thus, for example, of the approximately 13.5 million people who lived in the Spanish colonies in 1800 before independence only about one fifth were white. This contrasts with the U.S., where more than four-fifths were whites (out of a population of 5.3 million in 1801, 900,000 were slaves, plus approximately 60,000 free blacks). ... The fact of the recognition of a racial continuum in Hispanic American (sic) does not mean that there wasn't discrimination, which there was, or that there wasn't an obsession with race, or 'castes', as they were sometimes called. ... In areas with large indigenous Amerindian populations, a racial mixture resulted, which is known in Spanish as mestizos ... who are a majority in Mexico, Central America and most of South America. Similarly, when African slaves were brought to the Caribbean region and Brazil, where there was very little indigenous presence left, unions between them and Spanish produced a population of mixed mulatos ... who are a majority of the population in many of those Spanish-speaking Caribbean basin countries (Cuba, Dominican Republic, Puerto Rico, Colombia, and Venezuela).Aske has also written that: Spanish colonization was rather different from later English, or British, colonization of North America. They had different systems of colonization and different methods of subjugation. While the English were primarily interested in grabbing land, the Spanish in addition had a mandate to incorporate the land's inhabitants into their society, something which was achieved by religious conversion and sexual unions which produced a new 'race' of mestizos, a mixture of Europeans and indigenous peoples. mestizos (sic) form the majority of the population in Mexico, Central America, and much of South America. Racial mixing or miscegenation, after all, was something that the Spanish and Portuguese had been accustomed to during the hundreds of years of contact with Arabs and North Africans. Similarly, later on, when African slaves were introduced into the Caribbean basin region, unions between them and Spaniards produced a population of mulatos, who are a majority of the population in the Caribbean islands (the Antilles) (Cuba, Dominican Republic, Puerto Rico), as well as other areas of the Caribbean region (Colombia, Venezuela and parts of the Central American Caribbean coast). mestizos (sic) and mulatos may not have always have been first class citizens in their countries, but they were never disowned in the way the outcomes of unions of Europeans and Native Americans were in the British colonies, where interracial marriages were taboo and one drop of Black or Amerindian blood was enough to make the person 'impure'.In his famous 1963 book The Rise of the West, William Hardy McNeill wrote that: Racially mixed societies arose in most of Spanish and Portuguese America, compounded in varying proportions from European, Indian, and Negro strands. Fairly frequent resort to manumission mitigated the hardships of slavery in those areas; and the Catholic church positively encouraged marriages between white immigrants and Indian women as a remedy for sexual immorality. However, in the southern English colonies and in most of the Caribbean islands, the importation of Negro slaves created a much more sharply polarized biracial society. Strong race feeling and the servile status of nearly all Negroes interdicted intermarriage, practically if not legally. Such discrimination did not prevent interbreeding; but children of mixed parentage were assigned to the status of their mothers. Mulattoes and Indian half-breeds were thereby excluded from the white community. In Spanish (and, with some differences, Portuguese) territories a more elaborate and less oppressive principle of racial discrimination established itself. The handful of persons who had been born in the homelands claimed topmost social prestige; next came those of purely European descent; while beneath ranged the various racial blends to form a social pyramid whose numerous racial distinctions meant that no one barrier could become as ugly and inpenetrable as that dividing whites from Negroes in the English, Dutch, and French colonies.Thomas C. Wright, meanwhile, has written that: The demographic makeup of colonial Latin America became more complex when, as the native population declined, the Portuguese, Spanish, and the French in Haiti turned to Africa for labor, as did the British in North America. The tricontinental heritage that characterizes Latin America, then, is shared by the United States, but even a casual examination reveals that the outcome of the complex interaction of different peoples has varied. While miscegenation among the three races certainly occurred in North America, it appears to have been much less common than in Latin America. Furthermore, offspring of such liaisons were not recognized as belonging to new, distinct racial categories in North America as they were in Latin America. The terms mestizo or mameluco, mulatto, the general term castas, and dozens of subcategories of racial identity frankly recognized the outcomes of interracial sexual activity in Latin America and established a continuum of race rather than the unrealistic absolute categories of white, black, or Indian as used in the United States. (The U.S. Census Bureau's forms did not allow individuals to list more than one race until 2000.)
Spanish is the predominant language of Latin America. It is spoken as first language by about 60% of the population. Portuguese is spoken by about 30%, and about 10% speak other languages such as Quechua, Mayan languages, Guaraní, Aymara, Nahuatl, English, French, Dutch and Italian. Portuguese is spoken only in Brazil (Brazilian Portuguese), the biggest and most populous country in the region. Spanish is the official language of most of the rest of the countries and territories on the Latin American mainland (Spanish language in the Americas), as well as in Cuba, Puerto Rico (where it is co-official with English), and the Dominican Republic. French is spoken in Haiti and in the French overseas departments of Guadeloupe, Martinique and Guiana. It is also spoken by some Panamanians of Afro-Antillean descent. Dutch is the official language in Suriname, Aruba, Curaçao, and the Netherlands Antilles. (As Dutch is a Germanic language, these territories are not necessarily considered part of Latin America.) However, the native language of Aruba, Bonaire, and Curaçao, is Papiamento, a creole language largely based on Portuguese and Spanish and has a considerable influence coming from the Dutch language and Portuguese-based creole languages. Amerindian languages are widely spoken in Peru, Guatemala, Bolivia, Paraguay and Mexico, and to a lesser degree, in Panama, Ecuador, Brazil, Colombia, Venezuela, Argentina, and Chile amongst other countries. In Latin American countries not named above, the population of speakers of indigenous languages tend to be very small or even non-existent (e.g. Uruguay). Mexico is possibly the only country that contains a wider variety of indigenous languages than any Latin American country, but the most spoken language is Nahuatl. In Peru, Quechua is an official language, alongside Spanish and any other indigenous language in the areas where they predominate. In Ecuador, while holding no official status, the closely related Quichua is a recognized language of the indigenous people under the country's constitution; however, it is only spoken by a few groups in the country's highlands. In Bolivia, Aymara, Quechua and Guaraní hold official status alongside Spanish. Guaraní, along with Spanish, is an official language of Paraguay, and is spoken by a majority of the population (who are, for the most part, bilingual), and it is co-official with Spanish in the Argentine province of Corrientes. In Nicaragua, Spanish is the official language, but on the country's Caribbean coast English and indigenous languages such as Miskito, Sumo, and Rama also hold official status. Colombia recognizes all indigenous languages spoken within its territory as official, though fewer than 1% of its population are native speakers of these languages. Nahuatl is one of the 62 native languages spoken by indigenous people in Mexico, which are officially recognized by the government as "national languages" along with Spanish. Other European languages spoken in Latin America include: English, by some groups in Puerto Rico, as well as in nearby countries that may or may not be considered Latin American, like Belize and Guyana, and spoken by descendants of British settlers in Argentina & Chile; German, in southern Brazil, southern Chile, portions of Argentina, Venezuela and Paraguay; Italian, in Brazil, Argentina, Venezuela, and Uruguay; Ukrainian, Polish and Russian in southern Brazil and Argentina; and Welsh, in southern Argentina.Yiddish and Hebrew are possible to be heard around Buenos Aires and São Paulo especially. Non-European or Asian languages include Japanese in Brazil, Peru, Bolivia, and Paraguay, Korean in Brazil, Argentina, Paraguay, and Chile, Arabic in Argentina, Brazil, Colombia, Venezuela, and Chile, and Chinese throughout South America. In several nations, especially in the Caribbean region, creole languages are spoken. The most widely spoken creole language in Latin America and the Caribbean is Haitian Creole, the predominant language of Haiti; it is derived primarily from French and certain West African tongues with Amerindian, English, Portuguese and Spanish influences as well. Creole languages of mainland Latin America, similarly, are derived from European languages and various African tongues. The Garifuna language is spoken along the Caribbean coast in Honduras, Guatemala, Nicaragua and Belize mostly by the Garifuna people a mixed race Zambo people who were the result of mixing between Indigenous Caribbeans and escaped Black slaves. Primarily an Arawakan language, it has influences from Caribbean and European languages. Archaeologists have deciphered over 15 pre-Columbian distinct writing systems from mesoamerican societies. the ancient Maya had the most sophisticated textually written language, but since texts were largely confined to the religious and administrative elite, traditions were passed down orally. oral traditions also prevailed in other major indigenous groups including, but not limited to the Aztecs and other Nahuatl speakers, Quechua and Aymara of the Andean regions, the Quiché of Central America, the Tupi-Guaraní in today's Brazil, the Guaraní in Paraguay and the Mapuche in Chile.
The vast majority of Latin Americans are Christians (90%), mostly Roman Catholics belonging to the Latin Church. About 70% of the Latin American population consider themselves Catholic. Latin America constitute in absolute terms the second world's largest Christian population, after Europe.According to the detailed Pew multi-country survey in 2014, 69% of the Latin American population is Catholic and 19% is Protestant. Protestants are 26% in Brazil and over 40% in much of Central America. More than half of these are converts from Roman Catholicism.
Due to economic, social and security developments that are affecting the region in recent decades, the focus is now the change from net immigration to net emigration. About 10 million Mexicans live in the United States. 31.7 million Americans listed their ancestry as Mexican as of 2010, or roughly 10% of the population.During the initial stage of the Spanish colonization of the Philippines which were around the 1600s, about 16,500 soldiers levied from Peru and Mexico were sent together with 600 Spanish officers to fight wars, settle, colonize and build cities and presidios in the Philippines. These 16,500 Peruvians and Mexicans supplemented the Native Malay Population which then reached 667,612 people. This initial group of Latin American soldier-settler founders had spread their genes among the sparsely populated Philippines. This resulted into a spread of Latin American admixture among Filipinos as evidenced by a large number of Filipinos possessing Native American ancestry. A Y-DNA compilation organized by the Genetic Company "Applied Biosystems" found that 13.33% of the Filipino Male Population sampled from across the country had Y-DNA of Latin American and Spanish origins.Furthermore, according to a survey dated from 1870 conducted by German ethnologist Fedor Jagor of the population of Luzon island (Which holds half the citizens of the Philippines) 1/3rd of the people possess varying degrees of Spanish and Latin American ancestry. According to the 2005 Colombian census or DANE, about 3,331,107 Colombians currently live abroad.The number of Brazilians living overseas is estimated at about 2 million people. An estimated 1.5 to two million Salvadorans reside in the United States. At least 1.5 million Ecuadorians have gone abroad, mainly to the United States and Spain. Approximately 1.5 million Dominicans live abroad, mostly in the United States. More than 1.3 million Cubans live abroad, most of them in the United States. It is estimated that over 800,000 Chileans live abroad, mainly in Argentina, the United States, Canada, Australia and Sweden. An estimated 700,000 Bolivians were living in Argentina as of 2006 and another 33,000 in the United States.Japanese Brazilian immigrants to Japan numbered 250,000 in 2004, constituting Japan's second-largest immigrant population. Their experiences bear similarities to those of Japanese Peruvian immigrants, who are often relegated to low income jobs typically occupied by foreigners. Central Americans living abroad in 2005 were 3,314,300, of which 1,128,701 were Salvadorans, 685,713 were Guatemalans, 683,520 were Nicaraguans, 414,955 were Hondurans, 215,240 were Panamanians and 127,061 were Costa Ricans.For the period 2000–2005, Chile, Costa Rica, Panama, and Venezuela were the only countries with global positive migration rates, in terms of their yearly averages.As a result of the 2010 Haiti Earthquake and its social and economic impact, there was a significant migration of Haitians to other Latin American countries. During the presidency of Hugo Chávez and his successor Nicolás Maduro, over 3.2 million people fled Venezuela during the Venezuelan refugee crisis as socioeconomic conditions and the quality of life worsened.The countries of Latin America seek to strengthen links between migrants and their states of origin, while promoting their integration in the receiving state. These Emigrant Policies focus on the rights, obligations and opportunities for participation of emigrated citizens who already live outside the borders of the country of origin. Research on Latin America shows that the extension of policies towards migrants is linked to a focus on civil rights and state benefits that can positively influence integration in recipient countries. In addition, the tolerance of dual citizenship has spread more in Latin America than in any other region of the world.
Despite significant progress, education access and school completion remains unequal in Latin America. The region has made great progress in educational coverage; almost all children attend primary school and access to secondary education has increased considerably. Quality issues such as poor teaching methods, lack of appropriate equipment and overcrowding exist throughout the region. These issues lead to adolescents dropping out of the educational system early. Most educational systems in the region have implemented various types of administrative and institutional reforms that have enabled reach for places and communities that had no access to education services in the early 1990s. Compared to prior generations, Latin American youth have seen an increase in their levels of education. On average, they have completed two years schooling more than their parents.However, there are still 23 million children in the region between the ages of 4 and 17 outside of the formal education system. Estimates indicate that 30% of preschool age children (ages 4–5) do not attend school, and for the most vulnerable populations, the poor and rural, this calculation exceeds 40 percent. Among primary school age children (ages 6 to 12), coverage is almost universal; however there is still a need to incorporate 5 million children in the primary education system. These children live mostly in remote areas, are indigenous or Afro-descendants and live in extreme poverty.Among people between the ages of 13 and 17 years, only 80% are full-time students in the education system; among them only 66% advance to secondary school. These percentages are lower among vulnerable population groups: only 75% of the poorest youth between the ages of 13 and 17 years attend school. Tertiary education has the lowest coverage, with only 70% of people between the ages of 18 and 25 years outside of the education system. Currently, more than half of low income children or living in rural areas fail to complete nine years of education.
Latin America and the Caribbean have been cited by numerous sources to be the most dangerous regions in the world. Studies have shown that Latin America contains the majority of the world's most dangerous cities. Many analysts attribute the reason to why the region has such an alarming crime rate and criminal culture is largely due to social and income inequality within the region, they say that growing social inequality is fueling crime in the region. Many agree that the prison crisis will not be resolved until the gap between the rich and the poor is addressed. Crime and violence prevention and public security are now important issues for governments and citizens in Latin America and the Caribbean region. Homicide rates in Latin America are the highest in the world. From the early 1980s through the mid-1990s, homicide rates increased by 50 percent. Latin America and the Caribbean experienced more than 2.5 million murders between 2000 and 2017. There were a total of 63,880 murders in Brazil in 2018.The major victims of such homicides are young men, 69 percent of whom are between the ages of 15 and 19 years old. Countries with the highest homicide rate per year per 100,000 inhabitants as of 2015 were: El Salvador 109, Honduras 64, Venezuela 57, Jamaica 43, Belize 34.4, St. Kitts and Nevis 34, Guatemala 34, Trinidad and Tobago 31, the Bahamas 30, Brazil 26.7, Colombia 26.5, the Dominican Republic 22, St. Lucia 22, Guyana 19, Mexico 16, Puerto Rico 16, Ecuador 13, Grenada 13, Costa Rica 12, Bolivia 12, Nicaragua 12, Panama 11, Antigua and Barbuda 11, and Haiti 10. Most of the top countries with the highest homicide rates are in Africa and Latin America. Countries in Central America, like El Salvador and Honduras, top the list of homicides in the world.Brazil has more overall homicides than any country in the world, at 50,108, accounting for one in 10 globally. Crime-related violence in Latin America represents the most threat to public health, striking more victims than HIV/AIDS or other infectious diseases. Countries with lowest homicide rate per year per 100,000 inhabitants as of 2015 were: Chile 3, Peru 7, Argentina 7, Uruguay 8 and Paraguay 9.
According to Goldman Sachs' BRICS review of emerging economies, by 2050 the largest economies in the world will be as follows: China, United States, India, Japan, Germany, United Kingdom, Brazil and Mexico.
Over the past two centuries, Latin America's GDP per capita has fluctuated around world average. However, there is a substantial gap between Latin America and the developed economies. In the Andean region this gap can be a consequence of low human capital among Inca Indios in Pre-Columbian times. It is evident that the numeracy value of Peruvian Indios in the early 16th century was just half of the numeracy of the Spanish and Portuguese. Between 1820 and 2008, this gap widened from 0.8 to 2.7 times. Since 1980, Latin America also lost growth versus the world average. Many nations such as those in Asia have joined others on a rapid economic growth path, but Latin America has grown at slower pace and its share of world output declined from 9.5% in 1980 to 7.8% in 2008.
Latin America is the region with the highest levels of income inequality in the world. The following table lists all the countries in Latin America indicating a valuation of the country's Human Development Index, GDP at purchasing power parity per capita, measurement of inequality through the Gini index, measurement of poverty through the Human Poverty Index, measurement of extreme poverty based on people living under 1.25 dollars a day, life expectancy, murder rates and a measurement of safety through the Global Peace Index. Green cells indicate the best performance in each category while red indicates the lowest.
Wealth inequality in Latin America and the Caribbean remains a serious issue despite strong economic growth and improved social indicators over the past decade. A report released in 2013 by the UN Department of Economic and Social Affairs entitled Inequality Matters. Report of the World Social Situation, observed that: ‘Declines in the wage share have been attributed to the impact of labour-saving technological change and to a general weakening of labour market regulations and institutions. Such declines are likely to affect individuals in the middle and bottom of the income distribution disproportionately, since they rely mostly on labour income.’ In addition, the report noted that ‘highly-unequal land distribution has created social and political tensions and is a source of economic inefficiency, as small landholders frequently lack access to credit and other resources to increase productivity, while big owners may not have had enough incentive to do so.
The major trade blocs (or agreements) in the region are the Pacific Alliance and Mercosur. Minor blocs or trade agreements are the G3 Free Trade Agreement, the Dominican Republic – Central America Free Trade Agreement (DR-CAFTA), the Caribbean Community (CARICOM) and the Andean Community of Nations (CAN). However, major reconfigurations are taking place along opposing approaches to integration and trade; Venezuela has officially withdrawn from both the CAN and G3 and it has been formally admitted into the Mercosur (pending ratification from the Paraguayan legislature). The president-elect of Ecuador has manifested his intentions of following the same path. This bloc nominally opposes any Free Trade Agreement (FTA) with the United States, although Uruguay has manifested its intention otherwise. Chile, Peru, Colombia and Mexico are the only four Latin American nations that have an FTA with the United States and Canada, both members of the North American Free Trade Agreement (NAFTA).
Latin American culture is a mixture of many cultural expressions worldwide. It is the product of many diverse influences: Indigenous cultures of the people who inhabited the continent prior to European Colonization. Ancient and very advanced civilizations developed their own political, social and religious systems. The Mayas, the Aztecs and the Incas are examples of these. Indigenous legacies in music, dance, foods, arts and crafts, clothing, folk culture and traditions are very strong in Latin America. Linguistic effects on Spanish and Portuguese are also marked, such as in terms like pampa, taco, tamale, cacique. Western civilization, in particular the culture of Europe, was brought mainly by the colonial powers – the Spanish, Portuguese and French – between the 16th and 19th centuries. The most enduring European colonial influence is language and Roman Catholicism. More recently, additional cultural influences came from the United States and Europe during the nineteenth and twentieth centuries, due to the growing influence of the former on the world stage and immigration from the latter. The influence of the United States is particularly strong in northern Latin America, especially Puerto Rico, which is an American territory. Prior to 1959, Cuba, who fought for its independence along American soldiers in the Spanish–American War, was also known to have a close socioeconomic relation with the United States. In addition, the United States also helped Panama become an independent state from Colombia and built the twenty-mile-long Panama Canal Zone in Panama which held from 1903 (the Panama Canal opened to transoceanic freight traffic in 1914) to 1999, when the Torrijos-Carter Treaties restored Panamanian control of the Canal Zone. South America experienced waves of immigration of Europeans, especially Italians, Spaniards, Portuguese, Germans, Austrians, Poles, Ukrainians, French, Dutch, Russians, Croatians, Lithuanians and Ashkenazi Jews. With the end of colonialism, French culture was also able to exert a direct influence in Latin America, especially in the realms of high culture, science and medicine. This can be seen in any expression of the region's artistic traditions, including painting, literature and music, and in the realms of science and politics.Due to the impact of Enlightenment ideals after the French revolution, a certain number of Iberian-American countries decriminalized homosexuality after France and French territories in the Americas in 1791. Some of the countries that abolished sodomy laws or banned any reference to state interference in consensual adult sexuality in the 19th century were Dominican Republic (1822), Brazil (1824), Peru (1836), Mexico (1871), Paraguay (1880), Argentina (1887), Honduras (1899), Guatemala and El Salvador. Today same-sex marriage is legal in Argentina, Brazil, Colombia, Costa Rica, Ecuador, Uruguay, and French overseas departments, as well as in several states of Mexico. Civil unions can be held in Chile. African cultures, whose presence derives from a long history of New World slavery. Peoples of African descent have influenced the ethno-scapes of Latin America and the Caribbean. This is manifested for instance in music, dance and religion, especially in countries like Brazil, Puerto Rico, Venezuela, Colombia, Panama, Haiti, Costa Rica, Dominican Republic, and Cuba. Asian cultures, whose part of the presence derives from the long history of the Coolie trade mostly arriving during the 19th and 20th centuries, and most commonly Chinese workers in Peru and Venezuela. But also from Japanese and Korean immigration especially headed to Brazil. This has largely affected the cuisine, traditions including literature, art and lifestyles and politics. The effects of Asian influences have especially and mostly effected the nations of Brazil, Cuba, Panama and Peru.
Beyond the rich tradition of indigenous art, the development of Latin American visual art owed much to the influence of Spanish, Portuguese and French Baroque painting, which in turn often followed the trends of the Italian Masters. In general, this artistic Eurocentrism began to fade in the early twentieth century, as Latin Americans began to acknowledge the uniqueness of their condition and started to follow their own path. From the early twentieth century, the art of Latin America was greatly inspired by the Constructivist Movement. The Movement quickly spread from Russia to Europe and then into Latin America. Joaquín Torres García and Manuel Rendón have been credited with bringing the Constructivist Movement into Latin America from Europe.An important artistic movement generated in Latin America is muralism represented by Diego Rivera, David Alfaro Siqueiros, José Clemente Orozco and Rufino Tamayo in Mexico, Santiago Martinez Delgado and Pedro Nel Gómez in Colombia and Antonio Berni in Argentina. Some of the most impressive Muralista works can be found in Mexico, Colombia, New York City, San Francisco, Los Angeles and Philadelphia. Painter Frida Kahlo, one of the most famous Mexican artists, painted about her own life and the Mexican culture in a style combining Realism, Symbolism and Surrealism. Kahlo's work commands the highest selling price of all Latin American paintings.The Venezuelan Armando Reverón, whose work begins to be recognized internationally, is one of the most important artists of the 20th century in South America; he is a precursor of Arte Povera and Happening. From the 60s the kinetic art emerges in Venezuela, its main representatives are Jesús Soto, Carlos Cruz-Diez, Alejandro Otero and Gego. Colombian sculptor and painter Fernando Botero is also widely known by his works which, on first examination, are noted for their exaggerated proportions and the corpulence of the human and animal figures.
Latin American film is both rich and diverse. Historically, the main centers of production have been Mexico, Argentina, Brazil, and Cuba. Latin American film flourished after sound was introduced in cinema, which added a linguistic barrier to the export of Hollywood film south of the border. Mexican cinema started out in the silent era from 1896 to 1929 and flourished in the Golden Era of the 1940s. It boasted a huge industry comparable to Hollywood at the time with stars such as María Félix, Dolores del Río, and Pedro Infante. In the 1970s, Mexico was the location for many cult horror and action movies. More recently, films such as Amores Perros (2000) and Y tu mamá también (2001) enjoyed box office and critical acclaim and propelled Alfonso Cuarón and Alejandro González Iñárritu to the front rank of Hollywood directors. Alejandro González Iñárritu directed in 2010 Biutiful and Birdman (2014), Alfonso Cuarón directed Harry Potter and the Prisoner of Azkaban in 2004 and Gravity (2013). Close friend of both, Guillermo del Toro, a top rank Hollywood director in Hollywood and Spain, directed Pan's Labyrinth (2006) and produced El Orfanato (2007). Carlos Carrera (The Crime of Father Amaro), and screenwriter Guillermo Arriaga are also some of the most known present-day Mexican film makers. Rudo y Cursi released in December (2008) in Mexico was directed by Carlos Cuarón. Argentine cinema has also been prominenent since the first half of the 20th century and today averages over 60 full-length titles yearly. The industry suffered during the 1976–1983 military dictatorship; but re-emerged to produce the Academy Award winner The Official Story in 1985. A wave of imported U.S. films again damaged the industry in the early 1990s, though it soon recovered, thriving even during the Argentine economic crisis around 2001. Many Argentine movies produced during recent years have been internationally acclaimed, including Nueve reinas (2000), Son of the Bride (2001), El abrazo partido (2004), El otro (2007), the 2010 Foreign Language Academy Award winner El secreto de sus ojos and Wild Tales (2014). In Brazil, the Cinema Novo movement created a particular way of making movies with critical and intellectual screenplays, a clearer photography related to the light of the outdoors in a tropical landscape, and a political message. The modern Brazilian film industry has become more profitable inside the country, and some of its productions have received prizes and recognition in Europe and the United States, with movies such as Central do Brasil (1999), Cidade de Deus (2002) and Tropa de Elite (2007). Puerto Rican cinema has produced some notable films, such as Una Aventura Llamada Menudo, Los Diaz de Doris and Casi Casi. An influx of Hollywood films affected the local film industry in Puerto Rico during the 1980s and 1990s, but several Puerto Rican films have been produced since and it has been recovering. Cuban cinema has enjoyed much official support since the Cuban revolution and important film-makers include Tomás Gutiérrez Alea.
Pre-Columbian cultures were primarily oral, though the Aztecs and Mayans, for instance, produced elaborate codices. Oral accounts of mythological and religious beliefs were also sometimes recorded after the arrival of European colonizers, as was the case with the Popol Vuh. Moreover, a tradition of oral narrative survives to this day, for instance among the Quechua-speaking population of Peru and the Quiché (K'iche') of Guatemala. From the very moment of Europe's discovery of the continents, early explorers and conquistadores produced written accounts and crónicas of their experience – such as Columbus's letters or Bernal Díaz del Castillo's description of the conquest of Mexico. During the colonial period, written culture was often in the hands of the church, within which context Sor Juana Inés de la Cruz wrote memorable poetry and philosophical essays. Towards the end of the 18th Century and the beginning of the 19th, a distinctive criollo literary tradition emerged, including the first novels such as Lizardi's El Periquillo Sarniento (1816). The 19th century was a period of "foundational fictions" (in critic Doris Sommer's words), novels in the Romantic or Naturalist traditions that attempted to establish a sense of national identity, and which often focussed on the indigenous question or the dichotomy of "civilization or barbarism" (for which see, say, Domingo Sarmiento's Facundo (1845), Juan León Mera's Cumandá (1879), or Euclides da Cunha's Os Sertões (1902)). The 19th century also witnessed the realist work of Machado de Assis, who made use of surreal devices of metaphor and playful narrative construction, much admired by critic Harold Bloom. At the turn of the 20th century, modernismo emerged, a poetic movement whose founding text was Nicaraguan poet Rubén Darío's Azul (1888). This was the first Latin American literary movement to influence literary culture outside of the region, and was also the first truly Latin American literature, in that national differences were no longer so much at issue. José Martí, for instance, though a Cuban patriot, also lived in Mexico and the United States and wrote for journals in Argentina and elsewhere. However, what really put Latin American literature on the global map was no doubt the literary boom of the 1960s and 1970s, distinguished by daring and experimental novels (such as Julio Cortázar's Rayuela (1963)) that were frequently published in Spain and quickly translated into English. The Boom's defining novel was Gabriel García Márquez's Cien años de soledad (1967), which led to the association of Latin American literature with magic realism, though other important writers of the period such as the Peruvian Mario Vargas Llosa and Carlos Fuentes do not fit so easily within this framework. Arguably, the Boom's culmination was Augusto Roa Bastos's monumental Yo, el supremo (1974). In the wake of the Boom, influential precursors such as Juan Rulfo, Alejo Carpentier, and above all Jorge Luis Borges were also rediscovered. Contemporary literature in the region is vibrant and varied, ranging from the best-selling Paulo Coelho and Isabel Allende to the more avant-garde and critically acclaimed work of writers such as Diamela Eltit, Giannina Braschi, Ricardo Piglia, or Roberto Bolaño. There has also been considerable attention paid to the genre of testimonio, texts produced in collaboration with subaltern subjects such as Rigoberta Menchú. Finally, a new breed of chroniclers is represented by the more journalistic Carlos Monsiváis and Pedro Lemebel. The region boasts six Nobel Prize winners: in addition to the two Chilean poets Gabriela Mistral (1945) and Pablo Neruda (1971), there is also the Guatemalan novelist Miguel Angel Asturias (1967), the Colombian writer Gabriel García Márquez (1982), the Mexican poet and essayist Octavio Paz (1990), and the Peruvian novelist Mario Vargas Llosa (2010).
Latin America has produced many successful worldwide artists in terms of recorded global music sales. Among the most successful have been Juan Gabriel (Mexico) only Latin American musician to have sold over 200 million records worldwide, Gloria Estefan (Cuba), Carlos Santana, Luis Miguel (Mexico) of whom have sold over 90 million records, Shakira (Colombia) and Vicente Fernández (Mexico) with over 50 million records sold worldwide. Enrique Iglesias, although not a Latin American, has also contributed for the success of Latin music. Other notable successful mainstream acts through the years, include RBD, Celia Cruz, Soda Stereo, Thalía, Ricky Martin, Maná, Marc Anthony, Ricardo Arjona, Selena, and Menudo. Caribbean Hispanic music, such as merengue, bachata, salsa, and more recently reggaeton, from such countries as the Dominican Republic, Puerto Rico, Trinidad and Tobago, Cuba, and Panama, has been strongly influenced by African rhythms and melodies. Haiti's compas is a genre of music that is influenced by its Caribbean Hispanic counterparts, along with elements of jazz and modern sounds. Another well-known Latin American musical genre includes the Argentine and Uruguayan tango (with Carlos Gardel as the greatest exponent), as well as the distinct nuevo tango, a fusion of tango, acoustic and electronic music popularized by bandoneón virtuoso Ástor Piazzolla. Samba, North American jazz, European classical music and choro combined to form bossa nova in Brazil, popularized by guitarist João Gilberto with singer Astrud Gilberto and pianist Antonio Carlos Jobim. Other influential Latin American sounds include the Antillean soca and calypso, the Honduran (Garifuna) punta, the Colombian cumbia and vallenato, the Chilean cueca, the Ecuadorian boleros, and rockoleras, the Mexican ranchera and the mariachi which is the epitome of Mexican soul, the Nicaraguan palo de Mayo, the Peruvian marinera and tondero, the Uruguayan candombe, the French Antillean zouk (derived from Haitian compas) and the various styles of music from pre-Columbian traditions that are widespread in the Andean region. The classical composer Heitor Villa-Lobos (1887–1959) worked on the recording of native musical traditions within his homeland of Brazil. The traditions of his homeland heavily influenced his classical works. Also notable is the recent work of the Cuban Leo Brouwer and guitar work of the Venezuelan Antonio Lauro and the Paraguayan Agustín Barrios. Latin America has also produced world-class classical performers such as the Chilean pianist Claudio Arrau, Brazilian pianist Nelson Freire and the Argentine pianist and conductor Daniel Barenboim. Brazilian opera soprano Bidu Sayão, one of Brazil's most famous musicians, was a leading artist of the Metropolitan Opera in New York City from 1937 to 1952. Arguably, the main contribution to music entered through folklore, where the true soul of the Latin American and Caribbean countries is expressed. Musicians such as Yma Súmac, Chabuca Granda, Atahualpa Yupanqui, Violeta Parra, Víctor Jara, Jorge Cafrune, Facundo Cabral, Mercedes Sosa, Jorge Negrete, Luiz Gonzaga, Caetano Veloso, Susana Baca, Chavela Vargas, Simon Diaz, Julio Jaramillo, Toto la Momposina, Gilberto Gil, Maria Bethânia, Nana Caymmi, Nara Leão, Gal Costa, Ney Matogrosso as well as musical ensembles such as Inti Illimani and Los Kjarkas are magnificent examples of the heights that this soul can reach. Latin pop, including many forms of rock, is popular in Latin America today (see Spanish language rock and roll). A few examples are Café Tacuba, Soda Stereo, Maná, Rita Lee, Mutantes, Secos e Molhados Legião Urbana, Titãs, Paralamas do Sucesso, Cazuza, Barão Vermelho, Skank, Miranda!, Cansei de Ser Sexy or CSS, and Bajo Fondo. More recently, reggaeton, which blends Jamaican reggae and dancehall with Latin America genres such as bomba and plena, as well as hip hop, is becoming more popular, in spite of the controversy surrounding its lyrics, dance steps (Perreo) and music videos. It has become very popular among populations with a "migrant culture" influence – both Latino populations in the United States, such as southern Florida and New York City, and parts of Latin America where migration to the United States is common, such as Trinidad and Tobago, Dominican Republic, Colombia, Ecuador, El Salvador, and Mexico.
The following is a list of the ten countries with the most World Heritage Sites in Latin America.
IDB Education Initiative Latin American Network Information Center Latin America Data Base Washington Office on Latin America Council on Hemispheric Affairs Codigos De Barra Infolatam. Information and analysis of Latin America at the Library of Congress Web Archives (archived September 8, 2008) Map of Land Cover: Latin America and Caribbean (FAO) Lessons From Latin America by Benjamin Dangl, The Nation, March 4, 2009 Keeping Latin America on the World News Agenda – Interview with Michael Reid of The Economist at the Wayback Machine (archived June 24, 2010) Cold War in Latin America, CSU Pomona University at Archive.today (archived December 14, 2012) Latin America Cold War Resources, Yale University Latin America Cold War, Harvard University http://larc.ucalgary.ca/ Latin American Research Centre, University of Calgary The war on Democracy, by John Pilger
The London School of Economics (officially the London School of Economics and Political Science, often referred to as LSE or the LSE) is a public research university located in London, England, and a member institution of the federal University of London. Founded in 1895 by Fabian Society members Sidney Webb, Beatrice Webb, Graham Wallas, and George Bernard Shaw for the betterment of society, LSE joined the University of London in 1900 and established its first degree courses under the auspices of the university in 1901. LSE started awarding its own degrees in its own name in 2008, prior to which it awarded degrees of the University of London. The LSE is located in Westminster, Central London, near the boundary between Covent Garden and Holborn. The area is historically known as Clare Market. The LSE has more than 11,000 students, just under seventy percent of whom come from outside the UK, and 3,300 staff. It had an income of £415.1 million in 2018/19, of which £32.1 million was from research grants. One hundred and fifty-five nationalities are represented amongst the LSE's student body and the school has the second highest percentage of international students (70%) of all world universities. Despite its name, the school is organised into 25 academic departments and institutes which conduct teaching and research across a range of pure and applied social sciences.The LSE is a member of the Russell Group, Association of Commonwealth Universities, European University Association and is sometimes considered a part of the "Golden Triangle" of universities in south-east England. The LSE also forms part of CIVICA - The European University of Social Sciences, a network of eight European universities focused on research in the social sciences. In the 2014 Research Excellence Framework, the School had the highest proportion of world-leading research among research submitted of any British non-specialist university.The LSE has produced many notable alumni in the fields of law, history, anthropology, economics, philosophy, psychology, business, literature, media and politics. Alumni and staff include 55 past or present heads of state or government and 18 Nobel laureates. As of 2017, 27% (or 13 out of 49) of all the Nobel Memorial Prizes in Economics have been awarded or jointly awarded to LSE alumni, current staff or former staff, making up 16% (13 out of 79) of all laureates. LSE alumni and staff have also won 3 Nobel Peace Prizes and 2 Nobel Prizes in Literature. Out of all European universities, LSE has educated the most billionaires according to a 2014 global census of U.S dollar billionaires.
The School joined the federal University of London in 1900, and was recognised as a Faculty of Economics of the university. The University of London degrees of BSc (Econ) and DSc (Econ) were established in 1901, the first university degrees dedicated to the social sciences. Expanding rapidly over the following years, the school moved initially to the nearby 10 Adelphi Terrace, then to Clare Market and Houghton Street. The foundation stone of the Old Building, on Houghton Street, was laid by King George V in 1920; the building was opened in 1922. The 1930s economic debate between LSE and Cambridge is well known in academic circles. Rivalry between academic opinion at LSE and Cambridge goes back to the school's roots when LSE's Edwin Cannan (1861–1935), Professor of Economics, and Cambridge's Professor of Political Economy, Alfred Marshall (1842–1924), the leading economist of the day, argued about the bedrock matter of economics and whether the subject should be considered as an organic whole. (Marshall disapproved of LSE's separate listing of pure theory and its insistence on economic history.)The dispute also concerned the question of the economist's role, and whether this should be as a detached expert or a practical adviser. Despite the traditional view that the LSE and Cambridge were fierce rivals through the 1920s and 30s, they worked together in the 1920s on the London and Cambridge Economic Service. However, the 1930s brought a return to disputes as economists at the two universities argued over how best to address the economic problems caused by the Great Depression.The main figures in this debate were John Maynard Keynes from Cambridge and the LSE's Friedrich Hayek. The LSE Economist Lionel Robbins was also heavily involved. Starting off as a disagreement over whether demand management or deflation was the better solution to the economic problems of the time, it eventually embraced much wider concepts of economics and macroeconomics. Keynes put forward the theories now known as Keynesian economics, involving the active participation of the state and public sector, while Hayek and Robbins followed the Austrian School, which emphasised free trade and opposed state involvement.During World War II, the School decamped from London to the University of Cambridge, occupying buildings belonging to Peterhouse.The School's arms, including its motto and beaver mascot, were adopted in February 1922, on the recommendation of a committee of twelve, including eight students, which was established to research the matter. The Latin motto, rerum cognoscere causas, is taken from Virgil's Georgics. Its English translation is "to Know the Causes of Things" and it was suggested by Professor Edwin Cannan. The beaver mascot was selected for its associations with "foresight, constructiveness and industrious behaviour".
LSE continues to have a wide impact within British society, through its relationships and influence in politics, business and law. The Guardian described such influence in 2005 when it stated: Once again the political clout of the school, which seems to be closely wired into parliament, Whitehall and the Bank of England, is being felt by ministers.... The strength of LSE is that it is close to the political process: Mervyn King, was a former LSE professor. The former chairman of the House of Commons education committee, Barry Sheerman, sits on its board of governors, along with Labour peer Lord (Frank) Judd. Also on the board are Tory MPs Virginia Bottomley and Richard Shepherd, as well as Lord Saatchi and Lady Howe. Commenting in 2001 on the rising status of the LSE, the British magazine The Economist stated that "two decades ago the LSE was still the poor relation of the University of London's other colleges. Now... it regularly follows Oxford and Cambridge in league tables of research output and teaching quality and is at least as well-known abroad as Oxbridge". According to the magazine, the School "owes its success to the single-minded, American-style exploitation of its brand name and political connections by the recent directors, particularly Mr Giddens and his predecessor, John Ashworth" and raises money from foreign students' high fees, which are attracted by academic stars such as Richard Sennett.As of 2006, the School was active in opposing British government proposals to introduce compulsory ID cards, researching into the associated costs of the scheme, and shifting public and government opinion on the issue. The institution is also popular with politicians and MPs to launch new policy, legislation and manifesto pledges, prominently with the launch of the Liberal Democrats Manifesto Conference under Nick Clegg on 12 January 2008.
In the early 2010s, its academics have been at the forefront of both national and international government consultations, reviews and policy, including representation on the UK Airports Commission, Independent Police Commission, Migration Advisory Committee, UN Advisory Board on Water and Sanitation, London Finance Commission, HS2 Limited, the UK government's Infrastructure Commission and advising on Architecture and Urbanism for the London 2012 OlympicsCraig Calhoun took up the post of Director in September 2012. Its previous Director, Judith Rees, is also chair of the school's Grantham Institute on Climate Change, an adviser to the World Bank as well as sitting on the UN Secretary General's Advisory Board on Water and Sanitation and the International Scientific Advisory Council (ISAC). She is also a former Convenor of the Department of Geography and Environment and served as Deputy Director from 1998–2004. In February 2016, Calhoun announced his intention to step down at the end of the academic year, in order to become president of the Berggruen Institute. In September 2016, Bank of England Deputy Governor Dame Nemat (Minouche) Shafik was announced to replace Professor Julia Black as the School's director. Shafik began to lead the LSE in September 2017.
In February 2011, LSE had to face the consequences of matriculating one of Muammar Gaddafi's sons while accepting a £1.5m donation to the university from his family. LSE director Howard Davies resigned over allegations about the institution's links to the Libyan regime. The LSE announced in a statement that it had accepted his resignation with "great regret" and that it had set up an external inquiry into the school's relationship with the Libyan regime and Saif al-Islam Gaddafi, to be conducted by the former lord chief justice Harry Woolf.In 2013, the LSE was featured in a BBC Panorama documentary on North Korea, filmed inside the repressive regime by undercover journalists attached to a trip by the LSE's Grimshaw Club, a student society of the international relations department. The trip had been sanctioned by high-level North Korean officials. The trip caused international media attention as a BBC journalist was posing as a part of LSE. There was debate as to whether this put the student's lives in jeopardy in the repressive regime if a reporter had been exposed. The North Korea government made hostile threats towards the students and LSE after the publicity, which forced an apology from the BBC.In August 2015, it was revealed that the university was paid approximately £40,000 for a "glowing report" for Camila Batmanghelidjh's charity, Kids Company. The study was used by Batmanghelidjh to prove that the charity provided good value for money and was well managed. The university did not disclose that the study was funded by the charity. In the summer of 2017, dozens of campus cleaners contracted via Noonan Services went on weekly strikes, protesting outside key buildings and causing significant disruption during end-of-year examinations. The dispute organised by the UVW union was originally over unfair dismissals of cleaners, but had escalated into a broad demand for decent employment rights matching those of LSE's in-house employees. Owen Jones did not cross the picket line after arriving for a debate on grammar schools with Peter Hitchens. It was announced in June 2018 that some 200 outsourced workers at the LSE would be offered in-house contracts.
A sculpture by Mark Wallinger, The World Turned Upside Down, which features a globe resting on its north pole, was installed in Sheffield Street on the LSE campus on 26 March 2019. The artwork attracted controversy for showing the island of Taiwan as a sovereign entity rather than as part of the People’s Republic of China, Lhasa being denoted as a full capital, and depicting boundaries between India and China as recognised internationally. After protests and reactions from both sides, the school made the decision to alter the work of art over the objections of the Taiwanese students. The university decided later that year that it would retain the original design which chromatically displayed the PRC and Taiwan as different entities but with the addition of an asterisk beside the name of Taiwan and a corresponding placard that clarified the institution's position regarding the controversy.
Since 1902, LSE has been based at Clare Market and Houghton Street in Westminster. It is surrounded by a number of important institutions including the Royal Courts of Justice, all four Inns of Courts, Royal College of Surgeons, Sir John Soane's Museum, and the West End is immediately across Kingsway from campus, which also borders the City of London and is within walking distance to Trafalgar Square and the Houses of Parliament. In 1920, King George V laid the foundation of the Old Building. The campus now occupies an almost continuous group of around 30 buildings between Kingsway and the Aldwych. Alongside teaching and academic space, the institution also owns 11 student halls of residence across London, a West End theatre (the Peacock), early years centre, NHS medical centre and extensive sports ground in Berrylands, south London. LSE operates the George IV public house and the students' union operates the Three Tuns bar. The School's campus is noted for its numerous public art installations which include Richard Wilson's Square the Block, Michael Brown's Blue Rain, Christopher Le Brun's Desert Window.Since the early 2000s, the entire campus has undergone an extensive refurbishment project and a major fund-raising "Campaign for LSE" raised over £100 million in what was one of the largest university fund-raising exercises outside North America. This process was begun with the £35 million renovation of the Lionel Robbins Building by Sir Norman Foster to house the British Library of Political and Economic Science (BLPES), the world's largest social science library and the second largest single entity library in Britain, after the British Library at King's Cross. In 2003, LSE purchased the former Public Trustee building at 24 Kingsway, and engaged Sir Nicholas Grimshaw to redesign it into an ultra-modern educational facility at a total cost of over £45 million – increasing the size of the campus by 120,000 square feet (11,000 m2). The New Academic Building opened for teaching in October 2008, with an official opening by Her Majesty the Queen and the Duke of Edinburgh on 5 November 2008. In November 2009 the School purchased the adjacent Sardinia House to house three academic departments and the nearby Old White Horse public house, before acquiring the freehold of the grade-II listed Land Registry Building at 32 Lincoln's Inn Fields in October 2010, which was reopened in March 2013 by The Princess Royal as the new home for the Department of Economics, International Growth Centre and its associated economic research centres.
The first new building on the site for more than 40 years, the Saw Swee Hock Student Centre opened in January 2014 following an architectural design competition managed by RIBA Competitions. The building provides new accommodation for the LSE Students' Union, LSE accommodation office and LSE careers service as well as a bar, events space, gymnasium, rooftop terrace, learning café, dance studio and media centre. The building, designed as a showpiece for the City of Westminster and Midtown, was recognised as having a low environmental impact, receiving an 'Outstanding' status under BREEAM, and in 2012 was one of three winners of the New London Award in the Education category. In May 2014 the Saw Swee Hock Student Centre won the RIBA London Building of the Year Award. Centre Building The new Centre Building, situated opposite the British Library of Political and Economic Science, opened in June 2019. Designed as both a teaching and an academic space, the new 13-storey Centre Building includes 14 seminar rooms seating between 20 and 60, 234 study spaces, a 200-seater auditorium, as well as three lecture theatres. The building hosts the Department of Government on Levels 3 and 4, the International Inequalities Institute on Levels 4 and 5, and the Department of International Relations on Levels 7 through 10, and the Directorate on Level 1. The roof terraces on levels 2, 6 and 12 are also accessible to the public.
It is currently embarking on redevelopment and expansion with the development of a £120 million new facility designed by Rogers Stirk Harbour & Partners following the completion of a global design competition managed by RIBA Competitions. Completed in 2018, the Global Centre for the Social Sciences houses the Departments of Government, International Relations and the European Institute and feature a new square at the centre of the campus.In September 2013, LSE purchased the freehold of 44 Lincoln's Inn Fields, previously the home of the Francis Crick Institute's laboratories until 2016. The building will be demolished in 2017 to make way for the new Paul Marshall Building which will house academic departments (Management, Accounting and Finance), sports facilities and the new Marshall Institute for Philanthropy and Social Entrepreneurship. In 2015, LSE brought its ownership of buildings on Lincoln's Inn Fields to six with the purchase of 5 Lincoln's Inn Fields on the north side of the square, which has since been converted into faculty accommodation.On 15 November 2017, LSE announced that it has achieved contract completion on the purchase to acquire the Nuffield Building, which is adjacent to the Lincoln's Inn Fields, from the Royal College of Surgeons. According to the contract the building will be transferred to LSE after renovations in 2020.
The nearest London Underground stations are Holborn, Temple and Covent Garden. Charing Cross, at the Trafalgar Square end of Strand, and the City Thameslink entrance at Ludgate Hill are the nearest mainline stations, whilst London Waterloo is a walk or bus across the River Thames. Buses to Aldwych, Kingsway and the Royal Courts of Justice contain stops which are designated as 'alight here for LSE'.
Although LSE is a constituent college of the federal University of London, it is in many ways comparable with free-standing, self-governing and independently funded universities, and it awards its own degrees. LSE is incorporated under the Companies Act as a company limited by guarantee and is an exempt charity within the meaning of Schedule Two of the Charities Act 1993. The principal governance bodies of the LSE are: the LSE Council; the Court of Governors; the Academic Board; and the Director and Director's Management Team.The LSE Council is responsible for strategy and its members are company directors of the school. It has specific responsibilities in relation to areas including: the monitoring of institutional performance; finance and financial sustainability; audit arrangements; estate strategy; human resource and employment policy; health and safety; "educational character and mission", and student experience. The council is supported in carrying out its role by a number of committees that report directly to it.The Court of Governors deals with certain constitutional matters and has pre-decision discussions on key policy issues and the involvement of individual governors in the school's activities. The court has the following formal powers: the appointment of members of court, its subcommittees and of the council; election of the chair and vice chairs of the court and council and honorary fellows of the School; the amendment of the Memorandum and Articles of Association; and the appointment of external auditors.The Academic Board is LSE's principal academic body, and considers all major issues of general policy affecting the academic life of the School and its development. It is chaired by the director, with staff and student membership, and is supported by its own structure of committees. The Vice Chair of the Academic Board serves as a non-director member of the council and makes a termly report to the council.
The director is the head of LSE and its chief executive officer, responsible for executive management and leadership on academic issues. The director reports to and is accountable to the council. The director is also the accountable officer for the purposes of the Higher Education Funding Council for England Financial Memorandum. The LSE's current director is Dame Nemat Shafik, who replaced interim director, Professor Julia Black, on 1 September 2017. The director is supported by a deputy director and provost who oversees the heads of academic departments and institutes, three pro-directors each with designated portfolios (teaching and learning, research and planning and resources) and the School secretary who acts as company secretary. † Titled as Director and President
LSE's research and teaching is organised into a network of independent academic departments established by the LSE Council, the School's governing body, on the advice of the Academic Board, the School's senior academic authority. There are currently 27 academic departments or institutes.
The London School of Economics (LSE) is aiming to increase the size of its endowment fund to more than £1bn, which would make it one of the best resourced institutions in the UK and the world. The effort was initiated in 2016 by Lord Myners, then chairman of the LSE's Council and Court of Governors. The plan includes working with wealthy alumni of LSE to make large contributions, increasing the annual budget surplus, and launching a new, widescale alumni donor campaign. The plan to grow LSE's endowment to more than £1bn has been continued by Lord Myners' successors at the LSE. The LSE has stated that currently "limited endowment funding constrains our ability to offer 'needs blind' admission to students".
LSE continues to adopt a three-term structure and has not moved to semesters. Michaelmas Term runs from October to mid-December, Lent Term from mid-January to late March and Summer Term from late April to mid-June. Certain departments operate reading weeks in early November and mid-February.
The school's historic coat of arms is used on official documentation including degree certificates and transcripts and includes the motto – rerum cognoscere causas, a line taken from Virgil’s Georgics meaning "to know the causes of things", together with the school's mascot – a beaver. Both these symbols, adopted in February 1922, continue to be held in high regard to this day with the beaver chosen because of its representation as "a hard working and industrious yet sociable animal", attributes that the founders hoped LSE students to both possess and aspire to. The school's weekly newspaper is still entitled The Beaver, Rosebery residence hall's bar is called the Tipsy Beaver and LSE sports teams are known as the Beavers. The institution has two sets of colours – brand and academic – red being the brand colour used on signage, publications and in buildings across campus and purple, black and gold for academic purposes including presentation ceremonies and graduation dress. LSE's present 'red block' logo was adopted as part of a rebrand in the early 2000s, before which the school's coat of arms was used exclusively to represent the institution. As a trademarked brand, it is carefully protected but can be produced in various forms to reflect different requirements. In its full form it contains the full name of the institution to the right of the block with a further small empty red square at the end, but it is adapted for each academic department or professional service division to provide a cohesive brand across the institution.
The LSE received 20,000 applications for 1,600 undergraduate places in 2017, or 12.5 applicants per place. All undergraduate applications, including international applications, are made through UCAS. LSE had the 15th highest average entry qualification for undergraduates of any UK university in 2018–19, with new students averaging 168 UCAS points, equivalent to A*A*A* or ABBB in A-level grades. The university gave offers of admission to 37.0% of its applicants in 2015, the 3rd lowest amongst the Russell Group.Postgraduate students at the LSE are required to have a first or upper second Class UK honours degree, or its foreign equivalent, for master's degrees, while direct entry to the MPhil/PhD programme requires a UK taught master's with merit, or foreign equivalent. Admission to the diploma requires a UK degree or equivalent plus relevant experience. The intake to applications ratio for postgraduate degree programmes is very competitive; the MSc Financial Mathematics had a ratio of just over 4% in 2016.31.6% of LSE's undergraduates are privately educated, the ninth highest proportion amongst mainstream British universities. In the 2016–17 academic year, the university had a domicile breakdown of 33:18:50 of UK:EU:non-EU students respectively with a female-to-male ratio of 52:47.
LSE is the only university in the United Kingdom dedicated solely to the study and research of social sciences. LSE awards a range of academic degrees spanning bachelors, masters and PhDs. The post-nominals awarded are the degree abbreviations used commonly among British universities. The School offers over 140 MSc programmes, 5 MPA programmes, an LLM, 30 BSc programmes, an LLB, 4 BA programmes (including International History and Geography), and 35 PhD programmes. Subjects pioneered by LSE include anthropology, criminology, social psychology, sociology and social policy; international relations was first taught as a discipline at LSE. Courses are split across more than thirty research centres and nineteen departments, plus a Language Centre. Since programmes are all within the social sciences, they closely resemble each other, and undergraduate students usually take at least one course module in a subject outside of their degree for their first and second years of study, promoting a broader education in the social sciences. At undergraduate level, some departments have as few as 90 students across the three years of study. Since September 2010, it has been compulsory for first year undergraduates to participate in LSE 100: Understanding the Causes of Things alongside normal studies.From 1902, following its absorption into the University of London, until 2007, all degrees were awarded by the federal university in common with all other colleges of the university. This system was changed in 2007 to enable some colleges to award their own degrees. LSE was granted the power to begin awarding its own degrees from July 2008. All students entering from the 2007–08 academic year onwards received an LSE degree, while students who started before this date were issued University of London degrees. In conjunction with NYU Stern and HEC Paris, LSE also offers the TRIUM Executive MBA. This was globally ranked third among executive MBAs by the Financial Times in 2016.
In the 2014 Research Excellence Framework, LSE had the joint highest percentage of world-leading research among research submitted of any institution that entered more than one unit of assessment and was ranked third by cumulative grade point average with a score of 3.35, beating both Oxford and Cambridge. It was ranked 23rd in the country for research power by Research Fortnight based on its REF2014 results, and 28th in research power by the Times Higher Education. This followed the Research Assessment Exercise in 2008 where the School was placed second equal nationally on GPA, first for fraction of world-leading (4*) research and fourth for fraction of world-leading or internationally excellent (3* and 4*) research in LSE's analysis of the results, fourth equal for GPA and 29th for research power in Times Higher Education's analysis, and 27th in research power by Research Fortnight's analysis.According to analysis of the REF 2014 subject results by Times Higher Education, the School is the UK's top research university in terms of GPA of research submitted in business and management; area studies; and communication, cultural and media studies, library and information management, and second in law; politics and international studies; economics and econometrics; and social work and social policy.
The School houses a number of notable centres including the Centre for the Analysis of Social Exclusion, the Centre for Climate Change Economics and Policy, the Centre for Macroeconomics, Centre for Economic Performance, LSE Health and Social Care, the Financial Markets Group (founded by former Bank of England governor Sir Mervyn King), the Grantham Research Institute on Climate Change and the Environment (chaired by Lord Stern), LSE Cities, the UK Department for International Development funded International Growth Centre and one of the six the UK government-backed 'What Works Centres' – the What Works Centre for Local Economic Growth. The Greater London Group was influential research centre within LSE from the late 1950s on, before being subsumed into the LSE London research group.
In late 2014, LSE hired Erik Berglöf, former Chief Economist and Special Advisor to the EBRD to establish a new Institute of Global Affairs with seven regional research centres focusing on Africa, East Asia, Latin America and the Caribbean, the Middle East, South Asia, South East Asia and the United States. It is joined by the LSE IDEAS think tank, which in a global survey conducted by the University of Pennsylvania in 2015 was jointly ranked as world's second-best university think tank for the third year running alongside the LSE Public Policy Group, after Harvard University's Belfer Center for Science and International Affairs.In February 2015, Angelina Jolie and William Hague launched the UK's first academic Centre on Women, Peace and Security, based at the School. The Centre aims to contribute to global women's rights issues, including the prosecution of war rape and women's engagement in politics, through academic research, a post-graduate teaching program, public engagement, and collaboration with international organisations. Furthermore, in May 2016 it was announced that Jolie-Pitt and Hague would join Jane Connors and Madeleine Rees as Visiting Professors in Practice from September 2016.
LSE has academic partnerships in teaching and research with six universities – with Columbia University in New York City and University of California, Berkeley, in Asia with Peking University in Beijing and the National University of Singapore, in Africa with the University of Cape Town and Europe with Sciences Po in Paris.Together they offer a range of double or joint degree programmes including an MA in International and World History (with Columbia) and an MSc in International Affairs with Peking University, with graduates earning degrees from both institutions. The School also offers joint degrees for specific departments with various other universities including Fudan University in Shanghai, USC in Los Angeles and a Global Studies programme which is offered with a consortium of four European universities – Leipzig, Vienna, Roskilde and Wroclaw. It offers the TRIUM Global Executive MBA programme jointly with Stern School of Business of New York University and HEC School of Management, Paris. It is divided into six modules held in five international business locations over a 16-month period. LSE also offers a Dual Master of Public Administration (MPA) with Global Public Policy Network schools such as Sciences Po Paris, the Hertie School of Governance and National University of Singapore. The school also runs exchange programmes with a number of international business schools through the Global Master's in Management programme and an undergraduate student exchange programme with the University of California, Berkeley in Political Science. LSE is the only UK member school in the CEMS Alliance, and the LSE Global Master's in Management is the only programme in the UK to offer the CEMS Master's in International Management (CEMS MIM) as a double degree option, allowing students to study at one of 30 CEMS partner universities. It also participates in Key Action 1 of the European Union-wide Erasmus+ programme, encouraging staff and student mobility for teaching, although not the other Key Actions in the programme.The School is a member of the Association of Commonwealth Universities, the Association of Professional Schools of International Affairs, the European University Association, the G5, the Global Alliance in Management Education, the Russell Group and Universities UK, and is sometimes considered part of the 'Golden Triangle' of universities in south-east England, along with the University of Oxford, the University of Cambridge, University College London, Imperial College London, and King's College London.
The School's main library, the British Library of Political and Economic Science is located in the Lionel Robbins Building and contains over 4 million print volumes, 60,000 online journals and 29,000 electronic books. The Digital Library contains digitised material from LSE Library collections and also born-digital material that has been collected and preserved in digital formats. Founded in 1896, it is the world's largest social and political sciences library and the national social science library of the United Kingdom and Commonwealth. Its collections are recognised for their outstanding national and international status and hold 'Designation' status by the Museums, Libraries and Archives Council (MLA). BLPES responds to around 7,500 visits from students and staff each day. In addition, it provides a specialist international research collection, serving over 12,000 registered external users each year. The Shaw Library, housed in LSE's Founders Room in the Old Building contains the School's collection of fiction and general readings. It also hosts a weekly series of lunchtime music concerts and press launches and is the home of the Fabian Window which was unveiled by Tony Blair in 2003. In 2013, LSE purchased the Women's Library, Britain's main library and museum resource on women and the women's movement and a UNESCO classified resource from London Metropolitan University, moving the resources and artefacts into a new purpose-built facility within the Lionel Robbins Building complete with its own reading room and exhibition space. Several subject specific libraries also exist including the Seligman Library for Anthropology, the Himmelweit Library for Social Psychology, the Leverhulme Library for Statistics, the Robert McKenzie library for Sociology, the Michael Wise Library for Geography and the Gender Institute Library. Additionally, students are permitted to use the libraries of any other University of London college, and the extensive facilities at Senate House Library, off Russell Square.
The original LSE Summer School was established in 1989 and has since expanded to offer over 70 three-week courses in accounting, finance, economics, English language, international relations, government, law and management each July and August. It is advertised as the largest and one of the most well-established university Summer Schools of its kind in Europe.In recent years, the School has expanded its summer schools both abroad and into executive education with the LSE-PKU Summer School in Beijing (run with Peking University, the LSE-UCT July School in Cape Town (run with the University of Cape Town) and the Executive Summer School at its London campus. In 2011, it also launched a Methods Summer Programme. Together these courses welcome over 5,000 participants from over 130 countries and some of the top colleges and universities around the world, as well as professionals from several multinational institutions. Participants are housed in LSE halls of residence or their overseas equivalents, and the Summer School provides a full social programme including guest lectures and receptions.
Public lectures hosted by LSE Events office, are open to students, alumni and the general public. As well as leading academics and commentators, speakers frequently include prominent national and international figures such as ambassadors, CEOs, Members of Parliament, and heads of state. A number of these are broadcast live around the world via the School's website. LSE organises over 200 public events every year.Recent prominent speakers have included Kofi Annan, Ben Bernanke, Tony Blair, Gordon Brown, David Cameron, Noam Chomsky, Bill Clinton, Philip Craven, Niall Ferguson, Vicente Fox, Milton Friedman, Muammar Gaddafi, Julia Gillard, Alan Greenspan, Tenzin Gyatso, Lee Hsien Loong, Boris Johnson, David Harvey, Jean Tirole, Angelina Jolie, Paul Krugman, Dmitri Medvedev, Mario Monti, George Osborne, Robert Peston, Sebastián Piñera, Kevin Rudd, Jeffrey Sachs, Gerhard Schroeder, Carlos D. Mesa, Luiz Inácio Lula da Silva, Aung San Suu Kyi, Amartya Sen, George Soros and Rowan Williams. Previously, the School has hosted figures including Nelson Mandela and Margaret Thatcher.There are also a number of annual lecture series hosted by various departments. These include but are not limited to the Malinowski Memorial Lectures hosted by the Department of Anthropology, the Lionel Robbins Memorial Lectures and the Ralph Miliband programme.
The iXXi Briefings (from 9/11, written in Roman numerals) are private discussions which are attended by around 40 distinguished people, chaired by Lord Desai. At the briefings, two speakers talk for 15 minutes each before discussion is opened to all attendees, operating under Chatham House Rules. iXXi briefings provide an opportunity for the LSE to exhibit its resources and engage with experts and prominent figures. The iXXi briefings are run by LSE Enterprises.
In 2018, the university launched LSE Press in partnership with Ubiquity Press. This is intended to publish open-access journals and books in the social sciences. The first journal to be published by the press was the Journal of Illicit Economies and Development, edited by Dr John Collins, executive director of LSE's International Drug Policy Unit. The press is managed through the LSE Library.
In overall national rankings, the LSE consistently placed as a top 15 university until 2019, when it fell to 19th in the Guardian table for 2020. It was placed joint 8th in the Times Higher Education Table of Tables 2019. The LSE also ranked 3rd overall in the Sunday Times University Guide cumulative ranking over a ten-year period (1998–2007). LSE was one of only eight universities (along with the other members of the G5, Bath, St Andrews and Warwick) to have never left the top 15 in one of the three main domestic rankings between 2008–2017.The QS World University Rankings for 2021 rankings saw the LSE placed 49th overall. The 2020 Times Higher Education World University Rankings ranked LSE 27th globally and placed it 5th in the country. LSE was also ranked 24th for reputation by Times Higher Education in 2016. However, the 2019 Academic Ranking of World Universities placed the LSE 151–200 and 16–21 nationally. LSE climbs from 259th to 244th globally, and maintains 27th in the UK, in the US News & World Report Best Global Universities 2021. The citation-based CWTS Leiden Ranking placed LSE 43rd worldwide in 2019. Times Higher Education 2017 ranks social sciences at LSE at 15th globally and 4th in the country. The QS World University Rankings by Subject 2020 ranks the LSE second in the world for Social Sciences and Management and third in the world for Geography, Communication and Media Studies, Politics, and Social Policy and Administration. It is ranked in the top ten for Anthropology, Development studies, Accounting and Finance, History, Philosophy, Law, Economics, and Business and Management Studies, in the top 30 for Psychology, and the top 40 for Statistics.Disparities between national and international league tables have caused LSE to offer public explanations for the difference, including the statement in 2012: "At mid-2012, LSE has seen pleasing improvements over the last couple of years in our standing in all the main global rankings: those produced by Times Higher Education, QS and Shanghai Jiaotong University [the Academic Ranking of World Universities]. We have also seen good rises in the domestic UK rankings. But we remain concerned that all of the global rankings – by some way the most important for us, given our highly international orientation – suffer from inbuilt biases in favour of large multi-faculty universities with full STEM (Science, Technology, Engineering and Mathematics) offerings, and against small, specialist, mainly non-STEM universities such as LSE."According to data released by the Department for Education in 2018, LSE was rated as the best university for boosting graduate earnings, with male graduates seeing a 47.2% increase in earnings and female graduates seeing a 38.2% increase in earnings compared to the average graduate. According to Wealth-X and UBS's "Billionaire Census" in 2014, LSE ranked 10th in the list of 20 schools that have produced the most billionaire alumni. The LSE was the only UK university to make the list. In the 2017 National Student Survey LSE came 145th out of 148 for overall student satisfaction.
In the 2015–16 academic year there were 10,833 full-time students and around 700 part-time students at the university. Of these, approximately 7,500 came from outside the United Kingdom (approximately 70% of the total student body), making LSE a highly international school with over 160 countries represented. LSE had more countries represented by students than the UN. 32% of LSE's students come from Asia, 10% from North America, 2% each from South America and Africa. Combined over 100 languages are spoken at LSE. Over half of LSE's students are postgraduates, and there is approximately an equal split between genders with 51% male and 49% female students. Alumni total over 160,000, covering over 190 countries with more than 80 active alumni groups.
The LSE Students' Union (LSESU) is affiliated to the National Union of Students and is responsible for campaigning and lobbying the School on behalf of students as well providing student support and the organisation and undertaking of entertainment events and student societies. It is often regarded as the most politically active in Britain – a reputation it has held since the well documented LSE student riots in 1966–67 and 1968–69, which made international headlines. In 2015, the School was awarded the top spot for student nightlife by The Guardian newspaper due in part to its central location and provision of over 200 societies, 40 sports clubs, a Raising and Giving (RAG) branch and a thriving media group. In 2013, the Union moved into a purpose-built new building – the Saw Swee Hock Student Centre on the Aldwych campus.A weekly student newspaper The Beaver, is published each Tuesday during term time and is amongst the oldest student newspapers in the country. It sits alongside a radio station, Pulse! which has existed since 1999 and a television station LooSE Television since 2005. The Clare Market Review one of Britain's oldest student publications was revived in 2008. Over £150,000 is raised for charity each year through the RAG (Raising and Giving), the fundraising arm of the Students' Union, which was started in 1980 by then Student Union Entertainments Officer and former New Zealand MP Tim Barnett.Sporting activity is coordinated by the LSE Athletics Union, which is a constituent of British Universities & Colleges Sport (BUCS).
LSE owns or operates 10 halls of residence in and around central London and there are also two halls owned by urbanest and five intercollegiate halls (shared with other constituent colleges of the University of London) within a 3-mile radius of the School, for a total of over 4,000 places. Most residences take both undergraduates and postgraduates, although Carr-Saunders Hall and Passfield Hall are undergraduate only, and Butler's Wharf Residence, Grosvenor House and Lillian Knowles House are reserved for postgraduates. Sidney Webb House, managed by Unite Students, takes postgraduates and continuing students. There are also flats available on Anson and Carleton roads, which are reserved for students with children.The School guarantees accommodation for all first-year undergraduate students and many of the school's larger postgraduate population are also catered for, with some specific residences available for postgraduate living. Whilst none of the residences are located at the Aldwych campus, the closest, Grosvenor House is within a five-minute walk from the School in Covent Garden, whilst the farthest residences (Nutford and Butler's Wharf) are approximately forty-five minutes by Tube or Bus. Each residence accommodates a mixture of students both home and international, male and female, and, usually, undergraduate and postgraduate. New undergraduate students (including General Course students) occupy approximately 55% of all spaces, with postgraduates taking approximately 40% and continuing students about 5% of places. The largest LSE student residence, Bankside House, a refurbished early 1950s office block and former headquarters of the Central Electricity Generating Board, opened to students in 1996 and is fully catered, accommodating 617 students across eight floors overlooking the River Thames. It is located behind the Tate Modern art gallery on the south bank of the river. The second-largest residence, the High Holborn Residence in High Holborn, was opened in 1995 and is approximately 10 minutes walk from the main campus. It is self-catering, accommodating 447 students in flats of four our five bedrooms with shared facilities. Other accommodation is located in the surrounding area – Butler's Wharf is situated next to Tower Bridge, Rosebery Hall is located in the London Borough of Islington close to Sadler's Wells, and Carr-Saunders Hall, named after the LSE professor, is approximately 5 minutes from Telecom Tower in the heart of Fitzrovia. Since 2005, the school has opened three new residences to provide accommodation for all first-year students. Lilian Knowles, independently operated in Spitalfields, is home for approximately 360 students and opened in 2006. It is located in a converted Victorian night refuge; the remnants of which can still be seen on the outside facade. It is a common stop on Jack the Ripper tours as one of his victims is commonly believed to have been a one-time resident. Planning permission was sought to convert the Grade II listed Northumberland House, on Northumberland Avenue into a new residence in June 2005, and the accommodation opened to students in October 2006. It was formerly a Victorian grand hotel and lately government offices. The closest residence to the Aldwych campus is reserved for postgraduate students and is located on the eastern side of Drury Lane at the crossroads of Great Queen Street and Long Acre. Grosvenor House, converted from a Victorian office building, opened in September 2005. The residence is unique in that all of its 169 rooms are small, self-contained studios, with private toilet and shower facilities and a mini-kitchen.
LSE has a long list of notable alumni and staff, spanning the fields of scholarship provided by the school. The school has over 50 fellows of the British Academy on its staff, while other notable former staff members include Brian Barry, Maurice Cranston, Anthony Giddens, Harold Laski, Ralph Miliband, Michael Oakeshott, A. W. Philips, Karl Popper, Lionel Robbins, Susan Strange, Bob Ward and Charles Webster. Mervyn King, the former Governor of the Bank of England, is also a former professor of economics. In the political arena notable alumni and staff include 53 past or present heads of state, 20 members of the current British House of Commons and 46 members of the current House of Lords. Former British Prime Minister Clement Attlee taught at the school from 1912 to 1923. In recent British politics, former LSE students include Virginia Bottomley, Yvette Cooper, Edwina Currie, Frank Dobson, Margaret Hodge, Robert Kilroy-Silk, former UK Labour Party leader Ed Miliband and former UK Liberal Democrats leader Jo Swinson. Internationally, the current and first female President of the European Commission Ursula von der Leyen, Brazilian defence minister Celso Amorim, Costa Rican President Óscar Arias, Japanese Prime Minister Taro Aso, Queen Margrethe II of Denmark, architect of the Indian Constitution and eminent economist B. R. Ambedkar, President of India K. R. Narayanan, President of the Republic of China (Taiwan) Tsai Ing-wen, Italian Prime Minister and President of the European Commission, Romano Prodi, French Foreign Minister and President of the Constitutional Council Roland Dumas as well as Singapore's Deputy Prime Minister and Chairman of the International Monetary and Financial Committee at the International Monetary Fund (IMF), Tharman Shanmugaratnam all studied at LSE. A notable number of LSE students have also played a role in the Barack Obama administration, including Pete Rouse, Peter R. Orszag, Mona Sutphen, Paul Volcker and Jason Furman. Physician Vanessa Kerry and American journalist Susan Rasky are also alumnae of the LSE. Notable American Monica Lewinsky pursued her MSc in Social Psychology at the LSE. Business people who studied at LSE include the CEO of AirAsia Tony Fernandes, former CEO of General Motors Daniel Akerson, Director of Louis Vuitton Delphine Arnault, founder of easyJet Stelios Haji-Ioannou, CEO of Abercrombie & Fitch Michael S. Jeffries, Greek business magnate Spiros Latsis, American banker David Rockefeller, CEO of Newsmax Media Christopher Ruddy, founder of advertising agency Saatchi and Saatchi Maurice Saatchi, hedge fund managers George Soros and Michael Platt. Convicted British terrorist, Omar Saeed Sheikh, studied Statistics at LSE, but did not graduate. He served five years in an Indian prison for kidnapping British tourists in 1994. In 2002, he was arrested and convicted in the kidnapping and murder of Daniel Pearl. The Guardian reported that Sheikh came into contact with radical Islamists at the LSE.
As of 2019, 18 Nobel Prizes in economics, peace and literature are officially recognised as having been awarded to LSE alumni and staff.
Media related to London School of Economics at Wikimedia Commons Official website Catalogue of the archives of LSE Memorandum about the school by William Beveridge, 1935 Catalogue of School minute books, 1894–
Managerial Economics deals with the application of the economic concepts, theories, tools, and methodologies to solve practical problems in a business. In other words, managerial economics is the combination of economics theory and managerial theory. It helps the manager in decision-making and acts as a link between practice and theory. It is sometimes referred to as business economics and is a branch of economics that applies microeconomic analysis to decision methods of businesses or other management units. As such, it bridges economic theory and economics in practice. It draws heavily from quantitative techniques such as regression analysis, correlation and calculus. If there is a unifying theme that runs through most of managerial economics, it is the attempt to optimize business decisions given the firm's objectives and given constraints imposed by scarcity, for example through the use of operations research, mathematical programming, game theory for strategic decisions, and other computational methods.
Managerial decision areas include: assessment of investable funds selecting business area choice of product determining optimum output sales promotionAlmost any business decision can be analyzed with managerial economics techniques, but it is most commonly applied to: Risk analysis – various models are used to quantify risk and asymmetric information and to employ them in decision rules to manage risk. Production analysis – microeconomic techniques are used to analyze production efficiency, optimum factor allocation, costs, economies of scale and to estimate the firm's cost function. Pricing analysis – microeconomic techniques are used to analyze various pricing decisions including transfer pricing, joint product pricing, price discrimination, price elasticity estimations, and choosing the optimum pricing method. Capital budgeting – investment theory is used to examine a firm's capital purchasing decisions.At universities, the subject is taught primarily to advanced undergraduates and graduate business students. It is an integration of concepts and theories taken from management and economic subjects primarily used to teach students how to create and analyze optimized business decisions or strategies. In many countries it is possible to read for a degree in Business Economics which often covers managerial economics, financial economics, game theory, business forecasting and industrial economics.
Managerial economics to a certain degree is prescriptive in nature as it suggests a course of action to a managerial problem. Problems can be related to various departments in a firm like production, accounts, sales, etc.and it can also help in decision making. (a) Operational issues Demand decision Production decision Theory of exchange or price theory All human economic activity(b) Environmental issues Nature and trend of domestic business/ international environment Nature and impact of social costs and government policy
Demand is the willingness of potential customers to buy a commodity. It defines the market size for a commodity, and at a disaggregated level the composition of the customer base. Analysis of demand is important for a firm as its revenue, profits, and income of its employees depend on it.
Computational Economics. Aims and scope. Journal of Economics & Management Strategy. Aims and scope. Managerial and Decision Economics
https://web.archive.org/web/20111112021324/http://www.edushareonline.in/Management/eco%20new.pdf http://www.swlearning.com/economics/hirschey/managerial_econ/chap01.pdf
A market is one of a composition of systems, institutions, procedures, social relations or infrastructures whereby parties engage in exchange. While parties may exchange goods and services by barter, most markets rely on sellers offering their goods or services (including labour power) in exchange for money from buyers. It can be said that a market is the process by which the prices of goods and services are established. Markets facilitate trade and enable the distribution and resource allocation in a society. Markets allow any trade-able item to be evaluated and priced. A market emerges more or less spontaneously or may be constructed deliberately by human interaction in order to enable the exchange of rights (cf. ownership) of services and goods. Markets generally supplant gift economies and are often held in place through rules and customs, such as a booth fee, competitive pricing, and source of goods for sale (local produce or stock registration). Markets can differ by products (goods, services) or factors (labour and capital) sold, product differentiation, place in which exchanges are carried, buyers targeted, duration, selling process, government regulation, taxes, subsidies, minimum wages, price ceilings, legality of exchange, liquidity, intensity of speculation, size, concentration, exchange asymmetry, relative prices, volatility and geographic extension. The geographic boundaries of a market may vary considerably, for example the food market in a single building, the real estate market in a local city, the consumer market in an entire country, or the economy of an international trade bloc where the same rules apply throughout. Markets can also be worldwide, see for example the global diamond trade. National economies can also be classified as developed markets or developing markets. In mainstream economics, the concept of a market is any structure that allows buyers and sellers to exchange any type of goods, services and information. The exchange of goods or services, with or without money, is a transaction. Market participants consist of all the buyers and sellers of a good who influence its price, which is a major topic of study of economics and has given rise to several theories and models concerning the basic market forces of supply and demand. A major topic of debate is how much a given market can be considered to be a "free market", that is free from government intervention. Microeconomics traditionally focuses on the study of market structure and the efficiency of market equilibrium; when the latter (if it exists) is not efficient, then economists say that a market failure has occurred. However, it is not always clear how the allocation of resources can be improved since there is always the possibility of government failure.
In economics, a market is a coordinating mechanism that uses prices to convey information among economic entities (such as firms, households and individuals) to regulate production and distribution. In his seminal 1937 article "The Nature of the Firm", Ronald Coase wrote: "An economist thinks of the economic system as being coordinated by the price mechanism....in economic theory we find that the allocation of factors of production between different uses is determined by the price mechanism". Thus the usage of the price mechanism to convey information is the defining feature of the market. This is in contrast to a firm, which as Coase put it, "the distinguishing mark of the firm is the super-session of the price mechanism".Thus, Firms and Markets are two opposite forms of organizing production; Coase wrote: Outside the firm, price movements direct production, which is co-ordinated through a series of exchange transactions on the market. Within a firm, these market transactions are eliminated and in place of the complicated market structure with exchange transactions is substituted the entrepreneur-co-ordinator, who directs production.There are also other hybrid forms of coordinating mechanisms, in between the hierarchical firm and price-coordinating market(e.g. global value chains, Business Ventures, Joint Venture, and strategic alliances). The reasons for the existence of firms or other forms of co-ordinating mechanisms of production and distribution alongside the market are studied in "The Theory of the Firm" literature, with various complete and incomplete contract theories trying to explain the existence of the firm. Incomplete contract theories that are explicitly based on bounded rationality lead to the costs of writing complete contracts. Such theories include: Transaction Cost Economies by Oliver Williamson and Residual Rights Theory by Groomsman, Hart, and Moore. Market-Firms's dichotomy can be contrasted with the relationship between the agents transacting. While in a market the relationship is short term and restricted to the contract, in the case of firms and other co-ordinating mechanisms it is for a longer duration.In the modern world much economic activity takes place through fiat and not the market. Lafontaine and Slade (2007) estimates, in the US, that the total value added in transactions inside the firms equal the total value added of all market transactions. Similarly, 80% of all World Trade is conducted under Global Value Chains (2012 estimate), while 33% (1996 estimate) is intra-firm trade. Nearly 50% of US imports and 30% of exports take place within firms. While Rajan and Zingales (1998) have found that in 43 countries two-thirds of the growth in value added between 1980-90 came from increase in firm size.
A market is one of the many varieties of systems, institutions, procedures, social relations and infrastructures whereby parties engage in exchange. While parties may exchange goods and services by barter, most markets rely on sellers offering their goods or services (including labour) in exchange for money from buyers. It can be said that a market is the process by which the prices of goods and services are established. Markets facilitate trade and enable the distribution and allocation of resources in a society. Markets allow any trade-able item to be evaluated and priced. A market sometimes emerges more or less spontaneously or may be constructed deliberately by human interaction in order to enable the exchange of rights (cf. ownership) of services and goods. Markets of varying types can spontaneously arise whenever a party has interest in a good or service that some other party can provide. Hence there can be a market for cigarettes in correctional facilities, another for chewing gum in a playground, and yet another for contracts for the future delivery of a commodity. There can be black markets, where a good is exchanged illegally, for example markets for goods under a command economy despite pressure to repress them and virtual markets, such as eBay, in which buyers and sellers do not physically interact during negotiation. A market can be organized as an auction, as a private electronic market, as a commodity wholesale market, as a shopping center, as a complex institution such as a stock market and as an informal discussion between two individuals. Markets vary in form, scale (volume and geographic reach), location and types of participants as well as the types of goods and services traded. The following is a non exhaustive list:
Food retail markets: farmers' markets, fish markets, wet markets and grocery stores Retail marketplaces: public markets, market squares, Main Streets, High Streets, bazaars, souqs, night markets, shopping strip malls and shopping malls Big-box stores: supermarkets, hypermarkets and discount stores Ad hoc auction markets: process of buying and selling goods or services by offering them up for bid, taking bids and then selling the item to the highest bidder Used goods markets such as flea markets Temporary markets such as fairs Real estate markets
Physical wholesale markets: sale of goods or merchandise to retailers; to industrial, commercial, institutional, or other professional business users or to other wholesalers and related subordinated services Markets for intermediate goods used in production of other goods and services Labour markets: where people sell their labour to businesses in exchange for a wage Online auctions and Ad hoc auction markets: process of buying and selling goods or services by offering them up for bid, taking bids and then selling the item to the highest bidder Temporary markets such as trade fairs Energy markets
Media markets (broadcast market): is a region where the population can receive the same (or similar) television and radio station offerings and may also include other types of media including newspapers and Internet content Internet markets (electronic commerce): trading in products or services using computer networks, such as the Internet Artificial markets created by regulation to exchange rights for derivatives that have been designed to ameliorate externalities, such as pollution permits (see carbon trading)
Financial markets facilitate the exchange of liquid assets. Most investors prefer investing in two markets: The stock markets, for the exchange of shares in corporations (NYSE, AMEX and the NASDAQ are the most common stock markets in the United States) The bond marketsThere are also: Currency markets are used to trade one currency for another, and are often used for speculation on currency exchange rates The money market is the name for the global market for lending and borrowing Futures markets, where contracts are exchanged regarding the future delivery of goods are often an outgrowth of general commodity markets Prediction markets are a type of speculative market in which the goods exchanged are futures on the occurrence of certain events; they apply the market dynamics to facilitate information aggregation Insurance markets Debt markets
Grey markets (parallel markets): is the trade of a commodity through distribution channels which, while legal, are unofficial, unauthorized, or unintended by the original manufacturer markets in illegal goods such as the market for illicit drugs, illegal arms, infringing products, cigarettes sold to minors or untaxed cigarettes (in some jurisdictions), or the private sale of unpasteurized goat milk
In economics, a market that runs under laissez-faire policies is called a free market, it is "free" from the government, in the sense that the government makes no attempt to intervene through taxes, subsidies, minimum wages, price ceilings and so on. However, market prices may be distorted by a seller or sellers with monopoly power, or a buyer with monopsony power. Such price distortions can have an adverse effect on market participant's welfare and reduce the efficiency of market outcomes. The relative level of organization and negotiating power of buyers and sellers also markedly affects the functioning of the market. Markets are a system and systems have structure. The structure of a well-functioning market is defined by the theory of perfect competition. Well-functioning markets of the real world are never perfect, but basic structural characteristics can be approximated for real world markets, for example: Many small buyers and sellers Buyers and sellers have equal access to information Products are comparableMarkets where price negotiations meet equilibrium, but the equilibrium is not efficient are said to experience market failure. Market failures are often associated with time-inconsistent preferences, information asymmetries, non-perfectly competitive markets, principal–agent problems, externalities, or public goods. Among the major negative externalities which can occur as a side effect of production and market exchange, are air pollution (side-effect of manufacturing and logistics) and environmental degradation (side-effect of farming and urbanization). There exists a popular thought, especially among economists, that free markets would have a structure of a perfect competition. The logic behind this thought is that market failures are thought to be caused by other exogenic systems, and after removing those exogenic systems ("freeing" the markets) the free markets could run without market failures. For a market to be competitive, there must be more than a single buyer or seller. It has been suggested that two people may trade, but it takes at least three persons to have a market, so that there is competition in at least one of its two sides. However, competitive markets—as understood in formal economic theory—rely on much larger numbers of both buyers and sellers. A market with a single seller and multiple buyers is a monopoly. A market with a single buyer and multiple sellers is a monopsony. These are "the polar opposites of perfect competition". As an argument against such a logic, there is a second view that suggests that the source of market failures is inside the market system itself, therefore the removal of other interfering systems would not result in markets with a structure of perfect competition. As an analogy, such an argument may suggest that capitalists do not want to enhance the structure of markets, just like a coach of a football team would influence the referees or would break the rules if he could while he is pursuing his target of winning the game. Thus according to this view, capitalists are not enhancing the balance of their team versus the team of consumer-workers, so the market system needs a "referee" from outside that balances the game. In this second framework, the role of a "referee" of the market system is usually to be given to a democratic government.
Disciplines such as sociology, economic history, economic geography and marketing developed novel understandings of markets studying actual existing markets made up of persons interacting in diverse ways in contrast to an abstract and all-encompassing concepts of "the market". The term "the market" is generally used in two ways: "The market" denotes the abstract mechanisms whereby supply and demand confront each other and deals are made; in its place, reference to markets reflects ordinary experience and the places, processes and institutions in which exchanges occurs "The market" signifies an integrated, all-encompassing and cohesive capitalist world economy.
Microeconomics (from Greek prefix mikro- meaning "small" and economics) is a branch of economics that studies the behavior of individuals and small impacting organizations in making decisions on the allocation of limited resources (see scarcity). On the other hand, macroeconomics (from the Greek prefix makro- meaning "large" and economics) is a branch of economics dealing with the performance, structure, behavior and decision-making of an economy as a whole, rather than individual markets. The modern field of microeconomics arose as an effort of neoclassical economics school of thought to put economic ideas into mathematical mode. It began in the 19th century debates surrounding the works of Antoine Augustine Cournot, William Stanley Jevons, Carl Menger and Léon Walras—this period is usually denominated as the Marginal Revolution. A recurring theme of these debates was the contrast between the labor theory of value and the subjective theory of value, the former being associated with classical economists such as Adam Smith, David Ricardo and Karl Marx (Marx was a contemporary of the marginalists). In his Principles of Economics (1890), Alfred Marshall presented a possible solution to this problem, using the supply and demand model. Marshall's idea of solving the controversy was that the demand curve could be derived by aggregating individual consumer demand curves, which were themselves based on the consumer problem of maximizing utility. The supply curve could be derived by superimposing a representative firm supply curves for the factors of production and then market equilibrium would be given by the intersection of demand and supply curves. He also introduced the notion of different market periods: mainly long run and short run. This set of ideas gave way to what economists call perfect competition—now found in the standard microeconomics texts—even though Marshall himself was highly skeptical, it could be used as general model of all markets. Opposed to the model of perfect competition, some models of imperfect competition were proposed: The monopoly model, already considered by marginalist economists, describes a profit maximizing capitalist facing a market demand curve with no competitors, who may practice price discrimination. Oligopoly is a market form in which a market or industry is dominated by a small number of sellers. The oldest model was the duopoly of Cournot (1838). It was criticized by Harold Hotelling for its instability, by Joseph Bertrand for lacking equilibrium for prices as independent variables. Hotelling built a model of market located over a line with two sellers in each extreme of the line, in this case maximizing profit for both sellers leads to a stable equilibrium. From this model also follows that if a seller is to choose the location of his store so as to maximize his profit, he will place his store the closest to his competitor as "the sharper competition with his rival is offset by the greater number of buyers he has an advantage". He also argues that clustering of stores is wasteful from the point of view of transportation costs and that public interest would dictate more spatial dispersion. Monopolistic competition is a type of imperfect competition such that many producers sell products that are differentiated from one another (e.g. by branding or quality) and hence are not perfect substitutes. In monopolistic competition, a firm takes the prices charged by its rivals as given and ignores the impact of its own prices on the prices of other firms. The "founding father" of the theory of monopolistic competition is Edward Hastings Chamberlin, who wrote a pioneering book on the subject, Theory of Monopolistic Competition (1933). Joan Robinson published a book called The Economics of Imperfect Competition with a comparable theme of distinguishing perfect from imperfect competition. Chamberlin defined monopolistic competition as "challenge to traditional viewpoint of economics that competition and monopoly are alternatives and that individual prices are to be explained in terms of one or the other". He continues: "By contrast it is held that most economic situations are composite of both competition and monopoly, and that, wherever this is the case, a false view is given by neglecting either one of the two forces and regarding the situation as made up entirely of the other".William Baumol provided in his 1977 paper the current formal definition of a natural monopoly where “an industry in which multiform production is more costly than production by a monopoly”. Baumol defined a contestable market in his 1982 paper as a market where "entry is absolutely free and exit absolutely costless", freedom of entry in Stigler sense: the incumbent has no cost discrimination against entrants. He states that a contestable market will never have an economic profit greater than zero when in equilibrium and the equilibrium will also be efficient. According to Baumol, this equilibrium emerges endogenously due to the nature of contestable markets; that is, the only industry structure that survives in the long run is the one which minimizes total costs. This is in contrast to the older theory of industry structure since not only is industry structure not exogenously given, but equilibrium is reached without an ad hoc hypothesis on the behavior of firms, say using reaction functions in a duopoly. He concludes the paper commenting that regulators that seek to impede entry and/or exit of firms would do better to not interfere if the market in question resembles a contestable market. Around the 1970s the study of market failures came into focus with the study of information asymmetry. In particular, three authors emerged from this period: Akerlof, Spence and Stiglitz. Akerlof considered the problem of bad quality cars driving good quality cars out of the market in his classic "The Market for Lemons" (1970) because of the presence of asymmetrical information between buyers and sellers. Michael Spence explained that signaling was fundamental in the labour market since employers can't know beforehand which candidate is the most productive, a college degree becomes a signaling device that a firm uses to select new personnel.C.B. Macpherson identifies an underlying model of the market underlying Anglo-American liberal democratic political economy and philosophy in the seventeenth and eighteenth centuries: persons are cast as self-interested individuals, who enter into contractual relations with other such individuals, concerning the exchange of goods or personal capacities cast as commodities, with the motive of maximizing pecuniary interest. The state and its governance systems are cast as outside of this framework. This model came to dominant economic thinking in the later nineteenth century, as economists such as Ricardo, Mill, Jevons, Walras and later neo-classical economics shifted from reference to geographically located marketplaces to an abstract "market". This tradition is continued in contemporary neoliberalism, where the market is held up as optimal for wealth creation and human freedom and the states' role imagined as minimal, reduced to that of upholding and keeping stable property rights, contract and money supply. According to David Harvey, this allowed for boilerplate economic and institutional restructuring under structural adjustment and post-Communist reconstruction. Similar formalism occurs in a wide variety of social democratic and Marxist discourses that situate political action as antagonistic to the market. In particular, commodification theorists such as György Lukács insist that market relations necessarily lead to undue exploitation of labour and so need to be opposed in toto. A central theme of empirical analyses is the variation and proliferation of types of markets since the rise of capitalism and global scale economies. The Regulation school stresses the ways in which developed capitalist countries have implemented varying degrees and types of environmental, economic and social regulation, taxation and public spending, fiscal policy and government provisioning of goods, all of which have transformed markets in uneven and geographical varied ways and created a variety of mixed economies. Drawing on concepts of institutional variance and path dependence, varieties of capitalism theorists (such as Peter Hall and David Soskice) identify two dominant modes of economic ordering in the developed capitalist countries, "coordinated market economies" such as Germany and Japan and an Anglo-American "liberal market economies". However, such approaches imply that the Anglo-American liberal market economies in fact operate in a matter close to the abstract notion of "the market". While Anglo-American countries have seen increasing introduction of neo-liberal forms of economic ordering, this has not led to simple convergence, but rather a variety of hybrid institutional orderings. Rather, a variety of new markets have emerged, such as for carbon trading or rights to pollute. In some cases, such as emerging markets for water, different forms of privatization of different aspects of previously state run infrastructure have created hybrid private-public formations and graded degrees of commodification, commercialization, and privatization.
The marketing management school, evolved in the late 1950s and early 1960s, is fundamentally linked with the marketing mix framework, a business tool used in marketing and by marketers. In his paper "The Concept of the Marketing Mix", Neil H. Borden reconstructed the history of the term "marketing mix". He started teaching the term after an associate, James Culliton, described the role of the marketing manager in 1948 as a "mixer of ingredients"; one who sometimes follows recipes prepared by others, sometimes prepares his own recipe as he goes along, sometimes adapts a recipe from immediately available ingredients, and at other times invents new ingredients no one else has tried. The marketer E. Jerome McCarthy proposed a four Ps classification (product, price, promotion, place) in 1960, which has since been used by marketers throughout the world. Robert F. Lauterborn proposed a four Cs classification (consumer, price, promotion, place) in 1990 which is a more consumer-oriented version of the four Ps that attempts to better fit the movement from mass marketing to niche marketing. Koichi Shimizu proposed a 7Cs Compass Model (corporation, commodity, cost, communication, channel, consumer, circumstances) to provide a more complete picture of the nature of marketing in 1981. Businesses market their products/services to a specific segments of consumers. The defining factors of the markets are determined by demographics, interests and age/gender. A form of expansion is to enter a new market and sell/advertise to a different set of users.
A prominent entry-point for challenging the market model's applicability concerns exchange transactions and the homo economicus assumption of self-interest maximization. As of 2012, a number of streams of economic sociological analysis of markets focus on the role of the social in transactions and on the ways transactions involve social networks and relations of trust, cooperation and other bonds. Economic geographers in turn draw attention to the ways exchange transactions occur against the backdrop of institutional, social and geographic processes, including class relations, uneven development and historically contingent path-dependencies. Pierre Bourdieu has suggested the market model is becoming self-realizing in virtue of its wide acceptance in national and international institutions through the 1990s. Michel Callon's concept of framing provides a useful schema: each economic act or transaction occurs against, incorporates and also re-performs a geographically and cultural specific complex of social histories, institutional arrangements, rules and connections. These network relations are simultaneously bracketed, so that persons and transactions may be disentangled from thick social bonds. The character of calculability is imposed upon agents as they come to work in markets and are “formatted” as calculative agencies. Market exchanges contain a history of struggle and contestation that produced actors predisposed to exchange under certain sets of rules. Therefore, for Challon, market transactions can never be disembedded from social and geographic relations and there is no sense to talking of degrees of embeddedness and disembeddeness. An emerging theme is the interrelationship, inter-penetrability and variations of concepts of persons, commodities and modes of exchange under particular market formations. This is most pronounced in recent movement towards post-structuralist theorizing that draws on Michel Foucault and Actor Network Theory and stress relational aspects of person-hood, and dependence and integration into networks and practical systems. Commodity network approaches further both deconstruct and show alternatives to the market models concept of commodities.In social systems theory (cf. Niklas Luhmann), markets are also conceptualized as inner environments of the economy. As horizon of all potential investment decisions the market represents the environment of the actually realized investment decisions. However, such inner environments can also be observed in further function systems of society like in political, scientific, religious or mass media systems.
A widespread trend in economic history and sociology is skeptical of the idea that it is possible to develop a theory to capture an essence or unifying thread to markets. For economic geographers, reference to regional, local, or commodity specific markets can serve to undermine assumptions of global integration and highlight geographic variations in the structures, institutions, histories, path dependencies, forms of interaction and modes of self-understanding of agents in different spheres of market exchange. Reference to actual markets can show capitalism not as a totalizing force or completely encompassing mode of economic activity, but rather as "a set of economic practices scattered over a landscape, rather than a systemic concentration of power". Problematic for market formalism is the relationship between formal capitalist economic processes and a variety of alternative forms, ranging from semi-feudal and peasant economies widely operative in many developing economies, to informal markets, barter systems, worker cooperatives, or illegal trades that occur in most developed countries. Practices of incorporation of non-Western peoples into global markets in the nineteenth and twentieth century did not merely result in the quashing of former social economic institutions. Rather, various modes of articulation arose between transformed and hybridized local traditions and social practices and the emergent world economy. By their liberal nature, so called capitalist markets have almost always included a wide range of geographically situated economic practices that do not follow the market model. Economies are thus hybrids of market and non-market elements. Helpful here is J.K. Gibson-Graham's complex topology of the diversity of contemporary market economies describing different types of transactions, labour and economic agents. Transactions can occur in black markets (such as for marijuana) or be artificially protected (such as for patents). They can cover the sale of public goods under privatization schemes to co-operative exchanges and occur under varying degrees of monopoly power and state regulation. Likewise, there are a wide variety of economic agents, which engage in different types of transactions on different terms: one cannot assume the practices of a religious kindergarten, multinational corporation, state enterprise, or community-based cooperative can be subsumed under the same logic of calculability. This emphasis on proliferation can also be contrasted with continuing scholarly attempts to show underlying cohesive and structural similarities to different markets. Gibson-Graham thus read a variety of alternative markets for fair trade and organic foods or those using local exchange trading system as not only contributing to proliferation, but also forging new modes of ethical exchange and economic subjectivities.
Economic anthropology is a scholarly field that attempts to explain human economic behavior in its widest historic, geographic and cultural scope. It is practiced by anthropologists and has a complex relationship with the discipline of economics, of which it is highly critical. Its origins as a sub-field of anthropology begin with the Polish-British founder of anthropology, Bronisław Malinowski, and his French compatriot, Marcel Mauss, on the nature of gift-giving exchange (or reciprocity) as an alternative to market exchange. Studies in economic anthropology for the most part are focused on exchange. Bronisław Malinowski's path-breaking work, Argonauts of the Western Pacific (1922), addressed the question "why would men risk life and limb to travel across huge expanses of dangerous ocean to give away what appear to be worthless trinkets?". Malinowski carefully traced the network of exchanges of bracelets and necklaces across the Trobriand Islands and established that they were part of a system of exchange (the Kula ring). He stated that this exchange system was clearly linked to political authority. In the 1920s and later, Malinowski's study became the subject of debate with the French anthropologist, Marcel Mauss, author of The Gift (Essai sur le don, 1925). Malinowski emphasized the exchange of goods between individuals and their non-altruistic motives for giving: they expected a return of equal or greater value (colloquially referred to as "Indian giving"). In other words, reciprocity is an implicit part of gifting as no "free gift" is given without expectation of reciprocity. In contrast, Mauss has emphasized that the gifts were not between individuals, but between representatives of larger collectivities. He argued these gifts were a "total prestation" as they were not simple, alienable commodities to be bought and sold, but like the "Crown jewels" embodied the reputation, history and sense of identity of a "corporate kin group", such as a line of kings. Given the stakes, Mauss asked "why anyone would give them away?" and his answer was an enigmatic concept, "the spirit of the gift". A good part of the confusion (and resulting debate) was due to a bad translation. Mauss appeared to be arguing that a return gift is given to keep the very relationship between givers alive; a failure to return a gift ends the relationship; and the promise of any future gifts. Based on an improved translate, Jonathan Parry has demonstrated that Mauss was arguing that the concept of a "pure gift" given altruistically only emerges in societies with a well-developed market ideology.Rather than emphasize how particular kinds of objects are either gifts or commodities to be traded in restricted spheres of exchange, Arjun Appadurai and others began to look at how objects flowed between these spheres of exchange. They shifted attention away from the character of the human relationships formed through exchange and placed it on "the social life of things" instead. They examined the strategies by which an object could be "singularized" (made unique, special, one-of-a-kind) and so withdrawn from the market. A marriage ceremony that transforms a purchased ring into an irreplaceable family heirloom is one example whereas the heirloom in turn makes a perfect gift.
Although arithmetic has been used since the beginning of civilization to set prices, it was not until the 19th century that more advanced mathematical tools began to be used to study markets in the form of social statistics. More recent techniques include business intelligence, data mining and marketing engineering.
A market economy is an economic system in which the decisions regarding investment, production and distribution are guided by the price signals created by the forces of supply and demand. The major characteristic of a market economy is the existence of factor markets that play a dominant role in the allocation of capital and the factors of production.Market economies range from minimally regulated free-market and laissez-faire systems where state activity is restricted to providing public goods and services and safeguarding private ownership, to interventionist forms where the government plays an active role in correcting market failures and promoting social welfare. State-directed or dirigist economies are those where the state plays a directive role in guiding the overall development of the market through industrial policies or indicative planning—which guides yet does not substitute the market for economic planning—a form sometimes referred to as a mixed economy.Market economies are contrasted with planned economies where investment and production decisions are embodied in an integrated economy-wide economic plan. In a centrally planned economy, economic planning is the principal allocation mechanism between firms rather than markets, with the economy's means of production being owned and operated by a single organizational body.
For market economies to function efficiently, governments must establish clearly defined and enforceable property rights for assets and capital goods. However, property rights does not specifically mean private property rights and market economies do not logically presuppose the existence of private ownership of the means of production. Market economies can and often do include various types of cooperatives or autonomous state-owned enterprises that acquire capital goods and raw materials in capital markets. These enterprises utilize a market-determined free price system to allocate capital goods and labor. In addition, there are many variations of market socialism where the majority of capital assets are socially owned with markets allocating resources between socially owned firms. These models range from systems based on employee-owned enterprises based on self-management to a combination of public ownership of the means of production with factor markets.
Market economies rely upon a price system to signal market actors to adjust production and investment. Price formation relies on the interaction of supply and demand to reach or approximate an equilibrium where unit price for a particular good or service is at a point where the quantity demanded equals the quantity supplied. Governments can intervene by establishing price ceilings or price floors in specific markets (such as minimum wage laws in the labor market), or use fiscal policy to discourage certain consumer behavior or to address market externalities generated by certain transactions (Pigovian taxes). Different perspectives exist on the role of government in both regulating and guiding market economies and in addressing social inequalities produced by markets. Fundamentally, a market economy requires that a price system affected by supply and demand exists as the primary mechanism for allocating resources irrespective of the level of regulation.
Capitalism is an economic system where the means of production are largely or entirely privately owned and operated for a profit, structured on the process of capital accumulation. In general, in capitalist systems investment, distribution, income and prices are determined by markets, whether regulated or unregulated. There are different variations of capitalism with different relationships to markets. In laissez-faire and free-market variations of capitalism, markets are utilized most extensively with minimal or no state intervention and minimal or no regulation over prices and the supply of goods and services. In interventionist, welfare capitalism and mixed economies, markets continue to play a dominant role, but they are regulated to some extent by government in order to correct market failures or to promote social welfare. In state capitalist systems, markets are relied upon the least, with the state relying heavily on either indicative planning and/or state-owned enterprises to accumulate capital. Capitalism has been dominant in the Western world since the end of feudalism. However, it is argued that the term mixed economies more precisely describes most contemporary economies due to their containing both private-owned and state-owned enterprises. In capitalism, prices determine the demand-supply scale. Higher demand for certain goods and services lead to higher prices and lower demand for certain goods lead to lower prices.
A capitalist free-market economy is an economic system where prices for goods and services are set freely by the forces of supply and demand and are allowed to reach their point of equilibrium without intervention by government policy. It typically entails support for highly competitive markets, private ownership of productive enterprises. Laissez-faire is a more extensive form of free-market economy where the role of the state is limited to protecting property rights.
Laissez-faire is synonymous with what was referred to as strict capitalist free-market economy during the early and mid-19th century as a classical liberal ideal to achieve. It is generally understood that the necessary components for the functioning of an idealized free market include the complete absence of government regulation, subsidies, artificial price pressures and government-granted monopolies (usually classified as coercive monopoly by free market advocates) and no taxes or tariffs other than what is necessary for the government to provide protection from coercion and theft, maintaining peace and property rights and providing for basic public goods. Right-libertarian advocates of anarcho-capitalism see the state as morally illegitimate and economically unnecessary and destructive. Although laissez-faire has been commonly associated with capitalism, there is a similar left-wing laissez-faire system called free-market anarchism, also known as free-market anti-capitalism and free-market socialism to distinguish it from laissez-faire capitalism. Thus, critics of laissez-faire as commonly understood argues that a truly laissez-faire system would be anti-capitalist and socialist.
Welfare capitalism is a capitalist economy that includes public policies favoring extensive provisions for social welfare services. The economic mechanism involves a free market and the predominance of privately owned enterprises in the economy, but public provision of universal welfare services aimed at enhancing individual autonomy and maximizing equality. Examples of contemporary welfare capitalism include the Nordic model of capitalism predominant in Northern Europe.
Anglo-Saxon capitalism is the form of capitalism predominant in Anglophone countries and typified by the economy of the United States. It is contrasted with European models of capitalism such as the continental social market model and the Nordic model. Anglo-Saxon capitalism refers to a macroeconomic policy regime and capital market structure common to the Anglophone economies. Among these characteristics are low rates of taxation, more open financial markets, lower labor market protections and a less generous welfare state eschewing collective bargaining schemes found in the continental and northern European models of capitalism.
The East Asian model of capitalism involves a strong role for state investment and in some instances involves state-owned enterprises. The state takes an active role in promoting economic development through subsidies, the facilitation of "national champions" and an export-based model of growth. The actual practice of this model varies by country. This designation has been applied to the economies of China, Japan, Singapore, South Korea and Taiwan. A related concept in political science is the developmental state.
The social market economy was implemented by Alfred Müller-Armack and Ludwig Erhard after World War II in West Germany. The social market economic model, sometimes called Rhine capitalism, is based upon the idea of realizing the benefits of a free-market economy, especially economic performance and high supply of goods while avoiding disadvantages such as market failure, destructive competition, concentration of economic power and the socially harmful effects of market processes. The aim of the social market economy is to realize greatest prosperity combined with best possible social security. One difference from the free market economy is that the state is not passive, but instead takes active regulatory measures. The social policy objectives include employment, housing and education policies, as well as a socio-politically motivated balancing of the distribution of income growth. Characteristics of social market economies are a strong competition policy and a contractionary monetary policy. The philosophical background is neoliberalism or ordoliberalism.
Market socialism is a form of market economy where the means of production are socially owned. In a market socialist economy, firms operate according to the rules of supply and demand and operate to maximize profit; the principal difference between market socialism and capitalism being that the profits accrue to society as a whole as opposed to private owners.The distinguishing feature between non-market socialism and market socialism is the existence of a market for factors of production and the criteria of profitability for enterprises. Profits derived from publicly owned enterprises can variously be used to reinvest in further production, to directly finance government and social services, or be distributed to the public at large through a social dividend or basic income system.Advocates of market socialism such as Jaroslav Vanek argue that genuinely free markets are not possible under conditions of private ownership of productive property. Instead, he contends that the class differences and inequalities in income and power that result from private ownership enable the interests of the dominant class to skew the market to their favor, either in the form of monopoly and market power, or by utilizing their wealth and resources to legislate government policies that benefit their specific business interests. Additionally, Vanek states that workers in a socialist economy based on cooperative and self-managed enterprises have stronger incentives to maximize productivity because they would receive a share of the profits (based on the overall performance of their enterprise) in addition to receiving their fixed wage or salary. The stronger incentives to maximize productivity that he conceives as possible in a socialist economy based on cooperative and self-managed enterprises might be accomplished in a free-market economy if employee-owned companies were the norm as envisioned by various thinkers including Louis O. Kelso and James S. Albus.
Market socialism traces its roots to classical economics and the works of Adam Smith, the Ricardian socialists and mutualist philosophers.In the 1930s, the economists Oskar Lange and Abba Lerner developed a model of socialism that posited that a public body (dubbed the Central Planning Board) could set prices through a trial-and-error approach until they equaled the marginal cost of production in order to achieve perfect competition and pareto optimality. In this model of socialism, firms would be state-owned and managed by their employees and the profits would be disbursed among the population in a social dividend. This model came to be referred to as market socialism because it involved the use of money, a price system and simulated capital markets, all of which were absent from traditional of non-market socialism. A more contemporary model of market socialism is that put forth by the American economist John Roemer, referred to as economic democracy. In this model, social ownership is achieved through public ownership of equity in a market economy. A Bureau of Public Ownership would own controlling shares in publicly listed firms, so that the profits generated would be used for public finance and the provision of a basic income. Some anarchists and libertarian socialists promote a form of market socialism in which enterprises are owned and managed cooperatively by their workforce so that the profits directly remunerate the employee-owners. These cooperative enterprises would compete with each other in the same way private companies compete with each other in a capitalist market. The first major elaboration of this type of market socialism was made by Pierre-Joseph Proudhon and was called mutualism. Self-managed market socialism was promoted in Yugoslavia by economists Branko Horvat and Jaroslav Vanek. In the self-managed model of socialism, firms would be directly owned by their employees and the management board would be elected by employees. These cooperative firms would compete with each other in a market for both capital goods and for selling consumer goods.
Following the 1978 reforms, China developed what it calls a socialist market economy in which most of the economy is under state ownership, with the state enterprises organized as joint-stock companies with various government agencies owning controlling shares through a shareholder system. Prices are set by a largely free-price system and the state-owned enterprises are not subjected to micromanagement by a government planning agency. A similar system called socialist-oriented market economy has emerged in Vietnam following the Đổi Mới reforms in 1986. This system is frequently characterized as state capitalism instead of market socialism because there is no meaningful degree of employee self-management in firms, because the state enterprises retain their profits instead of distributing them to the workforce or government and because many function as de facto private enterprises. The profits neither finance a social dividend to benefit the population at large, nor do they accrue to their employees. In China, this economic model is presented as a preliminary stage of socialism to explain the dominance of capitalistic management practices and forms of enterprise organization in both the state and non-state sectors.
A wide range of philosophers and theologians have linked market economies to monotheistic values. Michael Novak described capitalism as being closely related to Catholicism, but Max Weber drew a connection between capitalism and Protestantism. The economist Jeffrey Sachs has stated that his work was inspired by the healing characteristics of Judaism. Chief Rabbi Lord Sacks of the United Synagogue draws a correlation between modern capitalism and the Jewish image of the Golden Calf.
In the Christian faith, the liberation theology movement advocated involving the church in labor market capitalism. Many priests and nuns integrated themselves into labor organizations while others moved into the slums to live among the poor. The Holy Trinity was interpreted as a call for social equality and the elimination of poverty. However, the Pope was highly active in his criticism of liberation theology. He was particularly concerned about the increased fusion between Christianity and Marxism. He closed Catholic institutions that taught liberation theology and dismissed some of its activists from the church.
The Buddhist approach to the market economy was dealt with in E. F. Schumacher’s 1966 essay "Buddhist Economics". Schumacher asserted that a market economy guided by Buddhist principles would more successfully meet the needs of its people. He emphasized the importance or pursuing occupations that adhered to Buddhist teachings. The essay would later become required reading for a course that Clair Brown offered at University of California, Berkeley.
The economist Joseph Stiglitz argues that markets suffer from informational inefficiency and the presumed efficiency of markets stems from the faulty assumptions of neoclassical welfare economics, particularly the assumption of perfect and costless information and related incentive problems. Neoclassical economics assumes static equilibrium and efficient markets require that there be no non-convexities, even though nonconvexities are pervasive in modern economies. Stiglitz's critique applies to both existing models of capitalism and to hypothetical models of market socialism. However, Stiglitz does not advocate replacing markets, but instead states that there is a significant role for government intervention to boost the efficiency of markets and to address the pervasive market failures that exist in contemporary economies. A fair market economy is in fact a martingale or a Brownian motion model and for a participant competitor in such a model there is no more than 50% of success chances at any given moment. Due to the fractal nature of any fair market and being market participants subject to the law of competition which impose reinvesting an increasing part of profits, the mean statistical chance of bankruptcy within the half life of any participant is also 50% and 100% whether an infinite sample of time is considered. Robin Hahnel and Michael Albert claim that "markets inherently produce class division". Albert states that even if everyone started out with a balanced job complex (doing a mix of roles of varying creativity, responsibility and empowerment) in a market economy, class divisions would arise, arguing: Without taking the argument that far, it is evident that in a market system with uneven distribution of empowering work, such as Economic Democracy, some workers will be more able than others to capture the benefits of economic gain. For example, if one worker designs cars and another builds them, the designer will use his cognitive skills more frequently than the builder. In the long term, the designer will become more adept at conceptual work than the builder, giving the former greater bargaining power in a firm over the distribution of income. A conceptual worker who is not satisfied with his income can threaten to work for a company that will pay him more. The effect is a class division between conceptual and manual laborers, and ultimately managers and workers, and a de facto labor market for conceptual workers. David McNally argues in the Marxist tradition that the logic of the market inherently produces inequitable outcomes and leads to unequal exchanges, arguing that Adam Smith's moral intent and moral philosophy espousing equal exchange was undermined by the practice of the free markets he championed. The development of the market economy involved coercion, exploitation and violence that Smith's moral philosophy could not countenance. McNally also criticizes market socialists for believing in the possibility of fair markets based on equal exchanges to be achieved by purging parasitical elements from the market economy such as private ownership of the means of production. McNally argues that market socialism is an oxymoron when socialism is defined as an end to wage-based labor.
Market Systems at Encyclopædia Britannica Online.
The degree may be offered as a terminal degree or as additional preparation for doctoral study, and is sometimes offered as a professional degree, such as the emerging degree, MPS in Applied Economics. The program emphases and curricula will differ correspondingly. The course of study for the master's degree lasts from one to two years. A thesis is often required, particularly for terminal degrees. Many universities (in the United States) do not offer the master's degree directly; rather, the degree is routinely awarded as a master's degree "en route", after completion of a designated phase of the PhD program in economics. Typically, the curriculum is structured around core topics, with any optional coursework complementary to the program focus. The core modules are usually in microeconomic theory, macroeconomic theory and econometrics. At this level, the topics covered are microfoundations and dynamic stochastic general equilibrium, and allow for heterogeneity, relaxing the idea of a representative agent. Sometimes, topics from heterodox economics are introduced. Econometrics extends the undergraduate domain to multiple linear regression and multivariate time series, and introduces simultaneous equation methods and generalized linear models. Game theory and computational economics are often included. Some (doctoral) programs include core work in economic history. See Economics education #Curriculum. Theory-focused degrees will tend to cover these core topics more mathematically, and emphasize econometric theory as opposed to econometric techniques and software; these will also require a separate course in mathematical economics. Note though that regardless of focus, most programs "now place a marked emphasis on the primacy of mathematics", and many universities thus also require "quantitative techniques", especially where mathematical economics is not a core course. The optional or additional coursework will depend on the program's emphasis. In theory-focused degrees, and those preparing students for doctoral work, this coursework is often in these same core topics, but in greater depth. In terminal or applied or career-focused degrees, options may include public finance, labour-, financial-, development-, industrial-, health- or agricultural economics. These degrees may also allow for a specialization in one of these areas, and may be named correspondingly (for example Master's in Financial Economics, Masters in International Economics, Masters in Development Economics, Master's in Sustainable Economic Development and Masters in Agricultural Economics.) Entry requirements are undergraduate work in (calculus-based) economics, at least at the "intermediate" level, and often as a major, and a sufficient level of mathematical training (including courses in probability and statistics; often (multivariable) calculus and linear algebra; and sometimes mathematical analysis.)
Bachelor of Economics Category:Economics schools Civilekonom; Civiløkonom; Siviløkonom; Cand.oecon. Economics education European Joint Master degree in Economics QEM EMLE Outline of economics
Discussion Graduate Training in Economics, The American Economic Association Masters degrees in economics: How to be a leader of the future, The Independent Is an M.A. in Economics a Waste of Time?, economics.about.com (archived) Getting an MBA vs. a Master's in Finance or Economics, mbapodcaster.com Applying to graduate school in economics, davidson.edu Books to Study Before Going to Graduate School in Economics, economics.about.comLists of programs Alphabetical List of U.S. Programs
A mixed economy is variously defined as an economic system blending elements of a market economy with elements of a planned economy, free markets with state interventionism, or private enterprise with public enterprise. While there is no single definition of a mixed economy, one definition is about a mixture of markets with state interventionism, referring specifically to a capitalist market economy with strong regulatory oversight and extensive interventions into markets. Another is that of an active collaboration of capitalist and socialist visions. Yet another definition is apolitical in nature, strictly referring to an economy containing a mixture of private enterprise with public enterprise. Alternatively, a mixed economy can refer to a socialist economy that allows a substantial role for private enterprise and contracting within a dominant economic framework of public ownership. This can extend to a Soviet-type planned economy that has been reformed to incorporate a greater role for markets in the allocation of factors of production.In most cases, particularly with reference to Western economies, a mixed economy is a capitalist economy characterized by the predominance of private ownership of the means of production, with profit-seeking enterprise and the accumulation of capital as its fundamental driving force. In such a system, markets are subject to varying degrees of regulatory control and governments wield indirect macroeconomic influence through fiscal and monetary policies with a view to counteracting capitalism's history of boom/bust cycles, unemployment and income disparities. In this framework, varying degrees of public utilities and essential services are provided by government, with state activity often limited to providing public goods and universal civic requirements, including healthcare, physical infrastructure and management of public lands. This contrasts with laissez-faire capitalism, where state activity is limited to providing public goods and services as well as the infrastructure and legal framework to protect property rights and enforce contracts.In reference to Western European economic models as championed by conservatives (Christian democrats), liberals (social liberals) and socialists (social democrats) as part of the post-war consensus, a mixed economy is a form of capitalism where most industries are privately owned, with only a small number of public utilities and essential services under public ownership, usually 15–20%. In the post-war era, Western European social democracy became associated with this economic model. As an economic ideal, mixed economies are supported by people of various political persuasions, typically centre-left and centre-right such as Christian democrats or social democrats. The contemporary capitalist welfare state has been described as a type of mixed economy—in the sense of state interventionism as opposed to a mixture of planning and markets—since economic planning was never a feature or key component of the welfare state.
While there is not a single, encompassing definition of a mixed economy, there are generally two major definitions, one being political and the other apolitical. The political definition of a mixed economy refers to the degree of state interventionism in a market economy, portraying the state as encroaching onto the market under the assumption that the market is the natural mechanism for allocating resources. The political definition is limited to capitalistic economies and precludes an extension to non-capitalist systems, being concerned with public policy and state influence in the market.The apolitical definition relates to patterns of ownership and management of economic enterprises in an economy. The apolitical definition of mixed economy strictly refers to a mix of public and private ownership of enterprises in the economy and is unconcerned with political forms and public policy.
The term mixed economy arose in the context of political debate in the United Kingdom in the postwar period, although the set of policies later associated with the term had been advocated from at least the 1930s. Supporters of the mixed economy, including R. H. Tawney, Anthony Crosland and Andrew Shonfield were mostly associated with the Labour Party, although similar views were expressed by Conservatives including Harold Macmillan. Critics of the mixed economy, including Ludwig von Mises and Friedrich von Hayek, argued that there can be no lasting middle ground between economic planning and a market economy and any move in the direction of socialist planning is an unintentional move toward what Hilaire Bloc called "the servile state".
In the apolitical sense, the term mixed economy is used to describe economic systems that combine various elements of market economies and planned economies. As most political-economic ideologies are defined in an idealized sense, what is described rarely—if ever—exists in practice. Most would not consider it unreasonable to label an economy that, while not being a perfect representation, very closely resembles an ideal by applying the rubric that denominates that ideal. When a system in question, however, diverges to a significant extent from an idealized economic model or ideology, the task of identifying it can become problematic. Hence, the term mixed economy was coined. As it is unlikely that an economy will contain a perfectly even mix, mixed economies are usually noted as being skewed towards either private ownership or public ownership, toward capitalism or socialism, or toward a market economy or command economy in varying degrees.
Jesuit author David Hollenbach has argued that Catholic social teaching calls for a "new form" of mixed economy. He refers back to Pope Pius XI's statement that government "should supply help to the members of the social body, but may never destroy or absorb them". Hollenbach writes that a socially just mixed economy involves labour, management and the state working together through a pluralistic system that distributes economic power widely.However, subsequent scholars have noted that conceiving of subsidiarity as a "top-down, government-driven political exercise" requires a selective reading of 1960s encyclicals. A more comprehensive reading of Catholic social teaching suggests a conceptualization of subsidiarity as a "bottom-up concept" that is "rooted in recognition of a common humanity, not in the political equivalent of noblese oblige".
Although fascism is primarily a political ideology that stresses the importance of cultural and social issues over economics, fascism is generally supportive of a broadly capitalistic mixed economy. Fascism supports a state interventionism into markets and private enterprise, alongside a corporatist framework referred to as the "third position" that ostensibly aims to be a middle-ground between socialism and capitalism by mediating labour and business disputes to promote national unity. 20th-century fascist regimes in Italy and Germany adopted large public works programs to stimulate their economies, state interventionism in largely private-sector dominated economies to promote re-armament and national interests. Scholars have drawn parallels between the American New Deal and public works programs promoted by fascism, arguing that fascism similarly arose in response to the threat of socialist revolution and similarly aimed to "save capitalism" and private property.
In the early post-war era in Western Europe, social democratic parties rejected the Stalinist political and economic model then current in the Soviet Union, committing themselves either to an alternate path to socialism or to a compromise between capitalism and socialism. In this period, social democrats embraced a mixed economy based on the predominance of private property, with only a minority of essential utilities and public services under public ownership. As a result, social democracy became associated with Keynesian economics, state interventionism and the welfare state while abandoning the prior goal of replacing the capitalist system (factor markets, private property and wage labor) with a qualitatively different socialist economic system.
Mixed economies understood as a mixture of socially owned and private enterprise have been predicted and advocated by various socialists as a necessary transitional form between capitalism and socialism. Additionally, a number of proposals for socialist systems call for a mixture of different forms of enterprise ownership including a role for private enterprise. For example, Alexander Nove's conception of feasible socialism outlines an economic system based on a combination of state-enterprises for large industries, worker and consumer cooperatives, private enterprises for small-scale operations and individually owned enterprises.The social democratic theorist Eduard Bernstein advocated a form of mixed economy, believing that a mixed system of public, cooperative and private enterprise would be necessary for a long period of time before capitalism would evolve of its own accord into socialism.The People's Republic of China adopted a socialist market economy which represents an early stage of socialist development according to the Communist Party of China. The communist party takes the Marxist–Leninist position that an economic system containing diverse forms of ownership—but with the public sector playing a decisive role—is a necessary characteristic of an economy in the preliminary stage of developing socialism.The Socialist Republic of Vietnam describes its economy as a socialist-oriented market economy that consists of a mixture of public, private and cooperative enterprise—a mixed economy that is oriented toward the long-term development of a socialist economy.
This meaning of a mixed economy refers to a combination of market forces with state intervention in the form of regulations, macroeconomic policies and social welfare interventions aimed at improving market outcomes. As such, this type of mixed economy falls under the framework of a capitalistic market economy, with macroeconomic interventions aimed at promoting the stability of capitalism. Other examples of common government activity in this form of mixed economy include environmental protection, maintenance of employment standards, a standardized welfare system and maintenance of competition. Most contemporary market-oriented economies fall under this category, including the economy of the United States. The term is also used to describe the economies of countries that feature extensive welfare states such as the Nordic model practiced by the Nordic countries which combine free markets with an extensive welfare state.The German social market economy is the economic policy of modern Germany that steers a middle path between the goals of social democracy and capitalism within the framework of a private market economy and aims at maintaining a balance between a high rate of economic growth, low inflation, low levels of unemployment, good working conditions, public welfare and public services by using state intervention. Under its influence, Germany emerged from desolation and defeat to become an industrial giant within the European Union.The American School is the economic philosophy that dominated United States national policies from the time of the American Civil War until the mid-twentieth century. It consisted of three core policy initiatives: protecting industry through high tariffs (1861–1932; changing to subsidies and reciprocity from 1932–1970s), government investment in infrastructure through internal improvements and a national bank to promote the growth of productive enterprises. During this period, the United States grew into the largest economy in the world, surpassing the United Kingdom by 1880.
This type of mixed economy specifically refers to a mixture of private and public ownership of industry and the means of production. As such, it is sometimes described as a "middle path" or transitional state between capitalism and socialism, but it can also refer to a mixture of state capitalism with private capitalism. Examples include the economies of China, Norway, Singapore and Vietnam—all of which feature large state-owned enterprise sectors operating alongside large private sectors. The French economy featured a large state sector from 1945 until 1986, mixing a substantial amount of state-owned enterprises and nationalized firms with private enterprise.Following the Chinese economic reforms initiated in 1978, the Chinese economy has reformed its state-owned enterprises and allowed greater scope for private enterprise to operate alongside the state and collective sectors. In the 1990s, the central government concentrated its ownership in strategic sectors of the economy, but local and provincial level state-owned enterprises continue to operate in almost every industry including information technology, automobiles, machinery and hospitality. The latest round of state-owned enterprise reform initiated in 2013 stressed increased dividend payouts of state enterprises to the central government and mixed ownership reform which includes partial private investment into state-owned firms. As a result, many nominally private-sector firms are actually partially state-owned by various levels of government and state institutional investors; and many state-owned enterprises are partially privately owned resulting in a mixed ownership economy.
This type of mixed economy refers to a combination of economic planning with market forces for the guiding of production in an economy and may coincide with a mixture of private and public enterprise. It can include capitalist economies with indicative macroeconomic planning policies and socialist planned economies that introduced market forces into their economies such as in Hungary. Dirigisme was an economic policy initiated under Charles de Gaulle in France, designating an economy where the government exerts strong directive influence through indicative economic planning. In the period of dirigisme, the French state used indicative economic planning to supplement market forces for guiding its market economy. It involved state control of industries such as transportation, energy and telecommunication infrastructures as well as various incentives for private corporations to merge or engage in certain projects. Under its influence, France experienced what is called Thirty Glorious Years of profound economic growth.Hungary inaugurated the New Economic Mechanism reforms in 1968 that introduced market processes into its planned economy. Under this system, firms were still publicly owned but not subject to physical production targets and output quotas specified by a national plan. Firms were attached to state ministries which had the power to merge, dissolve and reorganize them and which established the firm's operating sector. Enterprises had to acquire their inputs and sell their outputs in markets, eventually eroding away at the Soviet-style planned economy. In 2010, Australian economist John Quiggin wrote: "The experience of the twentieth century suggests that a mixed economy will outperform both central planning and laissez-faire. The real question for policy debates is one of determining the appropriate mix, and the way in which the public and private sectors should interact."
Numerous economists have questioned the validity of the entire concept of a mixed economy when understood to be a mixture of capitalism and socialism. In Human Action, Ludwig von Mises argued that there can be no mixture of capitalism and socialism—either market logic or economic planning must dominate an economy. Mises elaborated on this point by contending that even if a market economy contained numerous state-run or nationalized enterprises, this would not make the economy mixed because the existence of such organizations does not alter the fundamental characteristics of the market economy. These publicly owned enterprises would still be subject to market sovereignty as they would have to acquire capital goods through markets, strive to maximize profits or at the least try to minimize costs and utilize monetary accounting for economic calculation.Classical and orthodox Marxist theorists also dispute the viability of a mixed economy as a middle ground between socialism and capitalism. Irrespective of enterprise ownership, either the capitalist law of value and accumulation of capital drives the economy or conscious planning and non-monetary forms of valuation ultimately drive the economy. From the Great Depression onward, extant mixed economies in the Western world are still functionally capitalist because they operate on the basis of capital accumulation.
Mixed economy at Encyclopædia Britannica Online Mixed economy – a variety of definitions for mixed economy
Neoclassical economics is an approach to economics focusing on the determination of goods, outputs, and income distributions in markets through supply and demand. This determination is often mediated through a hypothesized maximization of utility by income-constrained individuals and of profits by firms facing production costs and employing available information and factors of production, in accordance with rational choice theory, a theory that has come under considerable question in recent years. Neoclassical economics dominated microeconomics and, together with Keynesian economics, formed the neoclassical synthesis which dominated mainstream economics as Neo-Keynesian economics from the 1950s to the 1970s. It competed with New Keynesian economics as New classical macroeconomics in explaining macroeconomic phenomenon from the 1970s till the 1990s, when it was identified as having became a part of the new neoclassical synthesis along with New Keynesianism. There have been many critiques of neoclassical economics, often incorporated into newer versions of neoclassical theory, but some remaining distinct fields.
The term was originally introduced by Thorstein Veblen in his 1900 article 'Preconceptions of Economic Science', in which he related marginalists in the tradition of Alfred Marshall et al. to those in the Austrian School. No attempt will here be made even to pass a verdict on the relative claims of the recognized two or three main "schools" of theory, beyond the somewhat obvious finding that, for the purpose in hand, the so-called Austrian school is scarcely distinguishable from the neo-classical, unless it be in the different distribution of emphasis. The divergence between the modernized classical views, on the one hand, and the historical and Marxist schools, on the other hand, is wider, so much so, indeed, as to bar out a consideration of the postulates of the latter under the same head of inquiry with the former. – Veblen It was later used by John Hicks, George Stigler, and others to include the work of Carl Menger, William Stanley Jevons, Léon Walras, John Bates Clark, and many others. Today it is usually used to refer to mainstream economics, although it has also been used as an umbrella term encompassing a number of other schools of thought, notably excluding institutional economics, various historical schools of economics, and Marxian economics, in addition to various other heterodox approaches to economics. Neoclassical economics is characterized by several assumptions common to many schools of economic thought. There is not a complete agreement on what is meant by neoclassical economics, and the result is a wide range of neoclassical approaches to various problem areas and domains—ranging from neoclassical theories of labor to neoclassical theories of demographic changes.
It was expressed by E. Roy Weintraub that neoclassical economics rests on three assumptions, although certain branches of neoclassical theory may have different approaches: People have rational preferences between outcomes that can be identified and associated with values. Individuals maximize utility and firms maximize profits. People act independently on the basis of full and relevant information.From these three assumptions, neoclassical economists have built a structure to understand the allocation of scarce resources among alternative ends—in fact understanding such allocation is often considered the definition of economics to neoclassical theorists. Here's how William Stanley Jevons presented "the problem of Economics". Given, a certain population, with various needs and powers of production, in possession of certain lands and other sources of material: required, the mode of employing their labour which will maximize the utility of their produce. From the basic assumptions of neoclassical economics comes a wide range of theories about various areas of economic activity. For example, profit maximization lies behind the neoclassical theory of the firm, while the derivation of demand curves leads to an understanding of consumer goods, and the supply curve allows an analysis of the factors of production. Utility maximization is the source for the neoclassical theory of consumption, the derivation of demand curves for consumer goods, and the derivation of labor supply curves and reservation demand.Market supply and demand are aggregated across firms and individuals. Their interactions determine equilibrium output and price. The market supply and demand for each factor of production is derived analogously to those for market final output to determine equilibrium income and the income distribution. Factor demand incorporates the marginal-productivity relationship of that factor in the output market.Neoclassical economics emphasizes equilibria, which are the solutions of agent maximization problems. Regularities in economies are explained by methodological individualism, the position that economic phenomena can be explained by aggregating over the behavior of agents. The emphasis is on microeconomics. Institutions, which might be considered as prior to and conditioning individual behavior, are de-emphasized. Economic subjectivism accompanies these emphases. See also general equilibrium.
Classical economics, developed in the 18th and 19th centuries, included a value theory and distribution theory. The value of a product was thought to depend on the costs involved in producing that product. The explanation of costs in classical economics was simultaneously an explanation of distribution. A landlord received rent, workers received wages, and a capitalist tenant farmer received profits on their investment. This classic approach included the work of Adam Smith and David Ricardo. However, some economists gradually began emphasizing the perceived value of a good to the consumer. They proposed a theory that the value of a product was to be explained with differences in utility (usefulness) to the consumer. (In England, economists tended to conceptualize utility in keeping with the utilitarianism of Jeremy Bentham and later of John Stuart Mill.) The third step from political economy to economics was the introduction of marginalism and the proposition that economic actors made decisions based on margins. For example, a person decides to buy a second sandwich based on how full he or she is after the first one, a firm hires a new employee based on the expected increase in profits the employee will bring. This differs from the aggregate decision making of classical political economy in that it explains how vital goods such as water can be cheap, while luxuries can be expensive.
The change in economic theory from classical to neoclassical economics has been called the "marginal revolution", although it has been argued that the process was slower than the term suggests. It is frequently dated from William Stanley Jevons's Theory of Political Economy (1871), Carl Menger's Principles of Economics (1871), and Léon Walras's Elements of Pure Economics (1874–1877). Historians of economics and economists have debated: Whether utility or marginalism was more essential to this revolution (whether the noun or the adjective in the phrase "marginal utility" is more important) Whether there was a revolutionary change of thought or merely a gradual development and change of emphasis from their predecessors Whether grouping these economists together disguises differences more important than their similarities.In particular, Jevons saw his economics as an application and development of Jeremy Bentham's utilitarianism and never had a fully developed general equilibrium theory. Menger did not embrace this hedonic conception, explained diminishing marginal utility in terms of subjective prioritization of possible uses, and emphasized disequilibrium and the discrete; further Menger had an objection to the use of mathematics in economics, while the other two modeled their theories after 19th century mechanics. Jevons built on the hedonic conception of Bentham or of Mill, while Walras was more interested in the interaction of markets than in explaining the individual psyche.Alfred Marshall's textbook, Principles of Economics (1890), was the dominant textbook in England a generation later. Marshall's influence extended elsewhere; Italians would compliment Maffeo Pantaleoni by calling him the "Marshall of Italy". Marshall thought classical economics attempted to explain prices by the cost of production. He asserted that earlier marginalists went too far in correcting this imbalance by overemphasizing utility and demand. Marshall thought that "We might as reasonably dispute whether it is the upper or the under blade of a pair of scissors that cuts a piece of paper, as whether value is governed by utility or cost of production". Marshall explained price by the intersection of supply and demand curves. The introduction of different market "periods" was an important innovation of Marshall's: Market period. The goods produced for sale on the market are taken as given data, e.g. in a fish market. Prices quickly adjust to clear markets. Short period. Industrial capacity is taken as given. The level of output, the level of employment, the inputs of raw materials, and prices fluctuate to equate marginal cost and marginal revenue, where profits are maximized. Economic rents exist in short period equilibrium for fixed factors, and the rate of profit is not equated across sectors. Long period. The stock of capital goods, such as factories and machines, is not taken as given. Profit-maximizing equilibria determine both industrial capacity and the level at which it is operated. Very long period. Technology, population trends, habits and customs are not taken as given, but allowed to vary in very long period models.Marshall took supply and demand as stable functions and extended supply and demand explanations of prices to all runs. He argued supply was easier to vary in longer runs, and thus became a more important determinant of price in the very long run.
An important change in neoclassical economics occurred around 1933. Joan Robinson and Edward H. Chamberlin, with the near simultaneous publication of their respective books, The Economics of Imperfect Competition (1933) and The Theory of Monopolistic Competition (1933), introduced models of imperfect competition. Theories of market forms and industrial organization grew out of this work. They also emphasized certain tools, such as the marginal revenue curve. Joan Robinson's work on imperfect competition, at least, was a response to certain problems of Marshallian partial equilibrium theory highlighted by Piero Sraffa. Anglo-American economists also responded to these problems by turning towards general equilibrium theory, developed on the European continent by Walras and Vilfredo Pareto. J. R. Hicks's Value and Capital (1939) was influential in introducing his English-speaking colleagues to these traditions. He, in turn, was influenced by the Austrian School economist Friedrich Hayek's move to the London School of Economics, where Hicks then studied. These developments were accompanied by the introduction of new tools, such as indifference curves and the theory of ordinal utility. The level of mathematical sophistication of neoclassical economics increased. Paul Samuelson's Foundations of Economic Analysis (1947) contributed to this increase in mathematical modelling. The interwar period in American economics has been argued to have been pluralistic, with neoclassical economics and institutionalism competing for allegiance. Frank Knight, an early Chicago school economist attempted to combine both schools. But this increase in mathematics was accompanied by greater dominance of neoclassical economics in Anglo-American universities after World War II. Some argue that outside political interventions, such as McCarthyism, and internal ideological bullying played an important role in this rise to dominance. Hicks' book, Value and Capital had two main parts. The second, which was arguably not immediately influential, presented a model of temporary equilibrium. Hicks was influenced directly by Hayek's notion of intertemporal coordination and paralleled by earlier work by Lindhal. This was part of an abandonment of disaggregated long run models. This trend probably reached its culmination with the Arrow–Debreu model of intertemporal equilibrium. The Arrow–Debreu model has canonical presentations in Gérard Debreu's Theory of Value (1959) and in Arrow and Hahn's "General Competitive Analysis" (1971). Many of these developments were against the backdrop of improvements in both econometrics, that is the ability to measure prices and changes in goods and services, as well as their aggregate quantities, and in the creation of macroeconomics, or the study of whole economies. The attempt to combine neo-classical microeconomics and Keynesian macroeconomics would lead to the neoclassical synthesis which was the dominant paradigm of economic reasoning in English-speaking countries from the 1950s till the 1970s. Hicks and Samuelson were for example instrumental in mainstreaming Keynesian economics. The dominance of Neo-Keynesian economics was upset by its inability to explain the economic crises of the 1970s- neoclassical economics emerged distinctly in macroeconomics as the new classical school, which sought to explain macroeconomic phenomenon using neoclassical microeconomics. It and its contemporary New Keynesian economics contributed to the new neoclassical synthesis of the 1990s, which informs much of mainstream economics today.
Criticism of neoclassical economics was offered by Leijonhufvud in the contention that "Instead of looking for an alternative to replace it, we should try to imagine an economic theory to transcend its limitations." In criticism, neoclassical economics is often conflated with all of mainstream economics. Neoclassical economics is sometimes criticized for having a normative bias. In this view, it does not focus on explaining actual economies, but instead on describing a theoretical world in which Pareto optimality applies.Criticisms of neoclassical economics are also directed at the rationality assumption, in particular on the basis of the view that the rationality assumption cannot be reconciled with altruisric behaviour. Many see the "economic man" as being quite different from real people, the Econ different from the Human. Many economists, even contemporaries, have criticized this model of economic man, with empirical evidence (as noted, especially in Behavioral Economics) growing in support of representing a person as a Human rather than an Econ. Thorstein Veblen claimed that neoclassical economics assumes a person to be: [A] lightning calculator of pleasures and pains, who oscillates like a homogeneous globule of desire of happiness under the impulse of stimuli that shift about the area, but leave him intact. Neoclassical economics, according to this criticism, has extreme difficulty explaining such things as voting behavior, or someone running into a burning building to save a complete stranger. Such so-called "non-rational" decision making has been examined in Behavioral Economics. G.A. Cory claims that behavioral Economics has demonstrated that while the Econ almost exclusively pursues only self-interest, the Human pursues a Dual Interest. The Dual Interest, according to Cory, includes both the Ego-based self-interest and the Empathy-based other (shared with others, yet internalized within the own-self)-interest.Problems exist with making the neoclassical general equilibrium theory compatible with an economy that develops over time and includes capital goods. This was explored in a major debate in the 1960s—the "Cambridge capital controversy"—about the validity of neoclassical economics, with an emphasis on economic growth, capital, aggregate theory, and the marginal productivity theory of distribution. There were also internal attempts by neoclassical economists to extend the Arrow–Debreu model to disequilibrium investigations of stability and uniqueness. However a result known as the Sonnenschein–Mantel–Debreu theorem suggests that the assumptions that must be made to ensure that equilibrium is stable and unique are quite restrictive. Neoclassical economics is also often seen as relying too heavily on complex mathematical models, such as those used in general equilibrium theory, without enough regard to whether these actually describe the real economy. Many see an attempt to model a system as complex as a modern economy by a mathematical model as unrealistic and doomed to failure. A famous answer to this criticism is Milton Friedman's claim that theories should be judged by their ability to predict events rather than by the realism of their assumptions. Mathematical models also include those in game theory, linear programming, and econometrics. Some see mathematical models used in contemporary research in mainstream economics as having transcended neoclassical economics, while others disagree. Critics of neoclassical economics are divided into those who think that highly mathematical method is inherently wrong and those who think that mathematical method is useful even if neoclassical economics has other problems.In general, allegedly overly unrealistic assumptions are one of the most common criticisms of neoclassical economics. It is fair to say that many (but not all) of these criticisms can only be directed towards a subset of the neoclassical models (for example, there are many neoclassical models where unregulated markets fail to achieve Pareto-optimality and there has recently been an increased interest in modeling non-rational decision making).It has been argued within the field of Ecological Economics that the Neoclassical Economics system is by nature dysfunctional. It, according to Ecological economics, considers the destruction of the natural world through the accelerating consumption of non-renewable resources as well as the exhaustion of the "waste sinks" of the ecosphere as mere "externalities." Such externalities, in turn, are allegedly viewed as occurring only occasionally, and easily rectified by shifting public property to private property: The Market will resolve any externalitity, given the opportunity to do so; so, there is no need for any kind of Government, or any other kind of Community "intervention." The need to consider "Empathy", in order to address the matter of achieving sustainability on a "Spaceship Earth", is also becoming a theme in the natural and environmental sciences.
Marginalism Market economy Microeconomics Neoclassical synthesis Static equilibrium (economics)
North America is a continent entirely within the Northern Hemisphere and almost all within the Western Hemisphere. It can also be described as a northern subcontinent of the Americas. It is bordered to the north by the Arctic Ocean, to the east by the Atlantic Ocean, to the southeast by South America and the Caribbean Sea, and to the west and south by the Pacific Ocean. North America covers an area of about 24,709,000 square kilometers (9,540,000 square miles), about 16.5% of the Earth's land area and about 4.8% of its total surface. North America is the third-largest continent by area, following Asia and Africa, and the fourth by population after Asia, Africa, and Europe. In 2013, its population was estimated at nearly 579 million people in 23 independent states, or about 7.5% of the world's population, if nearby islands (most notably around the Caribbean) are included. North America was reached by its first human populations during the last glacial period, via crossing the Bering land bridge approximately 40,000 to 17,000 years ago. The so-called Paleo-Indian period is taken to have lasted until about 10,000 years ago (the beginning of the Archaic or Meso-Indian period). The classic stage spans roughly the 6th to 13th centuries. The pre-Columbian era ended in 1492, with the beginning of the transatlantic migrations of European settlers during the Age of Discovery and the early modern period. Present-day cultural and ethnic patterns reflect interactions between European colonists, indigenous peoples, African slaves, immigrants, and the descendants of these groups. Owing to Europe's colonization of the Americas, most North Americans speak European languages such as English, Spanish or French, and their states' cultures commonly reflect Western traditions.
The Americas are usually accepted as having been named after the Italian explorer Amerigo Vespucci by the German cartographers Martin Waldseemüller and Matthias Ringmann. Vespucci, who explored South America between 1497 and 1502, was the first European to suggest that the Americas were not the East Indies, but a different landmass previously unknown by Europeans. In 1507, Waldseemüller produced a world map, in which he placed the word "America" on the continent of South America, in the middle of what is today Brazil. He explained the rationale for the name in the accompanying book Cosmographiae Introductio: ... ab Americo inventore ... quasi Americi terram sive Americam (from Americus the discoverer ... as if it were the land of Americus, thus America). For Waldseemüller, no one should object to the naming of the land after its discoverer. He used the Latinized version of Vespucci's name (Americus Vespucius), but in its feminine form "America", following the examples of "Europa", "Asia" and "Africa". Later, other mapmakers extended the name America to the northern continent. In 1538, Gerard Mercator used America on his map of the world for all the Western Hemisphere.Some argue that because the convention is to use the surname for naming discoveries (except in the case of royalty), the derivation from "Amerigo Vespucci" could be put in question. In 1874, Thomas Belt proposed a derivation from the Amerrique mountains of Central America; the next year, Jules Marcou suggested that the name of the mountain range stemmed from indigenous American languages. Marcou corresponded with Augustus Le Plongeon, who wrote: "The name AMERICA or AMERRIQUE in the Mayan language means, a country of perpetually strong wind, or the Land of the Wind, and ... the [suffixes] can mean ... a spirit that breathes, life itself."Mercator on his map called North America "America or New India" (America sive India Nova).
The United Nations formally recognizes "North America" as comprising three areas: Northern America, Central America, and The Caribbean. This has been formally defined by the UN Statistics Division."Northern America", as a term distinct from "North America", excludes Central America, which itself may or may not include Mexico (see Central America § Different definitions). In the limited context of the North American Free Trade Agreement, the term covers Canada, the United States, and Mexico, which are the three signatories of that treaty. France, Italy, Portugal, Spain, Romania, Greece, and the countries of Latin America use a six-continent model, with the Americas viewed as a single continent and North America designating a subcontinent comprising Canada, the United States, and Mexico, and often Greenland, Saint Pierre et Miquelon, and Bermuda.North America has been historically referred to by other names. Spanish North America (New Spain) was often referred to as Northern America, and this was the first official name given to Mexico.
Geographically the North American continent has many regions and subregions. These include cultural, economic, and geographic regions. Economic regions included those formed by trade blocs, such as the North American Trade Agreement bloc and Central American Trade Agreement. Linguistically and culturally, the continent could be divided into Anglo-America and Latin America. Anglo-America includes most of Northern America, Belize, and Caribbean islands with English-speaking populations (though sub-national entities, such as Louisiana and Quebec, have large Francophone populations; in Quebec, French is the sole official language). The southern North American continent is composed of two regions. These are Central America and the Caribbean. The north of the continent maintains recognized regions as well. In contrast to the common definition of "North America", which encompasses the whole continent, the term "North America" is sometimes used to refer only to Mexico, Canada, the United States, and Greenland.The term Northern America refers to the northernmost countries and territories of North America: the United States, Bermuda, St. Pierre and Miquelon, Canada and Greenland. Although the term does not refer to a unified region, Middle America—not to be confused with the Midwestern United States—groups the regions of Mexico, Central America, and the Caribbean.The largest countries of the continent, Canada and the United States, also contain well-defined and recognized regions. In the case of Canada these are (from east to west) Atlantic Canada, Central Canada, Canadian Prairies, the British Columbia Coast, and Northern Canada. These regions also contain many subregions. In the case of the United States – and in accordance with the US Census Bureau definitions – these regions are: New England, Mid-Atlantic, South Atlantic States, East North Central States, West North Central States, East South Central States, West South Central States, Mountain States, and Pacific States. Regions shared between both nations included the Great Lakes Region. Megalopolises have formed between both nations in the case of the Pacific Northwest and the Great Lakes Megaregion.
North America occupies the northern portion of the landmass generally referred to as the New World, the Western Hemisphere, the Americas, or simply America (which, in many countries is considered as a single continent with North America a subcontinent). North America is the third-largest continent by area, following Asia and Africa. North America's only land connection to South America is at the Isthmus of Darian/Isthmus of Panama. The continent is delimited on the southeast by most geographers at the Darién watershed along the Colombia-Panama border, placing almost all of Panama within North America. Alternatively, some geologists physiographically locate its southern limit at the Isthmus of Tehuantepec, Mexico, with Central America extending southeastward to South America from this point. The Caribbean islands, or West Indies, are considered part of North America. The continental coastline is long and irregular. The Gulf of Mexico is the largest body of water indenting the continent, followed by Hudson Bay. Others include the Gulf of Saint Lawrence and the Gulf of California. Before the Central American isthmus formed, the region had been underwater. The islands of the West Indies delineate a submerged former land bridge, which had connected North and South America via what are now Florida and Venezuela. There are numerous islands off the continent's coasts; principally, the Arctic Archipelago, the Bahamas, Turks & Caicos, the Greater and Lesser Antilles, the Aleutian Islands (some of which are in the Eastern Hemisphere proper), the Alexander Archipelago, the many thousand islands of the British Columbia Coast, and Newfoundland. Greenland, a self-governing Danish island, and the world's largest, is on the same tectonic plate (the North American Plate) and is part of North America geographically. In a geologic sense, Bermuda is not part of the Americas, but an oceanic island which was formed on the fissure of the Mid-Atlantic Ridge over 100 million years ago. The nearest landmass to it is Cape Hatteras, North Carolina. However, Bermuda is often thought of as part of North America, especially given its historical, political and cultural ties to Virginia and other parts of the continent. The vast majority of North America is on the North American Plate. Parts of western Mexico, including Baja California, and of California, including the cities of San Diego, Los Angeles, and Santa Cruz, lie on the eastern edge of the Pacific Plate, with the two plates meeting along the San Andreas fault. The southernmost portion of the continent and much of the West Indies lie on the Caribbean Plate, whereas the Juan de Fuca and Cocos plates border the North American Plate on its western frontier. The continent can be divided into four great regions (each of which contains many subregions): the Great Plains stretching from the Gulf of Mexico to the Canadian Arctic; the geologically young, mountainous west, including the Rocky Mountains, the Great Basin, California and Alaska; the raised but relatively flat plateau of the Canadian Shield in the northeast; and the varied eastern region, which includes the Appalachian Mountains, the coastal plain along the Atlantic seaboard, and the Florida peninsula. Mexico, with its long plateaus and cordilleras, falls largely in the western region, although the eastern coastal plain does extend south along the Gulf. The western mountains are split in the middle into the main range of the Rockies and the coast ranges in California, Oregon, Washington, and British Columbia, with the Great Basin—a lower area containing smaller ranges and low-lying deserts—in between. The highest peak is Denali in Alaska. The United States Geographical Survey (USGS) states that the geographic center of North America is "6 miles [10 km] west of Balta, Pierce County, North Dakota" at about 48°10′N 100°10′W, about 24 kilometres (15 mi) from Rugby, North Dakota. The USGS further states that "No marked or monumented point has been established by any government agency as the geographic center of either the 50 States, the conterminous United States, or the North American continent." Nonetheless, there is a 4.6-metre (15 ft) field stone obelisk in Rugby claiming to mark the center. The North American continental pole of inaccessibility is located 1,650 km (1,030 mi) from the nearest coastline, between Allen and Kyle, South Dakota at 43.36°N 101.97°W﻿ / 43.36; -101.97﻿ (Pole of Inaccessibility North America).
Laurentia is an ancient craton which forms the geologic core of North America; it formed between 1.5 and 1.0 billion years ago during the Proterozoic eon. The Canadian Shield is the largest exposure of this craton. From the Late Paleozoic to Early Mesozoic eras, North America was joined with the other modern-day continents as part of the supercontinent Pangaea, with Eurasia to its east. One of the results of the formation of Pangaea was the Appalachian Mountains, which formed some 480 million years ago, making it among the oldest mountain ranges in the world. When Pangaea began to rift around 200 million years ago, North America became part of Laurasia, before it separated from Eurasia as its own continent during the mid-Cretaceous period. The Rockies and other western mountain ranges began forming around this time from a period of mountain building called the Laramide orogeny, between 80 and 55 million years ago. The formation of the Isthmus of Panama that connected the continent to South America arguably occurred approximately 12 to 15 million years ago, and the Great Lakes (as well as many other northern freshwater lakes and rivers) were carved by receding glaciers about 10,000 years ago. North America is the source of much of what humanity knows about geologic time periods. The geographic area that would later become the United States has been the source of more varieties of dinosaurs than any other modern country. According to paleontologist Peter Dodson, this is primarily due to stratigraphy, climate and geography, human resources, and history. Much of the Mesozoic Era is represented by exposed outcrops in the many arid regions of the continent. The most significant Late Jurassic dinosaur-bearing fossil deposit in North America is the Morrison Formation of the western United States.
Geologically, Canada is one of the oldest regions in the world, with more than half of the region consisting of precambrian rocks that have been above sea level since the beginning of the Palaeozoic era. Canada's mineral resources are diverse and extensive. Across the Canadian Shield and in the north there are large iron, nickel, zinc, copper, gold, lead, molybdenum, and uranium reserves. Large diamond concentrations have been recently developed in the Arctic, making Canada one of the world's largest producers. Throughout the Shield there are many mining towns extracting these minerals. The largest, and best known, is Sudbury, Ontario. Sudbury is an exception to the normal process of forming minerals in the Shield since there is significant evidence that the Sudbury Basin is an ancient meteorite impact crater. The nearby, but less known Temagami Magnetic Anomaly has striking similarities to the Sudbury Basin. Its magnetic anomalies are very similar to the Sudbury Basin, and so it could be a second metal-rich impact crater. The Shield is also covered by vast boreal forests that support an important logging industry.
The lower 48 US states can be divided into roughly five physiographic provinces: The American cordillera The Canadian Shield Northern portion of the upper midwestern United States. The stable platform The coastal plain The Appalachian orogenic beltThe geology of Alaska is typical of that of the cordillera, while the major islands of Hawaii consist of Neogene volcanics erupted over a hot spot.
Central America is geologically active with volcanic eruptions and earthquakes occurring from time to time. In 1976 Guatemala was hit by a major earthquake, killing 23,000 people; Managua, the capital of Nicaragua, was devastated by earthquakes in 1931 and 1972, the last one killing about 5,000 people; three earthquakes devastated El Salvador, one in 1986 and two in 2001; one earthquake devastated northern and central Costa Rica in 2009, killing at least 34 people; in Honduras a powerful earthquake killed seven people in 2009. Volcanic eruptions are common in the region. In 1968 the Arenal Volcano, in Costa Rica, erupted and killed 87 people. Fertile soils from weathered volcanic lavas have made it possible to sustain dense populations in the agriculturally productive highland areas. Central America has many mountain ranges; the longest are the Sierra Madre de Chiapas, the Cordillera Isabelia, and the Cordillera de Talamanca. Between the mountain ranges lie fertile valleys that are suitable for the people; in fact, most of the population of Honduras, Costa Rica, and Guatemala live in valleys. Valleys are also suitable for the production of coffee, beans, and other crops.
North America is a very large continent which surpasses the Arctic Circle, and the Tropic of Cancer. Greenland, along with the Canadian Shield, is tundra with average temperatures ranging from 10 to 20 °C (50 to 68 °F), but central Greenland is composed of a very large ice sheet. This tundra radiates throughout Canada, but its border ends near the Rocky Mountains (but still contains Alaska) and at the end of the Canadian Shield, near the Great Lakes. Climate west of the Cascades is described as being a temperate weather with average precipitation 20 inches (510 mm). Climate in coastal California is described to be Mediterranean, with average temperatures in cities like San Francisco ranging from 57 to 70 °F (14 to 21 °C) over the course of the year.Stretching from the East Coast to eastern North Dakota, and stretching down to Kansas, is the continental-humid climate featuring intense seasons, with a large amount of annual precipitation, with places like New York City averaging 50 inches (1,300 mm). Starting at the southern border of the continental-humid climate and stretching to the Gulf of Mexico (whilst encompassing the eastern half of Texas) is the subtropical climate. This area has the wettest cities in the contiguous U.S. with annual precipitation reaching 67 inches (1,700 mm) in Mobile, Alabama. Stretching from the borders of the continental humid and subtropical climates, and going west to the Cascades Sierra Nevada, south to the southern tip of durango, north to the border with tundra climate, the steppe/desert climate is the driest climate in the U.S. Highland climates cut from north to south of the continent, where subtropical or temperate climates occur just below the tropics, as in central Mexico and Guatemala. Tropical climates appear in the island regions and in the subcontinent's bottleneck. Usually of the savannah type, with rains and high temperatures constants the whole year. Found in countries and states bathed by the Caribbean Sea or to south of the Gulf of Mexico and Pacific Ocean.
Notable North American fauna include the bison, black bear, prairie dog, turkey, pronghorn, raccoon, coyote and monarch butterfly. Notable plants that were domesticated in North America include tobacco, maize, squash, tomato, sunflower, blueberry, avocado, cotton, chile pepper and vanilla.
The indigenous peoples of the Americas have many creation myths by which they assert that they have been present on the land since its creation, but there is no evidence that humans evolved there. The specifics of the initial settlement of the Americas by ancient Asians are subject to ongoing research and discussion. The traditional theory has been that hunters entered the Beringia land bridge between eastern Siberia and present-day Alaska from 27,000 to 14,000 years ago. A growing viewpoint is that the first American inhabitants sailed from Beringia some 13,000 years ago, with widespread habitation of the Americas during the end of the Last Glacial Period, in what is known as the Late Glacial Maximum, around 12,500 years ago. The oldest petroglyphs in North America date from 15,000 to 10,000 years before present. Genetic research and anthropology indicate additional waves of migration from Asia via the Bering Strait during the Early-Middle Holocene.Before contact with Europeans, the natives of North America were divided into many different polities, from small bands of a few families to large empires. They lived in several "culture areas", which roughly correspond to geographic and biological zones and give a good indication of the main way of life of the people who lived there (e.g., the bison hunters of the Great Plains, or the farmers of Mesoamerica). Native groups can also be classified by their language family (e.g., Athapascan or Uto-Aztecan). Peoples with similar languages did not always share the same material culture, nor were they always allies. Anthropologists think that the Inuit people of the high Arctic came to North America much later than other native groups, as evidenced by the disappearance of Dorset culture artifacts from the archaeological record, and their replacement by the Thule people. During the thousands of years of native habitation on the continent, cultures changed and shifted. One of the oldest yet discovered is the Clovis culture (c. 9550–9050 BCE) in modern New Mexico. Later groups include the Mississippian culture and related Mound building cultures, found in the Mississippi river valley and the Pueblo culture of what is now the Four Corners. The more southern cultural groups of North America were responsible for the domestication of many common crops now used around the world, such as tomatoes, squash, and maize. As a result of the development of agriculture in the south, many other cultural advances were made there. The Mayans developed a writing system, built huge pyramids and temples, had a complex calendar, and developed the concept of zero around 400 CE.The first recorded European references to North America are in Norse sagas where it is referred to as Vinland. The earliest verifiable instance of pre-Columbian trans-oceanic contact by any European culture with the North America mainland has been dated to around 1000 CE. The site, situated at the northernmost extent of the island named Newfoundland, has provided unmistakable evidence of Norse settlement. Norse explorer Leif Erikson (c. 970–1020 CE) is thought to have visited the area. Erikson was the first European to make landfall on the continent (excluding Greenland). The Mayan culture was still present in southern Mexico and Guatemala when the Spanish conquistadors arrived, but political dominance in the area had shifted to the Aztec Empire, whose capital city Tenochtitlan was located further north in the Valley of Mexico. The Aztecs were conquered in 1521 by Hernán Cortés.
During the Age of Discovery, Europeans explored and staked claims to various parts of North America. Upon their arrival in the "New World", the Native American population declined substantially, because of violent conflicts with the invaders and the introduction of European diseases to which the Native Americans lacked immunity. Native culture changed drastically and their affiliation with political and cultural groups also changed. Several linguistic groups died out, and others changed quite quickly. In 1513, Juan Ponce de León, who had accompanied Columbus's second voyage, visited and named La Florida. As the colonial period unfolded, Britain, Spain, and France took over extensive territories in North America. In the late 18th and early 19th century, independence movements sprung up across the continent, leading to the founding of the modern countries in the area. The 13 British Colonies on the North Atlantic coast declared independence in 1776, becoming the United States of America. Canada was formed from the unification of northern territories controlled by Britain and France. New Spain, a territory that stretched from the modern-day southern US to Central America, declared independence in 1810, becoming the First Mexican Empire. In 1823 the former Captaincy General of Guatemala, then part of the Mexican Empire, became the first independent state in Central America, officially changing its name to the United Provinces of Central America. Over three decades of work on the Panama Canal led to the connection of Atlantic and Pacific waters in 1913, physically making North America a separate continent.
Economically, Canada and the United States are the wealthiest and most developed nations in the continent, followed by Mexico, a newly industrialized country. The countries of Central America and the Caribbean are at various levels of economic and human development. For example, small Caribbean island-nations, such as Barbados, Trinidad and Tobago, and Antigua and Barbuda, have a higher GDP (PPP) per capita than Mexico due to their smaller populations. Panama and Costa Rica have a significantly higher Human Development Index and GDP than the rest of the Central American nations. Additionally, despite Greenland's vast resources in oil and minerals, much of them remain untapped, and the island is economically dependent on fishing, tourism, and subsidies from Denmark. Nevertheless, the island is highly developed.Demographically, North America is ethnically diverse. Its three main groups are Caucasians, Mestizos and Blacks. There is a significant minority of Indigenous Americans and Asians among other less numerous groups.
The dominant languages in North America are English, Spanish, and French. Danish is prevalent in Greenland alongside Greenlandic, and Dutch is spoken side by side local languages in the Dutch Caribbean. The term Anglo-America is used to refer to the anglophone countries of the Americas: namely Canada (where English and French are co-official) and the United States, but also sometimes Belize and parts of the tropics, especially the Commonwealth Caribbean. Latin America refers to the other areas of the Americas (generally south of the United States) where the Romance languages, derived from Latin, of Spanish and Portuguese (but French speaking countries are not usually included) predominate: the other republics of Central America (but not always Belize), part of the Caribbean (not the Dutch-, English-, or French-speaking areas), Mexico, and most of South America (except Guyana, Suriname, French Guiana (France), and the Falkland Islands (UK)). The French language has historically played a significant role in North America and now retains a distinctive presence in some regions. Canada is officially bilingual. French is the official language of the Province of Quebec, where 95% of the people speak it as either their first or second language, and it is co-official with English in the Province of New Brunswick. Other French-speaking locales include the Province of Ontario (the official language is English, but there are an estimated 600,000 Franco-Ontarians), the Province of Manitoba (co-official as de jure with English), the French West Indies and Saint-Pierre et Miquelon, as well as the US state of Louisiana, where French is also an official language. Haiti is included with this group based on historical association but Haitians speak both Creole and French. Similarly, French and French Antillean Creole is spoken in Saint Lucia and the Commonwealth of Dominica alongside English. A significant number of Indigenous languages are spoken in North America, with 372,000 people in the United States speaking an indigenous language at home, about 225,000 in Canada and roughly 6 million in Mexico. In the United States and Canada, there are approximately 150 surviving indigenous languages of the 300 spoken prior to European contact.
Christianity is the largest religion in the United States, Canada and Mexico. According to a 2012 Pew Research Center survey, 77% of the population considered themselves Christians. Christianity also is the predominant religion in the 23 dependent territories in North America. The United States has the largest Christian population in the world, with nearly 247 million Christians (70%), although other countries have higher percentages of Christians among their populations. Mexico has the world's second largest number of Catholics, surpassed only by Brazil. A 2015 study estimates about 493,000 Christian believers from a Muslim background in North America, most of them belonging to some form of Protestantism.According to the same study religiously unaffiliated (include agnostic and atheist) make up about 17% of the population of Canada and the United States. No religion make up about 24% of the United States population, and 24% of Canada total population.Canada, the United States and Mexico host communities of both Jews (6 million or about 1.8%), Buddhists (3.8 million or 1.1%) and Muslims (3.4 million or 1.0%). The biggest number of Jewish individuals can be found in the United States (5.4 million), Canada (375,000) and Mexico (67,476). The United States host the largest Muslim population in North America with 2.7 million or 0.9%, While Canada host about one million Muslim or 3.2% of the population. While in Mexico there were 3,700 Muslims in the country. In 2012, U-T San Diego estimated U.S. practitioners of Buddhism at 1.2 million people, of whom 40% are living in Southern California.The predominant religion in Central America is Christianity (96%). Beginning with the Spanish colonization of Central America in the 16th century, Roman Catholicism became the most popular religion in the region until the first half of the 20th century. Since the 1960s, there has been an increase in other Christian groups, particularly Protestantism, as well as other religious organizations, and individuals identifying themselves as having no religion. Also Christianity is the predominant religion in the Caribbean (85%). Other religious groups in the region are Hinduism, Islam, Rastafari (in Jamaica), and Afro-American religions such as Santería and Vodou.
North America is the fourth most populous continent after Asia, Africa, and Europe. Its most populous country is the United States with 329.7 million persons. The second largest country is Mexico with a population of 112.3 million. Canada is the third most populous country with 37.0 million. The majority of Caribbean island-nations have national populations under a million, though Cuba, Dominican Republic, Haiti, Puerto Rico (a territory of the United States), Jamaica, and Trinidad and Tobago each have populations higher than a million. Greenland has a small population of 55,984 for its massive size (2,166,000 km2 or 836,300 mi2), and therefore, it has the world's lowest population density at 0.026 pop./km2 (0.067 pop./mi2).While the United States, Canada, and Mexico maintain the largest populations, large city populations are not restricted to those nations. There are also large cities in the Caribbean. The largest cities in North America, by far, are Mexico City and New York. These cities are the only cities on the continent to exceed eight million, and two of three in the Americas. Next in size are Los Angeles, Toronto, Chicago, Havana, Santo Domingo, and Montreal. Cities in the sun belt regions of the United States, such as those in Southern California and Houston, Phoenix, Miami, Atlanta, and Las Vegas, are experiencing rapid growth. These causes included warm temperatures, retirement of Baby Boomers, large industry, and the influx of immigrants. Cities near the United States border, particularly in Mexico, are also experiencing large amounts of growth. Most notable is Tijuana, a city bordering San Diego that receives immigrants from all over Latin America and parts of Europe and Asia. Yet as cities grow in these warmer regions of North America, they are increasingly forced to deal with the major issue of water shortages.Eight of the top ten metropolitan areas are located in the United States. These metropolitan areas all have a population of above 5.5 million and include the New York City metropolitan area, Los Angeles metropolitan area, Chicago metropolitan area, and the Dallas–Fort Worth metroplex. Whilst the majority of the largest metropolitan areas are within the United States, Mexico is host to the largest metropolitan area by population in North America: Greater Mexico City. Canada also breaks into the top ten largest metropolitan areas with the Toronto metropolitan area having six million people. The proximity of cities to each other on the Canada–United States border and Mexico–United States border has led to the rise of international metropolitan areas. These urban agglomerations are observed at their largest and most productive in Detroit–Windsor and San Diego–Tijuana and experience large commercial, economic, and cultural activity. The metropolitan areas are responsible for millions of dollars of trade dependent on international freight. In Detroit-Windsor the Border Transportation Partnership study in 2004 concluded US$13 billion was dependent on the Detroit–Windsor international border crossing while in San Diego-Tijuana freight at the Otay Mesa Port of Entry was valued at US$20 billion.North America has also been witness to the growth of megapolitan areas. In the United States exists eleven megaregions that transcend international borders and comprise Canadian and Mexican metropolitan regions. These are the Arizona Sun Corridor, Cascadia, Florida, Front Range, Great Lakes Megaregion, Gulf Coast Megaregion, Northeast, Northern California, Piedmont Atlantic, Southern California, and the Texas Triangle. Canada and Mexico are also the home of megaregions. These include the Quebec City – Windsor Corridor, Golden Horseshoe – both of which are considered part of the Great Lakes Megaregion – and megalopolis of Central Mexico. Traditionally the largest megaregion has been considered the Boston-Washington, DC Corridor, or the Northeast, as the region is one massive contiguous area. Yet megaregion criterion have allowed the Great Lakes Megalopolis to maintain status as the most populated region, being home to 53,768,125 people in 2000.The top ten largest North American metropolitan areas by population as of 2013, based on national census numbers from the United States and census estimates from Canada and Mexico. †2011 Census figures.
North America's GDP per capita was evaluated in October 2016 by the International Monetary Fund (IMF) to be $41,830, making it the richest continent in the world, followed by Oceania.Canada, Mexico, and the United States have significant and multifaceted economic systems. The United States has the largest economy of all three countries and in the world. In 2016, the U.S. had an estimated per capita gross domestic product (PPP) of $57,466 according to the World Bank, and is the most technologically developed economy of the three. The United States' services sector comprises 77% of the country's GDP (estimated in 2010), industry comprises 22% and agriculture comprises 1.2%. The U.S. economy is also the fastest growing economy in North America and the Americas as a whole, with the highest GDP per capita in the Americas as well. Canada shows significant growth in the sectors of services, mining and manufacturing. Canada's per capita GDP (PPP) was estimated at $44,656 and it had the 11th largest GDP (nominal) in 2014. Canada's services sector comprises 78% of the country's GDP (estimated in 2010), industry comprises 20% and agriculture comprises 2%. Mexico has a per capita GDP (PPP) of $16,111 and as of 2014 is the 15th largest GDP (nominal) in the world. Being a newly industrialized country, Mexico maintains both modern and outdated industrial and agricultural facilities and operations. Its main sources of income are oil, industrial exports, manufactured goods, electronics, heavy industry, automobiles, construction, food, banking and financial services.The North American economy is well defined and structured in three main economic areas. These areas are the North American Free Trade Agreement (NAFTA), Caribbean Community and Common Market (CARICOM), and the Central American Common Market (CACM). Of these trade blocs, the United States takes part in two. In addition to the larger trade blocs there is the Canada-Costa Rica Free Trade Agreement among numerous other free trade relations, often between the larger, more developed countries and Central American and Caribbean countries. The North America Free Trade Agreement (NAFTA) forms one of the four largest trade blocs in the world. Its implementation in 1994 was designed for economic homogenization with hopes of eliminating barriers of trade and foreign investment between Canada, the United States and Mexico. While Canada and the United States already conducted the largest bilateral trade relationship – and to present day still do – in the world and Canada–United States trade relations already allowed trade without national taxes and tariffs, NAFTA allowed Mexico to experience a similar duty-free trade. The free trade agreement allowed for the elimination of tariffs that had previously been in place on United States-Mexico trade. Trade volume has steadily increased annually and in 2010, surface trade between the three NAFTA nations reached an all-time historical increase of 24.3% or US$791 billion. The NAFTA trade bloc GDP (PPP) is the world's largest with US$17.617 trillion. This is in part attributed to the fact that the economy of the United States is the world's largest national economy; the country had a nominal GDP of approximately $14.7 trillion in 2010. The countries of NAFTA are also some of each other's largest trade partners. The United States is the largest trade partner of Canada and Mexico; while Canada and Mexico are each other's third largest trade partners. The Caribbean trade bloc – CARICOM – came into agreement in 1973 when it was signed by 15 Caribbean nations. As of 2000, CARICOM trade volume was US$96 billion. CARICOM also allowed for the creation of a common passport for associated nations. In the past decade the trade bloc focused largely on Free Trade Agreements and under the CARICOM Office of Trade Negotiations (OTN) free trade agreements have been signed into effect. Integration of Central American economies occurred under the signing of the Central American Common Market agreement in 1961; this was the first attempt to engage the nations of this area into stronger financial cooperation. Recent implementation of the Central American Free Trade Agreement (CAFTA) has left the future of the CACM unclear. The Central American Free Trade Agreement was signed by five Central American countries, the Dominican Republic, and the United States. The focal point of CAFTA is to create a free trade area similar to that of NAFTA. In addition to the United States, Canada also has relations in Central American trade blocs. Currently under proposal, the Canada – Central American Free Trade Agreement (CA4) would operate much the same as CAFTA with the United States does. These nations also take part in inter-continental trade blocs. Mexico takes a part in the G3 Free Trade Agreement with Colombia and Venezuela and has a trade agreement with the EU. The United States has proposed and maintained trade agreements under the Transatlantic Free Trade Area between itself and the European Union; the US-Middle East Free Trade Area between numerous Middle Eastern nations and itself; and the Trans-Pacific Strategic Economic Partnership between Southeast Asian nations, Australia, and New Zealand.
The Pan-American Highway route in the Americas is the portion of a network of roads nearly 48,000 km (30,000 mi) in length which travels through the mainland nations. No definitive length of the Pan-American Highway exists because the US and Canadian governments have never officially defined any specific routes as being part of the Pan-American Highway, and Mexico officially has many branches connecting to the US border. However, the total length of the portion from Mexico to the northern extremity of the highway is roughly 26,000 km (16,000 mi). The First Transcontinental Railroad in the United States was built in the 1860s, linking the railroad network of the eastern US with California on the Pacific coast. Finished on 10 May 1869 at the famous golden spike event at Promontory Summit, Utah, it created a nationwide mechanized transportation network that revolutionized the population and economy of the American West, catalyzing the transition from the wagon trains of previous decades to a modern transportation system. Although an accomplishment, it achieved the status of first transcontinental railroad by connecting myriad eastern US railroads to the Pacific and was not the largest single railroad system in the world. The Canadian Grand Trunk Railway (GTR) had, by 1867, already accumulated more than 2,055 km (1,277 mi) of track by connecting Ontario with the Canadian Atlantic provinces west as far as Port Huron, Michigan, through Sarnia, Ontario.
A shared telephone system known as the North American Numbering Plan (NANP) is an integrated telephone numbering plan of 24 countries and territories: the United States and its territories, Canada, Bermuda, and 17 Caribbean nations.
Canada and the United States were both former British colonies. There is frequent cultural interplay between the United States and English-speaking Canada. Greenland has experienced many immigration waves from Northern Canada, e.g. the Thule People. Therefore, Greenland shares some cultural ties with the indigenous people of Canada. Greenland is also considered Nordic and has strong Danish ties due to centuries of colonization by Denmark.Spanish-speaking North America shares a common past as former Spanish colonies. In Mexico and the Central American countries where civilizations like the Maya developed, indigenous people preserve traditions across modern boundaries. Central American and Spanish-speaking Caribbean nations have historically had more in common due to geographical proximity. Northern Mexico, particularly in the cities of Monterrey, Tijuana, Ciudad Juárez, and Mexicali, is strongly influenced by the culture and way of life of the United States. Of the aforementioned cities, Monterrey has been regarded as the most Americanized city in Mexico. Immigration to the United States and Canada remains a significant attribute of many nations close to the southern border of the US. The Anglophone Caribbean states have witnessed the decline of the British Empire and its influence on the region, and its replacement by the economic influence of Northern America in the Anglophone Caribbean. This is partly due to the relatively small populations of the English-speaking Caribbean countries, and also because many of them now have more people living abroad than those remaining at home. Northern Mexico, the Western United States and Alberta, Canada share a cowboy culture.
Canada, Mexico and the US submitted a joint bid to host the 2026 FIFA World Cup. The following table shows the most prominent sports leagues in North America, in order of average revenue.
Outline of North America Flags of North America List of cities in North America Table manners in North America Turtle Island (Native American folklore) – Name for North America among Native Americans
Footnotes Citations
Houghton Mifflin Company, "North America" Interactive SVG version of Non-Native American Nations Control over N America 1750–2008 animation
The Ocean-to-Ocean Bridge is a through truss bridge spanning the Colorado River in Yuma, Arizona. Built in 1915, it was the first highway crossing of the lower Colorado and is the earliest example of a through truss bridge in Arizona. It is also the only example of a Pennsylvania truss within Arizona. Originally the bridge carried the transcontinental Ocean-to-Ocean Highway and later carried its successor, US 80 until a new bridge was built to the west in 1956. Between 1988 and 2001, the bridge was closed to vehicular traffic and only traversable by pedestrians and bicyclists. After a major restoration, the bridge was rehabilitated and reopened to vehicular traffic in 2002, with a re-dedication by the Quechan nation and Yuma Crossing National Heritage Area. The bridge became part of Historic US 80 in 2018.
In 1913, following massive pressure against Arizona Congressman Carl Hayden by the citizens of Yuma, Hayden lobbied Congress, proposing federal aid be used for construction of a permanent highway crossing spanning the lower Colorado River. Congress authorized the construction of the bridge under the pretense that it was to be used in connecting Yuma to the Fort Yuma Indian Reservation located on the opposing bank of the river. It was therefore, the federal funding would be provided through the Office of Indian Affairs (OIA). Further capital was raised and provided by the State of Arizona and Imperial County, California; each providing $25,000. OIA engineers in Washington D.C. were tasked with designing the new bridge. The new bridge design was to be a steel, Pennsylvania through truss design complemented by a Warren deck truss second span, both resting upon concrete piers and abutments above the river. The location of the bridge was to be upstream from an existing ferry crossing at Prison Hill Road.The Omaha Structural Steel Works was awarded the construction contract by the OIA in June 1914, at a cost of $73,800. Construction on the bridge began in September 1914. Between October and February, problems arose constructing the bridge, as both Omaha Steel and the OIA were unfamiliar with the currents and flood patterns of the lower Colorado. Falsework erected to aid in the bridge's construction were twice washed downstream by floods. Omaha Steel decided to approach construction of the twin span bridge by a different method; constructing the spans on barges and floating each span down river into position. The 336 ft (102 m) span was floated down river and swung into place carefully and methodically on March 3, 1915, followed by local praise and celebration. Following completion, the Ocean-to-Ocean Bridge was ceremoniously opened to the public on May 22, 1915. The bridge became a crucial link in the nationwide transcontinental Ocean-to-Ocean Highway and was also the first highway bridge across the lower Colorado River.On November 11, 1926, the bridge became part of U.S. Route 80. The successor to the earlier Ocean-to-Ocean Highway, US 80 became the primary east to west transcontinental highway in Arizona and between the 1920s and 1930s, carried the majority of the state's auto traffic. During the Great Depression in the 1930s, the Ocean-to-Ocean Bridge was used by California state police officers to deny entry refugees of the dust bowl hailing from Oklahoma intending to find work in California. Often called "Okies", these people found work instead around Yuma, County between Yuma and Wellton. The refugees soon provided critically needed assistance to local farmers. In 1956, US 80 was re-routed off the Ocean-to-Ocean Bridge and onto a newer bridge built downstream at the foot of Fourth Avenue. Following construction of the Fourth Avenue Bridge and the construction of Interstate 8 in the early 1970s, vehicular traffic and importance of the Ocean-to-Ocean Bridge steadily declined. Similarly, the historic transcontinental highway which the bridge had once carried, US 80, declined and was removed from San Diego to Benson between 1964 and 1977, no longer running through Yuma. In 1978, the bridge was added to the National Register of Historic Places. Despite the newly gained honor, vehicular traffic was no longer allowed to use the bridge after 1988. In 2001, a $3 million restoration and rehabilitation project was begun, temporarily closing the bridge to pedestrians as well. Following the extensive restoration and a re-dedication ceremony, the bridge was re-opened to traffic in 2002 and now carries Penitentiary Avenue. The Ocean-to-Ocean Bridge was designated as part of Historic U.S. Route 80 by the Arizona Department of Transportation on September 21, 2018.
List of historic properties in Yuma, Arizona List of bridges on the National Register of Historic Places in Arizona National Register of Historic Places listings in Yuma County, Arizona Yuma Crossing Gillespie Dam Bridge
An ocean is a body of water that composes much of a planet's hydrosphere. On Earth, an ocean is one of the major conventional divisions of the World Ocean. These are, in descending order by area, the Pacific, Atlantic, Indian, Southern (Antarctic), and Arctic Oceans. The phrases "the ocean" or "the sea" used without specification refer to the interconnected body of salt water covering the majority of the Earth's surface. As a general term, "the ocean" is mostly interchangeable with "the sea" in American English, but not in British English. Strictly speaking, a sea is a body of water (generally a division of the world ocean) partly or fully enclosed by land.Saline seawater covers approximately 361,000,000 km2 (139,000,000 sq mi) and is customarily divided into several principal oceans and smaller seas, with the ocean covering approximately 71% of Earth's surface and 90% of the Earth's biosphere. The ocean contains 97% of Earth's water, and oceanographers have stated that less than 20% of the World Ocean has been mapped. The total volume is approximately 1.35 billion cubic kilometers (320 million cu mi) with an average depth of nearly 3,700 meters (12,100 ft).As the world ocean is the principal component of Earth's hydrosphere, it is integral to life, forms part of the carbon cycle, and influences climate and weather patterns. The World Ocean is the habitat of 230,000 known species, but because much of it is unexplored, the number of species that exist in the ocean is much larger, possibly over two million. The origin of Earth's oceans is unknown; oceans are thought to have formed in the Hadean eon and may have been the cause for the emergence of life. Extraterrestrial oceans may be composed of water or other elements and compounds. The only confirmed large stable bodies of extraterrestrial surface liquids are the lakes of Titan, although there is evidence for the existence of oceans elsewhere in the Solar System. Early in their geologic histories, Mars and Venus are theorized to have had large water oceans. The Mars ocean hypothesis suggests that nearly a third of the surface of Mars was once covered by water, and a runaway greenhouse effect may have boiled away the global ocean of Venus. Compounds such as salts and ammonia dissolved in water lower its freezing point so that water might exist in large quantities in extraterrestrial environments as brine or convecting ice. Unconfirmed oceans are speculated beneath the surface of many dwarf planets and natural satellites; notably, the ocean of the moon Europa is estimated to have over twice the water volume of Earth. The Solar System's giant planets are also thought to have liquid atmospheric layers of yet to be confirmed compositions. Oceans may also exist on exoplanets and exomoons, including surface oceans of liquid water within a circumstellar habitable zone. Ocean planets are a hypothetical type of planet with a surface completely covered with liquid.
The word ocean comes from the figure in classical antiquity, Oceanus (; Greek: Ὠκεανός Ōkeanós, pronounced [ɔːkeanós]), the elder of the Titans in classical Greek mythology, believed by the ancient Greeks and Romans to be the divine personification of the sea, an enormous river encircling the world. The concept of Ōkeanós has an Indo-European connection. Greek Ōkeanós has been compared to the Vedic epithet ā-śáyāna-, predicated of the dragon Vṛtra-, who captured the cows/rivers. Related to this notion, the Okeanos is represented with a dragon-tail on some early Greek vases.
Though generally described as several separate oceans, the global, interconnected body of salt water is sometimes referred to as the World Ocean or global ocean. The concept of a continuous body of water with relatively free interchange among its parts is of fundamental importance to oceanography.The major oceanic divisions – listed below in descending order of area and volume – are defined in part by the continents, various archipelagos, and other criteria. Oceans are fringed by smaller, adjoining bodies of water such as seas, gulfs, bays, bights, and straits.
The mid-ocean ridges of the world are connected and form a single global mid-oceanic ridge system that is part of every ocean and the longest mountain range in the world. The continuous mountain range is 65,000 km (40,000 mi) long (several times longer than the Andes, the longest continental mountain range).
Oceanographers divide the ocean into different vertical zones defined by physical and biological conditions. The pelagic zone includes all open ocean regions, and can be divided into further regions categorized by depth and light abundance. The photic zone includes the oceans from the surface to a depth of 200 m; it is the region where photosynthesis can occur and is, therefore, the most biodiverse. Because plants require photosynthesis, life found deeper than the photic zone must either rely on material sinking from above (see marine snow) or find another energy source. Hydrothermal vents are the primary source of energy in what is known as the aphotic zone (depths exceeding 200 m). The pelagic part of the photic zone is known as the epipelagic. The pelagic part of the aphotic zone can be further divided into vertical regions according to temperature. The mesopelagic is the uppermost region. Its lowermost boundary is at a thermocline of 12 °C (54 °F), which, in the tropics generally lies at 700–1,000 meters (2,300–3,300 ft). Next is the bathypelagic lying between 10 and 4 °C (50 and 39 °F), typically between 700–1,000 meters (2,300–3,300 ft) and 2,000–4,000 meters (6,600–13,100 ft), lying along the top of the abyssal plain is the abyssopelagic, whose lower boundary lies at about 6,000 meters (20,000 ft). The last zone includes the deep oceanic trench, and is known as the hadalpelagic. This lies between 6,000–11,000 meters (20,000–36,000 ft) and is the deepest oceanic zone. The benthic zones are aphotic and correspond to the three deepest zones of the deep-sea. The bathyal zone covers the continental slope down to about 4,000 meters (13,000 ft). The abyssal zone covers the abyssal plains between 4,000 and 6,000 m. Lastly, the hadal zone corresponds to the hadalpelagic zone, which is found in oceanic trenches. The pelagic zone can be further subdivided into two subregions: the neritic zone and the oceanic zone. The neritic zone encompasses the water mass directly above the continental shelves whereas the oceanic zone includes all the completely open water. In contrast, the littoral zone covers the region between low and high tide and represents the transitional area between marine and terrestrial conditions. It is also known as the intertidal zone because it is the area where tide level affects the conditions of the region. If a zone undergoes dramatic changes in temperature with depth, it contains a thermocline. The tropical thermocline is typically deeper than the thermocline at higher latitudes. Polar waters, which receive relatively little solar energy, are not stratified by temperature and generally lack a thermocline because surface water at polar latitudes are nearly as cold as water at greater depths. Below the thermocline, water is very cold, ranging from −1 °C to 3 °C. Because this deep and cold layer contains the bulk of ocean water, the average temperature of the world ocean is 3.9 °C. If a zone undergoes dramatic changes in salinity with depth, it contains a halocline. If a zone undergoes a strong, vertical chemistry gradient with depth, it contains a chemocline. The halocline often coincides with the thermocline, and the combination produces a pronounced pycnocline.
Oceanic maritime currents have different origins. Tidal currents are in phase with the tide, hence are quasiperiodic; they may form various knots in certain places, most notably around headlands. Non-periodic currents have for origin the waves, wind and different densities. The wind and waves create surface currents (designated as “drift currents”). These currents can decompose in one quasi-permanent current (which varies within the hourly scale) and one movement of Stokes drift under the effect of rapid waves movement (at the echelon of a couple of seconds).). The quasi-permanent current is accelerated by the breaking of waves, and in a lesser governing effect, by the friction of the wind on the surface.This acceleration of the current takes place in the direction of waves and dominant wind. Accordingly, when the sea depth increases, the rotation of the earth changes the direction of currents in proportion with the increase of depth, while friction lowers their speed. At a certain sea depth, the current changes direction and is seen inverted in the opposite direction with current speed becoming null: known as the Ekman spiral. The influence of these currents is mainly experienced at the mixed layer of the ocean surface, often from 400 to 800 meters of maximum depth. These currents can considerably alter, change and are dependent on the various yearly seasons. If the mixed layer is less thick (10 to 20 meters), the quasi-permanent current at the surface adopts an extreme oblique direction in relation to the direction of the wind, becoming virtually homogeneous, until the Thermocline.In the deep however, maritime currents are caused by the temperature gradients and the salinity between water density masses. In littoral zones, breaking waves are so intense and the depth measurement so low, that maritime currents reach often 1 to 2 knots.
Ocean currents greatly affect Earth's climate by transferring heat from the tropics to the polar regions. Transferring warm or cold air and precipitation to coastal regions, winds may carry them inland. Surface heat and freshwater fluxes create global density gradients that drive the thermohaline circulation part of large-scale ocean circulation. It plays an important role in supplying heat to the polar regions, and thus in sea ice regulation. Changes in the thermohaline circulation are thought to have significant impacts on Earth's energy budget. In so far as the thermohaline circulation governs the rate at which deep waters reach the surface, it may also significantly influence atmospheric carbon dioxide concentrations. For a discussion of the possibilities of changes to the thermohaline circulation under global warming, see shutdown of thermohaline circulation. The Antarctic Circumpolar Current encircles that continent, influencing the area's climate and connecting currents in several oceans. One of the most dramatic forms of weather occurs over the oceans: tropical cyclones (also called "typhoons" and "hurricanes" depending upon where the system forms).
The ocean has a significant effect on the biosphere. Oceanic evaporation, as a phase of the water cycle, is the source of most rainfall, and ocean temperatures determine climate and wind patterns that affect life on land. Life within the ocean evolved 3 billion years prior to life on land. Both the depth and the distance from shore strongly influence the biodiversity of the plants and animals present in each region.As it is thought that life evolved in the ocean, the diversity of life is immense, including: Bacteria : ubiquitous single-celled prokaryotes found throughout the world Archaea : prokaryotes distinct from bacteria, that inhabit many environments of the ocean, as well as many extreme environments Algae : algae is a "catch-all" term to include many photosynthetic, single-celled eukaryotes, such as green algae, diatoms, and dinoflagellates, but also multicellular algae, such as some red algae (including organisms like Pyropia, which is the source of the edible nori seaweed), and brown algae (including organisms like kelp). Plants : including sea grasses, or mangroves Fungi : many marine fungi with diverse roles are found in oceanic environments Animals : most animal phyla have species that inhabit the ocean, including many that are only found in marine environments such as sponges, Cnidaria (such as corals and jellyfish), comb jellies, Brachiopods, and Echinoderms (such as sea urchins and sea stars). Many other familiar animal groups primarily live in the ocean, including cephalopods (includes octopus and squid), crustaceans (includes lobsters, crabs, and shrimp), fish, sharks, cetaceans (includes whales, dolphins, and porpoises).In addition, many land animals have adapted to living a major part of their life on the oceans. For instance, seabirds are a diverse group of birds that have adapted to a life mainly on the oceans. They feed on marine animals and spend most of their lifetime on water, many only going on land for breeding. Other birds that have adapted to oceans as their living space are penguins, seagulls and pelicans. Seven species of turtles, the sea turtles, also spend most of their time in the oceans.
A zone of rapid salinity increase with depth is called a halocline. The temperature of maximum density of seawater decreases as its salt content increases. Freezing temperature of water decreases with salinity, and boiling temperature of water increases with salinity. Typical seawater freezes at around −2 °C at atmospheric pressure. If precipitation exceeds evaporation, as is the case in polar and temperate regions, salinity will be lower. If evaporation exceeds precipitation, as is the case in tropical regions, salinity will be higher. Thus, oceanic waters in polar regions have lower salinity content than oceanic waters in temperate and tropical regions.Salinity can be calculated using the chlorinity, which is a measure of the total mass of halogen ions (includes fluorine, chlorine, bromine, and iodine) in seawater. By international agreement, the following formula is used to determine salinity: Salinity (in ‰) = 1.80655 × Chlorinity (in ‰)The average chlorinity is about 19.2‰, and, thus, the average salinity is around 34.7‰
The motions of the ocean surface, known as undulations or waves, are the partial and alternate rising and falling of the ocean surface. The series of mechanical waves that propagate along the interface between water and air is called swell.
Although Earth is the only known planet with large stable bodies of liquid water on its surface and the only one in the Solar System, other celestial bodies are thought to have large oceans. In June 2020, NASA scientists reported that it's likely that exoplanets with oceans may be common in the Milky Way galaxy, based on mathematical modeling studies.
The gas giants, Jupiter and Saturn, are thought to lack surfaces and instead have a stratum of liquid hydrogen; however their planetary geology is not well understood. The possibility of the ice giants Uranus and Neptune having hot, highly compressed, supercritical water under their thick atmospheres has been hypothesised. Although their composition is still not fully understood, a 2006 study by Wiktorowicz and Ingersall ruled out the possibility of such a water "ocean" existing on Neptune, though some studies have suggested that exotic oceans of liquid diamond are possible.The Mars ocean hypothesis suggests that nearly a third of the surface of Mars was once covered by water, though the water on Mars is no longer oceanic (much of it residing in the ice caps). The possibility continues to be studied along with reasons for their apparent disappearance. Astronomers now think that Venus may have had liquid water and perhaps oceans for over 2 billion years.
A global layer of liquid water thick enough to decouple the crust from the mantle is thought to be present on the natural satellites Titan, Europa, Enceladus and, with less certainty, Callisto, Ganymede and Triton. A magma ocean is thought to be present on Io. Geysers have been found on Saturn's moon Enceladus, possibly originating from an ocean about 10 kilometers (6.2 mi) beneath the surface ice shell. Other icy moons may also have internal oceans, or may once have had internal oceans that have now frozen.Large bodies of liquid hydrocarbons are thought to be present on the surface of Titan, although they are not large enough to be considered oceans and are sometimes referred to as lakes or seas. The Cassini–Huygens space mission initially discovered only what appeared to be dry lakebeds and empty river channels, suggesting that Titan had lost what surface liquids it might have had. Later flybys of Titan provided radar and infrared images that showed a series of hydrocarbon lakes in the colder polar regions. Titan is thought to have a subsurface liquid-water ocean under the ice in addition to the hydrocarbon mix that forms atop its outer crust.
Ceres appears to be differentiated into a rocky core and icy mantle and may harbour a liquid-water ocean under its surface.Not enough is known of the larger trans-Neptunian objects to determine whether they are differentiated bodies capable of supporting oceans, although models of radioactive decay suggest that Pluto, Eris, Sedna, and Orcus have oceans beneath solid icy crusts approximately 100 to 180 km thick. In June 2020, astronomers reported evidence that the dwarf planet Pluto may have had a subsurface ocean, and consequently may have been habitable, when it was first formed.
Some planets and natural satellites outside the Solar System are likely to have oceans, including possible water ocean planets similar to Earth in the habitable zone or "liquid-water belt". The detection of oceans, even through the spectroscopy method, however is likely extremely difficult and inconclusive. Theoretical models have been used to predict with high probability that GJ 1214 b, detected by transit, is composed of exotic form of ice VII, making up 75% of its mass, making it an ocean planet. Other possible candidates are merely speculated based on their mass and position in the habitable zone include planet though little is actually known of their composition. Some scientists speculate Kepler-22b may be an "ocean-like" planet. Models have been proposed for Gliese 581 d that could include surface oceans. Gliese 436 b is speculated to have an ocean of "hot ice". Exomoons orbiting planets, particularly gas giants within their parent star's habitable zone may theoretically have surface oceans. Terrestrial planets will acquire water during their accretion, some of which will be buried in the magma ocean but most of it will go into a steam atmosphere, and when the atmosphere cools it will collapse on to the surface forming an ocean. There will also be outgassing of water from the mantle as the magma solidifies—this will happen even for planets with a low percentage of their mass composed of water, so "super-Earth exoplanets may be expected to commonly produce water oceans within tens to hundreds of millions of years of their last major accretionary impact."
Oceans, seas, lakes and other bodies of liquids can be composed of liquids other than water, for example the hydrocarbon lakes on Titan. The possibility of seas of nitrogen on Triton was also considered but ruled out. There is evidence that the icy surfaces of the moons Ganymede, Callisto, Europa, Titan and Enceladus are shells floating on oceans of very dense liquid water or water–ammonia. Earth is often called the ocean planet because it is 70% covered in water. Extrasolar terrestrial planets that are extremely close to their parent star will be tidally locked and so one half of the planet will be a magma ocean. It is also possible that terrestrial planets had magma oceans at some point during their formation as a result of giant impacts. Hot Neptunes close to their star could lose their atmospheres via hydrodynamic escape, leaving behind their cores with various liquids on the surface. Where there are suitable temperatures and pressures, volatile chemicals that might exist as liquids in abundant quantities on planets include ammonia, argon, carbon disulfide, ethane, hydrazine, hydrogen, hydrogen cyanide, hydrogen sulfide, methane, neon, nitrogen, nitric oxide, phosphine, silane, sulfuric acid, and water.Supercritical fluids, although not liquids, do share various properties with liquids. Underneath the thick atmospheres of the planets Uranus and Neptune, it is expected that these planets are composed of oceans of hot high-density fluid mixtures of water, ammonia and other volatiles. The gaseous outer layers of Jupiter and Saturn transition smoothly into oceans of supercritical hydrogen. The atmosphere of Venus is 96.5% carbon dioxide, which is a supercritical fluid at its surface.
Oceans at Curlie Smithsonian Ocean Portal NOAA – National Oceanic and Atmospheric Administration – Ocean Ocean :: Science Daily Ocean-bearing Planets: Looking For Extraterrestrial Life In All The Right Places Titan Likely To Have Huge Underground Ocean | Mind Blowing Science Origins of the oceans and continents". UN Atlas of the Oceans.
Though the peoples of Asia and Oceania have traveled the Pacific Ocean since prehistoric times, the eastern Pacific was first sighted by Europeans in the early 16th century when Spanish explorer Vasco Núñez de Balboa crossed the Isthmus of Panama in 1513 and discovered the great "southern sea" which he named Mar del Sur (in Spanish). The ocean's current name was coined by Portuguese explorer Ferdinand Magellan during the Spanish circumnavigation of the world in 1521, as he encountered favorable winds on reaching the ocean. He called it Mar Pacífico, which in both Portuguese and Spanish means "peaceful sea".
Important human migrations occurred in the Pacific in prehistoric times. About 3000 BC, the Austronesian peoples on the island of Taiwan mastered the art of long-distance canoe travel and spread themselves and their languages south to the Philippines, Indonesia, and maritime Southeast Asia; west towards Madagascar; southeast towards New Guinea and Melanesia (intermarrying with native Papuans); and east to the islands of Micronesia, Oceania and Polynesia.Long-distance trade developed all along the coast from Mozambique to Japan. Trade, and therefore knowledge, extended to the Indonesian islands but apparently not Australia. In 219 BC Xu Fu sailed out into the Pacific searching for the elixir of immortality. By at least 878 when there was a significant Islamic settlement in Canton much of this trade was controlled by Arabs or Muslims. From 1404 to 1433 Zheng He led expeditions into the Indian Ocean.
The first contact of European navigators with the western edge of the Pacific Ocean was made by the Portuguese expeditions of António de Abreu and Francisco Serrão, via the Lesser Sunda Islands, to the Maluku Islands, in 1512, and with Jorge Álvares's expedition to southern China in 1513, both ordered by Afonso de Albuquerque from Malacca. The eastern side of the ocean was discovered by Spanish explorer Vasco Núñez de Balboa in 1513 after his expedition crossed the Isthmus of Panama and reached a new ocean. He named it Mar del Sur (literally, "Sea of the South" or "South Sea") because the ocean was to the south of the coast of the isthmus where he first observed the Pacific. In 1520, navigator Ferdinand Magellan and his crew were the first to cross the Pacific in recorded history. They were part of a Spanish expedition to the Spice Islands that would eventually result in the first world circumnavigation. Magellan called the ocean Pacífico (or "Pacific" meaning, "peaceful") because, after sailing through the stormy seas off Cape Horn, the expedition found calm waters. The ocean was often called the Sea of Magellan in his honor until the eighteenth century. Magellan stopped at one uninhabited Pacific island before stopping at Guam in March 1521. Although Magellan himself died in the Philippines in 1521, Spanish navigator Juan Sebastián Elcano led the remains of the expedition back to Spain across the Indian Ocean and round the Cape of Good Hope, completing the first world circumnavigation in 1522. Sailing around and east of the Moluccas, between 1525 and 1527, Portuguese expeditions discovered the Caroline Islands, the Aru Islands, and Papua New Guinea. In 1542–43 the Portuguese also reached Japan.In 1564, five Spanish ships carrying 379 explorers crossed the ocean from Mexico led by Miguel López de Legazpi, and sailed to the Philippines and Mariana Islands. For the remainder of the 16th century, Spanish influence was paramount, with ships sailing from Mexico and Peru across the Pacific Ocean to the Philippines via Guam, and establishing the Spanish East Indies. The Manila galleons operated for two and a half centuries, linking Manila and Acapulco, in one of the longest trade routes in history. Spanish expeditions also discovered Tuvalu, the Marquesas, the Cook Islands, the Solomon Islands, and the Admiralty Islands in the South Pacific.Later, in the quest for Terra Australis ("the [great] Southern Land"), Spanish explorations in the 17th century, such as the expedition led by the Portuguese navigator Pedro Fernandes de Queirós, discovered the Pitcairn and Vanuatu archipelagos, and sailed the Torres Strait between Australia and New Guinea, named after navigator Luís Vaz de Torres. Dutch explorers, sailing around southern Africa, also engaged in discovery and trade; Willem Janszoon, made the first completely documented European landing in Australia (1606), in Cape York Peninsula, and Abel Janszoon Tasman circumnavigated and landed on parts of the Australian continental coast and discovered Tasmania and New Zealand in 1642.In the 16th and 17th centuries, Spain considered the Pacific Ocean a mare clausum—a sea closed to other naval powers. As the only known entrance from the Atlantic, the Strait of Magellan was at times patrolled by fleets sent to prevent entrance of non-Spanish ships. On the western side of the Pacific Ocean the Dutch threatened the Spanish Philippines.The 18th century marked the beginning of major exploration by the Russians in Alaska and the Aleutian Islands, such as the First Kamchatka expedition and the Great Northern Expedition, led by the Danish Russian navy officer Vitus Bering. Spain also sent expeditions to the Pacific Northwest, reaching Vancouver Island in southern Canada, and Alaska. The French explored and settled Polynesia, and the British made three voyages with James Cook to the South Pacific and Australia, Hawaii, and the North American Pacific Northwest. In 1768, Pierre-Antoine Véron, a young astronomer accompanying Louis Antoine de Bougainville on his voyage of exploration, established the width of the Pacific with precision for the first time in history. One of the earliest voyages of scientific exploration was organized by Spain in the Malaspina Expedition of 1789–1794. It sailed vast areas of the Pacific, from Cape Horn to Alaska, Guam and the Philippines, New Zealand, Australia, and the South Pacific.
Growing imperialism during the 19th century resulted in the occupation of much of Oceania by European powers, and later Japan and the United States. Significant contributions to oceanographic knowledge were made by the voyages of HMS Beagle in the 1830s, with Charles Darwin aboard; HMS Challenger during the 1870s; the USS Tuscarora (1873–76); and the German Gazelle (1874–76). In Oceania, France obtained a leading position as imperial power after making Tahiti and New Caledonia protectorates in 1842 and 1853, respectively. After navy visits to Easter Island in 1875 and 1887, Chilean navy officer Policarpo Toro negotiated the incorporation of the island into Chile with native Rapanui in 1888. By occupying Easter Island, Chile joined the imperial nations. By 1900 nearly all Pacific islands were in control of Britain, France, United States, Germany, Japan, and Chile.Although the United States gained control of Guam and the Philippines from Spain in 1898, Japan controlled most of the western Pacific by 1914 and occupied many other islands during the Pacific War; however, by the end of that war, Japan was defeated and the U.S. Pacific Fleet was the virtual master of the ocean. The Japanese-ruled Northern Mariana Islands came under the control of the United States. Since the end of World War II, many former colonies in the Pacific have become independent states.
The Pacific Ocean has most of the islands in the world. There are about 25,000 islands in the Pacific Ocean. The islands entirely within the Pacific Ocean can be divided into three main groups known as Micronesia, Melanesia and Polynesia. Micronesia, which lies north of the equator and west of the International Date Line, includes the Mariana Islands in the northwest, the Caroline Islands in the center, the Marshall Islands to the east and the islands of Kiribati in the southeast.Melanesia, to the southwest, includes New Guinea, the world's second largest island after Greenland and by far the largest of the Pacific islands. The other main Melanesian groups from north to south are the Bismarck Archipelago, the Solomon Islands, Santa Cruz, Vanuatu, Fiji and New Caledonia.The largest area, Polynesia, stretching from Hawaii in the north to New Zealand in the south, also encompasses Tuvalu, Tokelau, Samoa, Tonga and the Kermadec Islands to the west, the Cook Islands, Society Islands and Austral Islands in the center, and the Marquesas Islands, Tuamotu, Mangareva Islands, and Easter Island to the east.Islands in the Pacific Ocean are of four basic types: continental islands, high islands, coral reefs and uplifted coral platforms. Continental islands lie outside the andesite line and include New Guinea, the islands of New Zealand, and the Philippines. Some of these islands are structurally associated with nearby continents. High islands are of volcanic origin, and many contain active volcanoes. Among these are Bougainville, Hawaii, and the Solomon Islands.The coral reefs of the South Pacific are low-lying structures that have built up on basaltic lava flows under the ocean's surface. One of the most dramatic is the Great Barrier Reef off northeastern Australia with chains of reef patches. A second island type formed of coral is the uplifted coral platform, which is usually slightly larger than the low coral islands. Examples include Banaba (formerly Ocean Island) and Makatea in the Tuamotu group of French Polynesia.
The volume of the Pacific Ocean, representing about 50.1 percent of the world's oceanic water, has been estimated at some 714 million cubic kilometers (171 million cubic miles). Surface water temperatures in the Pacific can vary from −1.4 °C (29.5 °F), the freezing point of sea water, in the poleward areas to about 30 °C (86 °F) near the equator. Salinity also varies latitudinally, reaching a maximum of 37 parts per thousand in the southeastern area. The water near the equator, which can have a salinity as low as 34 parts per thousand, is less salty than that found in the mid-latitudes because of abundant equatorial precipitation throughout the year. The lowest counts of less than 32 parts per thousand are found in the far north as less evaporation of seawater takes place in these frigid areas. The motion of Pacific waters is generally clockwise in the Northern Hemisphere (the North Pacific gyre) and counter-clockwise in the Southern Hemisphere. The North Equatorial Current, driven westward along latitude 15°N by the trade winds, turns north near the Philippines to become the warm Japan or Kuroshio Current.Turning eastward at about 45°N, the Kuroshio forks and some water moves northward as the Aleutian Current, while the rest turns southward to rejoin the North Equatorial Current. The Aleutian Current branches as it approaches North America and forms the base of a counter-clockwise circulation in the Bering Sea. Its southern arm becomes the chilled slow, south-flowing California Current. The South Equatorial Current, flowing west along the equator, swings southward east of New Guinea, turns east at about 50°S, and joins the main westerly circulation of the South Pacific, which includes the Earth-circling Antarctic Circumpolar Current. As it approaches the Chilean coast, the South Equatorial Current divides; one branch flows around Cape Horn and the other turns north to form the Peru or Humboldt Current.
The climate patterns of the Northern and Southern Hemispheres generally mirror each other. The trade winds in the southern and eastern Pacific are remarkably steady while conditions in the North Pacific are far more varied with, for example, cold winter temperatures on the east coast of Russia contrasting with the milder weather off British Columbia during the winter months due to the preferred flow of ocean currents.In the tropical and subtropical Pacific, the El Niño Southern Oscillation (ENSO) affects weather conditions. To determine the phase of ENSO, the most recent three-month sea surface temperature average for the area approximately 3,000 km (1,900 mi) to the southeast of Hawaii is computed, and if the region is more than 0.5 °C (0.9 °F) above or below normal for that period, then an El Niño or La Niña is considered in progress. In the tropical western Pacific, the monsoon and the related wet season during the summer months contrast with dry winds in the winter which blow over the ocean from the Asian landmass. Worldwide, tropical cyclone activity peaks in late summer, when the difference between temperatures aloft and sea surface temperatures is the greatest; however, each particular basin has its own seasonal patterns. On a worldwide scale, May is the least active month, while September is the most active month. November is the only month in which all the tropical cyclone basins are active. The Pacific hosts the two most active tropical cyclone basins, which are the northwestern Pacific and the eastern Pacific. Pacific hurricanes form south of Mexico, sometimes striking the western Mexican coast and occasionally the southwestern United States between June and October, while typhoons forming in the northwestern Pacific moving into southeast and east Asia from May to December. Tropical cyclones also form in the South Pacific basin, where they occasionally impact island nations. In the arctic, icing from October to May can present a hazard for shipping while persistent fog occurs from June to December. A climatological low in the Gulf of Alaska keeps the southern coast wet and mild during the winter months. The Westerlies and associated jet stream within the Mid-Latitudes can be particularly strong, especially in the Southern Hemisphere, due to the temperature difference between the tropics and Antarctica, which records the coldest temperature readings on the planet. In the Southern hemisphere, because of the stormy and cloudy conditions associated with extratropical cyclones riding the jet stream, it is usual to refer to the Westerlies as the Roaring Forties, Furious Fifties and Shrieking Sixties according to the varying degrees of latitude.
The ocean was first mapped by Abraham Ortelius; he called it Maris Pacifici following Ferdinand Magellan's description of it as "a pacific sea" during his circumnavigation from 1519 to 1522. To Magellan, it seemed much more calm (pacific) than the Atlantic.The andesite line is the most significant regional distinction in the Pacific. A petrologic boundary, it separates the deeper, mafic igneous rock of the Central Pacific Basin from the partially submerged continental areas of felsic igneous rock on its margins. The andesite line follows the western edge of the islands off California and passes south of the Aleutian arc, along the eastern edge of the Kamchatka Peninsula, the Kuril Islands, Japan, the Mariana Islands, the Solomon Islands, and New Zealand's North Island. The dissimilarity continues northeastward along the western edge of the Andes Cordillera along South America to Mexico, returning then to the islands off California. Indonesia, the Philippines, Japan, New Guinea, and New Zealand lie outside the andesite line. Within the closed loop of the andesite line are most of the deep troughs, submerged volcanic mountains, and oceanic volcanic islands that characterize the Pacific basin. Here basaltic lavas gently flow out of rifts to build huge dome-shaped volcanic mountains whose eroded summits form island arcs, chains, and clusters. Outside the andesite line, volcanism is of the explosive type, and the Pacific Ring of Fire is the world's foremost belt of explosive volcanism. The Ring of Fire is named after the several hundred active volcanoes that sit above the various subduction zones. The Pacific Ocean is the only ocean which is almost totally bounded by subduction zones. Only the Antarctic and Australian coasts have no nearby subduction zones.
The Pacific Ocean was born 750 million years ago at the breakup of Rodinia, although it is generally called the Panthalassic Ocean until the breakup of Pangea, about 200 million years ago. The oldest Pacific Ocean floor is only around 180 Ma old, with older crust subducted by now.
The Pacific Ocean contains several long seamount chains, formed by hotspot volcanism. These include the Hawaiian–Emperor seamount chain and the Louisville Ridge.
The exploitation of the Pacific's mineral wealth is hampered by the ocean's great depths. In shallow waters of the continental shelves off the coasts of Australia and New Zealand, petroleum and natural gas are extracted, and pearls are harvested along the coasts of Australia, Japan, Papua New Guinea, Nicaragua, Panama, and the Philippines, although in sharply declining volume in some cases.
Fish are an important economic asset in the Pacific. The shallower shoreline waters of the continents and the more temperate islands yield herring, salmon, sardines, snapper, swordfish, and tuna, as well as shellfish. Overfishing has become a serious problem in some areas. For example, catches in the rich fishing grounds of the Okhotsk Sea off the Russian coast have been reduced by at least half since the 1990s as a result of overfishing.
The quantity of small plastic fragments floating in the north-east Pacific Ocean increased a hundredfold between 1972 and 2012. The ever-growing Great Pacific garbage patch between California and Japan is three times the size of France. An estimated 80,000 metric tons of plastic inhabit the patch, totaling 1.8 trillion pieces. Marine pollution is a generic term for the harmful entry into the ocean of chemicals or particles. The main culprits are those using the rivers for disposing of their waste. The rivers then empty into the ocean, often also bringing chemicals used as fertilizers in agriculture. The excess of oxygen-depleting chemicals in the water leads to hypoxia and the creation of a dead zone.Marine debris, also known as marine litter, is human-created waste that has ended up floating in a lake, sea, ocean, or waterway. Oceanic debris tends to accumulate at the center of gyres and coastlines, frequently washing aground where it is known as beach litter.From 1946 to 1958, Marshall Islands served as the Pacific Proving Grounds for the United States and was the site of 67 nuclear tests on various atolls. Several nuclear weapons were lost in the Pacific Ocean, including one-megaton bomb lost during the 1965 Philippine Sea A-4 incident.In addition, the Pacific Ocean has served as the crash site of satellites, including Mars 96, Fobos-Grunt, and Upper Atmosphere Research Satellite.
EPIC Pacific Ocean Data Collection Viewable on-line collection of observational data NOAA In-situ Ocean Data Viewer plot and download ocean observations NOAA PMEL Argo profiling floats Realtime Pacific Ocean data NOAA TAO El Niño data Realtime Pacific Ocean El Niño buoy data NOAA Ocean Surface Current Analyses – Realtime (OSCAR) Near-realtime Pacific Ocean Surface Currents derived from satellite altimeter and scatterometer data
A planned economy is a type of economic system where investment, production and the allocation of capital goods take place according to economy-wide economic plans and production plans. A planned economy may use centralized, decentralized, participatory or Soviet-type forms of economic planning. The level of centralization or decentralization in decision-making and participation depends on the specific type of planning mechanism employed.Socialist states based on the Soviet model have used central planning, although a minority such as the Socialist Federal Republic of Yugoslavia have adopted some degree of market socialism. Market abolitionist socialism replaces factor markets with direct calculation as the means to coordinate the activities of the various socially-owned economic enterprises that make up the economy. More recent approaches to socialist planning and allocation have come from some economists and computer scientists proposing planning mechanisms based on advances in computer science and information technology.Planned economies contrast with unplanned economies, specifically market economies, where autonomous firms operating in markets make decisions about production, distribution, pricing and investment. Market economies that use indicative planning are variously referred to as planned market economies, mixed economies and mixed market economies. A command economy follows an administrative-command system and uses Soviet-type economic planning which was characteristic of the former Soviet Union and Eastern Bloc before most of these countries converted to market economies. This highlights the central role of hierarchical administration and public ownership of production in guiding the allocation of resources in these economic systems.In command economies, important allocation decisions are made by government authorities and are imposed by law. This goes against the Marxist understanding of conscious planning. Decentralized planning has been proposed as a basis for socialism and has been variously advocated by anarchists, council communists, libertarian Marxists and other democratic and libertarian socialists who advocate a non-market form of socialism, in total rejection of the type of planning adopted in the economy of the Soviet Union.
In the Hellenistic and post-Hellenistic world, "compulsory state planning was the most characteristic trade condition for the Egyptian countryside, for Hellenistic India, and to a lesser degree the more barbaric regions of the Seleucid, the Pergamenian, the southern Arabian, and the Parthian empires". Scholars have argued that the Incan economy was a flexible type of command economy, centered around the movement and utilization of labor instead of goods. One view of mercantilism sees it as a planned economy.The Soviet-style planned economy started with war communism in 1918 and ended in 1921, when the Soviet government founded Gosplan in 1921. However, the period of the New Economic Policy intervened before regular five-year plans started in 1928. Dirigisme, or government direction of the economy through non-coercive means, was practiced in France and in Great Britain after World War II. The Swedish government planned public-housing models in a similar fashion as urban planning in a project called Million Programme, implemented from 1965 to 1974. Some decentralised participation in economic planning has been implemented across Revolutionary Spain, most notably in Catalonia, during the Spanish Revolution of 1936.
While socialism is not equivalent to economic planning or to the concept of a planned economy, an influential conception of socialism involves the replacement of capital markets with some form of economic planning in order to achieve ex-ante coordination of the economy. The goal of such an economic system would be to achieve conscious control over the economy by the population, specifically so that the use of the surplus product is controlled by the producers. The specific forms of planning proposed for socialism and their feasibility are subjects of the socialist calculation debate.
When the development of computer technology was still its early stages in 1971, the socialist Allende administration of Chile launched Project Cybersyn to install a telex machine in every corporation and organisation in the economy for the communication of economic data between firms and the government. The data was also fed into a computer-simulated economy for forecasting. A control room was built for realtime observation and management of the overall economy. The prototype-stage of the project showed promise when it was used to redirect supplies around a trucker's strike, but after CIA-backed Augusto Pinochet led a coup in 1973 that established a military dictatorship under his rule the program was abolished and Pinochet moved Chile towards a more liberalized market economy. In their book Towards a New Socialism (1993), the computer scientist Paul Cockshott from the University of Glasgow and the economist Allin Cottrell from the Wake Forest University claim to demonstrate how a democratically planned economy built on modern computer technology is possible and drives the thesis that it would be both economically more stable than the free-market economies and also morally desirable.
The use of computers to coordinate production in an optimal fashion has been variously proposed for socialist economies. The Polish economist Oskar Lange argued that the computer is more efficient than the market process at solving the multitude of simultaneous equations required for allocating economic inputs efficiently (either in terms of physical quantities or monetary prices).The 1970 Chilean distributed decision support system Project Cybersyn was pioneered by Salvador Allende's socialist government in an attempt to move towards a decentralised planned economy with the experimental viable system model of computed organisational structure of autonomous operative units though an algedonic feedback setting and bottom-up participative decision-making in the form of participative democracy by the Cyberfolk component.
The 1888 novel Looking Backward by Edward Bellamy depicts a fictional planned economy in a United States around the year 2000 which has become a socialist utopia. The World State in Aldous Huxley's Brave New World and Airstrip One in George Orwell's Nineteen Eighty-Four are both fictional examples of command economies, albeit with diametrically opposed aims. The former is a consumer economy designed to engender productivity while the latter is a shortage economy designed as an agent of totalitarian social control. Airstrip One is organized by the euphemistically named Ministry of Plenty. Other literary portrayals of planned economies were Yevgeny Zamyatin's We which was an influence on Orwell's work. Like Nineteen Eighty-Four, Ayn Rand's dystopian story Anthem was also an artistic portrayal of a command economy that was influenced by We. The difference is that it was a primitivist planned economy as opposed to the advanced technology of We or Brave New World.
The government can harness land, labour and capital to serve the economic objectives of the state. Consumer demand can be restrained in favor of greater capital investment for economic development in a desired pattern. In international comparisons, state-socialist nations compared favorably with capitalist nations in health indicators such as infant mortality and life expectancy. However, the reality of this, at least in regards to infant mortality, varied depending on whether official Soviet statistics or WHO definitions were used.The state can begin building a heavy industry at once in an underdeveloped economy without waiting years for capital to accumulate through the expansion of light industry and without reliance on external financing. This is what happened in the Soviet Union during the 1930s when the government forced the share of gross national income dedicated to private consumption from eighty percent to fifty percent. As a result of this development, the Soviet Union experienced massive growth in heavy industry, with a concurrent massive contraction of its agricultural sector due to labour shortage, in both relative and absolute terms.
Studies of command economies of the Eastern Bloc in the 1950s and 1960s by both American and Eastern European economists found that contrary to the expectations of both groups they showed greater fluctuations in output than market economies during the same period.
Critics of planned economies argue that planners cannot detect consumer preferences, shortages and surpluses with sufficient accuracy and therefore cannot efficiently co-ordinate production (in a market economy, a free price system is intended to serve this purpose). This difficulty was notably written about by economists Ludwig von Mises and Friedrich Hayek, who referred to subtly distinct aspects of the problem as the economic calculation problem and local knowledge problem, respectively.Whereas the former stressed the theoretical underpinnings of a market economy to subjective value theory while attacking the labor theory of value, the latter argued that the only way to satisfy individuals who have a constantly changing hierarchy of needs and are the only ones to possess their particular individual's circumstances is by allowing those with the most knowledge of their needs to have it in their power to use their resources in a competing marketplace to meet the needs of the most consumers most efficiently. This phenomenon is recognized as spontaneous order. Additionally, misallocation of resources would naturally ensue by redirecting capital away from individuals with direct knowledge and circumventing it into markets where a coercive monopoly influences behavior, ignoring market signals. According to Tibor Machan, "[w]ithout a market in which allocations can be made in obedience to the law of supply and demand, it is difficult or impossible to funnel resources with respect to actual human preferences and goals".
Economist Robin Hahnel, who supports participatory economics, a form of socialist decentralized planned economy, notes that even if central planning overcame its inherent inhibitions of incentives and innovation, it would nevertheless be unable to maximize economic democracy and self-management, which he believes are concepts that are more intellectually coherent, consistent and just than mainstream notions of economic freedom. Furthermore, Hahnel states: Combined with a more democratic political system, and redone to closer approximate a best case version, centrally planned economies no doubt would have performed better. But they could never have delivered economic self-management, they would always have been slow to innovate as apathy and frustration took their inevitable toll, and they would always have been susceptible to growing inequities and inefficiencies as the effects of differential economic power grew. Under central planning neither planners, managers, nor workers had incentives to promote the social economic interest. Nor did impeding markets for final goods to the planning system enfranchise consumers in meaningful ways. But central planning would have been incompatible with economic democracy even if it had overcome its information and incentive liabilities. And the truth is that it survived as long as it did only because it was propped up by unprecedented totalitarian political power.
Planned economies contrast with command economies. A planned economy is "an economic system in which the government controls and regulates production, distribution, prices, etc." whereas a command economy necessarily has substantial public ownership of industry while also having this type of regulation. Most of a command economy is organized in a top-down administrative model by a central authority, where decisions regarding investment and production output requirements are decided upon at the top in the chain of command, with little input from lower levels. Advocates of economic planning have sometimes been staunch critics of these command economies. Leon Trotsky believed that those at the top of the chain of command, regardless of their intellectual capacity, operated without the input and participation of the millions of people who participate in the economy and who understand/respond to local conditions and changes in the economy. Therefore, they would be unable to effectively coordinate all economic activity.Historians have associated planned economies with Marxist–Leninist states and the Soviet economic model. Since the 1980s, it was recognized that the Soviet economic model did not actually constitute a planned economy in that a comprehensive and binding plan did not guide production and investment. The further distinction of an administrative-command system emerged as a more accurate designation for the economic system that existed in the former Soviet Union and Eastern Bloc, highlighting the role of centralized hierarchical decision-making in the absence of popular control over the economy. The possibility of a digital planned economy was explored in Chile between 1971 and 1973 with the development of Project Cybersyn and by Alexander Kharkevich, head of the Department of Technical Physics in Kiev in 1962.While both economic planning and a planned economy can be either authoritarian or democratic and participatory, democratic socialist critics argue that command economies have been authoritarian or undemocratic in practice. Indicative planning is a form of economic planning in market economies that directs the economy through incentive-based methods. Economic planning can be practiced in a decentralized manner through different government authorities. In some predominantly market-oriented and Western mixed economies, the state utilizes economic planning in strategic industries such as the aerospace industry. Mixed economies usually employ macroeconomic planning while micro-economic affairs are left to the market and price system.
A decentralized-planned economy, occasionally called horizontally-planned economy due to its horizontalism, is a type of planned economy in which the investment and allocation of consumer and capital goods is explicated accordingly to an economy-wide plan built and operatively coordinated through a distributed network of disparate economic agents or even production units itself. Decentralized planning is usually held in contrast to centralized planning, in particular the Soviet-type economic planning of the Soviet Union's command economy, where economic information is aggregated and used to formulate a plan for production, investment and resource allocation by a single central authority. Decentralized planning can take shape both in the context of a mixed economy as well as in a post-capitalist economic system. This form of economic planning implies some process of democratic and participatory decision-making within the economy and within firms itself in the form of industrial democracy. Computer-based forms of democratic economic planning and coordination between economic enterprises have also been proposed by various computer scientists and radical economists. Proponents present decentralized and participatory economic planning as an alternative to market socialism for a post-capitalist society.Decentralized planning has been a feature of anarchist and other socialist economics. Variations of decentralized planning such as economic democracy, industrial democracy and participatory economics have been promoted by various political groups, most notably anarchists, democratic socialists, guild socialists, libertarian Marxists, libertarian socialists, revolutionary syndicalists and Trotskyists. During the Spanish Revolution, some areas where anarchist and libertarian socialist influence through the CNT and UGT was extensive, particularly rural regions, were run on the basis of decentralized planning resembling the principles laid out by anarcho-syndicalist Diego Abad de Santillan in the book After the Revolution.
Economist Pat Devine has created a model of decentralized economic planning called "negotiated coordination" which is based upon social ownership of the means of production by those affected by the use of the assets involved, with the allocation of consumer and capital goods made through a participatory form of decision-making by those at the most localized level of production. Moreover, organizations that utilize modularity in their production processes may distribute problem solving and decision making.
The planning structure of a decentralized planned economy is generally based on a consumers council and producer council (or jointly, a distributive cooperative) which is sometimes called a consumers' cooperative. Producers and consumers, or their representatives, negotiate the quality and quantity of what is to be produced. This structure is central to guild socialism, participatory economics and the economic theories related to anarchism.
Some decentralised participation in economic planning has been implemented in various regions and states in India, most notably in Kerala. Local level planning agencies assess the needs of people who are able to give their direct input through the Gram Sabhas (village-based institutions) and the planners subsequently seek to plan accordingly.
Some decentralised participation in economic planning has been implemented across Revolutionary Spain, most notably in Catalonia, during the Spanish Revolution of 1936.
The United Nations has developed local projects that promote participatory planning on a community level. Members of communities take decisions regarding community development directly.
Case studies (Soviet-type economies)Analysis of Soviet-type economic planning Eastern Bloc economies Economy of Cuba Economy of North Korea Five-year plans in the Soviet Union Five-year plans of China OGAS, a plan for creating a computer network to supervise the Soviet economy Project Cybersyn, a project for a computer network controlling the economy of Chile under Salvador AllendeCase studies (mixed-market economies)Dirigisme (indicative planning in France) Economy of India Economy of Singapore First Malaysia Plan Five-year plans of Argentina Five-year plans of South Korea
"The Myth of the Permanent Arms Economy" "The Stalin Model for the Control and Coordination of Enterprises in a Socialist Economy"
Political economy is the study of production and trade and their relations with law, custom and government; and with the distribution of national income and wealth. As a discipline, political economy originated in moral philosophy, in the 18th century, to explore the administration of states' wealth, with "political" signifying the Greek word polity and "economy" signifying the Greek word "oikonomía" (household management). The earliest works of political economy are usually attributed to the British scholars Adam Smith, Thomas Malthus, and David Ricardo, although they were preceded by the work of the French physiocrats, such as François Quesnay (1694–1774) and Anne-Robert-Jacques Turgot (1727–1781).In the late 19th century, the term "economics" gradually began to replace the term "political economy" with the rise of mathematical modelling coinciding with the publication of an influential textbook by Alfred Marshall in 1890. Earlier, William Stanley Jevons, a proponent of mathematical methods applied to the subject, advocated economics for brevity and with the hope of the term becoming "the recognised name of a science". Citation measurement metrics from Google Ngram Viewer indicate that use of the term "economics" began to overshadow "political economy" around roughly 1910, becoming the preferred term for the discipline by 1920. Today, the term "economics" usually refers to the narrow study of the economy absent other political and social considerations while the term "political economy" represents a distinct and competing approach. Political economy, where it isn't considered a synonym for economics, may refer to very different things. From an academic standpoint, the term may reference Marxian economics, applied public choice approaches emanating from the Chicago school and the Virginia school. In common parlance, "political economy" may simply refer to the advice given by economists to the government or public on general economic policy or on specific economic proposals developed by political scientists. A rapidly growing mainstream literature from the 1970s has expanded beyond the model of economic policy in which planners maximize utility of a representative individual toward examining how political forces affect the choice of economic policies, especially as to distributional conflicts and political institutions. It is available as a stand-alone area of study in certain colleges and universities. Most institutions of higher education offer the political economy as an area of specialization, either under economics or political science. Notable institutions includes the University of Warwick, London School of Economics, Graduate Institute Geneva, Paul H. Nitze School of Advanced International Studies, and Balsillie School of International Affairs, among others.
Originally, political economy meant the study of the conditions under which production or consumption within limited parameters was organized in nation-states. In that way, political economy expanded the emphasis of economics, which comes from the Greek oikos (meaning "home") and nomos (meaning "law" or "order"). Political economy was thus meant to express the laws of production of wealth at the state level, quite like economics concerns putting home to order. The phrase économie politique (translated in English to "political economy") first appeared in France in 1615 with the well-known book by Antoine de Montchrétien, Traité de l’economie politique. Other contemporary scholars attribute the roots of this study to the 13th Century Tunisian Arab Historian and Sociologist, Ibn Khaldun, for his work on making the distinction between "profit" and "sustenance", in modern political economy terms, surplus and that required for the reproduction of classes respectively. He also calls for the creation of a science to explain society and goes on to outline these ideas in his major work, the Muqaddimah. In Al-Muqaddimah Khaldun states, “Civilization and its well-being, as well as business prosperity, depend on productivity and people’s efforts in all directions in their own interest and profit” – seen as a modern precursor to Classical Economic thought. Leading on from this, the French physiocrats were the first major exponents of political economy, although the intellectual responses of Adam Smith, John Stuart Mill, David Ricardo, Henry George and Karl Marx to the physiocrats generally receives much greater attention. The world's first professorship in political economy was established in 1754 at the University of Naples Federico II in southern Italy. The Neapolitan philosopher Antonio Genovesi was the first tenured professor. In 1763, Joseph von Sonnenfels was appointed a Political Economy chair at the University of Vienna, Austria. Thomas Malthus, in 1805, became England's first professor of political economy, at the East India Company College, Haileybury, Hertfordshire. At present, political economy refers to different yet related approaches to studying economic and related behaviours, ranging from the combination of economics with other fields to the use of different, fundamental assumptions challenging earlier economic assumptions:
Political economy most commonly refers to interdisciplinary studies drawing upon economics, sociology and political science in explaining how political institutions, the political environment, and the economic system—capitalist, socialist, communist, or mixed—influence each other. The Journal of Economic Literature classification codes associate political economy with three sub-areas: (1) the role of government and/or class and power relationships in resource allocation for each type of economic system; (2) international political economy, which studies the economic impacts of international relations; and (3) economic models of political or exploitative class processes. Much of the political economy approach is derived from public choice theory on the one hand and radical political economics on the other hand, both dating from the 1960s. Public choice theory is a microfoundations theory closely intertwined with political economy. Both approaches model voters, politicians and bureaucrats as behaving in mainly self-interested ways, in contrast to a view, ascribed to earlier mainstream economists, of government officials trying to maximize individual utilities from some kind of social welfare function. As such, economists and political scientists often associate political economy with approaches using rational-choice assumptions, especially in game theory and in examining phenomena beyond economics' standard remit, such as government failure and complex decision making in which context the term "positive political economy" is common. Other "traditional" topics include analysis of such public policy issues as economic regulation, monopoly, rent-seeking, market protection, institutional corruption and distributional politics. Empirical analysis includes the influence of elections on the choice of economic policy, determinants and forecasting models of electoral outcomes, the political business cycles, central-bank independence and the politics of excessive deficits. A rather-recent focus has been put on modeling economic policy and political institutions concerning interactions between agents and economic and political institutions, including the seeming discrepancy of economic policy and economist's recommendations through the lens of transaction costs. From the mid-1990s, the field has expanded, in part aided by new cross-national data sets allowing tests of hypotheses on comparative economic systems and institutions. Topics have included the breakup of nations, the origins and rate of change of political institutions in relation to economic growth, development, financial markets and regulation, the importance of institutions, backwardness, reform and transition economies, the role of culture, ethnicity and gender in explaining economic outcomes, macroeconomic policy, the environment, fairness and the relation of constitutions to economic policy, theoretical and empirical.Other important landmarks in the development of political economy include: New political economy which may treat economic ideologies as the phenomenon to explain, per the traditions of Marxian political economy. Thus, Charles S. Maier suggests that a political economy approach "interrogates economic doctrines to disclose their sociological and political premises.... in sum, [it] regards economic ideas and behavior not as frameworks for analysis, but as beliefs and actions that must themselves be explained". This approach informs Andrew Gamble's The Free Economy and the Strong State (Palgrave Macmillan, 1988), and Colin Hay's The Political Economy of New Labour (Manchester University Press, 1999). It also informs much work published in New Political Economy, an international journal founded by Sheffield University scholars in 1996. International political economy (IPE) an interdisciplinary field comprising approaches to the actions of various actors. According to International Relations scholar Chris Brown, University of Warwick professor, Susan Strange, was "almost single-handedly responsible for creating international political economy as a field of study." In the United States, these approaches are associated with the journal International Organization, which in the 1970s became the leading journal of IPE under the editorship of Robert Keohane, Peter J. Katzenstein and Stephen Krasner. They are also associated with the journal The Review of International Political Economy. There also is a more critical school of IPE, inspired by thinkers such as Antonio Gramsci and Karl Polanyi; two major figures are Matthew Watson and Robert W. Cox. The use of a political economy approach by anthropologists, sociologists, and geographers used in reference to the regimes of politics or economic values that emerge primarily at the level of states or regional governance, but also within smaller social groups and social networks. Because these regimes influence and are influenced by the organization of both social and economic capital, the analysis of dimensions lacking a standard economic value (e.g. the political economy of language, of gender, or of religion) often draws on concepts used in Marxian critiques of capital. Such approaches expand on neo-Marxian scholarship related to development and underdevelopment postulated by André Gunder Frank and Immanuel Wallerstein. Historians have employed political economy to explore the ways in the past that persons and groups with common economic interests have used politics to effect changes beneficial to their interests. Political economy and law is a recent attempt within legal scholarship to engage explicitly with political economy literature. In the 1920s and 1930s, legal realists (e.g. Robert Hale) and intellectuals (e.g. John Commons) engaged themes related to political economy. In the second half of the 20th century, lawyers associated with the Chicago School incorporated certain intellectual traditions from economics. However, since the crisis in 2007 legal scholars especially related to international law, have turned to more explicitly engage with the debates, methodology and various themes within political economy texts. Thomas Piketty's approach and call to action which advocated for the re-introduction of political consideration and political science knowledge more generally into the discipline of economics as a way of improving the robustness of the discipline and remedying its shortcomings, which had become clear following the 2008 financial crisis. In 2010, the only Department of Political Economy in the United Kingdom formally established at King's College London. The rationale for this academic unit was that "the disciplines of Politics and Economics are inextricably linked", and that it was "not possible to properly understand political processes without exploring the economic context in which politics operates". In 2017, the Political Economy UK Group (abbreviated PolEconUK) was established as a research consortium in the field of political economy. It hosts an annual conference and counts among its member institutions Oxford, Cambridge, King's College London, Warwick University and the London School of Economics.
Because political economy is not a unified discipline, there are studies using the term that overlap in subject matter, but have radically different perspectives: Politics studies power relations and their relationship to achieving desired ends. Philosophy rigorously assesses and studies a set of beliefs and their applicability to reality. Economics studies the distribution of resources so that the material wants of a society are satisfied; enhance societal well-being. Sociology studies the effects of persons' involvement in society as members of groups and how that changes their ability to function. Many sociologists start from a perspective of production-determining relation from Karl Marx. Marx's theories on the subject of political economy are contained in his book Das Kapital. Anthropology studies political economy by investigating regimes of political and economic value that condition tacit aspects of sociocultural practices (e.g. the pejorative use of pseudo-Spanish expressions in the U.S. entertainment media) by means of broader historical, political and sociological processes. Analyses of structural features of transnational processes focus on the interactions between the world capitalist system and local cultures. Archaeology attempts to reconstruct past political economies by examining the material evidence for administrative strategies to control and mobilize resources. This evidence may include architecture, animal remains, evidence for craft workshops, evidence for feasting and ritual, evidence for the import or export of prestige goods, or evidence for food storage. Psychology is the fulcrum on which political economy exerts its force in studying decision making (not only in prices), but as the field of study whose assumptions model political economy. History documents change, often using it to argue political economy; some historical works take political economy as the narrative's frame. Ecology deals with political economy because human activity has the greatest effect upon the environment, its central concern being the environment's suitability for human activity. The ecological effects of economic activity spur research upon changing market economy incentives. Additionally and more recently, ecological theory has been used to examine economic systems as similar systems of interacting species (e.g., firms). Cultural studies examines social class, production, labor, race, gender and sex. Communications examines the institutional aspects of media and telecommunication systems. As the area of study focusing on aspects of human communication, it pays particular attention to the relationships between owners, labor, consumers, advertisers, structures of production and the state and the power relationships embedded in these relationships.
NBER (U.S.) "Political Economy" working-paper abstract links. VoxEU.org (Europe) "Politics and economics" article links. List, Friedrich. National System of Political Economy Carey, Henry C. Harmony of Interests – compares American and British systems of political economy International Political Economy at Jacobs University Bremen Global Political Economy at City University London Centre for Global Political Economy at the University of Sussex, UK O'Neil Center for Global Markets and Freedom at the SMU Cox School of Business Dallas, TX USA Institute for the study of Political Economy and Law (IPEL) at the International University College of Turin (IUC), Italy European Centre for International Political Economy Institute for Political Economy and Development (IPEAD)
Rabbit Niiname-no-Matsuri
There is another folk tradition which may use a variation "Rabbit", "Bunny", "I hate/love Grey Rabbits" or "White Rabbit" to ward off smoke that the wind is directing into your face when gathered around a campfire. It is thought that this tradition may be related to the tradition of invoking the rabbit on the first of the month. Others conjecture that it may originate with a North American First Nation story about smoke resembling rabbit fur. This tradition may be more of a social tradition in a group setting than a genuine belief that certain words will change the wind direction, and may be more of a childhood tradition than an adult one. Children have sometimes adapted from Rabbit to "Pink Elephant" or other comical derivatives. Because of this more mutable usage, historical record of this is even more scarce than other more static meanings. As with all folklore, its truth is made evident even in its only occasional fulfillment: should the wind then appear to change direction, others will interpret the use of such an expression as evidence of its effectiveness and will then tend to adopt and repeat its use. That multiple instances of its ineffectiveness also exist is discounted in light of the "fact" that it appeared to work once.
Three hares Rabbit's foot Stamping (custom)
On the White Rabbit Theory – An attempt to catalogue different "rabbit rabbit" variations and determine their origins. The Psychic Well Superstitions About Rabbits
Rabbits are small mammals in the family Leporidae of the order Lagomorpha (along with the hare and the pika). Oryctolagus cuniculus includes the European rabbit species and its descendants, the world's 305 breeds of domestic rabbit. Sylvilagus includes 13 wild rabbit species, among them the seven types of cottontail. The European rabbit, which has been introduced on every continent except Antarctica, is familiar throughout the world as a wild prey animal and as a domesticated form of livestock and pet. With its widespread effect on ecologies and cultures, the rabbit (or bunny) is, in many areas of the world, a part of daily life—as food, clothing, a companion, and a source of artistic inspiration. Although once considered rodents, lagomorphs like rabbits have been discovered to have diverged separately and earlier than their rodent cousins, and have a number of traits rodents lack, like two extra incisors.
Hares are precocial, born relatively mature and mobile with hair and good vision, while rabbits are altricial, born hairless and blind, and requiring closer care. Hares (and cottontail rabbits) live a relatively solitary life in a simple nest above the ground, while most rabbits live in social groups in burrows or warrens. Hares are generally larger than rabbits, with ears that are more elongated, and with hind legs that are larger and longer. Hares have not been domesticated, while descendants of the European rabbit are commonly bred as livestock and kept as pets.
Rabbits have long been domesticated. Beginning in the Middle Ages, the European rabbit has been widely kept as livestock, starting in ancient Rome. Selective breeding has generated a wide variety of rabbit breeds, many of which (since the early 19th century) are also kept as pets. Some strains of rabbit have been bred specifically as research subjects. As livestock, rabbits are bred for their meat and fur. The earliest breeds were important sources of meat, and so became larger than wild rabbits, but domestic rabbits in modern times range in size from dwarf to giant. Rabbit fur, prized for its softness, can be found in a broad range of coat colors and patterns, as well as lengths. The Angora rabbit breed, for example, was developed for its long, silky fur, which is often hand-spun into yarn. Other domestic rabbit breeds have been developed primarily for the commercial fur trade, including the Rex, which has a short plush coat.
Because the rabbit's epiglottis is engaged over the soft palate except when swallowing, the rabbit is an obligate nasal breather. Rabbits have two sets of incisor teeth, one behind the other. This way they can be distinguished from rodents, with which they are often confused. Carl Linnaeus originally grouped rabbits and rodents under the class Glires; later, they were separated as the scientific consensus is that many of their similarities were a result of convergent evolution. However, recent DNA analysis and the discovery of a common ancestor has supported the view that they do share a common lineage, and thus rabbits and rodents are now often referred to together as members of the superorder Glires.
The anatomy of rabbits' hind limbs are structurally similar to that of other land mammals and contribute to their specialized form of locomotion. The bones of the hind limbs consist of long bones (the femur, tibia, fibula, and phalanges) as well as short bones (the tarsals). These bones are created through endochondral ossification during development. Like most land mammals, the round head of the femur articulates with the acetabulum of the ox coxae. The femur articulates with the tibia, but not the fibula, which is fused to the tibia. The tibia and fibula articulate with the tarsals of the pes, commonly called the foot. The hind limbs of the rabbit are longer than the front limbs. This allows them to produce their hopping form of locomotion. Longer hind limbs are more capable of producing faster speeds. Hares, which have longer legs than cottontail rabbits, are able to move considerably faster. Rabbits stay just on their toes when moving this is called Digitigrade locomotion. The hind feet have four long toes that allow for this and are webbed to prevent them from spreading when hopping. Rabbits do not have paw pads on their feet like most other animals that use digitigrade locomotion. Instead, they have coarse compressed hair that offers protection.
Rabbits have muscled hind legs that allow for maximum force, maneuverability, and acceleration that is divided into three main parts; foot, thigh, and leg. The hind limbs of a rabbit are an exaggerated feature, that are much longer than the forelimbs providing more force. Rabbits run on their toes to gain the optimal stride during locomotion. The force put out by the hind limbs is contributed to both the structural anatomy of the fusion tibia and fibula, and muscular features. Bone formation and removal, from a cellular standpoint, is directly correlated to hind limb muscles. Action pressure from muscles creates force that is then distributed through the skeletal structures. Rabbits that generate less force, putting less stress on bones are more prone to osteoporosis due to bone rarefaction. In rabbits, the more fibers in a muscle, the more resistant to fatigue. For example, hares have a greater resistance to fatigue than cottontails. The muscles of rabbit's hind limbs can be classified into four main categories: hamstrings, quadriceps, dorsiflexors, or plantar flexors. The quadriceps muscles are in charge of force production when jumping. Complementing these muscles are the hamstrings which aid in short bursts of action. These muscles play off of one another in the same way as the plantar flexors and doriflexors, contributing to the generation and actions associated with force.
Within the order lagomorphs, the ears are utilized to detect and avoid predators. In the family leporidae, the ears are typically longer than they are wide. For example, in black tailed jack rabbits, their long ears cover a greater surface area relative to their body size that allow them to detect predators from far away. Contrasted to cotton tailed rabbits, their ears are smaller and shorter, requiring predators to be closer to detect them before they can flee. Evolution has favored rabbits to have shorter ears so the larger surface area does not cause them to lose heat in more temperate regions. The opposite can be seen in rabbits that live in hotter climates, mainly because they possess longer ears that have a larger surface area that help with dispersion of heat as well as the theory that sound does not travel well in more arid air, opposed to cooler air. Therefore, longer ears are meant to aid the organism in detecting predators sooner rather than later in warmer temperatures. The rabbit is characterized by its shorter ears while hares are characterized by their longer ears. Rabbits' ears are an important structure to aid thermoregulation and detect predators due to how the outer, middle, and inner ear muscles coordinate with one another. The ear muscles also aid in maintaining balance and movement when fleeing predators. Outer ear The Auricle (anatomy), also known as the pinna is a rabbit's outer ear. The rabbit's pinnae represent a fair part of the body surface area. It is theorized that the ears aid in dispersion of heat at temperatures above 30 °C with rabbits in warmer climates having longer pinnae due to this. Another theory is that the ears function as shock absorbers that could aid and stabilize rabbit's vision when fleeing predators, but this has typically only been seen in hares. The rest of the outer ear has bent canals that lead to the eardrum or tympanic membrane.Middle ear The middle ear is filled with three bones called ossicles and is separated by the outer eardrum in the back of the rabbit's skull. The three ossicles are called hammer, anvil, and stirrup and act to decrease sound before it hits the inner ear. In general, the ossicles act as a barrier to the inner ear for sound energy.Inner ear Inner ear fluid called endolymph receives the sound energy. After receiving the energy, later within the inner ear there are two parts: the cochlea that utilizes sound waves from the ossicles and the vestibular apparatus that manages the rabbit's position in regards to movement. Within the cochlea there is a basilar membrane that contains sensory hair structures utilized to send nerve signals to the brain so it can recognize different sound frequencies. Within the vestibular apparatus the rabbit possesses three semicircular canals to help detect angular motion.
Thermoregulation is the process that an organism utilizes to maintain an optimal body temperature independent of external conditions. This process is carried out by the pinnae which takes up most of the rabbit's body surface and contain a vascular network and arteriovenous shunts. In a rabbit, the optimal body temperature is around 38.5–40℃. If their body temperature exceeds or does not meet this optimal temperature, the rabbit must return to homeostasis. Homeostasis of body temperature is maintained by the use of their large, highly vascularized ears that are able to change the amount of blood flow that passes through the ears. Constriction and dilation of blood vessels in the ears are used to control the core body temperature of a rabbit. If the core temperature exceeds its optimal temperature greatly, blood flow is constricted to limit the amount of blood going through the vessels. With this constriction, there is only a limited amount of blood that is passing through the ears where ambient heat would be able to heat the blood that is flowing through the ears and therefore, increasing the body temperature. Constriction is also used when the ambient temperature is much lower than that of the rabbit's core body temperature. When the ears are constricted it again limits blood flow through the ears to conserve the optimal body temperature of the rabbit. If the ambient temperature is either 15 degrees above or below the optimal body temperature, the blood vessels will dilate. With the blood vessels being enlarged, the blood is able to pass through the large surface area which causes it to either heat or cool down. During the summer, the rabbit has the capability to stretch its pinnae which allows for greater surface area and increase heat dissipation. In the winter, the rabbit does the opposite and folds its ears in order to decrease its surface area to the ambient air which would decrease their body temperature. The jackrabbit has the largest ears within the Oryctolagus cuniculus group. Their ears contribute to 17% of their total body surface area. Their large pinna were evolved to maintain homeostasis while in the extreme temperatures of the desert.
The rabbit's nasal cavity lies dorsal to the oral cavity, and the two compartments are separated by the hard and soft palate. The nasal cavity itself is separated into a left and right side by a cartilage barrier, and it is covered in fine hairs that trap dust before it can enter the respiratory tract. As the rabbit breathes, air flows in through the nostrils along the alar folds. From there, the air moves into the nasal cavity, also known as the nasopharynx, down through the trachea, through the larynx, and into the lungs. The larynx functions as the rabbit's voice box, which enables it to produce a wide variety of sounds. The trachea is a long tube embedded with cartilaginous rings that prevent the tube from collapsing as air moves in and out of the lungs. The trachea then splits into a left and right bronchus, which meet the lungs at a structure called the hilum. From there, the bronchi split into progressively more narrow and numerous branches. The bronchi branch into bronchioles, into respiratory bronchioles, and ultimately terminate at the alveolar ducts. The branching that is typically found in rabbit lungs is a clear example of monopodial branching, in which smaller branches divide out laterally from a larger central branch.Rabbits breathe primarily through their noses due to the fact that the epiglottis is fixed to the backmost portion of the soft palate. Within the oral cavity, a layer of tissue sits over the opening of the glottis, which blocks airflow from the oral cavity to the trachea. The epiglottis functions to prevent the rabbit from aspirating on its food. Further, the presence of a soft and hard palate allow the rabbit to breathe through its nose while it feeds. Rabbits lungs are divided into four lobes: the cranial, middle, caudal, and accessory lobes. The right lung is made up of all four lobes, while the left lung only has two: the cranial and caudal lobes. In order to provide space for the heart, the left cranial lobe of the lungs is significantly smaller than that of the right. The diaphragm is a muscular structure that lies caudal to the lungs and contracts to facilitate respiration.
Rabbits are herbivores that feed by grazing on grass, forbs, and leafy weeds. In consequence, their diet contains large amounts of cellulose, which is hard to digest. Rabbits solve this problem via a form of hindgut fermentation. They pass two distinct types of feces: hard droppings and soft black viscous pellets, the latter of which are known as caecotrophs or "night droppings" and are immediately eaten (a behaviour known as coprophagy). Rabbits reingest their own droppings (rather than chewing the cud as do cows and numerous other herbivores) to digest their food further and extract sufficient nutrients.Rabbits graze heavily and rapidly for roughly the first half-hour of a grazing period (usually in the late afternoon), followed by about half an hour of more selective feeding. In this time, the rabbit will also excrete many hard fecal pellets, being waste pellets that will not be reingested. If the environment is relatively non-threatening, the rabbit will remain outdoors for many hours, grazing at intervals. While out of the burrow, the rabbit will occasionally reingest its soft, partially digested pellets; this is rarely observed, since the pellets are reingested as they are produced. Hard pellets are made up of hay-like fragments of plant cuticle and stalk, being the final waste product after redigestion of soft pellets. These are only released outside the burrow and are not reingested. Soft pellets are usually produced several hours after grazing, after the hard pellets have all been excreted. They are made up of micro-organisms and undigested plant cell walls.Rabbits are hindgut digesters. This means that most of their digestion takes place in their large intestine and cecum. In rabbits, the cecum is about 10 times bigger than the stomach and it along with the large intestine makes up roughly 40% of the rabbit's digestive tract. The unique musculature of the cecum allows the intestinal tract of the rabbit to separate fibrous material from more digestible material; the fibrous material is passed as feces, while the more nutritious material is encased in a mucous lining as a cecotrope. Cecotropes, sometimes called "night feces", are high in minerals, vitamins and proteins that are necessary to the rabbit's health. Rabbits eat these to meet their nutritional requirements; the mucous coating allows the nutrients to pass through the acidic stomach for digestion in the intestines. This process allows rabbits to extract the necessary nutrients from their food.The chewed plant material collects in the large cecum, a secondary chamber between the large and small intestine containing large quantities of symbiotic bacteria that help with the digestion of cellulose and also produce certain B vitamins. The pellets are about 56% bacteria by dry weight, largely accounting for the pellets being 24.4% protein on average. The soft feces form here and contain up to five times the vitamins of hard feces. After being excreted, they are eaten whole by the rabbit and redigested in a special part of the stomach. The pellets remain intact for up to six hours in the stomach; the bacteria within continue to digest the plant carbohydrates. This double-digestion process enables rabbits to use nutrients that they may have missed during the first passage through the gut, as well as the nutrients formed by the microbial activity and thus ensures that maximum nutrition is derived from the food they eat. This process serves the same purpose in the rabbit as rumination does in cattle and sheep. Rabbits are incapable of vomiting. Because rabbits cannot vomit, if buildup occurs within the intestines (due often to a diet with insufficient fiber), intestinal blockage can occur.
The adult male reproductive system forms the same as most mammals with the seminiferous tubular compartment containing the Sertoli cells and an adluminal compartment that contains the Leydig cells. The Leydig cells produce testosterone, which maintains libido and creates secondary sex characteristics such as the genital tubercle and penis. The Sertoli cells triggers the production of Anti-Müllerian duct hormone, which absorbs the Müllerian duct. In an adult male rabbit, the sheath of the penis is cylinder-like and can be extruded as early as two months of age. The scrotal sacs lay lateral to the penis and contain epididymal fat pads which protect the testes. Between 10–14 weeks, the testes descend and are able to retract into the pelvic cavity in order to thermoregulate. Furthermore, the secondary sex characteristics, such as the testes, are complex and secrete many compounds. These compounds includes fructose, citric acid, minerals, and a uniquely high amount of catalase. The adult female reproductive tract is bipartite, which prevents an embryo from translocating between uteri. The two uterine horns communicate to two cervixes and forms one vaginal canal. Along with being bipartite, the female rabbit does not go through an estrus cycle, which causes mating induced ovulation.The average female rabbit becomes sexually mature at 3 to 8 months of age and can conceive at any time of the year for the duration of her life. However, egg and sperm production can begin to decline after three years. During mating, the male rabbit will mount the female rabbit from behind and insert his penis into the female and make rapid pelvic hip thrusts. The encounter lasts only 20–40 seconds and after, the male will throw himself backwards off the female.The rabbit gestation period is short and ranges from 28 to 36 days with an average period of 31 days. A longer gestation period will generally yield a smaller litter while shorter gestation periods will give birth to a larger litter. The size of a single litter can range from four to 12 kits allowing a female to deliver up to 60 new kits a year. After birth, the female can become pregnant again as early as the next day.The mortality rates of embryos are high in rabbits and can be due to infection, trauma, poor nutrition and environmental stress so a high fertility rate is necessary to counter this.
Rabbits may appear to be crepuscular, but their natural inclination is toward nocturnal activity. In 2011, the average sleep time of a rabbit in captivity was calculated at 8.4 hours per day. As with other prey animals, rabbits often sleep with their eyes open, so that sudden movements will awaken the rabbit to respond to potential danger.
In addition to being at risk of disease from common pathogens such as Bordetella bronchiseptica and Escherichia coli, rabbits can contract the virulent, species-specific viruses RHD ("rabbit hemorrhagic disease", a form of calicivirus) or myxomatosis. Among the parasites that infect rabbits are tapeworms (such as Taenia serialis), external parasites (including fleas and mites), coccidia species, and Toxoplasma gondii. Domesticated rabbits with a diet lacking in high fiber sources, such as hay and grass, are susceptible to potentially lethal gastrointestinal stasis. Rabbits and hares are almost never found to be infected with rabies and have not been known to transmit rabies to humans.Encephalitozoon cuniculi, an obligate intracellular parasite is also capable of infecting many mammals including rabbits.
Rabbits are prey animals and are therefore constantly aware of their surroundings. For instance, in Mediterranean Europe, rabbits are the main prey of red foxes, badgers, and Iberian lynxes. If confronted by a potential threat, a rabbit may freeze and observe then warn others in the warren with powerful thumps on the ground. Rabbits have a remarkably wide field of vision, and a good deal of it is devoted to overhead scanning. They survive predation by burrowing, hopping away in a zig-zag motion, and, if captured, delivering powerful kicks with their hind legs. Their strong teeth allow them to eat and to bite in order to escape a struggle. The longest-lived rabbit on record, a domesticated European rabbit living in Tasmania, died at age 18. The lifespan of wild rabbits is much shorter; the average longevity of an eastern cottontail, for instance, is less than one year.
Rabbit habitats include meadows, woods, forests, grasslands, deserts and wetlands. Rabbits live in groups, and the best known species, the European rabbit, lives in burrows, or rabbit holes. A group of burrows is called a warren.More than half the world's rabbit population resides in North America. They are also native to southwestern Europe, Southeast Asia, Sumatra, some islands of Japan, and in parts of Africa and South America. They are not naturally found in most of Eurasia, where a number of species of hares are present. Rabbits first entered South America relatively recently, as part of the Great American Interchange. Much of the continent has just one species of rabbit, the tapeti, while most of South America's southern cone is without rabbits. The European rabbit has been introduced to many places around the world.
Rabbits have been a source of environmental problems when introduced into the wild by humans. As a result of their appetites, and the rate at which they breed, feral rabbit depredation can be problematic for agriculture. Gassing, barriers (fences), shooting, snaring, and ferreting have been used to control rabbit populations, but the most effective measures are diseases such as myxomatosis (myxo or mixi, colloquially) and calicivirus. In Europe, where rabbits are farmed on a large scale, they are protected against myxomatosis and calicivirus with a genetically modified virus. The virus was developed in Spain, and is beneficial to rabbit farmers. If it were to make its way into wild populations in areas such as Australia, it could create a population boom, as those diseases are the most serious threats to rabbit survival. Rabbits in Australia and New Zealand are considered to be such a pest that land owners are legally obliged to control them.
In some areas, wild rabbits and hares are hunted for their meat, a lean source of high quality protein. In the wild, such hunting is accomplished with the aid of trained falcons, ferrets, or dogs, as well as with snares or other traps, and rifles. A caught rabbit may be dispatched with a sharp blow to the back of its head, a practice from which the term rabbit punch is derived. Wild leporids comprise a small portion of global rabbit-meat consumption. Domesticated descendants of the European rabbit (Oryctolagus cuniculus) that are bred and kept as livestock (a practice called cuniculture) account for the estimated 200 million tons of rabbit meat produced annually. Approximately 1.2 billion rabbits are slaughtered each year for meat worldwide. In 1994, the countries with the highest consumption per capita of rabbit meat were Malta with 8.89 kg (19 lb 10 oz), Italy with 5.71 kg (12 lb 9 oz), and Cyprus with 4.37 kg (9 lb 10 oz), falling to 0.03 kg (1 oz) in Japan. The figure for the United States was 0.14 kg (5 oz) per capita. The largest producers of rabbit meat in 1994 were China, Russia, Italy, France, and Spain. Rabbit meat was once a common commodity in Sydney, Australia, but declined after the myxomatosis virus was intentionally introduced to control the exploding population of feral rabbits in the area. In the United Kingdom, fresh rabbit is sold in butcher shops and markets, and some supermarkets sell frozen rabbit meat. At farmers markets there, including the famous Borough Market in London, rabbit carcasses are sometimes displayed hanging, unbutchered (in the traditional style), next to braces of pheasant or other small game. Rabbit meat is a feature of Moroccan cuisine, where it is cooked in a tajine with "raisins and grilled almonds added a few minutes before serving". In China, rabbit meat is particularly popular in Sichuan cuisine, with its stewed rabbit, spicy diced rabbit, BBQ-style rabbit, and even spicy rabbit heads, which have been compared to spicy duck neck. Rabbit meat is comparatively unpopular elsewhere in the Asia-Pacific. An extremely rare infection associated with rabbits-as-food is tularemia (also known as rabbit fever), which may be contracted from an infected rabbit. Hunters are at higher risk for tularemia because of the potential for inhaling the bacteria during the skinning process. In addition to their meat, rabbits are used for their wool, fur, and pelts, as well as their nitrogen-rich manure and their high-protein milk. Production industries have developed domesticated rabbit breeds (such as the well-known Angora rabbit) to efficiently fill these needs.
Rabbits are often used as a symbol of fertility or rebirth, and have long been associated with spring and Easter as the Easter Bunny. The species' role as a prey animal with few defenses evokes vulnerability and innocence, and in folklore and modern children's stories, rabbits often appear as sympathetic characters, able to connect easily with youth of all kinds (for example, the Velveteen Rabbit, or Thumper in Bambi). With its reputation as a prolific breeder, the rabbit juxtaposes sexuality with innocence, as in the Playboy Bunny. The rabbit (as a swift prey animal) is also known for its speed, agility, and endurance, symbolized (for example) by the marketing icons the Energizer Bunny and the Duracell Bunny.
The rabbit often appears in folklore as the trickster archetype, as he uses his cunning to outwit his enemies. In Aztec mythology, a pantheon of four hundred rabbit gods known as Centzon Totochtin, led by Ometotchtli or Two Rabbit, represented fertility, parties, and drunkenness. In Central Africa, the common hare (Kalulu), is "inevitably described" as a trickster figure. In Chinese folklore, rabbits accompany Chang'e on the Moon. In the Chinese New Year, the zodiacal rabbit is one of the twelve celestial animals in the Chinese zodiac. Note that the Vietnamese zodiac includes a zodiacal cat in place of the rabbit, possibly because rabbits did not inhabit Vietnam. The most common explanation, however, is that the ancient Vietnamese word for "rabbit" (mao) sounds like the Chinese word for "cat" (卯, mao). In Japanese tradition, rabbits live on the Moon where they make mochi, the popular snack of mashed sticky rice. This comes from interpreting the pattern of dark patches on the moon as a rabbit standing on tiptoes on the left pounding on an usu, a Japanese mortar. In Jewish folklore, rabbits (shfanim שפנים) are associated with cowardice, a usage still current in contemporary Israeli spoken Hebrew (similar to the English colloquial use of "chicken" to denote cowardice). In Korean mythology, as in Japanese, rabbits live on the moon making rice cakes ("Tteok" in Korean). In Anishinaabe traditional beliefs, held by the Ojibwe and some other Native American peoples, Nanabozho, or Great Rabbit, is an important deity related to the creation of the world. A Vietnamese mythological story portrays the rabbit of innocence and youthfulness. The Gods of the myth are shown to be hunting and killing rabbits to show off their power. Buddhism, Christianity, and Judaism have associations with an ancient circular motif called the three rabbits (or "three hares"). Its meaning ranges from "peace and tranquility", to purity or the Holy Trinity, to Kabbalistic levels of the soul or to the Jewish diaspora. The tripartite symbol also appears in heraldry and even tattoos.The rabbit as trickster is a part of American popular culture, as Br'er Rabbit (from African-American folktales and, later, Disney animation) and Bugs Bunny (the cartoon character from Warner Bros.), for example. Anthropomorphized rabbits have appeared in film and literature, in Alice's Adventures in Wonderland (the White Rabbit and the March Hare characters), in Watership Down (including the film and television adaptations), in Rabbit Hill (by Robert Lawson), and in the Peter Rabbit stories (by Beatrix Potter). In the 1920s, Oswald the Lucky Rabbit, was a popular cartoon character.
Windling, Terri. The Symbolism of Rabbits and Hares
Rabbit at the Encyclopædia Britannica American Rabbit Breeders Association organization which promotes all phases of rabbit keeping House Rabbit Society an activist organization which promotes keeping rabbits indoors. RabbitShows.com an informational site on the hobby of showing rabbits. The (mostly) silent language of rabbits World Rabbit Science Association an international rabbit-health science-based organization The Year of the Rabbit – slideshow by Life magazine House Rabbit Society- FAQ: Aggression
Rabbiting (also rabbit hunting and cottontail hunting) is the sport of hunting rabbits. It often involves using ferrets or dogs to track or chase the prey. There are various methods used in capturing the rabbit, including trapping and shooting. Depending on where the hunting occurs, there may be licenses required and other rules in regards to methods being used.
Most rabbit hunters try to locate rabbit holes, which are usually found in wooded areas with higher grounds soft enough for the rabbits to burrow in. Hunters without hounds have the following options. A hunter, alone or with a partner, walks through the possible locations of rabbit hiding places, kicking or stomping possible covers to chase the rabbit out. In winter, an advantage is visible rabbit tracks after a fresh snow. Unraveling tracks allows the hunter to locate the hiding place: if no tracks lead out of a suspected location, then the quarry is located. After this, hunters with short-range arms (archers or the ones with small calibre) may scrutinize the location to find the rabbit and shoot it immobile. Alternatively, one may just as well scare the animal out and shoot it while it is on the run.
Spotlighting or lamping can refer to any form of rabbit hunting performed at night with the aid of powerful hand-held, rifle mounted or vehicle mounted search lights. The light is often used in conjunction with a dog such as a sighthound, (or lurcher) alongside an air rifle, or some other firearm such as a .22LR.17 HMR The rabbit is illuminated by the light and then shot, or a dog will chase and capture it. Most often lurchers are used to catch the prey, the most popular crosses involve greyhounds, border collies and salukis.Using a vehicle is a very popular method of spotlighting. Pick ups and 4×4 are preferred modes of transport. ATVS are also popular vehicles for rabbiting. They provide rapid acceleration making it easy to chase down rabbits.
There is a large variety of different traps that are used to capture rabbits and can be divided into categories. A lot of traps are typically used for pest control. When hunting for sport, long netting is the most common method of trapping. Many traps are illegal.
These traps have a high rate of success and are very easy to set up. The springs inside the trap are triggered by the weight of the rabbit, causing them to shorten and the door to shut behind the animal, leaving it safely enclosed. Homemade traps such as these do not have a great success rate, as the effectiveness depends entirely on the trap's quality. The way the traps operate vary, but ultimately the rabbit's movement is what triggers them to close.
These traps are quite advanced because they are able to capture a large number of rabbits and automatically reset themselves. They are buried into the ground and usually have a type of tunnel that lures the animal to a spring-loaded trap, which will then drop the rabbit into an enclosure once it is triggered by weight.
In medieval times, a hawk or falcon would have been used to catch the rabbit as it exited the warren burrow. For this type of hunt, an albino ferret would typically be used, allowing the bird-of-prey to more easily recognize it. While this hunting style is still occasionally used, especially in the UK where it remains popular (see Falconry), the methods above have almost entirely replaced it. Also around this time, the popularity of hare coursing sport was growing. Back then, two greyhounds would be released at the same time in pursuit of the rabbit and the one that kills it is declared the winner of the game; people typically placed bets on which dog would be the victor.In sixteenth-century Britain, hunting rabbits typically involved two hunters either on foot or horseback, a group of hounds, and a horn. The hunter leading the hounds used the horn to encourage them to chase after the rabbit, while the other stayed at the back of the group to motivate any dogs that fell behind. When the rabbit was caught, its death would be marked by a ritual dissection of its body following a blow of the horn. After the actual hunt, the meat would be taken home by the hunters, and the leftovers were given to the dogs as a reward. The rabbit's meat was not highly rated during this time period; huntsman still collected the meat, but the hunt was ultimately a form of entertainment.
In the United Kingdom it is not required for a hunter to have a game license to kill and take rabbits. The hunting season for rabbits runs through the entire year from January 1 to December 31. On the Isle of Man, a game license is required to shoot rabbits and a dealer's license is required for dealing any type of game; they can be obtained from the Treasury Office. The hunting season also runs throughout the entire year. In Jersey, no license is required because there is no hunting season for rabbits. Guernsey's laws require a shotgun or firearm certificate rather than a hunting license. In most of these places, it is considered an offence to kill any type of game on a Sunday.
In the United States, every person wishing to hunt must have a state hunting license (few states have exceptions to this). Some national wildlife refuges may have separate permits required. Each state has different hunting seasons for rabbits. In Virginia, the season lasts throughout November, December, and January.
Roger Bacon (; Latin: Rogerus or Rogerius Baconus, Baconis, also Frater Rogerus; c. 1219/20 – c. 1292), also known by the scholastic accolade Doctor Mirabilis, was a medieval English philosopher and Franciscan friar who placed considerable emphasis on the study of nature through empiricism. In the early modern era, he was regarded as a wizard and particularly famed for the story of his mechanical or necromantic brazen head. He is sometimes credited (mainly since the 19th century) as one of the earliest European advocates of the modern scientific method. Bacon applied the empirical method of Ibn al-Haytham (Alhazen) to observations in texts attributed to Aristotle. Bacon discovered the importance of empirical testing when the results he obtained were different than those that would have been predicted by Aristotle. (Aristotle had never performed experiments to verify his explanations of his observations of nature.) His linguistic work has been heralded for its early exposition of a universal grammar. However, more recent re-evaluations emphasise that Bacon was essentially a medieval thinker, with much of his "experimental" knowledge obtained from books in the scholastic tradition. He was, however, partially responsible for a revision of the medieval university curriculum, which saw the addition of optics to the traditional quadrivium.Bacon's major work, the Opus Majus, was sent to Pope Clement IV in Rome in 1267 upon the pope's request. Although gunpowder was first invented and described in China, Bacon was the first in Europe to record its formula.
Roger Bacon was born in Ilchester in Somerset, England, in the early 13th century, although his date of birth is sometimes narrowed down to c. 1210, "1213 or 1214", or "1215". However, modern scholars tend to argue for the date of c. 1220, but there are disagreements on this. The only source for his birth date is a statement from his 1267 Opus Tertium that "forty years have passed since I first learned the Alphabetum". The latest dates assume this referred to the alphabet itself, but elsewhere in the Opus Tertium it is clear that Bacon uses the term to refer to rudimentary studies, the trivium or quadrivium that formed the medieval curriculum. His family appears to have been well off.Bacon studied at Oxford. While Robert Grosseteste had probably left shortly before Bacon's arrival, his work and legacy almost certainly influenced the young scholar and it is possible Bacon subsequently visited him and William of Sherwood in Lincoln. Bacon became a master at Oxford, lecturing on Aristotle. There is no evidence he was ever awarded a doctorate. (The title Doctor Mirabilis was posthumous and figurative.) A caustic cleric named Roger Bacon is recorded speaking before the king at Oxford in 1233. In 1237 or at some point in the following decade, he accepted an invitation to teach at the University of Paris. While there, he lectured on Latin grammar, Aristotelian logic, arithmetic, geometry, and the mathematical aspects of astronomy and music. His faculty colleagues included Robert Kilwardby, Albertus Magnus, and Peter of Spain, the future Pope John XXI. The Cornishman Richard Rufus was a scholarly opponent. In 1247 or soon after, he left his position in Paris. As a private scholar, his whereabouts for the next decade are uncertain but he was likely in Oxford c. 1248–1251, where he met Adam Marsh, and in Paris in 1251. He seems to have studied most of the known Greek and Arabic works on optics (then known as "perspective", perspectiva). A passage in the Opus Tertium states that at some point he took a two-year break from his studies.By the late 1250s, resentment against the king's preferential treatment of his émigré Poitevin relatives led to a coup and the imposition of the Provisions of Oxford and Westminster, instituting a baronial council and more frequent parliaments. Pope Urban IV absolved the king of his oath in 1261 and, after initial abortive resistance, Simon de Montfort led a force, enlarged due to recent crop failures, that prosecuted the Second Barons' War. Bacon's own family were considered royal partisans: De Montfort's men seized their property and drove several members into exile. In 1256 or 1257, he became a friar in the Franciscan Order in either Paris or Oxford, following the example of scholarly English Franciscans such as Grosseteste and Marsh. After 1260, Bacon's activities were restricted by a statute prohibiting the friars of his order from publishing books or pamphlets without prior approval. He was likely kept at constant menial tasks to limit his time for contemplation and came to view his treatment as an enforced absence from scholarly life.By the mid-1260s, he was undertaking a search for patrons who could secure permission and funding for his return to Oxford. For a time, Bacon was finally able to get around his superiors' interference through his acquaintance with Guy de Foulques, bishop of Narbonne, cardinal of Sabina, and the papal legate who negotiated between England's royal and baronial factions.In 1263 or 1264, a message garbled by Bacon's messenger, Raymond of Laon, led Guy to believe that Bacon had already completed a summary of the sciences. In fact, he had no money to research, let alone copy, such a work and attempts to secure financing from his family were thwarted by the Second Barons' War. However, in 1265, Guy was summoned to a conclave at Perugia that elected him Pope Clement IV. William Benecor, who had previously been the courier between Henry III and the pope, now carried the correspondence between Bacon and Clement. Clement's reply of 22 June 1266 commissioned "writings and remedies for current conditions", instructing Bacon not to violate any standing "prohibitions" of his order but to carry out his task in utmost secrecy.While faculties of the time were largely limited to addressing disputes on the known texts of Aristotle, Clement's patronage permitted Bacon to engage in a wide-ranging consideration of the state of knowledge in his era. In 1267 or '68, Bacon sent the Pope his Opus Majus, which presented his views on how to incorporate Aristotelian logic and science into a new theology, supporting Grosseteste's text-based approach against the "sentence method" then fashionable.Bacon also sent his Opus Minus, De Multiplicatione Specierum, De Speculis Comburentibus, an optical lens, and possibly other works on alchemy and astrology. The entire process has been called "one of the most remarkable single efforts of literary productivity", with Bacon composing referenced works of around a million words in about a year.Pope Clement died in 1268 and Bacon lost his protector. The Condemnations of 1277 banned the teaching of certain philosophical doctrines, including deterministic astrology. Some time within the next two years, Bacon was apparently imprisoned or placed under house arrest. This was traditionally ascribed to Franciscan Minister General Jerome of Ascoli, probably acting on behalf of the many clergy, monks, and educators attacked by Bacon's 1271 Compendium Studii Philosophiae.Modern scholarship, however, notes that the first reference to Bacon's "imprisonment" dates from eighty years after his death on the charge of unspecified "suspected novelties" and finds it less than credible. Contemporary scholars who do accept Bacon's imprisonment typically associate it with Bacon's "attraction to contemporary prophesies", his sympathies for "the radical 'poverty' wing of the Franciscans", interest in certain astrological doctrines, or generally combative personality rather than from "any scientific novelties which he may have proposed".Sometime after 1278, Bacon returned to the Franciscan House at Oxford, where he continued his studies and is presumed to have spent most of the remainder of his life. His last dateable writing—the Compendium Studii Theologiae—was completed in 1292. He seems to have died shortly afterwards and been buried at Oxford.
Medieval European philosophy often relied on appeals to the authority of Church Fathers such as St Augustine, and on works by Plato and Aristotle only known at second hand or through (sometimes highly inaccurate) Latin translations. By the 13th century, new works and better versions – in Arabic or in new Latin translations from the Arabic – began to trickle north from Muslim Spain. In Roger Bacon's writings, he upholds Aristotle's calls for the collection of facts before deducing scientific truths, against the practices of his contemporaries, arguing that "thence cometh quiet to the mind". Bacon also called for reform with regard to theology. He argued that, rather than training to debate minor philosophical distinctions, theologians should focus their attention primarily on the Bible itself, learning the languages of its original sources thoroughly. He was fluent in several of these languages and was able to note and bemoan several corruptions of scripture, and of the works of the Greek philosophers that had been mistranslated or misinterpreted by scholars working in Latin. He also argued for the education of theologians in science ("natural philosophy") and its addition to the medieval curriculum.
Bacon's 1267 Greater Work, the Opus Majus, contains treatments of mathematics, optics, alchemy, and astronomy, including theories on the positions and sizes of the celestial bodies. It is divided into seven sections: "The Four General Causes of Human Ignorance" (Causae Erroris), "The Affinity of Philosophy with Theology" (Philosophiae cum Theologia Affinitas), "On the Usefulness of Grammar" (De Utilitate Grammaticae), "The Usefulness of Mathematics in Physics" (Mathematicae in Physicis Utilitas), "On the Science of Perspective" (De Scientia Perspectivae), "On Experimental Knowledge" (De Scientia Experimentali), and "A Philosophy of Morality" (Moralis Philosophia).It was not intended as a complete work but as a "persuasive preamble" (persuasio praeambula), an enormous proposal for a reform of the medieval university curriculum and the establishment of a kind of library or encyclopedia, bringing in experts to compose a collection of definitive texts on these subjects. The new subjects were to be "perspective" (i.e., optics), "astronomy" (inclusive of astronomy proper, astrology, and the geography necessary in order to use them), "weights" (likely some treatment of mechanics but this section of the Opus Majus has been lost), alchemy, agriculture (inclusive of botany and zoology), medicine, and "experimental science", a philosophy of science that would guide the others. The section on geography was allegedly originally ornamented with a map based on ancient and Arabic computations of longitude and latitude, but has since been lost. His (mistaken) arguments supporting the idea that dry land formed the larger proportion of the globe were apparently similar to those which later guided Columbus.In this work Bacon criticises his contemporaries Alexander of Hales and Albertus Magnus, who were held in high repute despite having only acquired their knowledge of Aristotle at second hand during their preaching careers. Albert was received at Paris as an authority equal to Aristotle, Avicenna and Averroes, a situation Bacon decried: "never in the world [had] such monstrosity occurred before."In Part I of the Opus Majus Bacon recognises some philosophers as the Sapientes, or gifted few, and saw their knowledge in philosophy and theology as superior to the vulgus philosophantium, or common herd of philosophers. He held Islamic thinkers between 1210 and 1265 in especially high regard calling them "both philosophers and sacred writers" and defended the integration of Islamic philosophy into Christian learning.
In Part IV of the Opus Majus, Bacon proposed a calendrical reform similar to the later system introduced in 1582 under Pope Gregory XIII. Drawing on ancient Greek and medieval Islamic astronomy recently introduced to western Europe via Spain, Bacon continued the work of Robert Grosseteste and criticised the then-current Julian calendar as "intolerable, horrible, and laughable". It had become apparent that Eudoxus and Sosigenes's assumption of a year of 365¼ days was, over the course of centuries, too inexact. Bacon charged that this meant the computation of Easter had shifted forward by 9 days since the First Council of Nicaea in 325. His proposal to drop one day every 125 years and to cease the observance of fixed equinoxes and solstices was not acted upon following the death of Pope Clement IV in 1268. The eventual Gregorian calendar drops one day from the first three centuries in each set of 400 years.
A passage in the Opus Majus and another in the Opus Tertium are usually taken as the first European descriptions of a mixture containing the essential ingredients of gunpowder. Partington and others have come to the conclusion that Bacon most likely witnessed at least one demonstration of Chinese firecrackers, possibly obtained by Franciscans—including Bacon's friend William of Rubruck—who visited the Mongol Empire during this period. The most telling passage reads: We have an example of these things (that act on the senses) in [the sound and fire of] that children's toy which is made in many [diverse] parts of the world; i.e. a device no bigger than one's thumb. From the violence of that salt called saltpetre [together with sulphur and willow charcoal, combined into a powder] so horrible a sound is made by the bursting of a thing so small, no more than a bit of parchment [containing it], that we find [the ear assaulted by a noise] exceeding the roar of strong thunder, and a flash brighter than the most brilliant lightning. At the beginning of the 20th century, Henry William Lovett Hime of the Royal Artillery published the theory that Bacon's Epistola contained a cryptogram giving a recipe for the gunpowder he witnessed. The theory was criticised by Thorndike in a 1915 letter to Science and several books, a position joined by Muir, Stillman, Steele, and Sarton. Needham et al. concurred with these earlier critics that the additional passage did not originate with Bacon and further showed that the proportions supposedly deciphered (a 7:5:5 ratio of saltpetre to charcoal to sulphur) as not even useful for firecrackers, burning slowly with a great deal of smoke and failing to ignite inside a gun barrel. The ~41% nitrate content is too low to have explosive properties.
Bacon attributed the Secret of Secrets (Secretum Secretorum), the Islamic "Mirror of Princes" (Arabic: Sirr al-ʿasrar‎), to Aristotle, thinking that he had composed it for Alexander the Great. Bacon produced an edition of Philip of Tripoli's Latin translation, complete with his own introduction and notes; and his writings of the 1260s and 1270s cite it far more than his contemporaries did. This led Easton and others, including Robert Steele, to argue that the text spurred Bacon's own transformation into an experimentalist. (Bacon never described such a decisive impact himself.) The dating of Bacon's edition of the Secret of Secrets is a key piece of evidence in the debate, with those arguing for a greater impact giving it an earlier date; but it certainly influenced the elder Bacon's conception of the political aspects of his work in the sciences.
Bacon has been credited with a number of alchemical texts.The Letter on the Secret Workings of Art and Nature and on the Vanity of Magic (Epistola de Secretis Operibus Artis et Naturae et de Nullitate Magiae), also known as On the Wonderful Powers of Art and Nature (De Mirabili Potestate Artis et Naturae), a likely-forged letter to an unknown "William of Paris," dismisses practices such as necromancy but contains most of the alchemical formulae attributed to Bacon, including one for a philosopher's stone and another possibly for gunpowder. It also includes several passages about hypothetical flying machines and submarines, attributing their first use to Alexander the Great. On the Vanity of Magic or The Nullity of Magic is a debunking of esoteric claims in Bacon's time, showing that they could be explained by natural phenomena.
Bacon's early linguistic and logical works are the Overview of Grammar (Summa Grammatica), Summa de Sophismatibus et Distinctionibus, and the Summulae Dialectices or Summulae super Totam Logicam. These are mature but essentially conventional presentations of Oxford and Paris's terminist and pre-modist logic and grammar. His later work in linguistics is much more idiosyncratic, using terminology and addressing questions unique in his era.In his Greek and Hebrew Grammars (Grammatica Graeca and Hebraica), in his work "On the Usefulness of Grammar" (Book III of the Opus Majus), and in his Compendium of the Study of Philosophy, Bacon stresses the need for scholars to know several languages. Europe's vernacular languages are not ignored—he considers them useful for practical purposes such as trade, proselytism, and administration—but Bacon is mostly interested in his era's languages of science and religion: Arabic, Greek, Hebrew and Latin.Bacon is less interested in a full practical mastery of the other languages than on a theoretical understanding of their grammatical rules, ensuring that a Latin reader will not misunderstand passages' original meaning. For this reason, his treatments of Greek and Hebrew grammar are not isolated works on their topic but contrastive grammars treating the aspects which influenced Latin or which were required for properly understanding Latin texts. He pointedly states, "I want to describe Greek grammar for the benefit of Latin speakers". It is likely only this limited sense which was intended by Bacon's boast that he could teach an interested pupil a new language within three days.Passages in the Overview and the Greek grammar have been taken as an early exposition of a universal grammar underlying all human languages. The Greek grammar contains the tersest and most famous exposition: Grammar is one and the same in all languages, substantially, though it may vary, accidentally, in each of them. However, Bacon's lack of interest in studying a literal grammar underlying the languages known to him and his numerous works on linguistics and comparative linguistics has prompted Hovdhaugen to question the usual literal translation of Bacon's grammatica in such passages. She notes the ambiguity in the Latin term, which could refer variously to the structure of language, to its description, and to the science underlying such descriptions: i.e., linguistics.
Bacon states that his Lesser Work (Opus Minus) and Third Work (Opus Tertium) were originally intended as summaries of the Opus Majus in case it was lost in transit. Easton's review of the texts suggests that they became separate works over the course of the laborious process of creating a fair copy of the Opus Majus, whose half-million words were copied by hand and apparently greatly revised at least once.Other works by Bacon include his "Tract on the Multiplication of Species" (Tractatus de Multiplicatione Specierum), "On Burning Lenses" (De Speculis Comburentibus), the Communia Naturalium and Mathematica, the "Compendium of the Study of Philosophy" and "of Theology" (Compendium Studii Philosophiae and Theologiae), and his Computus. The "Compendium of the Study of Theology", presumably written in the last years of his life, was an anticlimax: adding nothing new, it is principally devoted to the concerns of the 1260s.
The Mirror of Alchimy (Speculum Alchemiae), a short treatise on the origin and composition of metals, is traditionally credited to Bacon. It espouses the Arabian theory of mercury and sulphur forming the other metals, with vague allusions to transmutation. Stillman opined that "there is nothing in it that is characteristic of Roger Bacon's style or ideas, nor that distinguishes it from many unimportant alchemical lucubrations of anonymous writers of the thirteenth to the sixteenth centuries", and Muir and Lippmann also considered it a pseudepigraph.The cryptic Voynich manuscript has been attributed to Bacon by various sources, including by its first recorded owner, but historians of science Lynn Thorndike and George Sarton dismissed these claims as unsupported. and the vellum of the manuscript has since been dated to the 15th century.
Bacon was largely ignored by his contemporaries in favor of other scholars such as Albertus Magnus, Bonaventure, and Thomas Aquinas, although his works were studied by Bonaventure, John Pecham, and Peter of Limoges, through whom he may have influenced Raymond Lull. He was also partially responsible for the addition of optics (perspectiva) to the medieval university curriculum.By the early modern period, the English considered him the epitome of a wise and subtle possessor of forbidden knowledge, a Faust-like magician who had tricked the devil and so was able to go to heaven. Of these legends, one of the most prominent was that he created a talking brazen head which could answer any question. The story appears in the anonymous 16th-century account of The Famous Historie of Fryer Bacon, in which Bacon speaks with a demon but causes the head to speak by "the continuall fume of the six hottest Simples", testing his theory that speech is caused by "an effusion of vapors".Around 1589, Robert Greene adapted the story for the stage as The Honorable Historie of Frier Bacon and Frier Bongay, one of the most successful Elizabethan comedies. As late as the 1640s, Thomas Browne was still complaining that "Every ear is filled with the story of Frier Bacon, that made a brazen head to speak these words, Time is". Greene's Bacon spent seven years creating a brass head that would speak "strange and uncouth aphorisms" to enable him to encircle Britain with a wall of brass that would make it impossible to conquer. Unlike his source material, Greene does not cause his head to operate by natural forces but by "nigromantic charms" and "the enchanting forces of the devil": i.e., by entrapping a dead spirit or hobgoblin. Bacon collapses, exhausted, just before his device comes to life and announces "Time is", "Time was", and "Time is Past" before being destroyed in spectacular fashion: the stage direction instructs that "a lightening flasheth forth, and a hand appears that breaketh down the Head with a hammer".A necromantic head was ascribed to Pope Sylvester II as early as the 1120s, but Browne considered the legend to be a misunderstanding of a passage in Peter the Good's c. 1335 Precious Pearl where the negligent alchemist misses the birth of his creation and loses it forever. The story may also preserve the work by Bacon and his contemporaries to construct clockwork armillary spheres. Bacon had praised a "self-activated working model of the heavens" as "the greatest of all things which have been devised".As early as the 16th century, natural philosophers like Bruno, Dee, and Francis Bacon were attempting to rehabilitate Bacon's reputation and to portray him as a scientific pioneer who had avoided the petty bickering of his contemporaries to attempt a rational understanding of nature. By the 19th century, commenters following Whewell considered that "Bacon... was not appreciated in his age because he was so completely in advance of it; he is a 16th- or 17th-century philosopher, whose lot has been by some accident cast in the 13th century". His assertions in the Opus Majus that "theories supplied by reason should be verified by sensory data, aided by instruments, and corroborated by trustworthy witnesses" were (and still are) considered "one of the first important formulations of the scientific method on record".This idea that Bacon was a modern experimental scientist reflected two views of the period: that the principal form of scientific activity is experimentation and that 13th-century Europe still represented the "Dark Ages". This view, which is still reflected in some 21st-century popular science books, portrays Bacon as an advocate of modern experimental science who emerged as a solitary genius in an age hostile to his ideas. Based on Bacon's apocrypha, he is also portrayed as a visionary who predicted the invention of the submarine, aircraft, and automobile.However, in the course of the 20th century, Husserl, Heidegger and others emphasised the importance to modern science of Cartesian and Galilean projections of mathematics over sensory perceptions of nature; Heidegger in particular noted the lack of such an understanding in Bacon's works. Although Crombie, Kuhn and Schramm continued to argue for Bacon's importance to the development of "qualitative" areas of modern science, Duhem, Thorndike, Carton and Koyré emphasised the essentially medieval nature of Bacon's scientia experimentalis.Research also established that Bacon was not as isolated—and probably not as persecuted—as was once thought. Many medieval sources of and influences on Bacon's scientific activity have been identified. In particular, Bacon often mentioned his debt to the work of Robert Grosseteste: his work on optics and the calendar followed Grosseteste's lead, as did his idea that inductively-derived conclusions should be submitted for verification through experimental testing.Bacon noted of William of Sherwood that "nobody was greater in philosophy than he"; praised Peter of Maricourt (the author of "A Letter on Magnetism") and John of London as "perfect" mathematicians; Campanus of Novara (the author of works on astronomy, astrology, and the calendar) and a Master Nicholas as "good"; and acknowledged the influence of Adam Marsh and lesser figures. He was clearly not an isolated genius. The medieval church was also not generally opposed to scientific investigation and medieval science was both varied and extensive.As a result, the picture of Bacon has changed. Bacon is now seen as part of his age: a leading figure in the beginnings of the medieval universities at Paris and Oxford but one joined in the development of the philosophy of science by Robert Grosseteste, William of Auvergne, Henry of Ghent, Albert Magnus, Thomas Aquinas, John Duns Scotus, and William of Ockham. Lindberg summarised: Bacon was not a modern, out of step with his age, or a harbinger of things to come, but a brilliant, combative, and somewhat eccentric schoolman of the thirteenth century, endeavoring to take advantage of the new learning just becoming available while remaining true to traditional notions... of the importance to be attached to philosophical knowledge". A recent review of the many visions of Bacon across the ages says contemporary scholarship still neglects one of the most important aspects of his life and thought: his commitment to the Franciscan order. His Opus majus was a plea for reform addressed to the supreme spiritual head of the Christian faith, written against a background of apocalyptic expectation and informed by the driving concerns of the friars. It was designed to improve training for missionaries and to provide new skills to be employed in the defence of the Christian world against the enmity of non-Christians and of the Antichrist. It cannot usefully be read solely in the context of the history of science and philosophy. With regard to religion's influence on Bacon's philosophy, Charles Sanders Peirce noted, "To Roger Bacon,... the schoolmen's conception of reasoning appeared only an obstacle to truth... [but] Of all kinds of experience, the best, he thought, was interior illumination, which teaches many things about Nature which the external senses could never discover, such as the transubstantiation of bread."In Oxford lore, Bacon is credited as the namesake of Folly Bridge for having gotten himself placed under house arrest nearby. Although this is probably untrue, it had formerly been known as "Friar Bacon's Bridge". Bacon is also honoured at Oxford by a plaque affixed to the wall of the new Westgate shopping centre.
To commemorate the 700th anniversary of Bacon's approximate year of birth, Prof. J. Erskine wrote the biographical play A Pageant of the Thirteenth Century, which was performed and published by Columbia University in 1914. A fictionalised account of Bacon's life and times also appears in the second book of James Blish's After Such Knowledge trilogy, the 1964 Doctor Mirabilis. Bacon serves as a mentor to the protagonists of Thomas Costain's 1945 The Black Rose, and Umberto Eco's 1980 The Name of the Rose.Greene's play prompted a less successful sequel John of Bordeaux and was recast as a children's story for James Baldwin's 1905 Thirty More Famous Stories Retold. "The Brazen Head of Friar Bacon" also appears in Daniel Defoe's 1722 Journal of the Plague Year, Nathaniel Hawthorne's 1843 "The Birth-Mark" and 1844 "The Artist of the Beautiful", William Douglas O'Connor's 1891 "The Brazen Android" (where Bacon devises it to terrify King Henry into accepting Simon de Montfort's demands for greater democracy), John Cowper Powys's 1956 The Brazen Head, and Robertson Davies's 1970 Fifth Business. In the fan fiction serial Harry Potter and the Methods of Rationality, Harry is given Bacon's diary.
Baco, a lunar crater named for Roger Bacon History of geomagnetism, of translation, of the scientific method, and of science in the Middle Ages John of St Amand List of Catholic clergy scientists Oxford Franciscan school Roger Bacon High School Vitello Wilfrid Voynich
Six Degrees of Kevin Bacon or "Bacon's Law" is a parlor game based on the "six degrees of separation" concept, which posits that any two people on Earth are six or fewer acquaintance links apart. Movie buffs challenge each other to find the shortest path between an arbitrary actor and prolific actor Kevin Bacon. It rests on the assumption that anyone involved in the Hollywood film industry can be linked through their film roles to Bacon within six steps. In 2007, Bacon started a charitable organization called SixDegrees.org. In 2020, Bacon started a podcast called The Last Degree of Kevin Bacon.
While at the University of Virginia, Brett Tjaden created the Oracle of Bacon. A previous version of this computer program used information on some 800,000 people from the Internet Movie Database (IMDb), while the current implementation uses data drawn from Wikipedia. The algorithm calculates "how good a center" an individual IMDb personality is, i.e. a weighted average of the degree of separation of all the people that link to that particular person. The site returns an average personality number, e.g. for Clint Eastwood, it returns an average "Clint Eastwood Number." From there the Oracle site posits "The Center of the Hollywood Universe" as being the person with the lowest average personality number. Kevin Bacon, as it turns out, is not the "Center of the Hollywood Universe" (i.e. the most linkable actor). In fact, Bacon does not even make the top 100 list of average personality numbers. While he is not the most linkable actor, this still signifies being a better center than more than 99% of the people who have ever appeared in a film. Since each actor's average personality number can change with each new film made, the center can and does shift. "Centers" have included Rod Steiger, Donald Sutherland, Eric Roberts, Dennis Hopper, Christopher Lee and Harvey Keitel. A 2020 study from Cardiff University also used further measures to determine the center of the movie universe. These included degree centrality, closeness centrality and betweenness centrality, which were implemented using the NetworkX Python Library. The study also looked at movies released in different decades and found that, between 2000 and 2020, the most central actors included Angelina Jolie, Brahmanandam, Samuel L. Jackson, and Ben Kingsley.
Inspired by the game, the British photographer Andy Gotts tried to reach Kevin Bacon through photographic links instead of film links. Gotts wrote to 300 actors asking to take their pictures, and received permission only from Joss Ackland. Ackland then suggested that Gotts photograph Greta Scacchi, with whom he had appeared in the film White Mischief. Gotts proceeded from there, asking each actor to refer him to one or more friends or colleagues. Eventually, Christian Slater referred him to Bacon. Gotts' photograph of Bacon completed the project, eight years after it began. Gotts published the photos in a book, Degrees (ISBN 0-9546843-6-2), with text by Alan Bates, Pierce Brosnan, and Bacon.
Small-world experiment Morphy Number, connections via chess games to Paul Morphy Shusaku number, equivalent in the Go world with Honinbo Shusaku
The Oracle of Bacon computes the Bacon number of any actor or actress from Wikipedia data. A previous implementation used IMDB data. Six Degrees of James A. Conrad A how-to demonstration for those wishing to compile their own "degrees of" list by a Hollywood author who is three degrees of Bacon. Cinema FreeNet Movie Connector finds links between stars, but can also use directors and producers. Filmlovr.com browse the extensive film library to find films to connect via their actors aka determine the Bacon number. Six Degrees of Lois Weisberg suggests that Bacon connects to many actors because he acts in many different kinds of roles and films.
Sosie Ruth Bacon (born March 15, 1992) is an American actress. Her first role was playing 10-year-old Emily in the movie Loverboy (2005), which was directed by her father, Kevin Bacon. James Duff, producer of The Closer, was compelled by Bacon's performance in Loverboy to suggest that she play the role of Deputy Chief Brenda Leigh Johnson's niece Charlie in the fifth season of the show. Although her parents were opposed to her being involved in acting, Bacon accepted the role and appeared in four episodes alongside her mother, who played the role of Chief Johnson. Bacon portrayed the character Skye Miller in the TV series 13 Reasons Why.
Sosie Bacon was born on March 15, 1992, to married actors Kevin Bacon and Kyra Sedgwick. Sedgwick gave birth shortly after filming Miss Rose White, and named her newborn after the movie's art director, Sosie Hublitz. Despite her parents' successful acting careers, Bacon was provided with a "fairly ordinary" upbringing, according to producer James Duff, and her parents were determined that she not follow them into acting. During the filming of The Closer, Bacon's mother would spend half of the year in Los Angeles, while Bacon would stay in Manhattan with her father and brother. Sedgwick credited this as leading to a closer bond between Bacon and her father.
Sosie Bacon on IMDb
North America is a continent entirely within the Northern Hemisphere and almost all within the Western Hemisphere. It can also be described as a northern subcontinent of the Americas. It is bordered to the north by the Arctic Ocean, to the east by the Atlantic Ocean, to the southeast by South America and the Caribbean Sea, and to the west and south by the Pacific Ocean. North America covers an area of about 24,709,000 square kilometers (9,540,000 square miles), about 16.5% of the Earth's land area and about 4.8% of its total surface. North America is the third-largest continent by area, following Asia and Africa, and the fourth by population after Asia, Africa, and Europe. In 2013, its population was estimated at nearly 579 million people in 23 independent states, or about 7.5% of the world's population, if nearby islands (most notably around the Caribbean) are included. North America was reached by its first human populations during the last glacial period, via crossing the Bering land bridge approximately 40,000 to 17,000 years ago. The so-called Paleo-Indian period is taken to have lasted until about 10,000 years ago (the beginning of the Archaic or Meso-Indian period). The classic stage spans roughly the 6th to 13th centuries. The pre-Columbian era ended in 1492, with the beginning of the transatlantic migrations of European settlers during the Age of Discovery and the early modern period. Present-day cultural and ethnic patterns reflect interactions between European colonists, indigenous peoples, African slaves, immigrants, and the descendants of these groups. Owing to Europe's colonization of the Americas, most North Americans speak European languages such as English, Spanish or French, and their states' cultures commonly reflect Western traditions.
The Americas are usually accepted as having been named after the Italian explorer Amerigo Vespucci by the German cartographers Martin Waldseemüller and Matthias Ringmann. Vespucci, who explored South America between 1497 and 1502, was the first European to suggest that the Americas were not the East Indies, but a different landmass previously unknown by Europeans. In 1507, Waldseemüller produced a world map, in which he placed the word "America" on the continent of South America, in the middle of what is today Brazil. He explained the rationale for the name in the accompanying book Cosmographiae Introductio: ... ab Americo inventore ... quasi Americi terram sive Americam (from Americus the discoverer ... as if it were the land of Americus, thus America). For Waldseemüller, no one should object to the naming of the land after its discoverer. He used the Latinized version of Vespucci's name (Americus Vespucius), but in its feminine form "America", following the examples of "Europa", "Asia" and "Africa". Later, other mapmakers extended the name America to the northern continent. In 1538, Gerard Mercator used America on his map of the world for all the Western Hemisphere.Some argue that because the convention is to use the surname for naming discoveries (except in the case of royalty), the derivation from "Amerigo Vespucci" could be put in question. In 1874, Thomas Belt proposed a derivation from the Amerrique mountains of Central America; the next year, Jules Marcou suggested that the name of the mountain range stemmed from indigenous American languages. Marcou corresponded with Augustus Le Plongeon, who wrote: "The name AMERICA or AMERRIQUE in the Mayan language means, a country of perpetually strong wind, or the Land of the Wind, and ... the [suffixes] can mean ... a spirit that breathes, life itself."Mercator on his map called North America "America or New India" (America sive India Nova).
The United Nations formally recognizes "North America" as comprising three areas: Northern America, Central America, and The Caribbean. This has been formally defined by the UN Statistics Division."Northern America", as a term distinct from "North America", excludes Central America, which itself may or may not include Mexico (see Central America § Different definitions). In the limited context of the North American Free Trade Agreement, the term covers Canada, the United States, and Mexico, which are the three signatories of that treaty. France, Italy, Portugal, Spain, Romania, Greece, and the countries of Latin America use a six-continent model, with the Americas viewed as a single continent and North America designating a subcontinent comprising Canada, the United States, and Mexico, and often Greenland, Saint Pierre et Miquelon, and Bermuda.North America has been historically referred to by other names. Spanish North America (New Spain) was often referred to as Northern America, and this was the first official name given to Mexico.
Geographically the North American continent has many regions and subregions. These include cultural, economic, and geographic regions. Economic regions included those formed by trade blocs, such as the North American Trade Agreement bloc and Central American Trade Agreement. Linguistically and culturally, the continent could be divided into Anglo-America and Latin America. Anglo-America includes most of Northern America, Belize, and Caribbean islands with English-speaking populations (though sub-national entities, such as Louisiana and Quebec, have large Francophone populations; in Quebec, French is the sole official language). The southern North American continent is composed of two regions. These are Central America and the Caribbean. The north of the continent maintains recognized regions as well. In contrast to the common definition of "North America", which encompasses the whole continent, the term "North America" is sometimes used to refer only to Mexico, Canada, the United States, and Greenland.The term Northern America refers to the northernmost countries and territories of North America: the United States, Bermuda, St. Pierre and Miquelon, Canada and Greenland. Although the term does not refer to a unified region, Middle America—not to be confused with the Midwestern United States—groups the regions of Mexico, Central America, and the Caribbean.The largest countries of the continent, Canada and the United States, also contain well-defined and recognized regions. In the case of Canada these are (from east to west) Atlantic Canada, Central Canada, Canadian Prairies, the British Columbia Coast, and Northern Canada. These regions also contain many subregions. In the case of the United States – and in accordance with the US Census Bureau definitions – these regions are: New England, Mid-Atlantic, South Atlantic States, East North Central States, West North Central States, East South Central States, West South Central States, Mountain States, and Pacific States. Regions shared between both nations included the Great Lakes Region. Megalopolises have formed between both nations in the case of the Pacific Northwest and the Great Lakes Megaregion.
North America occupies the northern portion of the landmass generally referred to as the New World, the Western Hemisphere, the Americas, or simply America (which, in many countries is considered as a single continent with North America a subcontinent). North America is the third-largest continent by area, following Asia and Africa. North America's only land connection to South America is at the Isthmus of Darian/Isthmus of Panama. The continent is delimited on the southeast by most geographers at the Darién watershed along the Colombia-Panama border, placing almost all of Panama within North America. Alternatively, some geologists physiographically locate its southern limit at the Isthmus of Tehuantepec, Mexico, with Central America extending southeastward to South America from this point. The Caribbean islands, or West Indies, are considered part of North America. The continental coastline is long and irregular. The Gulf of Mexico is the largest body of water indenting the continent, followed by Hudson Bay. Others include the Gulf of Saint Lawrence and the Gulf of California. Before the Central American isthmus formed, the region had been underwater. The islands of the West Indies delineate a submerged former land bridge, which had connected North and South America via what are now Florida and Venezuela. There are numerous islands off the continent's coasts; principally, the Arctic Archipelago, the Bahamas, Turks & Caicos, the Greater and Lesser Antilles, the Aleutian Islands (some of which are in the Eastern Hemisphere proper), the Alexander Archipelago, the many thousand islands of the British Columbia Coast, and Newfoundland. Greenland, a self-governing Danish island, and the world's largest, is on the same tectonic plate (the North American Plate) and is part of North America geographically. In a geologic sense, Bermuda is not part of the Americas, but an oceanic island which was formed on the fissure of the Mid-Atlantic Ridge over 100 million years ago. The nearest landmass to it is Cape Hatteras, North Carolina. However, Bermuda is often thought of as part of North America, especially given its historical, political and cultural ties to Virginia and other parts of the continent. The vast majority of North America is on the North American Plate. Parts of western Mexico, including Baja California, and of California, including the cities of San Diego, Los Angeles, and Santa Cruz, lie on the eastern edge of the Pacific Plate, with the two plates meeting along the San Andreas fault. The southernmost portion of the continent and much of the West Indies lie on the Caribbean Plate, whereas the Juan de Fuca and Cocos plates border the North American Plate on its western frontier. The continent can be divided into four great regions (each of which contains many subregions): the Great Plains stretching from the Gulf of Mexico to the Canadian Arctic; the geologically young, mountainous west, including the Rocky Mountains, the Great Basin, California and Alaska; the raised but relatively flat plateau of the Canadian Shield in the northeast; and the varied eastern region, which includes the Appalachian Mountains, the coastal plain along the Atlantic seaboard, and the Florida peninsula. Mexico, with its long plateaus and cordilleras, falls largely in the western region, although the eastern coastal plain does extend south along the Gulf. The western mountains are split in the middle into the main range of the Rockies and the coast ranges in California, Oregon, Washington, and British Columbia, with the Great Basin—a lower area containing smaller ranges and low-lying deserts—in between. The highest peak is Denali in Alaska. The United States Geographical Survey (USGS) states that the geographic center of North America is "6 miles [10 km] west of Balta, Pierce County, North Dakota" at about 48°10′N 100°10′W, about 24 kilometres (15 mi) from Rugby, North Dakota. The USGS further states that "No marked or monumented point has been established by any government agency as the geographic center of either the 50 States, the conterminous United States, or the North American continent." Nonetheless, there is a 4.6-metre (15 ft) field stone obelisk in Rugby claiming to mark the center. The North American continental pole of inaccessibility is located 1,650 km (1,030 mi) from the nearest coastline, between Allen and Kyle, South Dakota at 43.36°N 101.97°W﻿ / 43.36; -101.97﻿ (Pole of Inaccessibility North America).
Laurentia is an ancient craton which forms the geologic core of North America; it formed between 1.5 and 1.0 billion years ago during the Proterozoic eon. The Canadian Shield is the largest exposure of this craton. From the Late Paleozoic to Early Mesozoic eras, North America was joined with the other modern-day continents as part of the supercontinent Pangaea, with Eurasia to its east. One of the results of the formation of Pangaea was the Appalachian Mountains, which formed some 480 million years ago, making it among the oldest mountain ranges in the world. When Pangaea began to rift around 200 million years ago, North America became part of Laurasia, before it separated from Eurasia as its own continent during the mid-Cretaceous period. The Rockies and other western mountain ranges began forming around this time from a period of mountain building called the Laramide orogeny, between 80 and 55 million years ago. The formation of the Isthmus of Panama that connected the continent to South America arguably occurred approximately 12 to 15 million years ago, and the Great Lakes (as well as many other northern freshwater lakes and rivers) were carved by receding glaciers about 10,000 years ago. North America is the source of much of what humanity knows about geologic time periods. The geographic area that would later become the United States has been the source of more varieties of dinosaurs than any other modern country. According to paleontologist Peter Dodson, this is primarily due to stratigraphy, climate and geography, human resources, and history. Much of the Mesozoic Era is represented by exposed outcrops in the many arid regions of the continent. The most significant Late Jurassic dinosaur-bearing fossil deposit in North America is the Morrison Formation of the western United States.
Geologically, Canada is one of the oldest regions in the world, with more than half of the region consisting of precambrian rocks that have been above sea level since the beginning of the Palaeozoic era. Canada's mineral resources are diverse and extensive. Across the Canadian Shield and in the north there are large iron, nickel, zinc, copper, gold, lead, molybdenum, and uranium reserves. Large diamond concentrations have been recently developed in the Arctic, making Canada one of the world's largest producers. Throughout the Shield there are many mining towns extracting these minerals. The largest, and best known, is Sudbury, Ontario. Sudbury is an exception to the normal process of forming minerals in the Shield since there is significant evidence that the Sudbury Basin is an ancient meteorite impact crater. The nearby, but less known Temagami Magnetic Anomaly has striking similarities to the Sudbury Basin. Its magnetic anomalies are very similar to the Sudbury Basin, and so it could be a second metal-rich impact crater. The Shield is also covered by vast boreal forests that support an important logging industry.
The lower 48 US states can be divided into roughly five physiographic provinces: The American cordillera The Canadian Shield Northern portion of the upper midwestern United States. The stable platform The coastal plain The Appalachian orogenic beltThe geology of Alaska is typical of that of the cordillera, while the major islands of Hawaii consist of Neogene volcanics erupted over a hot spot.
Central America is geologically active with volcanic eruptions and earthquakes occurring from time to time. In 1976 Guatemala was hit by a major earthquake, killing 23,000 people; Managua, the capital of Nicaragua, was devastated by earthquakes in 1931 and 1972, the last one killing about 5,000 people; three earthquakes devastated El Salvador, one in 1986 and two in 2001; one earthquake devastated northern and central Costa Rica in 2009, killing at least 34 people; in Honduras a powerful earthquake killed seven people in 2009. Volcanic eruptions are common in the region. In 1968 the Arenal Volcano, in Costa Rica, erupted and killed 87 people. Fertile soils from weathered volcanic lavas have made it possible to sustain dense populations in the agriculturally productive highland areas. Central America has many mountain ranges; the longest are the Sierra Madre de Chiapas, the Cordillera Isabelia, and the Cordillera de Talamanca. Between the mountain ranges lie fertile valleys that are suitable for the people; in fact, most of the population of Honduras, Costa Rica, and Guatemala live in valleys. Valleys are also suitable for the production of coffee, beans, and other crops.
North America is a very large continent which surpasses the Arctic Circle, and the Tropic of Cancer. Greenland, along with the Canadian Shield, is tundra with average temperatures ranging from 10 to 20 °C (50 to 68 °F), but central Greenland is composed of a very large ice sheet. This tundra radiates throughout Canada, but its border ends near the Rocky Mountains (but still contains Alaska) and at the end of the Canadian Shield, near the Great Lakes. Climate west of the Cascades is described as being a temperate weather with average precipitation 20 inches (510 mm). Climate in coastal California is described to be Mediterranean, with average temperatures in cities like San Francisco ranging from 57 to 70 °F (14 to 21 °C) over the course of the year.Stretching from the East Coast to eastern North Dakota, and stretching down to Kansas, is the continental-humid climate featuring intense seasons, with a large amount of annual precipitation, with places like New York City averaging 50 inches (1,300 mm). Starting at the southern border of the continental-humid climate and stretching to the Gulf of Mexico (whilst encompassing the eastern half of Texas) is the subtropical climate. This area has the wettest cities in the contiguous U.S. with annual precipitation reaching 67 inches (1,700 mm) in Mobile, Alabama. Stretching from the borders of the continental humid and subtropical climates, and going west to the Cascades Sierra Nevada, south to the southern tip of durango, north to the border with tundra climate, the steppe/desert climate is the driest climate in the U.S. Highland climates cut from north to south of the continent, where subtropical or temperate climates occur just below the tropics, as in central Mexico and Guatemala. Tropical climates appear in the island regions and in the subcontinent's bottleneck. Usually of the savannah type, with rains and high temperatures constants the whole year. Found in countries and states bathed by the Caribbean Sea or to south of the Gulf of Mexico and Pacific Ocean.
Notable North American fauna include the bison, black bear, prairie dog, turkey, pronghorn, raccoon, coyote and monarch butterfly. Notable plants that were domesticated in North America include tobacco, maize, squash, tomato, sunflower, blueberry, avocado, cotton, chile pepper and vanilla.
The indigenous peoples of the Americas have many creation myths by which they assert that they have been present on the land since its creation, but there is no evidence that humans evolved there. The specifics of the initial settlement of the Americas by ancient Asians are subject to ongoing research and discussion. The traditional theory has been that hunters entered the Beringia land bridge between eastern Siberia and present-day Alaska from 27,000 to 14,000 years ago. A growing viewpoint is that the first American inhabitants sailed from Beringia some 13,000 years ago, with widespread habitation of the Americas during the end of the Last Glacial Period, in what is known as the Late Glacial Maximum, around 12,500 years ago. The oldest petroglyphs in North America date from 15,000 to 10,000 years before present. Genetic research and anthropology indicate additional waves of migration from Asia via the Bering Strait during the Early-Middle Holocene.Before contact with Europeans, the natives of North America were divided into many different polities, from small bands of a few families to large empires. They lived in several "culture areas", which roughly correspond to geographic and biological zones and give a good indication of the main way of life of the people who lived there (e.g., the bison hunters of the Great Plains, or the farmers of Mesoamerica). Native groups can also be classified by their language family (e.g., Athapascan or Uto-Aztecan). Peoples with similar languages did not always share the same material culture, nor were they always allies. Anthropologists think that the Inuit people of the high Arctic came to North America much later than other native groups, as evidenced by the disappearance of Dorset culture artifacts from the archaeological record, and their replacement by the Thule people. During the thousands of years of native habitation on the continent, cultures changed and shifted. One of the oldest yet discovered is the Clovis culture (c. 9550–9050 BCE) in modern New Mexico. Later groups include the Mississippian culture and related Mound building cultures, found in the Mississippi river valley and the Pueblo culture of what is now the Four Corners. The more southern cultural groups of North America were responsible for the domestication of many common crops now used around the world, such as tomatoes, squash, and maize. As a result of the development of agriculture in the south, many other cultural advances were made there. The Mayans developed a writing system, built huge pyramids and temples, had a complex calendar, and developed the concept of zero around 400 CE.The first recorded European references to North America are in Norse sagas where it is referred to as Vinland. The earliest verifiable instance of pre-Columbian trans-oceanic contact by any European culture with the North America mainland has been dated to around 1000 CE. The site, situated at the northernmost extent of the island named Newfoundland, has provided unmistakable evidence of Norse settlement. Norse explorer Leif Erikson (c. 970–1020 CE) is thought to have visited the area. Erikson was the first European to make landfall on the continent (excluding Greenland). The Mayan culture was still present in southern Mexico and Guatemala when the Spanish conquistadors arrived, but political dominance in the area had shifted to the Aztec Empire, whose capital city Tenochtitlan was located further north in the Valley of Mexico. The Aztecs were conquered in 1521 by Hernán Cortés.
During the Age of Discovery, Europeans explored and staked claims to various parts of North America. Upon their arrival in the "New World", the Native American population declined substantially, because of violent conflicts with the invaders and the introduction of European diseases to which the Native Americans lacked immunity. Native culture changed drastically and their affiliation with political and cultural groups also changed. Several linguistic groups died out, and others changed quite quickly. In 1513, Juan Ponce de León, who had accompanied Columbus's second voyage, visited and named La Florida. As the colonial period unfolded, Britain, Spain, and France took over extensive territories in North America. In the late 18th and early 19th century, independence movements sprung up across the continent, leading to the founding of the modern countries in the area. The 13 British Colonies on the North Atlantic coast declared independence in 1776, becoming the United States of America. Canada was formed from the unification of northern territories controlled by Britain and France. New Spain, a territory that stretched from the modern-day southern US to Central America, declared independence in 1810, becoming the First Mexican Empire. In 1823 the former Captaincy General of Guatemala, then part of the Mexican Empire, became the first independent state in Central America, officially changing its name to the United Provinces of Central America. Over three decades of work on the Panama Canal led to the connection of Atlantic and Pacific waters in 1913, physically making North America a separate continent.
Economically, Canada and the United States are the wealthiest and most developed nations in the continent, followed by Mexico, a newly industrialized country. The countries of Central America and the Caribbean are at various levels of economic and human development. For example, small Caribbean island-nations, such as Barbados, Trinidad and Tobago, and Antigua and Barbuda, have a higher GDP (PPP) per capita than Mexico due to their smaller populations. Panama and Costa Rica have a significantly higher Human Development Index and GDP than the rest of the Central American nations. Additionally, despite Greenland's vast resources in oil and minerals, much of them remain untapped, and the island is economically dependent on fishing, tourism, and subsidies from Denmark. Nevertheless, the island is highly developed.Demographically, North America is ethnically diverse. Its three main groups are Caucasians, Mestizos and Blacks. There is a significant minority of Indigenous Americans and Asians among other less numerous groups.
The dominant languages in North America are English, Spanish, and French. Danish is prevalent in Greenland alongside Greenlandic, and Dutch is spoken side by side local languages in the Dutch Caribbean. The term Anglo-America is used to refer to the anglophone countries of the Americas: namely Canada (where English and French are co-official) and the United States, but also sometimes Belize and parts of the tropics, especially the Commonwealth Caribbean. Latin America refers to the other areas of the Americas (generally south of the United States) where the Romance languages, derived from Latin, of Spanish and Portuguese (but French speaking countries are not usually included) predominate: the other republics of Central America (but not always Belize), part of the Caribbean (not the Dutch-, English-, or French-speaking areas), Mexico, and most of South America (except Guyana, Suriname, French Guiana (France), and the Falkland Islands (UK)). The French language has historically played a significant role in North America and now retains a distinctive presence in some regions. Canada is officially bilingual. French is the official language of the Province of Quebec, where 95% of the people speak it as either their first or second language, and it is co-official with English in the Province of New Brunswick. Other French-speaking locales include the Province of Ontario (the official language is English, but there are an estimated 600,000 Franco-Ontarians), the Province of Manitoba (co-official as de jure with English), the French West Indies and Saint-Pierre et Miquelon, as well as the US state of Louisiana, where French is also an official language. Haiti is included with this group based on historical association but Haitians speak both Creole and French. Similarly, French and French Antillean Creole is spoken in Saint Lucia and the Commonwealth of Dominica alongside English. A significant number of Indigenous languages are spoken in North America, with 372,000 people in the United States speaking an indigenous language at home, about 225,000 in Canada and roughly 6 million in Mexico. In the United States and Canada, there are approximately 150 surviving indigenous languages of the 300 spoken prior to European contact.
Christianity is the largest religion in the United States, Canada and Mexico. According to a 2012 Pew Research Center survey, 77% of the population considered themselves Christians. Christianity also is the predominant religion in the 23 dependent territories in North America. The United States has the largest Christian population in the world, with nearly 247 million Christians (70%), although other countries have higher percentages of Christians among their populations. Mexico has the world's second largest number of Catholics, surpassed only by Brazil. A 2015 study estimates about 493,000 Christian believers from a Muslim background in North America, most of them belonging to some form of Protestantism.According to the same study religiously unaffiliated (include agnostic and atheist) make up about 17% of the population of Canada and the United States. No religion make up about 24% of the United States population, and 24% of Canada total population.Canada, the United States and Mexico host communities of both Jews (6 million or about 1.8%), Buddhists (3.8 million or 1.1%) and Muslims (3.4 million or 1.0%). The biggest number of Jewish individuals can be found in the United States (5.4 million), Canada (375,000) and Mexico (67,476). The United States host the largest Muslim population in North America with 2.7 million or 0.9%, While Canada host about one million Muslim or 3.2% of the population. While in Mexico there were 3,700 Muslims in the country. In 2012, U-T San Diego estimated U.S. practitioners of Buddhism at 1.2 million people, of whom 40% are living in Southern California.The predominant religion in Central America is Christianity (96%). Beginning with the Spanish colonization of Central America in the 16th century, Roman Catholicism became the most popular religion in the region until the first half of the 20th century. Since the 1960s, there has been an increase in other Christian groups, particularly Protestantism, as well as other religious organizations, and individuals identifying themselves as having no religion. Also Christianity is the predominant religion in the Caribbean (85%). Other religious groups in the region are Hinduism, Islam, Rastafari (in Jamaica), and Afro-American religions such as Santería and Vodou.
North America is the fourth most populous continent after Asia, Africa, and Europe. Its most populous country is the United States with 329.7 million persons. The second largest country is Mexico with a population of 112.3 million. Canada is the third most populous country with 37.0 million. The majority of Caribbean island-nations have national populations under a million, though Cuba, Dominican Republic, Haiti, Puerto Rico (a territory of the United States), Jamaica, and Trinidad and Tobago each have populations higher than a million. Greenland has a small population of 55,984 for its massive size (2,166,000 km2 or 836,300 mi2), and therefore, it has the world's lowest population density at 0.026 pop./km2 (0.067 pop./mi2).While the United States, Canada, and Mexico maintain the largest populations, large city populations are not restricted to those nations. There are also large cities in the Caribbean. The largest cities in North America, by far, are Mexico City and New York. These cities are the only cities on the continent to exceed eight million, and two of three in the Americas. Next in size are Los Angeles, Toronto, Chicago, Havana, Santo Domingo, and Montreal. Cities in the sun belt regions of the United States, such as those in Southern California and Houston, Phoenix, Miami, Atlanta, and Las Vegas, are experiencing rapid growth. These causes included warm temperatures, retirement of Baby Boomers, large industry, and the influx of immigrants. Cities near the United States border, particularly in Mexico, are also experiencing large amounts of growth. Most notable is Tijuana, a city bordering San Diego that receives immigrants from all over Latin America and parts of Europe and Asia. Yet as cities grow in these warmer regions of North America, they are increasingly forced to deal with the major issue of water shortages.Eight of the top ten metropolitan areas are located in the United States. These metropolitan areas all have a population of above 5.5 million and include the New York City metropolitan area, Los Angeles metropolitan area, Chicago metropolitan area, and the Dallas–Fort Worth metroplex. Whilst the majority of the largest metropolitan areas are within the United States, Mexico is host to the largest metropolitan area by population in North America: Greater Mexico City. Canada also breaks into the top ten largest metropolitan areas with the Toronto metropolitan area having six million people. The proximity of cities to each other on the Canada–United States border and Mexico–United States border has led to the rise of international metropolitan areas. These urban agglomerations are observed at their largest and most productive in Detroit–Windsor and San Diego–Tijuana and experience large commercial, economic, and cultural activity. The metropolitan areas are responsible for millions of dollars of trade dependent on international freight. In Detroit-Windsor the Border Transportation Partnership study in 2004 concluded US$13 billion was dependent on the Detroit–Windsor international border crossing while in San Diego-Tijuana freight at the Otay Mesa Port of Entry was valued at US$20 billion.North America has also been witness to the growth of megapolitan areas. In the United States exists eleven megaregions that transcend international borders and comprise Canadian and Mexican metropolitan regions. These are the Arizona Sun Corridor, Cascadia, Florida, Front Range, Great Lakes Megaregion, Gulf Coast Megaregion, Northeast, Northern California, Piedmont Atlantic, Southern California, and the Texas Triangle. Canada and Mexico are also the home of megaregions. These include the Quebec City – Windsor Corridor, Golden Horseshoe – both of which are considered part of the Great Lakes Megaregion – and megalopolis of Central Mexico. Traditionally the largest megaregion has been considered the Boston-Washington, DC Corridor, or the Northeast, as the region is one massive contiguous area. Yet megaregion criterion have allowed the Great Lakes Megalopolis to maintain status as the most populated region, being home to 53,768,125 people in 2000.The top ten largest North American metropolitan areas by population as of 2013, based on national census numbers from the United States and census estimates from Canada and Mexico. †2011 Census figures.
North America's GDP per capita was evaluated in October 2016 by the International Monetary Fund (IMF) to be $41,830, making it the richest continent in the world, followed by Oceania.Canada, Mexico, and the United States have significant and multifaceted economic systems. The United States has the largest economy of all three countries and in the world. In 2016, the U.S. had an estimated per capita gross domestic product (PPP) of $57,466 according to the World Bank, and is the most technologically developed economy of the three. The United States' services sector comprises 77% of the country's GDP (estimated in 2010), industry comprises 22% and agriculture comprises 1.2%. The U.S. economy is also the fastest growing economy in North America and the Americas as a whole, with the highest GDP per capita in the Americas as well. Canada shows significant growth in the sectors of services, mining and manufacturing. Canada's per capita GDP (PPP) was estimated at $44,656 and it had the 11th largest GDP (nominal) in 2014. Canada's services sector comprises 78% of the country's GDP (estimated in 2010), industry comprises 20% and agriculture comprises 2%. Mexico has a per capita GDP (PPP) of $16,111 and as of 2014 is the 15th largest GDP (nominal) in the world. Being a newly industrialized country, Mexico maintains both modern and outdated industrial and agricultural facilities and operations. Its main sources of income are oil, industrial exports, manufactured goods, electronics, heavy industry, automobiles, construction, food, banking and financial services.The North American economy is well defined and structured in three main economic areas. These areas are the North American Free Trade Agreement (NAFTA), Caribbean Community and Common Market (CARICOM), and the Central American Common Market (CACM). Of these trade blocs, the United States takes part in two. In addition to the larger trade blocs there is the Canada-Costa Rica Free Trade Agreement among numerous other free trade relations, often between the larger, more developed countries and Central American and Caribbean countries. The North America Free Trade Agreement (NAFTA) forms one of the four largest trade blocs in the world. Its implementation in 1994 was designed for economic homogenization with hopes of eliminating barriers of trade and foreign investment between Canada, the United States and Mexico. While Canada and the United States already conducted the largest bilateral trade relationship – and to present day still do – in the world and Canada–United States trade relations already allowed trade without national taxes and tariffs, NAFTA allowed Mexico to experience a similar duty-free trade. The free trade agreement allowed for the elimination of tariffs that had previously been in place on United States-Mexico trade. Trade volume has steadily increased annually and in 2010, surface trade between the three NAFTA nations reached an all-time historical increase of 24.3% or US$791 billion. The NAFTA trade bloc GDP (PPP) is the world's largest with US$17.617 trillion. This is in part attributed to the fact that the economy of the United States is the world's largest national economy; the country had a nominal GDP of approximately $14.7 trillion in 2010. The countries of NAFTA are also some of each other's largest trade partners. The United States is the largest trade partner of Canada and Mexico; while Canada and Mexico are each other's third largest trade partners. The Caribbean trade bloc – CARICOM – came into agreement in 1973 when it was signed by 15 Caribbean nations. As of 2000, CARICOM trade volume was US$96 billion. CARICOM also allowed for the creation of a common passport for associated nations. In the past decade the trade bloc focused largely on Free Trade Agreements and under the CARICOM Office of Trade Negotiations (OTN) free trade agreements have been signed into effect. Integration of Central American economies occurred under the signing of the Central American Common Market agreement in 1961; this was the first attempt to engage the nations of this area into stronger financial cooperation. Recent implementation of the Central American Free Trade Agreement (CAFTA) has left the future of the CACM unclear. The Central American Free Trade Agreement was signed by five Central American countries, the Dominican Republic, and the United States. The focal point of CAFTA is to create a free trade area similar to that of NAFTA. In addition to the United States, Canada also has relations in Central American trade blocs. Currently under proposal, the Canada – Central American Free Trade Agreement (CA4) would operate much the same as CAFTA with the United States does. These nations also take part in inter-continental trade blocs. Mexico takes a part in the G3 Free Trade Agreement with Colombia and Venezuela and has a trade agreement with the EU. The United States has proposed and maintained trade agreements under the Transatlantic Free Trade Area between itself and the European Union; the US-Middle East Free Trade Area between numerous Middle Eastern nations and itself; and the Trans-Pacific Strategic Economic Partnership between Southeast Asian nations, Australia, and New Zealand.
The Pan-American Highway route in the Americas is the portion of a network of roads nearly 48,000 km (30,000 mi) in length which travels through the mainland nations. No definitive length of the Pan-American Highway exists because the US and Canadian governments have never officially defined any specific routes as being part of the Pan-American Highway, and Mexico officially has many branches connecting to the US border. However, the total length of the portion from Mexico to the northern extremity of the highway is roughly 26,000 km (16,000 mi). The First Transcontinental Railroad in the United States was built in the 1860s, linking the railroad network of the eastern US with California on the Pacific coast. Finished on 10 May 1869 at the famous golden spike event at Promontory Summit, Utah, it created a nationwide mechanized transportation network that revolutionized the population and economy of the American West, catalyzing the transition from the wagon trains of previous decades to a modern transportation system. Although an accomplishment, it achieved the status of first transcontinental railroad by connecting myriad eastern US railroads to the Pacific and was not the largest single railroad system in the world. The Canadian Grand Trunk Railway (GTR) had, by 1867, already accumulated more than 2,055 km (1,277 mi) of track by connecting Ontario with the Canadian Atlantic provinces west as far as Port Huron, Michigan, through Sarnia, Ontario.
A shared telephone system known as the North American Numbering Plan (NANP) is an integrated telephone numbering plan of 24 countries and territories: the United States and its territories, Canada, Bermuda, and 17 Caribbean nations.
Canada and the United States were both former British colonies. There is frequent cultural interplay between the United States and English-speaking Canada. Greenland has experienced many immigration waves from Northern Canada, e.g. the Thule People. Therefore, Greenland shares some cultural ties with the indigenous people of Canada. Greenland is also considered Nordic and has strong Danish ties due to centuries of colonization by Denmark.Spanish-speaking North America shares a common past as former Spanish colonies. In Mexico and the Central American countries where civilizations like the Maya developed, indigenous people preserve traditions across modern boundaries. Central American and Spanish-speaking Caribbean nations have historically had more in common due to geographical proximity. Northern Mexico, particularly in the cities of Monterrey, Tijuana, Ciudad Juárez, and Mexicali, is strongly influenced by the culture and way of life of the United States. Of the aforementioned cities, Monterrey has been regarded as the most Americanized city in Mexico. Immigration to the United States and Canada remains a significant attribute of many nations close to the southern border of the US. The Anglophone Caribbean states have witnessed the decline of the British Empire and its influence on the region, and its replacement by the economic influence of Northern America in the Anglophone Caribbean. This is partly due to the relatively small populations of the English-speaking Caribbean countries, and also because many of them now have more people living abroad than those remaining at home. Northern Mexico, the Western United States and Alberta, Canada share a cowboy culture.
Canada, Mexico and the US submitted a joint bid to host the 2026 FIFA World Cup. The following table shows the most prominent sports leagues in North America, in order of average revenue.
Outline of North America Flags of North America List of cities in North America Table manners in North America Turtle Island (Native American folklore) – Name for North America among Native Americans
Footnotes Citations
Houghton Mifflin Company, "North America" Interactive SVG version of Non-Native American Nations Control over N America 1750–2008 animation
The Southern Ocean, also known as the Antarctic Ocean or the Austral Ocean, comprises the southernmost waters of the World Ocean, generally taken to be south of 60° S latitude and encircling Antarctica. As such, it is regarded as the second-smallest of the five principal oceanic divisions: smaller than the Pacific, Atlantic, and Indian Oceans but larger than the Arctic Ocean. This oceanic zone is where cold, northward flowing waters from the Antarctic mix with warmer subantarctic waters. By way of his voyages in the 1770s, James Cook proved that waters encompassed the southern latitudes of the globe. Since then, geographers have disagreed on the Southern Ocean's northern boundary or even existence, considering the waters as various parts of the Pacific, Atlantic, and Indian Oceans, instead. However, according to Commodore John Leech of the International Hydrographic Organization (IHO), recent oceanographic research has discovered the importance of Southern Circulation, and the term Southern Ocean has been used to define the body of water which lies south of the northern limit of that circulation. This remains the current official policy of the IHO, since a 2000 revision of its definitions including the Southern Ocean as the waters south of the 60th parallel has not yet been adopted. Others regard the seasonally-fluctuating Antarctic Convergence as the natural boundary.The maximum depth of the Southern Ocean, using the definition that it lies south of 60th parallel, was surveyed by the Five Deeps Expedition in early February 2019. The expedition's multibeam sonar team identified the deepest point at 60° 28' 46"S, 025° 32' 32"W, with a depth of 7,434 metres (24,390 ft). The expedition leader and chief submersible pilot Victor Vescovo, has proposed naming this deepest point in the Southern Ocean the "Factorian Deep", based on the name of the manned submersible DSV Limiting Factor, in which he successfully visited the bottom for the first time on February 3, 2019.
Borders and names for oceans and seas were internationally agreed when the International Hydrographic Bureau, the precursor to the IHO, convened the First International Conference on 24 July 1919. The IHO then published these in its Limits of Oceans and Seas, the first edition being 1928. Since the first edition, the limits of the Southern Ocean have moved progressively southwards; since 1953, it has been omitted from the official publication and left to local hydrographic offices to determine their own limits. The IHO included the ocean and its definition as the waters south of the 60th parallel south in its 2000 revisions, but this has not been formally adopted, due to continuing impasses about some of the content, such as the naming dispute over the Sea of Japan. The 2000 IHO definition, however, was circulated in a draft edition in 2002, and is used by some within the IHO and by some other organizations such as the CIA World Factbook and Merriam-Webster.The Australian Government regards the Southern Ocean as lying immediately south of Australia (see § Australian standpoint).The National Geographic Society does not recognize the ocean, depicting it in a typeface different from the other world oceans; instead, it shows the Pacific, Atlantic, and Indian Oceans extending to Antarctica on both its print and online maps. Map publishers using the term Southern Ocean on their maps include Hema Maps and GeoNova.
"Southern Ocean" is an obsolete name for the Pacific Ocean or South Pacific, coined by Vasco Núñez de Balboa, the first European to discover it, who approached it from the north. The "South Seas" is a less archaic synonym. A 1745 British Act of Parliament established a prize for discovering a Northwest Passage to "the Western and Southern Ocean of America".Authors using "Southern Ocean" to name the waters encircling the unknown southern polar regions used varying limits. James Cook's account of his second voyage implies New Caledonia borders it. Peacock's 1795 Geographical Dictionary said it lay "to the southward of America and Africa"; John Payne in 1796 used 40 degrees as the northern limit; the 1827 Edinburgh Gazetteer used 50 degrees. The Family Magazine in 1835 divided the "Great Southern Ocean" into the "Southern Ocean" and the "Antarctick [sic] Ocean" along the Antarctic Circle, with the northern limit of the Southern Ocean being lines joining Cape Horn, the Cape of Good Hope, Van Diemen's Land and the south of New Zealand.The United Kingdom's South Australia Act 1834 described the waters forming the southern limit of the new province of South Australia as "the Southern Ocean". The Colony of Victoria's Legislative Council Act 1881 delimited part of the division of Bairnsdale as "along the New South Wales boundary to the Southern ocean".
In the 1928 first edition of Limits of Oceans and Seas, the Southern Ocean was delineated by land-based limits: Antarctica to the south, and South America, Africa, Australia, and Broughton Island, New Zealand to the north. The detailed land-limits used were from Cape Horn in Chile eastwards to Cape Agulhas in Africa, then further eastwards to the southern coast of mainland Australia to Cape Leeuwin, Western Australia. From Cape Leeuwin, the limit then followed eastwards along the coast of mainland Australia to Cape Otway, Victoria, then southwards across Bass Strait to Cape Wickham, King Island, along the west coast of King Island, then the remainder of the way south across Bass Strait to Cape Grim, Tasmania. The limit then followed the west coast of Tasmania southwards to the South East Cape and then went eastwards to Broughton Island, New Zealand, before returning to Cape Horn.
The northern limits of the Southern Ocean were moved southwards in the IHO's 1937 second edition of the Limits of Oceans and Seas. From this edition, much of the ocean's northern limit ceased to abut land masses. In the second edition, the Southern Ocean then extended from Antarctica northwards to latitude 40°S between Cape Agulhas in Africa (long. 20°E) and Cape Leeuwin in Western Australia (long. 115°E), and extended to latitude 55°S between Auckland Island of New Zealand (165 or 166°E east) and Cape Horn in South America (67°W).As is discussed in more detail below, prior to the 2002 edition the limits of oceans explicitly excluded the seas lying within each of them. The Great Australian Bight was unnamed in the 1928 edition, and delineated as shown in the figure above in the 1937 edition. It therefore encompassed former Southern Ocean waters—as designated in 1928—but was technically not inside any of the three adjacent oceans by 1937. In the 2002 draft edition, the IHO have designated 'seas' as being subdivisions within 'oceans', so the Bight would have still been within the Southern Ocean in 1937 if the 2002 convention were in place then. To perform direct comparisons of current and former limits of oceans it is necessary to consider, or at least be aware of, how the 2002 change in IHO terminology for 'seas' can affect the comparison.
The Southern Ocean did not appear in the 1953 third edition of Limits of Oceans and Seas, a note in the publication read: The Antarctic or Southern Ocean has been omitted from this publication as the majority of opinions received since the issue of the 2nd Edition in 1937 are to the effect that there exists no real justification for applying the term Ocean to this body of water, the northern limits of which are difficult to lay down owing to their seasonal change. The limits of the Atlantic, Pacific and Indian Oceans have therefore been extended South to the Antarctic Continent.Hydrographic Offices who issue separate publications dealing with this area are therefore left to decide their own northern limits (Great Britain uses Latitude of 55 South.) Instead, in the IHO 1953 publication, the Atlantic, Indian and Pacific Oceans were extended southward, the Indian and Pacific Oceans (which had not previously touched pre 1953, as per the first and second editions) now abutted at the meridian of South East Cape, and the southern limits of the Great Australian Bight and the Tasman Sea were moved northwards.
The IHO readdressed the question of the Southern Ocean in a survey in 2000. Of its 68 member nations, 28 responded, and all responding members except Argentina agreed to redefine the ocean, reflecting the importance placed by oceanographers on ocean currents. The proposal for the name Southern Ocean won 18 votes, beating the alternative Antarctic Ocean. Half of the votes supported a definition of the ocean's northern limit at the 60th parallel south—with no land interruptions at this latitude—with the other 14 votes cast for other definitions, mostly the 50th parallel south, but a few for as far north as the 35th parallel south. A draft fourth edition of Limits of Oceans and Seas was circulated to IHO member states in August 2002 (sometimes referred to as the "2000 edition" as it summarized the progress to 2000). It has yet to be published due to 'areas of concern' by several countries relating to various naming issues around the world – primarily the Sea of Japan naming dispute – and there have been various changes, 60 seas were given new names, and even the name of the publication was changed. A reservation had also been lodged by Australia regarding the Southern Ocean limits. Effectively, the third edition—which did not delineate the Southern Ocean leaving delineation to local hydrographic offices—has yet to be superseded. Despite this, the fourth edition definition has partial de facto usage by many nations, scientists and organisations such as the U.S. (the CIA World Factbook uses "Southern Ocean" but none of the other new sea names within the "Southern Ocean" such as "Cosmonauts Sea") and Merriam-Webster, scientists and nations – and even by some within the IHO. Some nations' hydrographic offices have defined their own boundaries; the United Kingdom used the 55th parallel south for example. Other organisations favour more northerly limits for the Southern Ocean. For example, Encyclopædia Britannica describes the Southern Ocean as extending as far north as South America, and confers great significance on the Antarctic Convergence, yet its description of the Indian Ocean contradicts this, describing the Indian Ocean as extending south to Antarctica.Other sources, such as the National Geographic Society, show the Atlantic, Pacific and Indian Oceans as extending to Antarctica on its maps, although articles on the National Geographic web site have begun to reference the Southern Ocean.A radical shift from past IHO practices (1928–1953) was also seen in the 2002 draft edition when the IHO delineated 'seas' as being subdivisions that lay within the boundaries of 'oceans'. While the IHO are often considered the authority for such conventions, the shift brought them into line with the practices of other publications (e.g. the CIA World Fact Book) which already adopted the principle that seas are contained within oceans. This difference in practice is markedly seen for the Pacific Ocean in the adjacent figure. Thus, for example, previously the Tasman Sea between Australia and New Zealand was not regarded by the IHO as being part of the Pacific, but as of the 2002 draft edition it is. The new delineation of seas being subdivisions of oceans has avoided the need to interrupt the northern boundary of the Southern Ocean where intersected by Drake Passage which includes all of the waters from South America to the Antarctic coast, nor interrupt it for the Scotia Sea, which also extends below the 60th parallel south. The new delineation of seas has also meant that the long-time named seas around Antarctica, excluded from the 1953 edition (the 1953 map did not even extend that far south), are 'automatically' part of the Southern Ocean.
In Australia, cartographical authorities define the Southern Ocean as including the entire body of water between Antarctica and the south coasts of Australia and New Zealand, and up to 60°S elsewhere. Coastal maps of Tasmania and South Australia label the sea areas as Southern Ocean and Cape Leeuwin in Western Australia is described as the point where the Indian and Southern Oceans meet.
Exploration of the Southern Ocean was inspired by a belief in the existence of a Terra Australis – a vast continent in the far south of the globe to "balance" the northern lands of Eurasia and North Africa – which had existed since the times of Ptolemy. The doubling of the Cape of Good Hope in 1487 by Bartolomeu Dias first brought explorers within touch of the Antarctic cold, and proved that there was an ocean separating Africa from any Antarctic land that might exist. Ferdinand Magellan, who passed through the Strait of Magellan in 1520, assumed that the islands of Tierra del Fuego to the south were an extension of this unknown southern land. In 1564, Abraham Ortelius published his first map, Typus Orbis Terrarum, an eight-leaved wall map of the world, on which he identified the Regio Patalis with Locach as a northward extension of the Terra Australis, reaching as far as New Guinea.European geographers continued to connect the coast of Tierra del Fuego with the coast of New Guinea on their globes, and allowing their imaginations to run riot in the vast unknown spaces of the south Atlantic, south Indian and Pacific oceans they sketched the outlines of the Terra Australis Incognita ("Unknown Southern Land"), a vast continent stretching in parts into the tropics. The search for this great south land was a leading motive of explorers in the 16th and the early part of the 17th centuries.The Spaniard Gabriel de Castilla, who claimed having sighted "snow-covered mountains" beyond the 64° S in 1603, is recognized as the first explorer that discovered the continent of Antarctica, although he was ignored in his time. In 1606, Pedro Fernández de Quirós took possession for the king of Spain all of the lands he had discovered in Australia del Espiritu Santo (the New Hebrides) and those he would discover "even to the Pole".Francis Drake, like Spanish explorers before him, had speculated that there might be an open channel south of Tierra del Fuego. When Willem Schouten and Jacob Le Maire discovered the southern extremity of Tierra del Fuego and named it Cape Horn in 1615, they proved that the Tierra del Fuego archipelago was of small extent and not connected to the southern land, as previously thought. Subsequently, in 1642, Abel Tasman showed that even New Holland (Australia) was separated by sea from any continuous southern continent.
The first land south of the parallel 60° south latitude was discovered by the Englishman William Smith, who sighted Livingston Island on 19 February 1819. A few months later Smith returned to explore the other islands of the South Shetlands archipelago, landed on King George Island, and claimed the new territories for Britain. In the meantime, the Spanish Navy ship San Telmo sank in September 1819 when trying to cross Cape Horn. Parts of her wreckage were found months later by sealers on the north coast of Livingston Island (South Shetlands). It is unknown if some survivor managed to be the first to set foot on these Antarctic islands. The first confirmed sighting of mainland Antarctica cannot be accurately attributed to one single person. It can, however, be narrowed down to three individuals. According to various sources, three men all sighted the ice shelf or the continent within days or months of each other: Fabian Gottlieb von Bellingshausen, a captain in the Russian Imperial Navy; Edward Bransfield, a captain in the Royal Navy; and Nathaniel Palmer, an American sealer out of Stonington, Connecticut. It is certain that the expedition, led by von Bellingshausen and Lazarev on the ships Vostok and Mirny, reached a point within 32 km (20 mi) from Princess Martha Coast and recorded the sight of an ice shelf at 69°21′28″S 2°14′50″W that became known as the Fimbul Ice Shelf. On 30 January 1820, Bransfield sighted Trinity Peninsula, the northernmost point of the Antarctic mainland, while Palmer sighted the mainland in the area south of Trinity Peninsula in November 1820. Von Bellingshausen's expedition also discovered Peter I Island and Alexander I Island, the first islands to be discovered south of the circle.
In December 1839, as part of the United States Exploring Expedition of 1838–42 conducted by the United States Navy (sometimes called "the Wilkes Expedition"), an expedition sailed from Sydney, Australia, on the sloops-of-war USS Vincennes and USS Peacock, the brig USS Porpoise, the full-rigged ship Relief, and two schooners Sea Gull and USS Flying Fish. They sailed into the Antarctic Ocean, as it was then known, and reported the discovery "of an Antarctic continent west of the Balleny Islands" on 25 January 1840. That part of Antarctica was later named "Wilkes Land", a name it maintains to this day. Explorer James Clark Ross passed through what is now known as the Ross Sea and discovered Ross Island (both of which were named for him) in 1841. He sailed along a huge wall of ice that was later named the Ross Ice Shelf. Mount Erebus and Mount Terror are named after two ships from his expedition: HMS Erebus and HMS Terror. The Imperial Trans-Antarctic Expedition of 1914, led by Ernest Shackleton, set out to cross the continent via the pole, but their ship, Endurance, was trapped and crushed by pack ice before they even landed. The expedition members survived after an epic journey on sledges over pack ice to Elephant Island. Then Shackleton and five others crossed the Southern Ocean, in an open boat called James Caird, and then trekked over South Georgia to raise the alarm at the whaling station Grytviken. In 1946, US Navy Rear Admiral Richard E. Byrd and more than 4,700 military personnel visited the Antarctic in an expedition called Operation Highjump. Reported to the public as a scientific mission, the details were kept secret and it may have actually been a training or testing mission for the military. The expedition was, in both military or scientific planning terms, put together very quickly. The group contained an unusually high amount of military equipment, including an aircraft carrier, submarines, military support ships, assault troops and military vehicles. The expedition was planned to last for eight months but was unexpectedly terminated after only two months. With the exception of some eccentric entries in Admiral Byrd's diaries, no real explanation for the early termination has ever been officially given. Captain Finn Ronne, Byrd's executive officer, returned to Antarctica with his own expedition in 1947–1948, with Navy support, three planes, and dogs. Ronne disproved the notion that the continent was divided in two and established that East and West Antarctica was one single continent, i.e. that the Weddell Sea and the Ross Sea are not connected. The expedition explored and mapped large parts of Palmer Land and the Weddell Sea coastline, and identified the Ronne Ice Shelf, named by Ronne after his wife Edith "Jackie" Ronne. Ronne covered 3,600 miles (5,790 km) by ski and dog sled – more than any other explorer in history. The Ronne Antarctic Research Expedition discovered and mapped the last unknown coastline in the world and was the first Antarctic expedition to ever include women.
The Antarctic Treaty was signed on 1 December 1959 and came into force on 23 June 1961. Among other provisions, this treaty limits military activity in the Antarctic to the support of scientific research. The first person to sail single-handed to Antarctica was the New Zealander David Henry Lewis, in 1972, in a 10-metre (30 ft) steel sloop Ice Bird. A baby, named Emilio Marcos de Palma, was born near Hope Bay on 7 January 1978, becoming the first baby born on the continent. He also was born further south than anyone in history.The MV Explorer was a cruise ship operated by the Swedish explorer Lars-Eric Lindblad. Observers point to Explorer's 1969 expeditionary cruise to Antarctica as the frontrunner for today's sea-based tourism in that region. Explorer was the first cruise ship used specifically to sail the icy waters of the Antarctic Ocean and the first to sink there when she struck an unidentified submerged object on 23 November 2007, reported to be ice, which caused a 10 by 4 inches (25 by 10 cm) gash in the hull. Explorer was abandoned in the early hours of 23 November 2007 after taking on water near the South Shetland Islands in the Southern Ocean, an area which is usually stormy but was calm at the time. Explorer was confirmed by the Chilean Navy to have sunk at approximately position: 62° 24′ South, 57° 16′ West, in roughly 600 m of water.British engineer Richard Jenkins designed an unmanned surface vehicle called a "saildrone" that completed the first autonomous circumnavigation of the Southern Ocean on 3 August 2019 after 196 days at sea.The first completely human-powered expedition on the Southern Ocean was accomplished on 25 December 2019 by a team of rowers comprising captain Fiann Paul (Iceland), first mate Colin O'Brady (US), Andrew Towne (US), Cameron Bellamy (South Africa), Jamie Douglas-Hamilton (UK) and John Petersen (US).
The Southern Ocean probably contains large, and possibly giant, oil and gas fields on the continental margin. Placer deposits, accumulation of valuable minerals such as gold, formed by gravity separation during sedimentary processes are also expected to exist in the Southern Ocean.Manganese nodules are expected to exist in the Southern Ocean. Manganese nodules are rock concretions on the sea bottom formed of concentric layers of iron and manganese hydroxides around a core. The core may be microscopically small and is sometimes completely transformed into manganese minerals by crystallization. Interest in the potential exploitation of polymetallic nodules generated a great deal of activity among prospective mining consortia in the 1960s and 1970s.The icebergs that form each year around in the Southern Ocean hold enough fresh water to meet the needs of every person on Earth for several months. For several decades there have been proposals, none yet to be feasible or successful, to tow Southern Ocean icebergs to more arid northern regions (such as Australia) where they can be harvested.
Icebergs can occur at any time of year throughout the ocean. Some may have drafts up to several hundred meters; smaller icebergs, iceberg fragments and sea-ice (generally 0.5 to 1 m thick) also pose problems for ships. The deep continental shelf has a floor of glacial deposits varying widely over short distances. Sailors know latitudes from 40 to 70 degrees south as the "Roaring Forties", "Furious Fifties" and "Shrieking Sixties" due to high winds and large waves that form as winds blow around the entire globe unimpeded by any land-mass. Icebergs, especially in May to October, make the area even more dangerous. The remoteness of the region makes sources of search and rescue scarce.
The Antarctic Circumpolar Current moves perpetually eastward – chasing and joining itself, and at 21,000 km (13,000 mi) in length – it comprises the world's longest ocean current, transporting 130 million cubic metres per second (4.6×10^9 cu ft/s) of water – 100 times the flow of all the world's rivers. Several processes operate along the coast of Antarctica to produce, in the Southern Ocean, types of water masses not produced elsewhere in the oceans of the Southern Hemisphere. One of these is the Antarctic Bottom Water, a very cold, highly saline, dense water that forms under sea ice. Associated with the Circumpolar Current is the Antarctic Convergence encircling Antarctica, where cold northward-flowing Antarctic waters meet the relatively warmer waters of the subantarctic, Antarctic waters predominantly sink beneath subantarctic waters, while associated zones of mixing and upwelling create a zone very high in nutrients. These nurture high levels of phytoplankton with associated copepods and Antarctic krill, and resultant foodchains supporting fish, whales, seals, penguins, albatrosses and a wealth of other species.The Antarctic Convergence is considered to be the best natural definition of the northern extent of the Southern Ocean.
Large-scale upwelling is found in the Southern Ocean. Strong westerly (eastward) winds blow around Antarctica, driving a significant flow of water northwards. This is actually a type of coastal upwelling. Since there are no continents in a band of open latitudes between South America and the tip of the Antarctic Peninsula, some of this water is drawn up from great depths. In many numerical models and observational syntheses, the Southern Ocean upwelling represents the primary means by which deep dense water is brought to the surface. Shallower, wind-driven upwelling is also found off the west coasts of North and South America, northwest and southwest Africa, and southwest and southeast Australia, all associated with oceanic subtropical high pressure circulations. Some models of the ocean circulation suggest that broad-scale upwelling occurs in the tropics, as pressure driven flows converge water toward the low latitudes where it is diffusively warmed from above. The required diffusion coefficients, however, appear to be larger than are observed in the real ocean. Nonetheless, some diffusive upwelling does probably occur.
The Ross Gyre and Weddell Gyre are two gyres that exist within the Southern Ocean. The gyres are located in the Ross Sea and Weddell Sea respectively, and both rotate clockwise. The gyres are formed by interactions between the Antarctic Circumpolar Current and the Antarctic Continental Shelf. Sea ice has been noted to persist in the central area of the Ross Gyre. There is some evidence that global warming has resulted in some decrease of the salinity of the waters of the Ross Gyre since the 1950s.Due to the Coriolis effect acting to the left in the Southern Hemisphere and the resulting Ekman transport away from the centres of the Weddell Gyre, these regions are very productive due to upwelling of cold, nutrient rich water.
Sea temperatures vary from about −2 to 10 °C (28 to 50 °F). Cyclonic storms travel eastward around the continent and frequently become intense because of the temperature contrast between ice and open ocean. The ocean-area from about latitude 40 south to the Antarctic Circle has the strongest average winds found anywhere on Earth. In winter the ocean freezes outward to 65 degrees south latitude in the Pacific sector and 55 degrees south latitude in the Atlantic sector, lowering surface temperatures well below 0 degrees Celsius. At some coastal points, however, persistent intense drainage winds from the interior keep the shoreline ice-free throughout the winter.
A variety of marine animals exist and rely, directly or indirectly, on the phytoplankton in the Southern Ocean. Antarctic sea life includes penguins, blue whales, orcas, colossal squids and fur seals. The emperor penguin is the only penguin that breeds during the winter in Antarctica, while the Adélie penguin breeds farther south than any other penguin. The rockhopper penguin has distinctive feathers around the eyes, giving the appearance of elaborate eyelashes. King penguins, chinstrap penguins, and gentoo penguins also breed in the Antarctic. The Antarctic fur seal was very heavily hunted in the 18th and 19th centuries for its pelt by sealers from the United States and the United Kingdom. The Weddell seal, a "true seal", is named after Sir James Weddell, commander of British sealing expeditions in the Weddell Sea. Antarctic krill, which congregates in large schools, is the keystone species of the ecosystem of the Southern Ocean, and is an important food organism for whales, seals, leopard seals, fur seals, squid, icefish, penguins, albatrosses and many other birds.The benthic communities of the seafloor are diverse and dense, with up to 155,000 animals found in 1 square metre (10.8 sq ft). As the seafloor environment is very similar all around the Antarctic, hundreds of species can be found all the way around the mainland, which is a uniquely wide distribution for such a large community. Deep-sea gigantism is common among these animals.A census of sea life carried out during the International Polar Year and which involved some 500 researchers was released in 2010. The research is part of the global Census of Marine Life (CoML) and has disclosed some remarkable findings. More than 235 marine organisms live in both polar regions, having bridged the gap of 12,000 km (7,500 mi). Large animals such as some cetaceans and birds make the round trip annually. More surprising are small forms of life such as mudworms, sea cucumbers and free-swimming snails found in both polar oceans. Various factors may aid in their distribution – fairly uniform temperatures of the deep ocean at the poles and the equator which differ by no more than 5 °C (9.0 °F), and the major current systems or marine conveyor belt which transport egg and larva stages. However, among smaller marine animals generally assumed to be the same in the Antarctica and the Arctic, more detailed studies of each population have often—but not always—revealed differences, showing that they are closely related cryptic species rather than a single bipolar species.
The rocky shores of mainland Antarctica and its offshore islands provide nesting space for over 100 million birds every spring. These nesters include species of albatrosses, petrels, skuas, gulls and terns. The insectivorous South Georgia pipit is endemic to South Georgia and some smaller surrounding islands. Freshwater ducks inhabit South Georgia and the Kerguelen Islands.The flightless penguins are all located in the Southern Hemisphere, with the greatest concentration located on and around Antarctica. Four of the 18 penguin species live and breed on the mainland and its close offshore islands. Another four species live on the subantarctic islands. Emperor penguins have four overlapping layers of feathers, keeping them warm. They are the only Antarctic animal to breed during the winter.
There are relatively few fish species in few families in the Southern Ocean. The most species-rich family are the snailfish (Liparidae), followed by the cod icefish (Nototheniidae) and eelpout (Zoarcidae). Together the snailfish, eelpouts and notothenioids (which includes cod icefish and several other families) account for almost ​9⁄10 of the more than 320 described fish species of the Southern Ocean (tens of undescribed species also occur in the region, especially among the snailfish). Southern Ocean snailfish are generally found in deep waters, while the icefish also occur in shallower waters.
Cod icefish (Nototheniidae), as well as several other families, are part of the Notothenioidei suborder, collectively sometimes referred to as icefish. The suborder contains many species with antifreeze proteins in their blood and tissue, allowing them to live in water that is around or slightly below 0 °C (32 °F). Antifreeze proteins are also known from Southern Ocean snailfish.The crocodile icefish (family Channichthyidae), also known as white-blooded fish, are only found in the Southern Ocean. They lack hemoglobin in their blood, resulting in their blood being colourless. One Channichthyidae species, the mackerel icefish (Champsocephalus gunnari), was once the most common fish in coastal waters less than 400 metres (1,312 ft) deep, but was overfished in the 1970s and 1980s. Schools of icefish spend the day at the seafloor and the night higher in the water column eating plankton and smaller fish.There are two species from the genus Dissostichus, the Antarctic toothfish (Dissostichus mawsoni) and the Patagonian toothfish (Dissostichus eleginoides). These two species live on the seafloor 100–3,000 metres (328–9,843 ft) deep, and can grow to around 2 metres (7 ft) long weighing up to 100 kilograms (220 lb), living up to 45 years. The Antarctic toothfish lives close to the Antarctic mainland, whereas the Patagonian toothfish lives in the relatively warmer subantarctic waters. Toothfish are commercially fished, and overfishing has reduced toothfish populations.Another abundant fish group is the genus Notothenia, which like the Antarctic toothfish have antifreeze in their bodies.An unusual species of icefish is the Antarctic silverfish (Pleuragramma antarcticum), which is the only truly pelagic fish in the waters near Antarctica.
Seven pinniped species inhabit Antarctica. The largest, the elephant seal (Mirounga leonina), can reach up to 4,000 kilograms (8,818 lb), while females of the smallest, the Antarctic fur seal (Arctocephalus gazella), reach only 150 kilograms (331 lb). These two species live north of the sea ice, and breed in harems on beaches. The other four species can live on the sea ice. Crabeater seals (Lobodon carcinophagus) and Weddell seals (Leptonychotes weddellii) form breeding colonies, whereas leopard seals (Hydrurga leptonyx) and Ross seals (Ommatophoca rossii) live solitary lives. Although these species hunt underwater, they breed on land or ice and spend a great deal of time there, as they have no terrestrial predators.The four species that inhabit sea ice are thought to make up 50% of the total biomass of the world's seals. Crabeater seals have a population of around 15 million, making them one of the most numerous large animals on the planet. The New Zealand sea lion (Phocarctos hookeri), one of the rarest and most localised pinnipeds, breeds almost exclusively on the subantarctic Auckland Islands, although historically it had a wider range. Out of all permanent mammalian residents, the Weddell seals live the furthest south.There are 10 cetacean species found in the Southern Ocean; six baleen whales, and four toothed whales. The largest of these, the blue whale (Balaenoptera musculus), grows to 24 metres (79 ft) long weighing 84 tonnes. Many of these species are migratory, and travel to tropical waters during the Antarctic winter.
Many aquatic molluscs are present in Antarctica. Bivalves such as Adamussium colbecki move around on the seafloor, while others such as Laternula elliptica live in burrows filtering the water above. There are around 70 cephalopod species in the Southern Ocean, the largest of which is the colossal squid (Mesonychoteuthis hamiltoni), which at up to 14 metres (46 ft) is among the largest invertebrate in the world. Squid makes up most of the diet of some animals, such as grey-headed albatrosses and sperm whales, and the warty squid (Moroteuthis ingens) is one of the subantarctic's most preyed upon species by vertebrates.The sea urchin genus Abatus burrow through the sediment eating the nutrients they find in it. Two species of salps are common in Antarctic waters, Salpa thompsoni and Ihlea racovitzai. Salpa thompsoni is found in ice-free areas, whereas Ihlea racovitzai is found in the high latitude areas near ice. Due to their low nutritional value, they are normally only eaten by fish, with larger animals such as birds and marine mammals only eating them when other food is scarce.Antarctic sponges are long lived, and sensitive to environmental changes due to the specificity of the symbiotic microbial communities within them. As a result, they function as indicators of environmental health.
Increased solar ultraviolet radiation resulting from the Antarctic ozone hole has reduced marine primary productivity (phytoplankton) by as much as 15% and has started damaging the DNA of some fish. Illegal, unreported and unregulated fishing, especially the landing of an estimated five to six times more Patagonian toothfish than the regulated fishery, likely affects the sustainability of the stock. Long-line fishing for toothfish causes a high incidence of seabird mortality.
All international agreements regarding the world's oceans apply to the Southern Ocean. In addition, it is subject to these agreements specific to the region: The Southern Ocean Whale Sanctuary of the International Whaling Commission (IWC) prohibits commercial whaling south of 40 degrees south (south of 60 degrees south between 50 degrees and 130 degrees west). Japan regularly does not recognize this provision, because the sanctuary violates IWC charter. Since the scope of the sanctuary is limited to commercial whaling, in regard to its whaling permit and whaling for scientific research, a Japanese fleet carried out an annual whale-hunt in the region. On 31 March 2014, the International Court of Justice ruled that Japan's whaling program, which Japan has long claimed is for scientific purposes, was a cloak for commercial whaling, and no further permits would be granted. Convention for the Conservation of Antarctic Seals is part of the Antarctic Treaty System. It was signed at the conclusion of a multilateral conference in London on 11 February 1972. Convention for the Conservation of Antarctic Marine Living Resources (CCAMLR) is part of the Antarctic Treaty System. The Convention was entered into force on 7 April 1982 and has its goal is to preserve marine life and environmental integrity in and near Antarctica. It was established in large part to concerns that an increase in krill catches in the Southern Ocean could have a serious impact on populations of other marine life which are dependent upon krill for food.Many nations prohibit the exploration for and the exploitation of mineral resources south of the fluctuating Antarctic Convergence, which lies in the middle of the Antarctic Circumpolar Current and serves as the dividing line between the very cold polar surface waters to the south and the warmer waters to the north. The Antarctic Treaty covers the portion of the globe south of sixty degrees south, it prohibits new claims to Antarctica.The Convention for the Conservation of Antarctic Marine Living Resources applies to the area south of 60° South latitude as well as the areas further north up to the limit of the Antarctic Convergence.
Between 1 July 1998 and 30 June 1999, fisheries landed 119,898 tonnes, of which 85% consisted of krill and 14% of Patagonian toothfish. International agreements came into force in late 1999 to reduce illegal, unreported, and unregulated fishing, which in the 1998–99 season landed five to six times more Patagonian toothfish than the regulated fishery.
Major operational ports include: Rothera Station, Palmer Station, Villa Las Estrellas, Esperanza Base, Mawson Station, McMurdo Station, and offshore anchorages in Antarctica. Few ports or harbors exist on the southern (Antarctic) coast of the Southern Ocean, since ice conditions limit use of most shores to short periods in midsummer; even then some require icebreaker escort for access. Most Antarctic ports are operated by government research stations and, except in an emergency, remain closed to commercial or private vessels; vessels in any port south of 60 degrees south are subject to inspection by Antarctic Treaty observers. The Southern Ocean's southernmost port operates at McMurdo Station at 77°50′S 166°40′E. Winter Quarters Bay forms a small harbor, on the southern tip of Ross Island where a floating ice pier makes port operations possible in summer. Operation Deep Freeze personnel constructed the first ice pier at McMurdo in 1973.Based on the original 1928 IHO delineation of the Southern Ocean (and the 1937 delineation if the Great Australian Bight is considered integral), Australian ports and harbors between Cape Leeuwin and Cape Otway on the Australian mainland and along the west coast of Tasmania would also be identified as ports and harbors existing in the Southern Ocean. These would include the larger ports and harbors of Albany, Thevenard, Port Lincoln, Whyalla, Port Augusta, Port Adelaide, Portland, Warrnambool, and Macquarie Harbour. Even though organizers of several Yacht races define their routes as involving the Southern Ocean, the actual routes don't enter the actual geographical boundaries of the Southern Ocean. The routes involve instead South Atlantic, South Pacific and Indian Ocean.
Borders of the oceans List of countries by southernmost point List of seamounts in the Southern Ocean Seven Seas
Oceanography Image of the Day, from the Woods Hole Oceanographic Institution The CIA World Factbook's entry on the Southern Ocean The Fifth Ocean from Geography.About.com The International Bathymetric Chart of the Southern Ocean (IBCSO) National Geophysical Data Center U.S. National Oceanic and Atmospheric Administration (NOAA): Limits of Oceans and Seas (2nd Edition), extant 1937 to 1953, with limits of Southern Ocean. NOAA In-situ Ocean Data Viewer Plot and download ocean observations NOAA FAQ about the number of oceans Commission for the Conservation of Antarctic Marine Living Resources
India is a federal union comprising 28 states and 8 union territories, for a total of 36 entities. The states and union territories are further subdivided into districts and smaller administrative divisions.
The Indian subcontinent has been ruled by many different ethnic groups throughout its history, each instituting their own policies of administrative division in the region. During the British Raj, the preceding Mughal administrative structure was mostly kept. India was divided into provinces (also called Presidencies) that were directly governed by the British and princely states which were nominally controlled by a local prince or raja loyal to the British Empire, which held de facto sovereignty (suzerainty) over the princely states.
Between 1947 and 1950 the territories of the princely states were politically integrated into the Indian Union. Most were merged into existing provinces; others were organised into new provinces, such as Rajasthan, Himachal Pradesh, Madhya Bharat, and Vindhya Pradesh, made up of multiple princely states; a few, including Mysore, Hyderabad, Bhopal, and Bilaspur, became separate provinces. The new Constitution of India, which came into force on 26 January 1950, made India a sovereign democratic republic. The new republic was also declared to be a "Union of States". The constitution of 1950 distinguished between three main types of states: Part A states, which were the former governors' provinces of British India, were ruled by an elected governor and state legislature. The nine Part A states were Assam (formerly Assam Province), Bihar (formerly Bihar Province), Bombay (formerly Bombay Province), East Punjab (formerly Punjab Province), Madhya Pradesh (formerly the Central Provinces and Berar), Madras (formerly Madras Province), Orissa (formerly Orissa Province), Uttar Pradesh (formerly the United Provinces), and West Bengal (formerly Bengal Province). The eight Part B states were former princely states or groups of princely states, governed by a rajpramukh, who was usually the ruler of a constituent state, and an elected legislature. The rajpramukh was appointed by the President of India. The Part B states were Hyderabad (formerly Hyderabad Princely State), Jammu and Kashmir (formerly Jammu and Kashmir Princely State), Madhya Bharat (formerly Central India Agency), Mysore (formerly Mysore Princely State), Patiala and East Punjab States Union (PEPSU), Rajasthan (formerly Rajputana Agency), Saurashtra (formerly Baroda, Western India and Gujarat States Agency), and Travancore-Cochin (formerly Travancore Princely State and Cochin Princely State). The ten Part C states included both the former chief commissioners' provinces and some princely states, and each was governed by a chief commissioner appointed by the President of India. The Part C states were Ajmer (formerly Ajmer-Merwara Province), Bhopal (formerly Bhopal Princely State), Bilaspur (formerly Bilaspur Princely State), Coorg State (formerly Coorg Province), Delhi, Himachal Pradesh, Kutch (formerly Cutch Princely State), Manipur (formerly Manipur Princely State), Tripura (formerly Tripura Princely State), and Vindhya Pradesh (formerly Central India Agency). The only Part D state was the Andaman and Nicobar Islands, which were administered by a lieutenant governor appointed by the union government.
Andhra State was created on 1 October 1953 from the Telugu-speaking northern districts of Madras State.Pondicherry, comprising the previous French enclaves of Pondichéry, Karikal, Yanaon and Mahé was transferred to India in 1954 and became a union territory in 1962. The French enclave of Chandernagore was transferred to West Bengal.In the same year, pro-India forces liberated the Portuguese-held enclaves of Dadrá and Nagar Aveli, declaring the short-lived de facto state Free Dadra and Nagar Haveli. In 1961, it was annexed by India as the Union Territory of Dadra and Nagar Haveli.The States Reorganisation Act, 1956 reorganised the states based on linguistic lines resulting in the creation of the new states. As a result of this act, Madras State retained its name with Kanyakumari district added to form Travancore-Cochin. Andhra Pradesh was created with the merger of Andhra State with the Telugu-speaking districts of Hyderabad State in 1956. Kerala was created with the merger of Malabar district and the Kasaragod taluk of South Canara districts of Madras State with Travancore-Cochin. Mysore State was re-organized with the addition of districts of Bellary and South Canara (excluding Kasaragod taluk) and the Kollegal taluk of Coimbatore district from the Madras State, the districts of Belgaum, Bijapur, North Canara and Dharwad from Bombay State, the Kannada-majority districts of Bidar, Raichur and Gulbarga from Hyderabad State and the Coorg State. The Laccadive Islands, Aminidivi Islands and Minicoy Island which were divided between South Canara and Malabar districts of Madras State were united and organised into the union territory of Lakshadweep. Bombay State was enlarged by the addition of Saurashtra State and Kutch State, the Marathi-speaking districts of Nagpur Division of Madhya Pradesh and Marathwada region of Hyderabad State. Rajasthan and Punjab gained territories from Ajmer State and Patiala and East Punjab States Union respectively and certain territories of Bihar was transferred to West Bengal.
Bombay State was split into the linguistic states of Gujarat and Maharashtra on 1 May 1960 by the Bombay Reorganisation Act. Former Union Territory of Nagaland achieved statehood on 1 December 1963. The Punjab Reorganisation Act, 1966 resulted in the creation of Haryana on 1 November and the transfer of the northern districts of Punjab to Himachal Pradesh. The act also designated Chandigarh as a union territory and the shared capital of Punjab and Haryana.Madras State was renamed Tamil Nadu in 1969. North-eastern states of Manipur, Meghalaya and Tripura were formed on 21 January 1972. Mysore State was renamed as Karnataka in 1973. On 16 May 1975, Sikkim became the 22nd state of the Indian Union and the state's monarchy was abolished. In 1987, Arunachal Pradesh and Mizoram became states on 20 February, followed by Goa on 30 May, while erstwhile union territory of Goa, Daman and Diu's northern exclaves Damão and Diu became separate union territory as Daman and Diu.In November 2000, three new states were created; namely, Chhattisgarh from eastern Madhya Pradesh, Uttaranchal from northwest Uttar Pradesh (renamed Uttarakhand in 2007) and Jharkhand from southern districts of Bihar with the enforcement of Madhya Pradesh Reorganisation Act, 2000, Uttar Pradesh Reorganisation Act, 2000 and Bihar Reorganisation Act, 2000 respectively. Pondicherry was renamed as Puducherry in 2007 and Orissa was renamed as Odisha in 2011. Telangana was created on 2 June 2014 as ten former districts of north-western Andhra Pradesh.In August 2019, the Parliament of India passed the Jammu and Kashmir Reorganisation Act, 2019, which contains provisions to reorganise the state of Jammu and Kashmir into two union territories; Jammu and Kashmir and Ladakh, effective from 31 October 2019. Later that year in November, the Government of India introduced legislation to merge the union territories of Daman and Diu and Dadra and Nagar Haveli into a single union territory to be known as Dadra and Nagar Haveli and Daman and Diu, effective from 26 January 2020.
The Sixth Schedule of the Constitution of India allows for the formation of autonomous councils to administer areas which have been given autonomy within their respective states. Most of these autonomous areas are located in Northeast India.
The Constitution of India distributes the sovereign executive and legislative powers exercisable with respect to the territory of any State between the Union and that State.
Administrative divisions of India Autonomous administrative divisions of India List of states and union territories of India by area List of states and union territories of India by population List of states in India by past population List of states of India by wildlife population List of adjectives and demonyms for states and territories of India List of Indian state and union territory name etymologies List of princely states of British India (alphabetical)
Official Government of India website: States and Union Territories
Trickle-down economics, also called trickle-down theory, refers to the economic proposition that taxes on businesses and the wealthy in society should be reduced as a means to stimulate business investment in the short term and benefit society at large in the long term. In recent history, the term has been used by critics of supply-side economic policies, such as "Reaganomics". Whereas general supply-side theory favors lowering taxes overall, trickle-down theory more specifically targets taxes on the upper end of the economic spectrum.The term "trickle-down" originated as a joke by humorist Will Rogers and today is often used to criticize economic policies that favor the wealthy or privileged while being framed as good for the average citizen. David Stockman, who as Ronald Reagan's budget director championed Reagan's tax cuts at first, later became critical of them and told journalist William Greider that "supply-side economics" is the trickle-down idea: It's kind of hard to sell 'trickle down,' so the supply-side formula was the only way to get a tax policy that was really 'trickle down.' Supply-side is 'trickle-down' theory. Political opponents of the Reagan administration soon seized on this language in an effort to brand the administration as caring only about the wealthy. Some studies suggest a link between trickle-down economics and reduced growth. Trickle-down economics has been widely criticized, particularly by left-wing, centre-left and moderate politicians and economists, but also some right-wing politicians.
In 1896, Democratic presidential candidate William Jennings Bryan described the concept using the metaphor of a "leak" in his Cross of Gold speech: "There are two ideas of government. There are those who believe that if you just legislate to make the well-to-do prosperous, that their prosperity will leak through on those below. The Democratic idea has been that if you legislate to make the masses prosperous their prosperity will find its way up and through every class that rests upon it." Humorist Will Rogers jokingly advised in a column in 1932: This election was lost four and six years ago, not this year. They [Republicans] didn’t start thinking of the old common fellow till just as they started out on the election tour. The money was all appropriated for the top in the hopes that it would trickle down to the needy. Mr. Hoover was an engineer. He knew that water trickles down. Put it uphill and let it go and it will reach the driest little spot. But he didn’t know that money trickled up. Give it to the people at the bottom and the people at the top will have it before night, anyhow. But it will at least have passed through the poor fellow's hands. They saved the big banks, but the little ones went up the flue. William J. Bennett wrote:Humorist Will Rogers referred to the theory that cutting taxes for higher earners and businesses was a "trickle-down" policy, a term that has stuck over the years. Presidential speechwriter Samuel Rosenman wrote:The philosophy that had prevailed in Washington since 1921, that the object of government was to provide prosperity for those who lived and worked at the top of the economic pyramid, in the belief that prosperity would trickle down to the bottom of the heap and benefit all. The Merriam-Webster Dictionary notes that the first known use of "trickle-down" as an adjective meaning "relating to or working on the principle of trickle-down theory" was in 1944 while the first known use of "trickle-down theory" was in 1954.After leaving the presidency, Democrat Lyndon B. Johnson alleged "Republicans [...] simply don't know how to manage the economy. They're so busy operating the trickle-down theory, giving the richest corporations the biggest break, that the whole thing goes to hell in a handbasket."Although "trickle-down" is commonly mentioned in reference to income, it has been used by economist Arthur Okun to refer to the flow of the benefits of innovation, which do not accrue entirely to the "great entrepreneurs and inventors," but trickle down to the masses. More precisely, Nobel prize economist William Nordhaus estimates that innovators are able to capture only an extremely low 2.2 percent of the total surplus from innovation. Circling back to income "trickle-down," the surplus from innovation can take the form of gains in real wages, which are spread throughout the economy, according to Nobel prize economist Paul Romer. In particular, economist William Baumol argues that "the bulk of the unprecedented and widespread rise in the developed world’s living standards since the Industrial Revolution could not have occurred without the revolution’s innovations."Speaking on the Senate floor in 1992, Senator Hank Brown (R-Colorado) said: "Mr. President, the trickle-down theory attributed to the Republican Party has never been articulated by President Reagan and has never been articulated by President Bush and has never been advocated by either one of them. One might argue whether trickle-down makes any sense or not. To attribute to people who have advocated the opposite in policies is not only inaccurate but poisons the debate on public issues."Economist Thomas Sowell has written extensively on trickle-down economics and loathes its characterization, citing that supply-side economics has never claimed to work in a "trickle-down" fashion. Rather, the economic theory of reducing marginal tax rates works in precisely the opposite direction: "Workers are always paid first and then profits flow upward later – if at all."
The economist John Kenneth Galbraith noted that "trickle-down economics" had been tried before in the United States in the 1890s under the name "horse-and-sparrow theory", writing: Mr. David Stockman has said that supply-side economics was merely a cover for the trickle-down approach to economic policy—what an older and less elegant generation called the horse-and-sparrow theory: 'If you feed the horse enough oats, some will pass through to the road for the sparrows.' Galbraith claimed that the horse-and-sparrow theory was partly to blame for the Panic of 1896. While running against Ronald Reagan for the Presidential nomination in 1980, George H. W. Bush had derided the trickle-down approach as "voodoo economics". In the 1992 presidential election, independent candidate Ross Perot also referred to trickle-down economics "political voodoo". In the same election during a presidential town hall debate, Bill Clinton said: What I want you to understand is the national debt is not the only cause of [declining economic conditions in America]. It is because America has not invested in its people. It is because we have not grown. It is because we've had 12 years of trickle-down economics. We've gone from first to twelfth in the world in wages. We've had four years where we’ve produced no private-sector jobs. Most people are working harder for less money than they were making 10 years ago. A 2012 study by the Tax Justice Network indicates that wealth of the super-rich does not trickle down to improve the economy, but it instead tends to be amassed and sheltered in tax havens with a negative effect on the tax bases of the home economy. In 2013, Pope Francis referred to "trickle-down theories" in his apostolic exhortation Evangelii Gaudium with the following statement (No. 54): Some people continue to defend trickle-down theories which assume that economic growth, encouraged by a free market, will inevitably succeed in bringing about greater justice and inclusiveness in the world. This opinion, which has never been confirmed by the facts, expresses a crude and naïve trust in the goodness of those wielding economic power and in the sacralized workings of the prevailing economic system. A 2015 paper by researchers for the International Monetary Fund argues that there is no trickle-down effect as the rich get richer: [I]f the income share of the top 20 percent (the rich) increases, then GDP growth actually declines over the medium term, suggesting that the benefits do not trickle down. In contrast, an increase in the income share of the bottom 20 percent (the poor) is associated with higher GDP growth. A 2015 report on policy by economist Pavlina R. Tcherneva described the failings of increasing economic gains of the rich without commensurate participation by the working and middle classes, referring to the problematic policies as "Reagan-style trickle-down economics," and "a trickle-down, financial-sector-driven policy regime."In 2016, Nobel laureate Joseph Stiglitz wrote that the post-World War II evidence does not support trickle-down economics, but rather "trickle-up economics" whereby more money in the pockets of the poor or the middle benefits everyone.A 2019 study in the Journal of Political Economy found, contrary to trickle-down theory, that "the positive relationship between tax cuts and employment growth is largely driven by tax cuts for lower-income groups and that the effect of tax cuts for the top 10 percent on employment growth is small."
In New Zealand, Labour Party Member of Parliament Damien O'Connor has called trickle-down economics "the rich pissing on the poor" in the Labour Party campaign launch video for the 2011 general election. In a 2016 presidential candidates debate, Hillary Clinton accused Donald Trump of supporting the "most extreme" version of trickle-down economics with his tax plan, calling it "trumped-up trickle-down" as a pun on his name.
The United States of America (USA), commonly known as the United States (US or U.S.) or America, is a country primarily located in central North America, between Canada and Mexico. It consists of 50 states, a federal district, five self-governing territories, and several other island possessions. At 3.8 million square miles (9.8 million square kilometers), it is the world's third- or fourth-largest country by total area. With a population of over 328 million, it is the third most populous country in the world. The national capital is Washington, D.C., and the most populous city is New York City. Paleo-Indians migrated from Siberia to the North American mainland at least 12,000 years ago, and European colonization began in the 16th century. The United States emerged from the thirteen British colonies established along the East Coast. Disputes over taxation and political representation with Great Britain led to the American Revolutionary War (1775–1783), which established independence. In the late 18th century, the U.S. began vigorously expanding across North America, gradually acquiring new territories, conquering and displacing Native Americans, and admitting new states; by 1848, the United States spanned the continent. Slavery was legal in the southern United States until the second half of the 19th century, when the American Civil War led to its abolition. The Spanish–American War and World War I established the U.S. as a world power, a status confirmed by the outcome of World War II. During the Cold War, the United States and the Soviet Union engaged in various proxy wars, but avoided direct military conflict. They also competed in the Space Race, culminating in the 1969 spaceflight that first landed humans on the Moon. The Soviet Union's collapse in 1991 ended the Cold War and left the United States as the world's sole superpower, with immense power in global geopolitics. The United States is a federal republic and a representative democracy with three separate branches of government, including a bicameral legislature. It is a founding member of the United Nations, World Bank, International Monetary Fund, Organization of American States (OAS), NATO, and other international organizations. It is a permanent member of the United Nations Security Council. The U.S. ranks high in international measures of economic freedom, lack of government corruption, quality of life, and quality of higher education. Despite income and wealth disparities, the United States continuously ranks high in measures of socioeconomic performance. It is one of the most racially and ethnically diverse nations in the world. Considered a melting pot of cultures, religions, and ethnicities, its population has been profoundly shaped by centuries of immigration. A highly developed country, the United States accounts for approximately a quarter of global gross domestic product (GDP) and is the world's largest economy by nominal GDP. By value, the United States is the world's largest importer and the second-largest exporter of goods. Although its population is only 4.3% of the world total, it holds 29.4% of the total wealth in the world, the largest share held by any country. Making up more than a third of global military spending, it is the foremost military power in the world, and is a leading political, cultural, and scientific force internationally.
It has been generally accepted that the first inhabitants of North America migrated from Siberia by way of the Bering land bridge and arrived at least 12,000 years ago; however, increasing evidence suggests an even earlier arrival. The Clovis culture, which appeared around 11,000 BC, is believed to represent the first wave of human settlement of the Americas. This was likely the first of three major waves of migration into North America; later waves brought the forerunners of present-day Athabaskans, Aleuts and Eskimos.Over time, indigenous cultures in North America grew increasingly complex, and some, such as the pre-Columbian Mississippian culture in the southeast, developed advanced agriculture, grand architecture, and state-level societies. Its city state Cahokia is the largest, most complex pre-Columbian archaeological site in the modern-day United States. In the Four Corners region, Ancestral Puebloan culture developed from centuries of agricultural experimentation. The Iroquois Confederacy, located in the southern Great Lakes region, was established at some point between the twelfth and fifteenth centuries. Most prominent along the Atlantic coast were the Algonquian tribes, who practiced hunting and trapping, along with limited cultivation. Estimating the native population of North America at the time of European contact is difficult. Douglas H. Ubelaker of the Smithsonian Institution estimated that there was a population of 92,916 in the south Atlantic states and a population of 473,616 in the Gulf states, but most academics regard this figure as too low. Anthropologist Henry F. Dobyns believed the populations were much higher, suggesting 1,100,000 along the shores of the gulf of Mexico, 2,211,000 people living between Florida and Massachusetts, 5,250,000 in the Mississippi Valley and tributaries and 697,000 people in the Florida peninsula.
The first Europeans to arrive in the contiguous United States were Spanish conquistadors such as Juan Ponce de León, who made his first visit to Florida in 1513. Even earlier, Christopher Columbus landed in Puerto Rico on his 1493 voyage. The Spanish set up the first settlements in Florida and New Mexico such as Saint Augustine and Santa Fe. The French established their own as well along the Mississippi River, notably New Orleans. Successful English settlement of the eastern coast of North America began with the Virginia Colony in 1607 at Jamestown and with the Pilgrims' Plymouth Colony in 1620. Many English settlers were dissenting Christian groups who came seeking religious freedom. The continent's first elected legislative assembly, Virginia's House of Burgesses, was created in 1619. Documents such as the Mayflower Compact and the Fundamental Orders of Connecticut established precedents for representative self-government and constitutionalism that would develop throughout the American colonies. In the early days of colonization, many European settlers were subject to food shortages, disease, and attacks from Native Americans. Native Americans were also often at war with neighboring tribes and allied with Europeans in their colonial wars. In many cases, however, natives and settlers came to depend on each other. Settlers traded for food and animal pelts; natives for guns, ammunition, and other European goods. Natives taught many settlers to cultivate corn, beans, and squash. European missionaries and others felt it was important to "civilize" the Native Americans and urged them to adopt European agricultural techniques and lifestyles. However, with the advancement of European colonization in North America, the Native Americans were often conquered and displaced. The native population of America declined after European arrival for various reasons, primarily diseases such as smallpox and measles.A large-scale slave trade with English privateers began. Because of less disease and better food and treatment, the life expectancy of slaves was much higher in North America than further south, leading to a rapid increase in the numbers of slaves. Colonial society was largely divided over the religious and moral implications of slavery, and colonies passed acts for and against the practice. But by the turn of the 18th century, African slaves were replacing European indentured servants for cash crop labor, especially in the South.The Thirteen Colonies (New Hampshire, Massachusetts, Connecticut, Rhode Island, New York, New Jersey, Pennsylvania, Delaware, Maryland, Virginia, North Carolina, South Carolina and Georgia) that would become the United States of America were administered by the British as overseas dependencies. All nonetheless had local governments with elections open to most free men. With extremely high birth rates, low death rates, and steady settlement, the colonial population grew rapidly, eclipsing Native American populations. The Christian revivalist movement of the 1730s and 1740s known as the Great Awakening fueled interest both in religion and in religious liberty.During the Seven Years' War (1756–63), known in the U.S. as the French and Indian War, British forces seized Canada from the French. With the creation of the Province of Quebec (1763–1791), Canada's francophone population would remain politically and culturally isolated from the English-speaking colonial dependencies of Nova Scotia, Newfoundland and the Thirteen Colonies. Excluding the Native Americans who lived there, the Thirteen Colonies had a population of over 2.1 million in 1770, about a third that of Britain. Despite continuing new arrivals, the rate of natural increase was such that by the 1770s only a small minority of Americans had been born overseas. The colonies' distance from Britain had allowed the development of self-government, but their unprecedented success motivated British monarchs to periodically seek to reassert royal authority.
The American Revolutionary War fought by the Thirteen Colonies against the British Empire was the first successful colonial war of independence against a European power. Americans had developed an ideology of "republicanism", asserting that government rested on the will of the people as expressed in their local legislatures. They demanded their rights as Englishmen and "no taxation without representation". The British insisted on administering the empire through Parliament, and the conflict escalated into war.The Second Continental Congress unanimously adopted the Declaration of Independence on July 4, 1776; this day is celebrated annually as Independence Day. In 1777, the Articles of Confederation established a decentralized government that operated until 1789.After its defeat at the Battle of Yorktown in 1781, Britain signed a peace treaty. American sovereignty became internationally recognized and the country was granted all lands east of the Mississippi River. Tensions with Britain remained, however, leading to the War of 1812, which was fought to a draw. Nationalists led the Philadelphia Convention of 1787 in writing the United States Constitution, ratified in state conventions in 1788. The federal government was reorganized into three branches in 1789, on the principle of creating salutary checks and balances. George Washington, who had led the Continental Army to victory, was the first president elected under the new constitution. The Bill of Rights, forbidding federal restriction of personal freedoms and guaranteeing a range of legal protections, was adopted in 1791. Although the federal government criminalized the international slave trade in 1808, after 1820, cultivation of the highly profitable cotton crop exploded in the Deep South, and along with it, the slave population. The Second Great Awakening, especially in the period 1800–1840, converted millions to evangelical Protestantism. In the North, it energized multiple social reform movements, including abolitionism; in the South, Methodists and Baptists proselytized among slave populations.Beginning in the late 18th century, Americans began to expand westward, prompting a long series of American Indian Wars. The 1803 Louisiana Purchase almost doubled the nation's area, Spain ceded Florida and other Gulf Coast territory in 1819, the Republic of Texas was annexed in 1845 during a period of expansionism, and the 1846 Oregon Treaty with Britain led to U.S. control of the present-day American Northwest. Victory in the Mexican–American War resulted in the 1848 Mexican Cession of California and much of the present-day American Southwest, making the U.S. span the continent.The California Gold Rush of 1848–49 spurred migration to the Pacific coast, which led to the California Genocide and the creation of additional western states. After the Civil War, new transcontinental railways made relocation easier for settlers, expanded internal trade and increased conflicts with Native Americans. In 1869, a new Peace Policy nominally promised to protect Native Americans from abuses, avoid further war, and secure their eventual U.S. citizenship. Nonetheless, large-scale conflicts continued throughout the West into the 1900s.
Irreconcilable sectional conflict regarding the slavery of Africans and African Americans ultimately led to the American Civil War. With the 1860 election of Republican Abraham Lincoln, conventions in thirteen slave states declared secession and formed the Confederate States of America (the "South" or the "Confederacy"), while the federal government (the "Union") maintained that secession was illegal. In order to bring about this secession, military action was initiated by the secessionists, and the Union responded in kind. The ensuing war would become the deadliest military conflict in American history, resulting in the deaths of approximately 618,000 soldiers as well as many civilians. The Union initially simply fought to keep the country united. Nevertheless, as casualties mounted after 1863 and Lincoln delivered his Emancipation Proclamation, the main purpose of the war from the Union's viewpoint became the abolition of slavery. Indeed, when the Union ultimately won the war in April 1865, each of the states in the defeated South was required to ratify the Thirteenth Amendment, which prohibited slavery. Two amendments ensuring citizenship and, in theory, voting rights for blacks were also ratified. Reconstruction began in earnest following the war. While President Lincoln attempted to foster friendship and forgiveness between the Union and the former Confederacy, his assassination on April 14, 1865 drove a wedge between North and South again. Republicans in the federal government made it their goal to oversee the rebuilding of the South and to ensure the rights of African Americans. They persisted until the Compromise of 1877 when the Republicans agreed to cease protecting the rights of African Americans in the South in order for Democrats to concede the presidential election of 1876. Southern white Democrats, calling themselves "Redeemers", took control of the South after the end of Reconstruction. From 1890 to 1910 the Redeemers established so-called Jim Crow laws, disenfranchising most blacks and some poor whites throughout the region. Blacks faced racial segregation, especially in the South. They also occasionally experienced vigilante violence, including lynching.
In the North, urbanization and an unprecedented influx of immigrants from Southern and Eastern Europe supplied a surplus of labor for the country's industrialization and transformed its culture. National infrastructure including telegraph and transcontinental railroads spurred economic growth and greater settlement and development of the American Old West. The later invention of electric light and the telephone would also affect communication and urban life.The United States fought Indian Wars west of the Mississippi River from 1810 to at least 1890. Most of these conflicts ended with the cession of Native American territory and their confinement to Indian reservations. Additionally, the Trail of Tears in the 1830s exemplified the Indian removal policy that forcibly resettled Indians. This further expanded acreage under mechanical cultivation, increasing surpluses for international markets. Mainland expansion also included the purchase of Alaska from Russia in 1867. In 1893, pro-American elements in Hawaii overthrew the monarchy and formed the Republic of Hawaii, which the U.S. annexed in 1898. Puerto Rico, Guam, and the Philippines were ceded by Spain in the same year, following the Spanish–American War. American Samoa was acquired by the United States in 1900 after the end of the Second Samoan Civil War. The U.S. Virgin Islands were purchased from Denmark in 1917.Rapid economic development during the late 19th and early 20th centuries fostered the rise of many prominent industrialists. Tycoons like Cornelius Vanderbilt, John D. Rockefeller, and Andrew Carnegie led the nation's progress in railroad, petroleum, and steel industries. Banking became a major part of the economy, with J. P. Morgan playing a notable role. The American economy boomed, becoming the world's largest. These dramatic changes were accompanied by social unrest and the rise of populist, socialist, and anarchist movements. This period eventually ended with the advent of the Progressive Era, which saw significant reforms including women's suffrage, alcohol prohibition, regulation of consumer goods, greater antitrust measures to ensure competition and attention to worker conditions.
The United States remained neutral from the outbreak of World War I in 1914 until 1917, when it joined the war as an "associated power" alongside the formal Allies of World War I, helping to turn the tide against the Central Powers. In 1919, President Woodrow Wilson took a leading diplomatic role at the Paris Peace Conference and advocated strongly for the U.S. to join the League of Nations. However, the Senate refused to approve this and did not ratify the Treaty of Versailles that established the League of Nations.In 1920, the women's rights movement won passage of a constitutional amendment granting women's suffrage. The 1920s and 1930s saw the rise of radio for mass communication and the invention of early television. The prosperity of the Roaring Twenties ended with the Wall Street Crash of 1929 and the onset of the Great Depression. After his election as president in 1932, Franklin D. Roosevelt responded with the New Deal. The Great Migration of millions of African Americans out of the American South began before World War I and extended through the 1960s; whereas the Dust Bowl of the mid-1930s impoverished many farming communities and spurred a new wave of western migration.At first effectively neutral during World War II, the United States began supplying materiel to the Allies in March 1941 through the Lend-Lease program. On December 7, 1941, the Empire of Japan launched a surprise attack on Pearl Harbor, prompting the United States to join the Allies against the Axis powers and, the following year, to intern about 120,000 U.S. residents (including American citizens) of Japanese descent. Although Japan attacked the United States first, the U.S. nonetheless pursued a "Europe first" defense policy. The United States thus left its vast Asian colony, the Philippines, isolated and fighting a losing struggle against Japanese invasion and occupation. During the war, the United States was referred to as one of the "Four Policemen" of Allies power who met to plan the postwar world, along with Britain, the Soviet Union and China. Although the nation lost around 400,000 military personnel, it emerged relatively undamaged from the war with even greater economic and military influence.The United States played a leading role in the Bretton Woods and Yalta conferences, which signed agreements on new international financial institutions and Europe's postwar reorganization. As an Allied victory was won in Europe, a 1945 international conference held in San Francisco produced the United Nations Charter, which became active after the war. The United States and Japan then fought each other in the largest naval battle in history, the Battle of Leyte Gulf. The United States eventually developed the first nuclear weapons and used them on Japan in the cities of Hiroshima and Nagasaki; the Japanese surrendered on September 2, ending World War II.
After World War II, the United States and the Soviet Union competed for power, influence, and prestige during what became known as the Cold War, driven by an ideological divide between capitalism and communism. They dominated the military affairs of Europe, with the U.S. and its NATO allies on one side and the Soviet Union and its Warsaw Pact allies on the other. The U.S. developed a policy of containment towards the expansion of communist influence. While the U.S. and Soviet Union engaged in proxy wars and developed powerful nuclear arsenals, the two countries avoided direct military conflict.The United States often opposed Third World movements that it viewed as Soviet-sponsored, and occasionally pursued direct action for regime change against left-wing governments, even occasionally supporting authoritarian right-wing regimes. American troops fought communist Chinese and North Korean forces in the Korean War of 1950–53. The Soviet Union's 1957 launch of the first artificial satellite and its 1961 launch of the first crewed spaceflight initiated a "Space Race" in which the United States became the first nation to land a man on the Moon in 1969. A proxy war in Southeast Asia eventually evolved into the Vietnam War (1955–1975), with full American participation.At home, the U.S. had experienced sustained economic expansion and a rapid growth of its population and middle class following World War II. After a surge in female labor participation, especially in the 1970s, by 1985 the majority of women aged 16 and over were employed. Construction of an Interstate Highway System transformed the nation's infrastructure over the following decades. Millions moved from farms and inner cities to large suburban housing developments. In 1959 Hawaii became the 50th and last U.S. state added to the country. The growing Civil Rights Movement used nonviolence to confront segregation and discrimination, with Martin Luther King Jr. becoming a prominent leader and figurehead. A combination of court decisions and legislation, culminating in the Civil Rights Act of 1968, sought to end racial discrimination. Meanwhile, a counterculture movement grew which was fueled by opposition to the Vietnam war, the Black Power movement, and the sexual revolution. The launch of a "War on Poverty" expanded entitlements and welfare spending, including the creation of Medicare and Medicaid, two programs that provide health coverage to the elderly and poor, respectively, and the means-tested Food Stamp Program and Aid to Families with Dependent Children.The 1970s and early 1980s saw the onset of stagflation. After his election in 1980, President Ronald Reagan responded to economic stagnation with free-market oriented reforms. Following the collapse of détente, he abandoned "containment" and initiated the more aggressive "rollback" strategy towards the Soviet Union. The late 1980s brought a "thaw" in relations with the Soviet Union, and its collapse in 1991 finally ended the Cold War. This brought about unipolarity with the U.S. unchallenged as the world's dominant superpower.
After the Cold War, the conflict in the Middle East triggered a crisis in 1990, when Iraq invaded and attempted to annex Kuwait, an ally of the United States. Fearing the spread of instability, in August President George H. W. Bush launched and led the Gulf War against Iraq; waged until January 1991 by coalition forces from 34 nations, it ended in the expulsion of Iraqi forces from Kuwait and restoration of the monarchy.Originating within U.S. military defense networks, the Internet spread to international academic platforms and then to the public in the 1990s, greatly affecting the global economy, society, and culture. Due to the dot-com boom, stable monetary policy, and reduced social welfare spending, the 1990s saw the longest economic expansion in modern U.S. history. Beginning in 1994, the U.S. signed the North American Free Trade Agreement (NAFTA), causing trade among the U.S., Canada, and Mexico to soar.On September 11, 2001, Al-Qaeda terrorist hijackers flew passenger planes into the World Trade Center in New York City and the Pentagon near Washington, D.C., killing nearly 3,000 people. In response, the United States launched the War on Terror, which included a war in Afghanistan and the 2003–11 Iraq War. A 2011 military operation in Pakistan led to the death of the leader of Al-Qaeda.Government policy designed to promote affordable housing, widespread failures in corporate and regulatory governance, and historically low interest rates set by the Federal Reserve led to the mid-2000s housing bubble, which culminated with the 2008 financial crisis, the nation's largest economic contraction since the Great Depression. During the crisis, assets owned by Americans lost about a quarter of their value. Barack Obama, the first African-American and multiracial president, was elected in 2008 amid the crisis, and subsequently passed stimulus measures and the Dodd–Frank Act in an attempt to mitigate its negative effects and ensure there would not be a repeat of the crisis. In 2010, President Obama led efforts to pass the Affordable Care Act, the most sweeping reform to the nation's healthcare system in nearly five decades.In the presidential election of 2016, Republican Donald Trump was elected as the 45th president of the United States. On January 20, 2020, the first case of COVID-19 in the United States was confirmed. As of September 2020, the United States has over 6.2 million COVID-19 cases and over 180,000 deaths. The United States is by far the country with the most cases of COVID-19 since April 11, 2020.
The 48 contiguous states and the District of Columbia occupy a combined area of 3,119,885 square miles (8,080,470 km2). Of this area, 2,959,064 square miles (7,663,940 km2) is contiguous land, composing 83.65% of total U.S. land area. Hawaii, occupying an archipelago in the central Pacific, southwest of North America, is 10,931 square miles (28,311 km2) in area. The populated territories of Puerto Rico, American Samoa, Guam, Northern Mariana Islands, and U.S. Virgin Islands together cover 9,185 square miles (23,789 km2). Measured by only land area, the United States is third in size behind Russia and China, just ahead of Canada.The United States is the world's third- or fourth-largest nation by total area (land and water), ranking behind Russia and Canada and nearly equal to China. The ranking varies depending on how two territories disputed by China and India are counted, and how the total size of the United States is measured.The coastal plain of the Atlantic seaboard gives way further inland to deciduous forests and the rolling hills of the Piedmont. The Appalachian Mountains divide the eastern seaboard from the Great Lakes and the grasslands of the Midwest. The Mississippi–Missouri River, the world's fourth longest river system, runs mainly north–south through the heart of the country. The flat, fertile prairie of the Great Plains stretches to the west, interrupted by a highland region in the southeast.The Rocky Mountains, west of the Great Plains, extend north to south across the country, peaking around 14,000 feet (4,300 m) in Colorado. Farther west are the rocky Great Basin and deserts such as the Chihuahua and Mojave. The Sierra Nevada and Cascade mountain ranges run close to the Pacific coast, both ranges reaching altitudes higher than 14,000 feet (4,300 m). The lowest and highest points in the contiguous United States are in the state of California, and only about 84 miles (135 km) apart. At an elevation of 20,310 feet (6,190.5 m), Alaska's Denali is the highest peak in the country and in North America. Active volcanoes are common throughout Alaska's Alexander and Aleutian Islands, and Hawaii consists of volcanic islands. The supervolcano underlying Yellowstone National Park in the Rockies is the continent's largest volcanic feature.The United States, with its large size and geographic variety, includes most climate types. To the east of the 100th meridian, the climate ranges from humid continental in the north to humid subtropical in the south. The Great Plains west of the 100th meridian are semi-arid. Much of the Western mountains have an alpine climate. The climate is arid in the Great Basin, desert in the Southwest, Mediterranean in coastal California, and oceanic in coastal Oregon and Washington and southern Alaska. Most of Alaska is subarctic or polar. Hawaii and the southern tip of Florida are tropical, as well as its territories in the Caribbean and the Pacific. States bordering the Gulf of Mexico are prone to hurricanes, and most of the world's tornadoes occur in the country, mainly in Tornado Alley areas in the Midwest and South. Overall, the United States receives more high-impact extreme weather incidents than any other country in the world.
The U.S. ecology is megadiverse: about 17,000 species of vascular plants occur in the contiguous United States and Alaska, and more than 1,800 species of flowering plants are found in Hawaii, few of which occur on the mainland. The United States is home to 428 mammal species, 784 bird species, 311 reptile species, and 295 amphibian species, as well as about 91,000 insect species.There are 62 national parks and hundreds of other federally managed parks, forests, and wilderness areas. Altogether, the government owns about 28% of the country's land area, mostly in the western states. Most of this land is protected, though some is leased for oil and gas drilling, mining, logging, or cattle ranching, and about .86% is used for military purposes.Environmental issues include debates on oil and nuclear energy, dealing with air and water pollution, the economic costs of protecting wildlife, logging and deforestation, and international responses to global warming. The most prominent environmental agency is the Environmental Protection Agency (EPA), created by presidential order in 1970. The idea of wilderness has shaped the management of public lands since 1964, with the Wilderness Act. The Endangered Species Act of 1973 is intended to protect threatened and endangered species and their habitats, which are monitored by the United States Fish and Wildlife Service.
The First Amendment of the U.S. Constitution guarantees the free exercise of religion and forbids Congress from passing laws respecting its establishment. United states has the world's largest Christian population. In a 2014 survey, 70.6% of adults in the United States identified themselves as Christians; Protestants accounted for 46.5%, while Roman Catholics, at 20.8%, formed the largest single Christian group. In 2014, 5.9% of the U.S. adult population claimed a non-Christian religion. These include Judaism (1.9%), Islam (0.9%), Hinduism (0.7%), and Buddhism (0.7%). The survey also reported that 22.8% of Americans described themselves as agnostic, atheist or simply having no religion—up from 8.2% in 1990.Protestantism is the largest Christian religious grouping in the United States, accounting for almost half of all Americans. Baptists collectively form the largest branch of Protestantism at 15.4%, and the Southern Baptist Convention is the largest individual Protestant denomination at 5.3% of the U.S. population. Apart from Baptists, other Protestant categories include nondenominational Protestants, Methodists, Pentecostals, unspecified Protestants, Lutherans, Presbyterians, Congregationalists, other Reformed, Episcopalians/Anglicans, Quakers, Adventists, Holiness, Christian fundamentalists, Anabaptists, Pietists, and multiple others.The Bible Belt is an informal term for a region in the Southern United States in which socially conservative evangelical Protestantism is a significant part of the culture and Christian church attendance across the denominations is generally higher than the nation's average. By contrast, religion plays the least important role in New England and in the Western United States.
The United States had a life expectancy of 78.6 years at birth in 2017, which was the third year of declines in life expectancy following decades of continuous increase. The recent decline, primarily among the age group 25 to 64, is largely due to record highs in the drug overdose and suicide rates; the country has one of the highest suicide rates among wealthy countries. From 1999 to 2019, more than 770,000 Americans died from drug overdoses. Life expectancy was highest among Asians and Hispanics and lowest among blacks.Increasing obesity in the United States and improvements in health and longevity outside the U.S. contributed to lowering the country's rank in life expectancy from 11th in the world in 1987 to 42nd in 2007. In 2017, the United States had the lowest life expectancy among Japan, Canada, Australia, the United Kingdom, and seven nations in western Europe. Obesity rates have more than doubled in the last 30 years and are the highest in the industrialized world. Approximately one-third of the adult population is obese and an additional third is overweight. Obesity-related type 2 diabetes is considered epidemic by health care professionals.In 2010, coronary artery disease, lung cancer, stroke, chronic obstructive pulmonary diseases, and traffic accidents caused the most years of life lost in the U.S. Low back pain, depression, musculoskeletal disorders, neck pain, and anxiety caused the most years lost to disability. The most harmful risk factors were poor diet, tobacco smoking, obesity, high blood pressure, high blood sugar, physical inactivity, and alcohol use. Alzheimer's disease, drug abuse, kidney disease, cancer, and falls caused the most additional years of life lost over their age-adjusted 1990 per-capita rates. U.S. teenage pregnancy and abortion rates are substantially higher than in other Western nations, especially among blacks and Hispanics.Health-care coverage in the United States is a combination of public and private efforts and is not universal. In 2017, 12.2% of the population did not carry health insurance. The subject of uninsured and underinsured Americans is a major political issue. The Affordable Care Act, passed in early 2010, roughly halved the uninsured share of the population, though the bill and its ultimate effect are issues of controversy. The U.S. health-care system far outspends any other nation, measured both in per capita spending and as percentage of GDP. However, the U.S. is a global leader in medical innovation.
American public education is operated by state and local governments and regulated by the United States Department of Education through restrictions on federal grants. In most states, children are required to attend school from the age of six or seven (generally, kindergarten or first grade) until they turn 18 (generally bringing them through twelfth grade, the end of high school); some states allow students to leave school at 16 or 17.About 12% of children are enrolled in parochial or nonsectarian private schools. Just over 2% of children are homeschooled. The U.S. spends more on education per student than any nation in the world, spending an average of $12,794 per year on public elementary and secondary school students in the 2016–2017 school year. Some 80% of U.S. college students attend public universities.Of Americans 25 and older, 84.6% graduated from high school, 52.6% attended some college, 27.2% earned a bachelor's degree, and 9.6% earned graduate degrees. The basic literacy rate is approximately 99%. The United Nations assigns the United States an Education Index of 0.97, tying it for 12th in the world.The United States has many private and public institutions of higher education. The majority of the world's top universities, as listed by various ranking organizations, are in the U.S. There are also local community colleges with generally more open admission policies, shorter academic programs, and lower tuition. In 2018, U21, a network of research-intensive universities, ranked the United States first in the world for breadth and quality of higher education, and 15th when GDP was a factor. As for public expenditures on higher education, the U.S. trails some other OECD (Organization for Cooperation and Development) nations but spends more per student than the OECD average, and more than all nations in combined public and private spending. As of 2018, student loan debt exceeded 1.5 trillion dollars.
The United States is a federal republic of 50 states, a federal district, five territories and several uninhabited island possessions. It is the world's oldest surviving federation. It is a federal republic and a representative democracy "in which majority rule is tempered by minority rights protected by law." The U.S. ranked 25th on the Democracy Index in 2018. On Transparency International's 2019 Corruption Perceptions Index, its public sector position deteriorated from a score of 76 in 2015 to 69 in 2019.In the American federalist system, citizens are usually subject to three levels of government: federal, state, and local. The local government's duties are commonly split between county and municipal governments. In almost all cases, executive and legislative officials are elected by a plurality vote of citizens by district. The government is regulated by a system of checks and balances defined by the U.S. Constitution, which serves as the country's supreme legal document. The original text of the Constitution establishes the structure and responsibilities of the federal government and its relationship with the individual states. Article One protects the right to the writ of habeas corpus. The Constitution has been amended 27 times; the first ten amendments, which make up the Bill of Rights, and the Fourteenth Amendment form the central basis of Americans' individual rights. All laws and governmental procedures are subject to judicial review and any law ruled by the courts to be in violation of the Constitution is voided. The principle of judicial review, not explicitly mentioned in the Constitution, was established by the Supreme Court in Marbury v. Madison (1803) in a decision handed down by Chief Justice John Marshall.The federal government comprises three branches: Legislative: The bicameral Congress, made up of the Senate and the House of Representatives, makes federal law, declares war, approves treaties, has the power of the purse, and has the power of impeachment, by which it can remove sitting members of the government. Executive: The president is the commander-in-chief of the military, can veto legislative bills before they become law (subject to congressional override), and appoints the members of the Cabinet (subject to Senate approval) and other officers, who administer and enforce federal laws and policies. Judicial: The Supreme Court and lower federal courts, whose judges are appointed by the president with Senate approval, interpret laws and overturn those they find unconstitutional.The House of Representatives has 435 voting members, each representing a congressional district for a two-year term. House seats are apportioned among the states by population. Each state then draws single-member districts to conform with the census apportionment. The District of Columbia and the five major U.S. territories each have one member of Congress—these members are not allowed to vote.The Senate has 100 members with each state having two senators, elected at-large to six-year terms; one-third of Senate seats are up for election every two years. The District of Columbia and the five major U.S. territories do not have senators. The president serves a four-year term and may be elected to the office no more than twice. The president is not elected by direct vote, but by an indirect electoral college system in which the determining votes are apportioned to the states and the District of Columbia. The Supreme Court, led by the chief justice of the United States, has nine members, who serve for life.
The 50 states are the principal administrative divisions in the country. These are subdivided into counties or county equivalents and further divided into municipalities. The District of Columbia is a federal district that contains the capital of the United States, Washington, D.C. The states and the District of Columbia choose the president of the United States. Each state has presidential electors equal to the number of their representatives and senators in Congress; the District of Columbia has three (because of the 23rd Amendment). Territories of the United States such as Puerto Rico do not have presidential electors, and so people in those territories cannot vote for the president.The United States also observes tribal sovereignty of the American Indian nations to a limited degree, as it does with the states' sovereignty. American Indians are U.S. citizens and tribal lands are subject to the jurisdiction of the U.S. Congress and the federal courts. Like the states they have a great deal of autonomy, but also like the states, tribes are not allowed to make war, engage in their own foreign relations, or print and issue currency.Citizenship is granted at birth in all states, the District of Columbia, and all major U.S. territories except American Samoa.
The United States has operated under a two-party system for most of its history. For elective offices at most levels, state-administered primary elections choose the major party nominees for subsequent general elections. Since the general election of 1856, the major parties have been the Democratic Party, founded in 1824, and the Republican Party, founded in 1854. Since the Civil War, only one third-party presidential candidate—former president Theodore Roosevelt, running as a Progressive in 1912—has won as much as 20% of the popular vote. The president and vice president are elected by the Electoral College.In American political culture, the center-right Republican Party is considered "conservative" and the center-left Democratic Party is considered "liberal". The states of the Northeast and West Coast and some of the Great Lakes states, known as "blue states", are relatively liberal. The "red states" of the South and parts of the Great Plains and Rocky Mountains are relatively conservative. Republican Donald Trump, the winner of the 2016 presidential election, is serving as the 45th president of the United States. Leadership in the Senate includes vice president Mike Pence, president pro tempore Chuck Grassley, Majority Leader Mitch McConnell, and Minority Leader Chuck Schumer. Leadership in the House includes Speaker of the House Nancy Pelosi, Majority Leader Steny Hoyer, and Minority Leader Kevin McCarthy. In the 116th United States Congress, the House of Representatives is controlled by the Democratic Party and the Senate is controlled by the Republican Party, giving the U.S. a split Congress. The Senate consists of 53 Republicans and 45 Democrats with two Independents who caucus with the Democrats; the House consists of 233 Democrats, 196 Republicans, and 1 Libertarian. Of state governors, there are 26 Republicans and 24 Democrats. Among the D.C. mayor and the five territorial governors, there are four Democrats, one Republican, and one New Progressive.
The United States has an established structure of foreign relations. It is a permanent member of the United Nations Security Council. New York City is home to the United Nations Headquarters. Almost all countries have embassies in Washington, D.C., and many have consulates around the country. Likewise, nearly all nations host American diplomatic missions. However, Iran, North Korea, Bhutan, and the Republic of China (Taiwan) do not have formal diplomatic relations with the United States (although the U.S. still maintains unofficial relations with Bhutan and Taiwan). It is a member of the G7, G20, and OECD. The United States has a "Special Relationship" with the United Kingdom and strong ties with India, Canada, Australia, New Zealand, the Philippines, Japan, South Korea, Israel, and several European Union countries, including France, Italy, Germany, Spain and Poland. It works closely with fellow NATO members on military and security issues and with its neighbors through the Organization of American States and free trade agreements such as the trilateral North American Free Trade Agreement with Canada and Mexico. Colombia is traditionally considered by the United States as its most loyal ally in South America.The U.S. exercises full international defense authority and responsibility for Micronesia, the Marshall Islands and Palau through the Compact of Free Association.
The president is the commander-in-chief of the United States Armed Forces and appoints its leaders, the secretary of defense and the Joint Chiefs of Staff. The Department of Defense administers five of the six service branches, which are made up of the Army, Marine Corps, Navy, Air Force, and Space Force. The Coast Guard, also a branch of the armed forces, is administered by the Department of Homeland Security in peacetime and by the Department of the Navy in wartime. In 2019, all six branches of the U.S. Armed Forces reported 1.4 million personnel on active duty. The Reserves and National Guard brought the total number of troops to 2.3 million. The Department of Defense also employed about 700,000 civilians, not including contractors.Military service is voluntary, though conscription may occur in wartime through the Selective Service System. American forces can be rapidly deployed by the Air Force's large fleet of transport aircraft, the Navy's 11 active aircraft carriers, and Marine expeditionary units at sea with the Navy's Atlantic and Pacific fleets. The military operates about 800 bases and facilities abroad, and maintains deployments greater than 100 active duty personnel in 25 foreign countries. The United States spent $649 billion on its military in 2019, 36% of global military spending. At 4.7% of GDP, the rate was the second-highest among the top 15 military spenders, after Saudi Arabia. Defense spending plays a major role in science and technology investment, with roughly half of U.S. federal research and development funded by the Department of Defense. Defense's share of the overall U.S. economy has generally declined in recent decades, from early Cold War peaks of 14.2% of GDP in 1953 and 69.5% of federal spending in 1954 to 4.7% of GDP and 18.8% of federal spending in 2011.The country is one of the five recognized nuclear weapons states and one of nine countries to possess nuclear weapons. The United States possesses the second-largest stockpile of nuclear weapons in the world. More than 40% of the world's 14,000 nuclear weapons are held by the United States.
Law enforcement in the United States is primarily the responsibility of local police departments and sheriff's offices, with state police providing broader services. Federal agencies such as the Federal Bureau of Investigation (FBI) and the U.S. Marshals Service have specialized duties, including protecting civil rights, national security and enforcing U.S. federal courts' rulings and federal laws. State courts conduct most criminal trials while federal courts handle certain designated crimes as well as certain appeals from the state criminal courts. A cross-sectional analysis of the World Health Organization Mortality Database from 2010 showed that United States homicide rates "were 7.0 times higher than in other high-income countries, driven by a gun homicide rate that was 25.2 times higher." In 2016, the U.S. murder rate was 5.4 per 100,000. The United States has the highest documented incarceration rate and largest prison population in the world. As of 2020, the Prison Policy Initiative reported that there were some 2.3 million people incarcerated. According to the Federal Bureau of Prisons, the majority of inmates held in federal prisons are convicted of drug offenses. The imprisonment rate for all prisoners sentenced to more than a year in state or federal facilities is 478 per 100,000 in 2013. About 9% of prisoners are held in privatized prisons, a practice beginning in the 1980s and a subject of contention.Capital punishment is sanctioned in the United States for certain federal and military crimes, and at the state level in 28 states, though three states have moratoriums on carrying out the penalty imposed by their governors. In 2019, the country had the sixth-highest number of executions in the world, following China, Iran, Saudi Arabia, Iraq, and Egypt. No executions took place from 1967 to 1977, owing in part to a U.S. Supreme Court ruling striking down the practice. Since the decision, however, there have been more than 1,500 executions. In recent years the number of executions and presence of capital punishment statute on whole has trended down nationally, with several states recently abolishing the penalty.
According to the International Monetary Fund, the U.S. GDP of $16.8 trillion constitutes 24% of the gross world product at market exchange rates and over 19% of the gross world product at purchasing power parity. The United States is the largest importer of goods and second-largest exporter, though exports per capita are relatively low. In 2010, the total U.S. trade deficit was $635 billion. Canada, China, Mexico, Japan, and Germany are its top trading partners. From 1983 to 2008, U.S. real compounded annual GDP growth was 3.3%, compared to a 2.3% weighted average for the rest of the G7. The country ranks ninth in the world in nominal GDP per capita and sixth in GDP per capita at PPP. The U.S. dollar is the world's primary reserve currency. In 2009, the private sector was estimated to constitute 86.4% of the economy. While its economy has reached a postindustrial level of development, the United States remains an industrial power. In August 2010, the American labor force consisted of 154.1 million people (50%). With 21.2 million people, government is the leading field of employment. The largest private employment sector is health care and social assistance, with 16.4 million people. It has a smaller welfare state and redistributes less income through government action than most European nations.The United States is the only advanced economy that does not guarantee its workers paid vacation and is one of a few countries in the world without paid family leave as a legal right. 74% of full-time American workers get paid sick leave, according to the Bureau of Labor Statistics, although only 24% of part-time workers get the same benefits. In 2009, the United States had the third-highest workforce productivity per person in the world, behind Luxembourg and Norway.
The United States has been a leader in technological innovation since the late 19th century and scientific research since the mid-20th century. Methods for producing interchangeable parts were developed by the U.S. War Department by the Federal Armories during the first half of the 19th century. This technology, along with the establishment of a machine tool industry, enabled the U.S. to have large-scale manufacturing of sewing machines, bicycles, and other items in the late 19th century and became known as the American system of manufacturing. Factory electrification in the early 20th century and introduction of the assembly line and other labor-saving techniques created the system of mass production. In the 21st century, approximately two-thirds of research and development funding comes from the private sector. The United States leads the world in scientific research papers and impact factor.In 1876, Alexander Graham Bell was awarded the first U.S. patent for the telephone. Thomas Edison's research laboratory, one of the first of its kind, developed the phonograph, the first long-lasting light bulb, and the first viable movie camera. The latter led to emergence of the worldwide entertainment industry. In the early 20th century, the automobile companies of Ransom E. Olds and Henry Ford popularized the assembly line. The Wright brothers, in 1903, made the first sustained and controlled heavier-than-air powered flight.The rise of fascism and Nazism in the 1920s and 30s led many European scientists, including Albert Einstein, Enrico Fermi, and John von Neumann, to immigrate to the United States. During World War II, the Manhattan Project developed nuclear weapons, ushering in the Atomic Age, while the Space Race produced rapid advances in rocketry, materials science, and aeronautics.The invention of the transistor in the 1950s, a key active component in practically all modern electronics, led to many technological developments and a significant expansion of the U.S. technology industry. This, in turn, led to the establishment of many new technology companies and regions around the country such as Silicon Valley in California. Advancements by American microprocessor companies such as Advanced Micro Devices (AMD) and Intel, along with both computer software and hardware companies such as Adobe Systems, Apple Inc., IBM, Microsoft, and Sun Microsystems, created and popularized the personal computer. The ARPANET was developed in the 1960s to meet Defense Department requirements, and became the first of a series of networks which evolved into the Internet.
Accounting for 4.24% of the global population, Americans collectively possess 29.4% of the world's total wealth, the largest percentage of any country. Americans also make up roughly half of the world's population of millionaires. The Global Food Security Index ranked the U.S. number one for food affordability and overall food security in March 2013. Americans on average have more than twice as much living space per dwelling and per person as EU residents. For 2017 the United Nations Development Programme ranked the United States 13th among 189 countries in its Human Development Index (HDI) and 25th among 151 countries in its inequality-adjusted HDI (IHDI).Wealth, like income and taxes, is highly concentrated; the richest 10% of the adult population possess 72% of the country's household wealth, while the bottom half possess only 2%. According to the Federal Reserve, the top 1% controlled 38.6% of the country's wealth in 2016. In 2017, Forbes found that just three individuals (Jeff Bezos, Warren Buffett and Bill Gates) held more money than the bottom half of the population. According to a 2018 study by the OECD, the United States has a larger percentage of low-income workers than almost any other developed nation, largely because of a weak collective bargaining system and lack of government support for at-risk workers. The top one percent of income-earners accounted for 52 percent of the income gains from 2009 to 2015, where income is defined as market income excluding government transfers. After years of stagnation, median household income reached a record high in 2016 following two consecutive years of record growth. Income inequality remains at record highs however, with the top fifth of earners taking home more than half of all overall income. The rise in the share of total annual income received by the top one percent, which has more than doubled from nine percent in 1976 to 20 percent in 2011, has significantly affected income inequality, leaving the United States with one of the widest income distributions among OECD nations. The extent and relevance of income inequality is a matter of debate.There were about 567,715 sheltered and unsheltered homeless persons in the U.S. in January 2019, with almost two-thirds staying in an emergency shelter or transitional housing program. In 2011, 16.7 million children lived in food-insecure households, about 35% more than 2007 levels, though only 845,000 U.S. children (1.1%) saw reduced food intake or disrupted eating patterns at some point during the year, and most cases were not chronic. As of June 2018, 40 million people, roughly 12.7% of the U.S. population, were living in poverty, including 13.3 million children. Of those impoverished, 18.5 million live in deep poverty (family income below one-half of the poverty threshold) and over five million live "in 'Third World' conditions". In 2017, the U.S. states or territories with the lowest and highest poverty rates were New Hampshire (7.6%) and American Samoa (65%), respectively. The economic impact and mass unemployment caused by the COVID-19 pandemic has raised fears of a mass eviction crisis, with an analysis by the Aspen Institute indicating that between 30 and 40 million people are at risk for eviction by the end of 2020.
Personal transportation is dominated by automobiles, which operate on a network of 4 million miles (6.4 million kilometers) of public roads. The United States has the world's second-largest automobile market, and has the highest vehicle ownership per capita in the world, with 816.4 vehicles per 1,000 Americans (2014). In 2017, there were 255,009,283 non-two wheel motor vehicles, or about 910 vehicles per 1,000 people.The civil airline industry is entirely privately owned and has been largely deregulated since 1978, while most major airports are publicly owned. The three largest airlines in the world by passengers carried are U.S.-based; American Airlines is number one after its 2013 acquisition by US Airways. Of the world's 50 busiest passenger airports, 16 are in the United States, including the busiest, Hartsfield–Jackson Atlanta International Airport.
The United States energy market is about 29,000 terawatt hours per year. In 2005, 40% of this energy came from petroleum, 23% from coal, and 22% from natural gas. The remainder was supplied by nuclear and renewable energy sources.Since 2007, the total greenhouse gas emissions by the United States are the second highest by country, exceeded only by China. The United States has historically been the world's largest producer of greenhouse gases, and greenhouse gas emissions per capita remain high.
The United States is home to many cultures and a wide variety of ethnic groups, traditions, and values. Aside from the Native American, Native Hawaiian, and Native Alaskan populations, nearly all Americans or their ancestors immigrated within the past five centuries. Mainstream American culture is a Western culture largely derived from the traditions of European immigrants with influences from many other sources, such as traditions brought by slaves from Africa. More recent immigration from Asia and especially Latin America has added to a cultural mix that has been described as both a homogenizing melting pot, and a heterogeneous salad bowl in which immigrants and their descendants retain distinctive cultural characteristics.Americans have traditionally been characterized by a strong work ethic, competitiveness, and individualism, as well as a unifying belief in an "American creed" emphasizing liberty, equality, private property, democracy, rule of law, and a preference for limited government. Americans are extremely charitable by global standards: according to a 2006 British study, Americans gave 1.67% of GDP to charity, more than any other nation studied.The American Dream, or the perception that Americans enjoy high social mobility, plays a key role in attracting immigrants. Whether this perception is accurate has been a topic of debate. While mainstream culture holds that the United States is a classless society, scholars identify significant differences between the country's social classes, affecting socialization, language, and values. Americans tend to greatly value socioeconomic achievement, but being ordinary or average is also generally seen as a positive attribute.
In the 18th and early 19th centuries, American art and literature took most of its cues from Europe. Writers such as Washington Irving, Nathaniel Hawthorne, Edgar Allan Poe, and Henry David Thoreau established a distinctive American literary voice by the middle of the 19th century. Mark Twain and poet Walt Whitman were major figures in the century's second half; Emily Dickinson, virtually unknown during her lifetime, is now recognized as an essential American poet. A work seen as capturing fundamental aspects of the national experience and character—such as Herman Melville's Moby-Dick (1851), Twain's The Adventures of Huckleberry Finn (1885), F. Scott Fitzgerald's The Great Gatsby (1925) and Harper Lee's To Kill a Mockingbird (1960)—may be dubbed the "Great American Novel."Thirteen U.S. citizens have won the Nobel Prize in Literature. William Faulkner, Ernest Hemingway and John Steinbeck are often named among the most influential writers of the 20th century. Popular literary genres such as the Western and hardboiled crime fiction developed in the United States. The Beat Generation writers opened up new literary approaches, as have postmodernist authors such as John Barth, Thomas Pynchon, and Don DeLillo.The transcendentalists, led by Thoreau and Ralph Waldo Emerson, established the first major American philosophical movement. After the Civil War, Charles Sanders Peirce and then William James and John Dewey were leaders in the development of pragmatism. In the 20th century, the work of W. V. O. Quine and Richard Rorty, and later Noam Chomsky, brought analytic philosophy to the fore of American philosophical academia. John Rawls and Robert Nozick also led a revival of political philosophy. In the visual arts, the Hudson River School was a mid-19th-century movement in the tradition of European naturalism. The 1913 Armory Show in New York City, an exhibition of European modernist art, shocked the public and transformed the U.S. art scene. Georgia O'Keeffe, Marsden Hartley, and others experimented with new, individualistic styles. Major artistic movements such as the abstract expressionism of Jackson Pollock and Willem de Kooning and the pop art of Andy Warhol and Roy Lichtenstein developed largely in the United States. The tide of modernism and then postmodernism has brought fame to American architects such as Frank Lloyd Wright, Philip Johnson, and Frank Gehry. Americans have long been important in the modern artistic medium of photography, with major photographers including Alfred Stieglitz, Edward Steichen, Edward Weston, and Ansel Adams.
Mainstream American cuisine is similar to that in other Western countries. Wheat is the primary cereal grain with about three-quarters of grain products made of wheat flour and many dishes use indigenous ingredients, such as turkey, venison, potatoes, sweet potatoes, corn, squash, and maple syrup which were consumed by Native Americans and early European settlers. These homegrown foods are part of a shared national menu on one of America's most popular holidays, Thanksgiving, when some Americans make traditional foods to celebrate the occasion.The American fast food industry, the world's largest, pioneered the drive-through format in the 1940s. Characteristic dishes such as apple pie, fried chicken, pizza, hamburgers, and hot dogs derive from the recipes of various immigrants. French fries, Mexican dishes such as burritos and tacos, and pasta dishes freely adapted from Italian sources are widely consumed. Americans drink three times as much coffee as tea. Marketing by U.S. industries is largely responsible for making orange juice and milk ubiquitous breakfast beverages.
Although little known at the time, Charles Ives's work of the 1910s established him as the first major U.S. composer in the classical tradition, while experimentalists such as Henry Cowell and John Cage created a distinctive American approach to classical composition. Aaron Copland and George Gershwin developed a new synthesis of popular and classical music. The rhythmic and lyrical styles of African-American music have deeply influenced American music at large, distinguishing it from European and African traditions. Elements from folk idioms such as the blues and what is now known as old-time music were adopted and transformed into popular genres with global audiences. Jazz was developed by innovators such as Louis Armstrong and Duke Ellington early in the 20th century. Country music developed in the 1920s, and rhythm and blues in the 1940s.Elvis Presley and Chuck Berry were among the mid-1950s pioneers of rock and roll. Rock bands such as Metallica, the Eagles, and Aerosmith are among the highest grossing in worldwide sales. In the 1960s, Bob Dylan emerged from the folk revival to become one of America's most celebrated songwriters and James Brown led the development of funk. More recent American creations include hip hop and house music. American pop stars such as Elvis Presley, Michael Jackson, Madonna and Whitney Houston have become global celebrities, as have contemporary musical artists such as Katy Perry, Taylor Swift, Lady Gaga, Britney Spears, Mariah Carey, Beyoncé, Jay-Z, Eminem, and Kanye West.
Hollywood, a northern district of Los Angeles, California, is one of the leaders in motion picture production. The world's first commercial motion picture exhibition was given in New York City in 1894, using Thomas Edison's Kinetoscope. Since the early 20th century, the U.S. film industry has largely been based in and around Hollywood, although in the 21st century an increasing number of films are not made there, and film companies have been subject to the forces of globalization.Director D. W. Griffith, the top American filmmaker during the silent film period, was central to the development of film grammar, and producer/entrepreneur Walt Disney was a leader in both animated film and movie merchandising. Directors such as John Ford redefined the image of the American Old West, and, like others such as John Huston, broadened the possibilities of cinema with location shooting. The industry enjoyed its golden years, in what is commonly referred to as the "Golden Age of Hollywood", from the early sound period until the early 1960s, with screen actors such as John Wayne and Marilyn Monroe becoming iconic figures. In the 1970s, "New Hollywood" or the "Hollywood Renaissance" was defined by grittier films influenced by French and Italian realist pictures of the post-war period. In more recent times, directors such as Steven Spielberg, George Lucas and James Cameron have gained renown for their blockbuster films, often characterized by high production costs and earnings. Notable films topping the American Film Institute's AFI 100 list include Orson Welles's Citizen Kane (1941), which is frequently cited as the greatest film of all time, Casablanca (1942), The Godfather (1972), Gone with the Wind (1939), Lawrence of Arabia (1962), The Wizard of Oz (1939), The Graduate (1967), On the Waterfront (1954), Schindler's List (1993), Singin' in the Rain (1952), It's a Wonderful Life (1946) and Sunset Boulevard (1950). The Academy Awards, popularly known as the Oscars, have been held annually by the Academy of Motion Picture Arts and Sciences since 1929, and the Golden Globe Awards have been held annually since January 1944.
American football is by several measures the most popular spectator sport; the National Football League (NFL) has the highest average attendance of any sports league in the world, and the Super Bowl is watched by tens of millions globally. Baseball has been regarded as the U.S. national sport since the late 19th century, with Major League Baseball (MLB) being the top league. Basketball and ice hockey are the country's next two leading professional team sports, with the top leagues being the National Basketball Association (NBA) and the National Hockey League (NHL). College football and basketball attract large audiences. In soccer (a sport that has gained a footing in the United States since the mid-1990's), the country hosted the 1994 FIFA World Cup, the men's national soccer team qualified for ten World Cups and the women's team has won the FIFA Women's World Cup four times; Major League Soccer is the sport's highest league in the United States (featuring 23 American and three Canadian teams). The market for professional sports in the United States is roughly $69 billion, roughly 50% larger than that of all of Europe, the Middle East, and Africa combined.Eight Olympic Games have taken place in the United States. The 1904 Summer Olympics in St. Louis, Missouri, were the first ever Olympic Games held outside of Europe. As of 2017, the United States has won 2,522 medals at the Summer Olympic Games, more than any other country, and 305 in the Winter Olympic Games, the second most behind Norway. While most major U.S. sports such as baseball and American football have evolved out of European practices, basketball, volleyball, skateboarding, and snowboarding are American inventions, some of which have become popular worldwide. Lacrosse and surfing arose from Native American and Native Hawaiian activities that predate Western contact. The most watched individual sports are golf and auto racing, particularly NASCAR.
The four major broadcasters in the U.S. are the National Broadcasting Company (NBC), Columbia Broadcasting System (CBS), American Broadcasting Company (ABC), and Fox Broadcasting Company (FOX). The four major broadcast television networks are all commercial entities. Cable television offers hundreds of channels catering to a variety of niches. Americans listen to radio programming, also largely commercial, on average just over two-and-a-half hours a day.In 1998, the number of U.S. commercial radio stations had grown to 4,793 AM stations and 5,662 FM stations. In addition, there are 1,460 public radio stations. Most of these stations are run by universities and public authorities for educational purposes and are financed by public or private funds, subscriptions, and corporate underwriting. Much public-radio broadcasting is supplied by NPR. NPR was incorporated in February 1970 under the Public Broadcasting Act of 1967; its television counterpart, PBS, was created by the same legislation. As of September 30, 2014, there are 15,433 licensed full-power radio stations in the U.S. according to the U.S. Federal Communications Commission (FCC).Well-known newspapers include The Wall Street Journal, The New York Times, and USA Today. Although the cost of publishing has increased over the years, the price of newspapers has generally remained low, forcing newspapers to rely more on advertising revenue and on articles provided by a major wire service, such as the Associated Press or Reuters, for their national and world coverage. With very few exceptions, all the newspapers in the U.S. are privately owned, either by large chains such as Gannett or McClatchy, which own dozens or even hundreds of newspapers; by small chains that own a handful of papers; or in a situation that is increasingly rare, by individuals or families. Major cities often have "alternative weeklies" to complement the mainstream daily papers, such as New York City's The Village Voice or Los Angeles' LA Weekly. Major cities may also support a local business journal, trade papers relating to local industries, and papers for local ethnic and social groups. Aside from web portals and search engines, the most popular websites are Facebook, YouTube, Wikipedia, Yahoo!, eBay, Amazon, and Twitter.More than 800 publications are produced in Spanish, the second most commonly used language in the United States behind English.
Index of United States-related articles Lists of U.S. state topics Outline of the United States
Internet sources
"United States". The World Factbook. Central Intelligence Agency. United States, from the BBC News Key Development Forecasts for the United States from International FuturesGovernmentOfficial U.S. Government Web Portal Gateway to government sites House Official site of the United States House of Representatives Senate Official site of the United States Senate White House Official site of the president of the United States Supreme Court Official site of the Supreme Court of the United StatesHistoryHistorical Documents Collected by the National Center for Public Policy Research U.S. National Mottos: History and Constitutionality Analysis by the Ontario Consultants on Religious Tolerance USA Collected links to historical dataMapsNational Atlas of the United States Official maps from the U.S. Department of the Interior Wikimedia Atlas of the United States Measure of America A variety of mapped information relating to health, education, income, and demographics for the U.S.PhotosPhotos of the USA
USA Network (on-air simply as USA, stylized as usa since 2005) is an American basic cable channel owned by the NBCUniversal Television and Streaming division of NBCUniversal, a subsidiary of Comcast. It was originally launched in 1977 as Madison Square Garden Sports Network, one of the first national sports cable television channels, before being relaunched as USA Network in 1980. Once a minor player in basic-tier pay television, USA has steadily gained popularity due to its original programming; it is one of four major subscription-television networks (with TBS, TNT and FX) that also broadcasts syndicated reruns of current and former network television series and theatrically-released feature films, as well as limited sports programming and WWE. As of September 2018, USA Network is available to about 90.4 million households (98% of households with pay television) in the US.
USA Network originally launched on September 22, 1977 as the Madison Square Garden Sports Network (not to be confused with the New York City-area regional sports network of the same name now simply known as the MSG Network). The network was founded by cable provider UA-Columbia Cablevision and Kay Koplovitz. The channel was one of the first national cable television channels, utilizing satellite delivery as opposed to the then-industry standard microwave relay to distribute its programming to cable systems. Initially, the network ran a mix of college and less well-known professional sports, similar to those found during the early years of ESPN. The channel began its broadcast day after 5:00 p.m. Eastern Time on weekdays and 12:00 p.m. Eastern Time on weekends. On April 9, 1980, the channel changed its name to USA Network after the ownership structure was reorganized under a joint operating agreement by UA-Columbia and the then-MCA Inc./Universal City Studios. That fall, USA began signing on at noon Eastern Time on weekdays; it also added some talk shows and a children's program called Calliope to its schedule. Sports programming began airing at 5:00 p.m. Eastern Time weekdays, and aired all day on weekends. In the fall of 1981, USA began its daily programming at 6:00 a.m. Eastern Time, with talk shows and children's programs running until noon, sports airing from noon onward during weekends and until 3:00 p.m. weekdays, talk shows from 3:00 to 6:00 p.m. weekdays, and sports airing again after 6:00 p.m. Eastern Time. Later, in 1982, Time Inc. and Gulf+Western's Paramount Pictures unit (now part of ViacomCBS) would buy stakes in the venture. The three partners had a non-compete clause that would prevent them from owning other basic cable networks independently from the USA joint venture, but the said clause would cause Time Inc. to drop out of the venture in 1987, as the company attempted (but failed) to buy CNN from Ted Turner and run it independently from USA. MCA and Paramount subsequently became the sole owners of the channel (with each company owning a 50% interest). In the fall of 1982, USA began operating on a 24-hour-a-day schedule, running a mix of talk shows, children's shows, and a low-budget movie from 6:00 a.m. to 6:00 p.m. Eastern Time. The channel began running a mix of 1960s and 1970s Hanna-Barbera cartoons each weekday evening from 6:00 to 7:00 p.m. as part of the USA Cartoon Express block, with sports programming airing after 7:00 p.m., which were rebroadcast during the overnight hours. Weekends featured a mix of movies, some older drama series and talk shows during the morning hours, and sports during the afternoons and evenings. Overnights consisted of old low-budget films and film shorts, and music as part of a show called Night Flight. Between 1984 and 1986, USA's programming focus began shifting away from sports, and shifted towards general entertainment programs not found on broadcast stations, including some less common network drama series and cartoons. For the 1985-1986 season, the channel had 4 hours of original and exclusive shows. One original series from the 1985-1986 season was the comedy Check It Out!. USA, wanting to become the flagship cable channel and compete directly with the broadcast networks, committed to 26 half-hours of part exclusive off-broadcast network and part original programming for the 1986-1987 season at an increase of $30 million. In one case, the channel picked up Airwolf for 58 off-network episodes, while commissioning 24 new episodes without the original cast.One tradition on USA was an afternoon lineup of game show reruns mixed in with several original low-budget productions that aired over the years. It began in October 1984 with reruns of The Gong Show and Make Me Laugh. In September 1985, the network began airing its first original game show, a revival of the mid-1970s game show Jackpot; two more original game shows, Love Me, Love Me Not, and a revival of the short-lived 1980 series Chain Reaction, were added in September 1986. More shows were progressively added soon afterward such as The Joker's Wild, Tic-Tac-Dough, Press Your Luck, High Rollers, and Hollywood Squares (with John Davidson as its "Square-Master", or host), along with Wipeout, Face the Music, and Name That Tune. In June 1987, the channel debuted another original game show, Bumper Stumpers. (All four USA original game shows in this era were taped in Canada.) When it began, the game-show block ran for an hour, but it expanded significantly the following year. By 1989, the network ran game shows Monday through Fridays from 12:00 to 5:00 p.m. Eastern Time. In January 1989, USA debuted USA Up All Night, a showcase of low-budget feature films that aired as part of its weekend overnight schedule. Up All Night became a cult favorite among viewers for the comedic wraparound segments that were usually shown during breaks leading into (and sometimes, out of) commercials and between films that were hosted by comedian Gilbert Gottfried and model/actress Rhonda Shear, the latter of whom had replaced original co-host Caroline Schlitt in 1991. Though this program was discontinued on March 7, 1998, late-night movie telecasts on USA continued to be branded under the "Up All Night" banner until 2002. Short news updates, branded as USA Updates, were shown from as early as 1989 until 2000. These segments were first produced out of KYW-TV in Philadelphia, owing to the fact that the station had already produced a number of syndicated news services (including the Group W Newsfeed) and Steve Bell, the former newsreader on Good Morning America, was employed as a primary anchor at the station. However, when KYW's news operations were heavily revamped in response to falling ratings in 1991, production of USA Updates was then taken over by the "All News Channel" (operated as a joint venture of Hubbard Broadcasting's and Viacom's CONUS Communications). The ANC-produced updates continued through 2000 (ANC was suffering heavily around this time due to competition with other cable news channels such as CNN and the then-similarly formatted Headline News, and ended up shutting down in 2002); USA Network has not carried any news programming since the news updates were discontinued. USA was the first basic cable channel to pre-empt the syndicated TV market by purchasing a package of 26 films from the Touchstone Pictures library in October 1989. To obtain the package, it spent an estimated $50 million to $60 million, with films including such box office hits as Dead Poets Society, Good Morning, Vietnam, and Three Men and a Baby.The tradition of game-show reruns continued into the 1990s with the $25,000 and $100,000 Pyramids, the early 1990s revivals of The Joker's Wild and Tic-Tac-Dough, and other well-known shows such as Scrabble, Sale of the Century, Talk About, and Caesars Challenge. Additionally, two more original game shows were added in June 1994; these were Free 4 All and Quicksilver. In September 1991, the block was reduced to three hours, from 2:00 to 5:00 p.m. Eastern. However, an additional hour was added in March 1993. In November 1994, the game-show block was cut back to only two hours, from 2:00 to 4:00 p.m. On September 24, 1992, USA launched a sister network, the Sci-Fi Channel (now Syfy), focusing on science fiction series and films. In January 1993, the channel began showing WWF Monday Night Raw, which was the first major professional wrestling program to show storylines playing out in front of an audience. In September 1993, USA adopted a new on-air look centering on the slogan "The Remote Stops Here," with flat graphics suggesting a television camera's in-lens symbols and music consisting of electric guitar and synthesized noises, though the movie presentation openers were retained from the previous design.
In 1994, Paramount Pictures parent Paramount Communications was sold to the original iteration of Viacom; the following year, MCA was acquired by Seagram. In April 1996, Viacom, which also owned MTV Networks, launched a new classic television network called TV Land. MCA subsequently sued Viacom for breach of contract, claiming that it had violated the non-compete clause in its joint venture agreement with MCA. A judge presiding over the case sided with MCA, and Viacom subsequently sold its stake in USA and the Sci-Fi Channel to Seagram for $1.7 billion. In turn, Seagram sold a controlling interest in the networks to Barry Diller in February 1998, which led to the creation of USA Networks, Inc.; the company also merged the cable channels with Diller's existing television properties including the Home Shopping Network and its broadcasting unit Silver King Broadcasting (which was restructured as USA Broadcasting, and eventually sold its stations to Univision Communications in 2001 to form the nucleus of Telefutura/UniMás). In October 1995, the network dropped the entire game show block; it was replaced with a block called USA Live, which carried reruns of Love Connection and The People's Court, with live hosted wraparound segments between shows; that block was dropped by 1997 (some of the game shows that USA had aired can still be seen on GSN and Buzzr). In 1994, USA began simulcasting the upstart business news channel Bloomberg Information TV each weekday morning from 5:00 to 8:00 a.m. Eastern and Pacific Time (and later, from 5:00 to 6:00 a.m. Eastern and Pacific on Saturdays); in 2004, the Bloomberg simulcast moved to E!, where it ran until 2007 (USA was actually the second television network to simulcast Bloomberg's programming, the now-defunct American Independent Network also carried a simulcast of the channel during the mid-1990s). On June 17, 1996, the network unveiled a new on-air appearance, which included the introduction of a new logo (incorporating a star ridged into the "U" of the now-serifed "USA" logotype, replacing the Futura-typeface logo that had been in use since the network's start under the USA Network name in 1980), and a three-note jingle. Network IDs, feature presentation intros for movies and promo graphics were based around a behind-the-scenes look at the fictional "USA Studios"; some of the IDs showed people in the control room, while a studio that was being set-up by a crew was the backdrop for the "Tonight" menu that displayed the evening's schedule. Opening sequences leading into movie telecasts showed people running through the "USA Studios Film Vault". The new look coincided with a shift in focus, more towards off-network reruns and original programming; game shows and court shows were dropped from the schedule, while cartoons were phased out. USA Studios also became the branding for USA-produced programming at this point. This logo was replaced in July 1999 in favor of a 'USA flag'-styled logo (whose design was slightly modified in 2002). In September 1996, USA replaced the USA Cartoon Express with the action-oriented children's block, USA Action Extreme Team; the channel discontinued its animation block outright in September 1998 (other than airing the first-run teen sitcom USA High and reruns of Saved by the Bell: The New Class from 1997 to 2001, USA has not aired children's programming since that time), and replaced it with a block called "USAM", which advertised itself as "Primetime Comedy in the Morning". The block mainly featured sitcoms originally aired on network television that were cancelled before making it to 100 episodes (such as The Jeff Foxworthy Show, Hearts Afire and Something So Right); however, for a time, the block also included the 1989–1994 episodes of the Bob Saget run of America's Funniest Home Videos. "USAM" was discontinued in 2002; by that point, the only sitcoms airing on USA were daytime and late night reruns of Martin and overnight airings of Living Single, Cheers and Wings, with drama series and movies populating much of the channel's daytime and primetime schedule. In 2000, USA Networks bought Canadian media company North American Television, Inc. (a joint partnership between the Canadian Broadcasting Corporation and Power Corporation of Canada), owner of cable television channels Trio and Newsworld International (the CBC continued to handle programming responsibilities for NWI until 2005, when eventual USA owner Vivendi sold the channel to a group led by Al Gore, who relaunched it as Current TV). One major shock happened when USA lost the broadcasting rights of the WWF to Viacom in June 2000; Raw (which had been retitled Raw is War) was moved to TNN in September of that year.
In 2001, USA Networks sold its non-shopping television and film assets (including USA Network, the Sci-Fi Channel, Trio, USA Films (which was rechristened as Focus Features and Studios USA) to Vivendi Universal. USA and the other channels were folded into Vivendi's Universal Television Group. In July 2002, the channel debuted Monk, which became one of USA Network's first breakout hit series. It is the comedy-drama police procedural that starred Tony Shalhoub as Adrian Monk, a former San Francisco police inspector-turned-consultant who suffers from various obsessive-compulsive behaviors that include the ability to pay attention to detail when solving crimes. It ran for eight seasons until it ended on December 4, 2009.
In 2003, General Electric agreed to merge NBC and its sibling companies with Vivendi Universal's North American-based filmed entertainment assets, including Universal Pictures and Universal Television Group in a multibillion-dollar purchase, renaming the merged company NBC Universal. GE retained an 80% ownership stake in the new company, while Vivendi retained a 20% stake. NBC Universal officially took over as owner of USA and its sibling cable channels (except for Newsworld International) in 2004. That year, USA premiered the sci-fi series The 4400.
In 2005, USA Network introduced a new logo and associated marketing campaign, "Characters Welcome". The slogan was designed to help emphasize the wide range of programming the network offered, and to help USA Network establish itself more prominently as a brand. The launch of the campaign featured promos themed around the daily lives of characters from the network's programs. To contrast itself from the "grittier" offerings of other mainstream cable networks, USA Network's original programming during this era was marked by a focus on comedic and "optimistic" action and drama series, referred to as a "blue sky" approach. Notable examples of this programming strategy included Psych (2006) (which ran for eight seasons, becoming the network's longest-running series), Burn Notice (2007), and Royal Pains (2009). In October 2005, Raw returned to USA after Viacom did not renew its broadcasting agreement with the WWE. On May 13, 2007 (in advance of NBC's 2007–08 fall upfronts presentation), NBC Universal announced that new episodes of Law & Order: Criminal Intent would be moved to USA beginning with the drama's seventh season in the fall of 2007; episodes would then be re-aired later in the season on NBC, most likely to shore up any programming holes created by the cancellation of a failed new series. Although this is not the first time a broadcast series has moved to cable (USA had acquired first-run rights to the revival of Alfred Hitchcock Presents from NBC in 1987, while The Paper Chase had moved beforehand from CBS to Showtime in 1983), it marked the first time that a series which moved its first-run episodes from broadcast to cable television would continue to air episodes on a broadcast network while it was still a first-run program. On December 7, 2007, it was announced that USA Network would continue broadcasting first-run episodes of Raw through at least 2010.The June 1, 2008 premiere of In Plain Sight, starring Mary McCormack, was USA's highest-rated series premiere since the 2006 debut of Psych, with 5.3 million viewers. In early 2009, USA Network acquired the network television rights for 24 recent and upcoming Universal Pictures films, including Duplicity, Funny People, Frost/Nixon, Land of the Lost, Milk, and State of Play.In 2011, control and majority ownership of then-parent NBC Universal passed from General Electric to Comcast. Comcast would buy out GE's remaining ownership in NBCU two years later. USA Network was considered the key piece of the NBC-Comcast merger; Wunderlich Securities analyst Matthew Harrigan projected that USA contributed $9.5 billion to NBCUniversal's $44.8 billion value, with NBC contributing only $408 million. In 2014, the channel had dropped 18% in viewership and out of first place among the major cable channels. USA has been a key NBCUniversal asset accounting for one-third of advertising revenue for NBCUniversal Cable Entertainment Group and $1 billion in annual earnings over the past few years.In April 2015, it was announced that WWE SmackDown would move to USA from sister network Syfy.
In April 2016, USA Network unveiled a new branding campaign and slogan, "We the Bold". The campaign was designed to reflect the channel's current focus on "rich, captivating stories about unlikely heroes who defy the status quo, push boundaries and are willing to risk everything for what they believe in". USA had quietly discontinued the "Characters Welcome" tagline in the lead-up to the rebranding, whose associated programming shift was led by the premieres of Mr. Robot and Colony. Variety reported that the new programming strategy was designed to appeal to themes of "authenticity, resiliency, bravery and innovation". The Washington Post felt that the re-branding symbolically marked the end of USA's "blue sky" era, as the channel had been increasingly producing more "intense" series with darker themes. NBCUniversal marketing executive Alexandra Shapiro explained that the "Characters Welcome" campaign and associated programming was reflective of the "weirdly optimistic" mood of the network's key demographic at the time.In August 2016, NBCUniversal acquired the television rights to the Harry Potter film franchise from 2018 through 2025, including the main film series and their spin-offs (with the first, Fantastic Beasts and Where to Find Them, to have its cable premiere in 2019), and other content. On cable, the films are to primarily be aired by USA Network and Syfy, and the deal also includes the ability for Universal Parks & Resorts to offer "exclusive content and events" related to the franchise (Universal Parks had already been involved in The Wizarding World of Harry Potter attractions). The deal succeeded one with Freeform; The Wall Street Journal reported the deal was valued around $250 million over the length of the agreement, making it one of the highest-valued film franchise deals. To launch the new rights, Syfy and USA aired a joint marathon over the July 13–15, 2018 weekend, airing all eight films (including directors' cuts of the first six) with limited commercial interruption.
USA Network has achieved a viewership foothold with its original programming; this began in the 1990s with initial hits such as Silk Stalkings and La Femme Nikita, which were gradually followed in the following two decades by series such as Monk, Psych, Shooter, White Collar, Covert Affairs, Mr. Robot, Suits, Burn Notice and Royal Pains. Most of its original series are scripted dramas, some of which incorporate comedic elements. In addition to its original productions, the network airs syndicated reruns of current and former network series such as Law & Order: Special Victims Unit, Law & Order: Criminal Intent (which spent the final four seasons of its run as a first-run program on USA) and NCIS. The network also broadcasts a variety of films from the Universal Pictures library and select films from other movie studios (such as Sony Pictures Entertainment, Paramount Pictures, Walt Disney Studios Motion Pictures and Warner Bros. Entertainment), airing primarily as part of its overnight and weekend schedule, and occasionally during primetime on nights when original programming or marathons of its acquired programs are not scheduled. From 1984 to 2016, the network was the longtime home of the Westminster Kennel Club Dog Show. USA is also the home of WWE's flagship cable program Raw; the series originally aired on the channel from its debut in January 1993 (when the promotion was known as the World Wrestling Federation; Raw itself replaced longtime Monday night standby WWF Prime Time Wrestling) until the series moved to TNN in September 2000, before returning to the channel in October 2005. On January 7, 2016, WWE's second flagship program SmackDown moved to USA Network from Syfy. In 2018, USA renewed its rights to Raw for five additional years, but lost the rights for SmackDown to Fox beginning October 2019. In August 2019, WWE announced that its tertiary weekly program WWE NXT would return to USA Network on September 18, 2019, airing on Wednesday nights in a two-hour live format.
USA Network has a longstanding history with sports, dating back to its existence as the Madison Square Garden Network. The network carried Major League Baseball games on Thursday nights from 1979 to 1983, and the NHL on USA ran from 1979 to 1985. College Football on USA ran from 1980 to 1986, and its telecast of the 1981 Liberty Bowl was the first college bowl game to be exclusively broadcast on cable television. The NBA on USA also aired from 1979 to 1984, the first time that the NBA had a cable television partner. Professional wrestling company WWE has had a longstanding relationship with the network; WWF Prime Time Wrestling broadcast on USA from 1985-1993 until it was superseded by WWE Raw from 1993-2000, and again since 2005. WWE SmackDown aired on the network from January 2016 until October 2019, when it moved to FOX. For 17 years from 1981 to 1998, USA aired a weekly boxing show, USA Tuesday Night Fights, which showcased bouts featuring up-and-coming boxers. Tennis on USA aired professional tournaments in the United States from 1984 to 2008, and was the longtime cable home of the US Open before its cable television rights moved to ESPN2 and the Tennis Channel in 2009. The PGA Tour on USA covered the opening two rounds of the Masters Tournament from 1982 to 2007, Ryder Cup matches from 1989 to 2010, and various other events. The USA Network aired most games of the World League of American Football (later NFL Europe) in its first two seasons of operation in 1991 and 1992; one innovation introduced for the network's WLAF telecasts was the in-helmet camera. Upon the 2004 purchase of Vivendi Universal by NBC, USA's sports division was immediately merged into NBC Sports. Since 2004, the network has broadcast select events from the Olympic Games, as part of an expansion of NBCUniversal's broadcast rights to the Summer and Winter Olympics that allowed several of the company's cable channels rights to telecast Olympic events live (some of which are later re-aired on tape delay on NBC as part of the network's primetime and late night Olympic coverage). USA Network also carried games from the International Ice Hockey Federation in 2006 and 2010. During the 2014 Winter Olympics, USA aired English Premier League soccer matches in lieu of sister channel NBCSN, due to that channel's full devotion to carrying coverage of Olympic events. After ratings success with those matches, USA began to air mid-afternoon Saturday games weekly during the 2015–16 season. USA also participates in NBC Sports' broader effort of carrying all ten Survival Sunday matches across its numerous channels the second week of May each year. Starting in 2015, USA Network became used as an overflow feed for coverage of NHL playoff games that cannot be aired by either NBCSN or CNBC. In 2016, USA aired three NASCAR races (the Sprint Cup Series race at Watkins Glen International and two Xfinity Series races at Mid-Ohio Sports Car Course and Bristol Motor Speedway) due to NBC broadcasting the 2016 Summer Olympics.
USA Network operates a high definition simulcast feed of the channel, that broadcasts in the 1080i resolution format, and is available on nearly all pay-TV providers.
In February 2007, Shaw Communications submitted an application to the Canadian Radio-television and Telecommunications Commission (CRTC), to carry the USA Network in Canada as a foreign service that would be eligible for carriage by domestic cable and satellite providers (and to automatically allow all English-language general interest cable networks from the United States into Canada). However, because of programming rights issues with other Canadian specialty channels, certain programs would be subjected to blackout restrictions, including WWE Raw.In September 2007, the CRTC refused Shaw's request to carry USA Network in Canada on the basis that the channel carried too much programming that overlapped with the English language digital cable specialty channel Mystery TV (which is then owned by Canwest – later Shaw Media – and formerly, Groupe TVA). However, on September 20, the CRTC stated that it would reconsider their denial of the eligible foreign carriage proposal for USA Network at a later date, when Shaw instead offered to carry the channel on the digital cable tiers of its Shaw Cable systems. In spite of this, the CRTC has since rejected the restructured proposal on the basis that USA's programming would be competitive with Mystery TV. Many of USA's original programs currently air on either Showcase or CTV Drama Channel. WWE programming that airs on USA also airs on Rogers Media-owned Sportsnet 360.
Regional versions of USA Network previously operated in certain South American countries (such as Argentina and Brazil); in September 2004, most of these services were renamed under the Universal Channel banner to take advantage of the more well-known brand, and to reduce the awkwardness of a channel branded with the initials of another nation.
Official website
USA Today is an internationally distributed American daily middle-market newspaper that is the flagship publication of its owner, Gannett. Founded by Al Neuharth on September 15, 1982, it operates from Gannett's corporate headquarters in McLean, Virginia. It is printed at 37 sites across the United States and at five additional sites internationally. Its dynamic design influenced the style of local, regional, and national newspapers worldwide through its use of concise reports, colorized images, informational graphics, and inclusion of popular culture stories, among other distinct features.With a weekly print circulation of 726,906, a digital only subscriber base of 504,000, and an approximate daily readership of 2.6 million, USA Today is ranked first by circulation on the list of newspapers in the United States. It has been shown to maintain a generally centrist audience, in regards to political persuasion. USA Today is distributed in all 50 states, Washington, D.C., and Puerto Rico, and an international edition is distributed in Asia, Canada, Europe, and the Pacific Islands.
The genesis of USA Today was on February 29, 1980, when a company task force known as "Project NN" met with Gannett chairman Al Neuharth in Cocoa Beach, Florida to develop a national newspaper. Early regional prototypes included East Bay Today, an Oakland, California-based publication published in the late 1970s to serve as the morning edition of the Oakland Tribune, an afternoon newspaper which Gannett owned at the time. On June 11, 1981, Gannett printed the first prototypes of the proposed publication. The two proposed design layouts were mailed to newsmakers and prominent leaders in journalism, for review and feedback. Gannett's board of directors approved the launch of the national newspaper, titled USA Today, on December 5, 1981. At launch, Neuharth was appointed president and publisher of the newspaper, adding those responsibilities to his existing position as Gannett's chief executive officer.Gannett announced the launch of the paper on April 20, 1982. USA Today began publishing on September 15, 1982, initially in the Baltimore and Washington, D.C. metropolitan areas for a newsstand price of 25¢ (equivalent to 66¢ today). After selling out the first issue, Gannett gradually expanded the national distribution of the paper, reaching an estimated circulation of 362,879 copies by the end of 1982, double the amount of sales that Gannett projected. The design uniquely incorporated color graphics and photographs. Initially, only its front news section pages were rendered in four-color, while the remaining pages were printed in a spot color format. The paper's overall style and elevated use of graphics – developed by Neuharth, in collaboration with staff graphics designers George Rorick, Sam Ward, Suzy Parker, John Sherlock and Web Bryant – was derided by critics, who referred to it as a "McPaper" or "television you can wrap fish in", because it opted to incorporate concise nuggets of information more akin to the style of television news, rather than in-depth stories like traditional newspapers, which many in the newspaper industry considered to be a dumbing down of content. Although USA Today had been profitable for just ten years as of 1997, it changed the appearance and feel of newspapers around the world.On July 2, 1984, the newspaper switched from predominantly black-and-white to full color photography and graphics in all four sections. The next week on July 10, USA Today launched an international edition intended for U.S. readers abroad, followed four months later on October 8 with the rollout of the first transmission via satellite of its international version to Singapore. On April 8, 1985, the paper published its first special bonus section, a 12-page section called "Baseball '85", which previewed the 1985 Major League Baseball season.By the fourth quarter of 1985, USA Today had become the second-largest newspaper in the United States, reaching a daily circulation of 1.4 million copies. Total daily readership of the paper by 1987 (according to Simmons Market Research Bureau statistics) had reached 5.5 million, the largest of any daily newspaper in the U.S. On May 6, 1986, USA Today began production of its international edition in Switzerland. USA Today operated at a loss for most of its first four years of operation, accumulating a total deficit of $233 million after taxes, according to figures released by Gannett in July 1987; the newspaper began turning its first profit in May 1987, six months ahead of Gannett corporate revenue projections.On January 29, 1988, USA Today published the largest edition in its history, a 78-page weekend edition featuring a section previewing Super Bowl XXII; the edition included 44.38 pages of advertising and sold 2,114,055 copies, setting a single-day record for an American newspaper (and surpassed seven months later on September 2, when its Labor Day weekend edition sold 2,257,734 copies). On April 15, USA Today launched a third international printing site, based in Hong Kong. The international edition set circulation and advertising records during August 1988, with coverage of the 1988 Summer Olympics, selling more than 60,000 copies and 100 pages of advertising.By July 1991, Simmons Market Research Bureau estimated that USA Today had a total daily readership of nearly 6.6 million, an all-time high and the largest readership of any daily newspaper in the United States. On September 1, 1991, USA Today launched a fourth printsite for its international edition in London for the United Kingdom and the British Isles. The international edition's schedule was changed as of April 1, 1994 to Monday through Friday, rather than from Tuesday through Saturday, in order to accommodate business travelers; on February 1, 1995, USA Today opened its first editorial bureau outside the United States at its Hong Kong publishing facility; additional editorial bureaus were launched in London and Moscow in 1996.On April 17, 1995, USA Today launched its website, www.usatoday.com to provide real-time news coverage; in June 2002 the site expanded to include USATODAY.com Travel, providing travel information and booking tools. On August 28, 1995, a fifth international publishing site was launched in Frankfurt, Germany, to print and distribute the international edition throughout most of Europe.On October 4, 1999, USA Today began running advertisements on its front page for the first time. In 2017, some pages of USA Today's website features Auto-Play functionality for video or audio-aided stories. On February 8, 2000, Gannett launched USA Today Live, a broadcast and Internet initiative designed to provide coverage from the newspaper to broadcast television stations nationwide for use in their local newscasts and their websites; the venture also provided integration with the USA Today website, which transitioned from a text-based format to feature audio and video clips of news content.The paper launched a sixth printing site for its international edition on May 15, 2000, in Milan, Italy, followed on July 10 by the launch of an international printing facility in Charleroi, Belgium.In 2001, two interactive units were launched: on June 19, USA Today and Gannett Newspapers launched the USA Today Careers Network (now Careers.com), a website featuring localized employment listings, then on July 18, the USA Today News Center was launched as an interactive television news service developed through a joint venture with the On Command Corporation that was distributed to hotels around the United States. On September 12 of that year, the newspaper set an all-time single day circulation record, selling 3,638,600 copies for its edition covering the September 11 attacks. That November, USA Today migrated its operations from Gannett's previous corporate headquarters in Arlington, Virginia to the company's new headquarters in nearby McLean.On December 12, 2005, Gannett announced that it would combine the separate newsroom operations of the online and print entities of USA Today, with USAToday.com's vice president and editor-in-chief Kinsey Wilson promoted to co-executive editor, alongside existing executive editor John Hillkirk.In December 2010, USA Today launched the USA Today API for sharing data with partners of all types.
On August 27, 2010, USA Today announced that it would undergo a reorganization of its newsroom, announcing the layoffs of 130 staffers. It also announced that the paper would shift its focus away from print and place more emphasis on its digital platforms (including USAToday.com and its related mobile applications) and launch of a new publication called USA Today Sports. On January 24, 2011, to reverse a revenue slide, the paper introduced a tweaked format that modified the appearance of its front section pages, which included a larger logo at the top of each page; coloring tweaks to section front pages; a new sans-serif font, called Prelo, for certain headlines of main stories (replacing the Gulliver typeface that had been implemented for story headers in April 2000); an updated "Newsline" feature featuring larger, "newsier" headline entry points; and the increasing and decreasing of mastheads and white space to present a cleaner style.
On September 14, 2012, USA Today underwent the first major redesign in its history, in commemoration for the 30th anniversary of the paper's first edition. Developed in conjunction with brand design firm Wolff Olins, the print edition of USA Today added a page covering technology stories and expanded travel coverage within the Life section and increased the number of color pages included in each edition, while retaining longtime elements. The "globe" logo used since the paper's inception was replaced with a new logo featuring a large circle rendered in colors corresponding to each of the sections, serving as an infographic that changes with news stories, containing images representing that day's top stories.The paper's website was also extensively overhauled using a new, in-house content management system known as Presto and a design created by Fantasy Interactive, that incorporates flipboard-style navigation to switch between individual stories (which obscure most of the main and section pages), clickable video advertising and a responsive design layout. The site was designed to be more interactive, provide optimizations for mobile and touchscreen devices, provide "high impact" advertising units, and provide the ability for Gannett to syndicate USA Today content to the websites of its local properties, and vice versa. To accomplish this goal, Gannett migrated its newspaper and television station websites to the Presto platform and the USA Today site design throughout 2013 and 2014 (although archive content accessible through search engines remains available through the pre-relaunch design).
On October 6, 2013, Gannett test launched a condensed daily edition of USA Today (part of what was internally known within Gannett as the "Butterfly" initiative) for distribution as an insert in four of its newspapers – The Indianapolis Star, the Rochester Democrat & Chronicle, the Fort Myers-based The News-Press and the Appleton, Wisconsin-based The Post-Crescent. The launch of the syndicated insert caused USA Today to restructure its operations to allow seven-day-a-week production to accommodate the packaging of its national and international news content and enterprise stories (comprising about 10 pages for the weekday and Saturday editions, and up to 22 pages for the Sunday edition) into the pilot insert. Gannett later announced on December 11, that it would formally launch the condensed daily edition of USA Today in 31 additional local newspapers nationwide through April 2014 (with the Palm Springs, California-based The Desert Sun and the Lafayette, Louisiana-based Advertiser being the first newspapers outside of the pilot program participants to add the supplement on December 15), citing "positive feedback" to the feature from readers and advertisers of the initial four papers. Gannett was given permission from the Alliance for Audited Media to count the circulation figures from the syndicated local insert with the total circulation count for the flagship national edition of USA Today.On January 4, 2014, USA Today acquired the consumer product review website Reviewed. In the first quarter of 2014, Gannett launched a condensed USA Today insert into 31 other newspapers in its network, thereby increasing the number of inserts to 35, in an effort to shore up circulation after it regained its position as the highest circulated week daily newspaper in the United States in October 2013. On September 3, 2014, USA Today announced that it would lay off roughly 70 employees in a restructuring of its newsroom and business operations. In October 2014, USA Today and OpenWager Inc. entered into a partnership to release a Bingo mobile app called USA TODAY Bingo Cruise.On December 3, 2015, Gannett formally launched the USA Today Network, a national digital newsgathering service providing shared content between USA Today and the company's 92 local newspapers throughout the United States as well as pooling advertising services on both a hyperlocal and national reach. The Louisville Courier-Journal had earlier soft-launched the service as part of a pilot program started on November 17, coinciding with an imaging rebrand for the Louisville, Kentucky-based newspaper; Gannett's other local newspaper properties, as well as those it acquired through its merger with the Journal Media Group, gradually began identifying themselves as part of the USA Today Network (foregoing use of the Gannett name outside of requisite ownership references) through early January 2016.
USA Today is known for synthesizing news down to easy-to-read-and-comprehend stories. In the main edition circulated in the United States and Canada, each edition consists of four sections: News (the oft-labeled "front page" section), Money, Sports, and Life. Since March 1998, the Friday edition of Life has been separated into two distinct sections: the regular Life focusing on entertainment (subtitled Weekend; section E), which features television reviews and listings, a DVD column, film reviews and trends, and a travel supplement called Destinations & Diversions (section D). The international edition of the paper features two sections: News and Money in one; with Sports and Life in the other. Atypical of most daily newspapers, the paper does not print on Saturdays and Sundays; the Friday edition serves as the weekend edition (although USA Today has published special Saturday and Sunday editions in the past, the first being published on January 19, 1991, when it released a Saturday "Extra" edition updating coverage of the Gulf War from the previous day; the paper published special seven-day-a-week editions for the first time on July 19, 1996, when it published special editions for exclusive distribution in the host city of Atlanta and surrounding areas for the two-week duration of the 1996 Summer Olympics). USA Today prints each complete story on the front page of the respective section with the exception of the cover story. The cover story is a longer story that requires a jump (readers must turn to another page in the paper to complete the story, usually the next page of that section). On certain days, the news or sports section will take up two paper sections, and there will be a second cover story within the second section. Each section is denoted by a certain color to differentiate sections beyond lettering and is seen in a box the top-left corner of the first page; the principal section colors are blue for News (section A), green for Money (section B), red for Sports (section C), and purple for Life (section D); in the paper's early years, the Life and Money sections were also assigned blue nameplates and spot color, as the presses used at USA Today' printing facilities did not yet accommodate the use of other colors to denote all four original sections. Orange is used for bonus sections (section E or above), which are published occasionally such as for business travel trends and the Olympics; other bonus sections for sports (such as for the PGA Tour preview, NCAA Basketball Tournaments, Memorial Day auto races (Indianapolis 500 and Coca-Cola 600), NFL opening weekend and the Super Bowl) previously used the orange color, but now use the red designated for sports in their bonus sections. To increase their ties to USA Today, Gannett incorporated the USA Today coloring scheme into an internally created graphics package for news programming that the company began phasing in across its television station group – which were spun-off in July 2015 into the separate broadcast and digital media company Tegna – in late 2012 (the package utilizes the color scheme for a rundown graphic used on most stations – outside those that Gannett acquired in 2014 from London Broadcasting, which began implementing the package in late 2015 – that persists throughout its stations' newscasts, as well as bumpers for individual story topics). Gannett's television stations began to a new on-air appearance that uses a color-coding system identical to that of the paper. In many ways, USA Today is set up to break the typical newspaper layout. Some examples of that divergence from tradition include using the left-hand quarter of each section as reefers (front-page paragraphs referring to stories on inside pages), sometimes using sentence-length blurbs to describe stories inside; the lead reefer is the cover page feature "Newsline", which shows summarized descriptions of headline stories featured in all four main sections and any special sections. As a national newspaper, USA Today cannot focus on the weather for any one city. Therefore, the entire back page of the News section is used for weather maps for the continental United States, Puerto Rico and the United States Virgin Islands, and temperature lists for many cities throughout the U.S. and the world (temperatures for individual cities on the primary forecast map and temperature lists are suffixed with a one- or two-letter code, such as "t" for thunderstorms, referencing the expected weather conditions); the colorized forecast map, originally created by staff designer George Rorick (who left USA Today for a similar position at The Detroit News in 1986), was copied by newspapers around the world, breaking from the traditional style of using monochrome contouring or simplistic text to denote temperature ranges. National precipitation maps for the next three days (previously five days until the 2012 redesign), and four-day forecasts and air quality indexes for 36 major U.S. cities (originally 16 cities prior to 1999) – with individual cities color-coded by the temperature contour corresponding to the given area on the forecast map – are also featured. Weather data is provided by AccuWeather, which has served as the forecast provider for USA Today for most of the paper's existence (with an exception from January 2002 to September 2012, when The Weather Channel provided data through a long-term multimedia content agreement with Gannett). In the bottom left-hand corner of the weather page is "Weather Focus", a graphic which explains various meteorological phenomena. On some days, the Weather Focus could be a photo of a rare meteorological event. On Mondays, the Money section uses its back page for "Market Trends", a feature that launched in June 2002 and presents an unusual graphic depicting the performance of various industry groups as a function of quarterly, monthly, and weekly movements against the S&P 500. On days featuring bonus sections or business holidays, the Money and Life sections are usually combined into one section, while combinations of the Friday Life editions into one section are common during quiet weeks. Advertising coverage is seen in the Monday Money section, which often includes a review of a current television ad, and after Super Bowl Sunday, a review of the ads aired during the broadcast with the results of the Ad Track live survey. Stock tables for individual stock exchanges (comprising one subsection for companies traded on the New York Stock Exchange, and another for companies trading on NASDAQ and the American Stock Exchange) and mutual indexes were discontinued with the 2012 redesign due to the myriad of electronic ways to check individual stock prices, in line with most newspapers. Book coverage, including reviews and a national sales chart (the latter of which debuted on October 28, 1994), is seen on Thursdays in Life, with the official full A.C. Nielsen television ratings chart printed on Wednesdays or Thursdays, depending on release. The paper also publishes the Mediabase survey for several genres of music, based on radio airplay spins on Tuesdays, along with their own chart of the top ten singles in general on Wednesdays. Because of the same limitations cited for its nationalized forecasts, the television page in Life – which provides prime time and late night listings (running from 8:00 p.m. to 12:30 a.m. Eastern Time Zone) – incorporates a boilerplate "Local news" or "Local programming" descriptions to denote time periods in which the five major English language broadcast networks (ABC, NBC, CBS, Fox and The CW) cede airtime to allow their affiliates to carry syndicated programs or local newscasts; the television page has never been accompanied by a weekly listings supplement with broader scheduling information similar to those featured in local newspapers. Like most national papers, USA Today does not carry comic strips. One of the staples of the News section is "Across the USA", a state-by-state roundup of headlines. The summaries consist of paragraph-length Associated Press reports highlighting one story of note in each state, the District of Columbia, and one U.S. territory. Similarly, the "For the Record" page of the Sports section (which features sports scores for both the previous four days of league play and individual non-league events, seasonal league statistics and wagering lines for the current day's games). The page previously featured a rundown of winning numbers from the previous deadline date for all participating state lotteries and individual multi-state lotteries. Some traditions have been retained. The lead story still appears on the upper-right hand of the front page. Commentary and political cartoons occupy the last few pages of the News section. Stock and mutual fund data are presented in the Money section. But USA Today is sufficiently different in aesthetics to be recognized on sight, even in a mix of other newspapers, such as at a newsstand. The overall design and layout of USA Today has been described as neo-Victorian.Also, in most of the sections' front pages, on the lower left-hand corner, are "USA Today Snapshots", which give statistics of various lifestyle interests according to the section it is in (for example, a snapshot in "Life" could show how many people tend to watch a certain genre of television show based upon the type of mood they are in at the time). These "Snapshots" are shown through graphs that are made up of various illustrations of objects that roughly pertain to the graphs subject matter (using the example above, the graph's bars could be made up of several TV sets, or ended by one). These are usually loosely based on research by a national institute (with the credited source mentioned in fine print in the box below the graph). The newspaper also features an occasional magazine supplement called Open Air, which launched on March 7, 2008 and appears several times a year. Various other advertorials appear throughout the year, mainly on Fridays.
The opinion section prints USA Today editorials, columns by guest writers and members of the Editorial Board of Contributors, letters to the editor, and editorial cartoons. One unique feature of the USA Today editorial page is the publication of opposing points of view; alongside the editorial board's piece on the day's topic runs an opposing view by a guest writer, often an expert in the field. The opinion pieces featured in each edition are decided by the Board of Contributors, which are separate from the paper's news staff.From 1999 to 2002 and from 2004 to 2015, the editorial page editor was Brian Gallagher, who has worked for the newspaper since its founding in 1982. Other members of the Editorial Board included deputy editorial page editor Bill Sternberg, executive forum editor John Siniff, op-ed/forum page editor Glen Nishimura, operations editor Thuan Le Elston, letters editor Michelle Poblete, web content editor Eileen Rivers, and editorial writers Dan Carney, George Hager, and Saundra Torry. The newspaper's website calls this group "demographically and ideologically diverse."Beginning with the 1984 United States presidential election, USA Today has traditionally maintained a policy not to endorse candidates for the President of the United States or any other state or federal political office, which has been since re-evaluated by the paper's Board of Contributors through an independent process during each four-year election cycle, with any decision to circumvent the policy based on a consensus vote in which fewer than two of the editorial board's members dissent or hold differing opinions. For most of its history, the paper's political editorials (most of them linked to the then-current Presidential election cycle) had focused instead on providing opinion on major issues based on the differing concerns of voters, the vast amount of information on these themes, and the board's aim to provide a fair viewpoint through the diverse political ideologies of its members and avoid reader perceptions of bias. Such avoidance of doing political editorials played a great part in USA Today's long-standing reputation for "fluff", but after its 30th anniversary revamp, the paper took a more active stance on political issues, calling for stronger gun laws after the Sandy Hook Elementary School shooting in 2012. It heavily criticized the Republican Party for both the 2013 government shutdown and the 2015 revolts in the United States House of Representatives that ended with the resignation of John Boehner as House Speaker. It also called out then-President Barack Obama and other top members of the Democratic Party for what they perceived as "inaction" over several issues during 2013–14, particularly over the NSA scandal and the ISIL beheading incidents. The editorial board broke from the "non-endorsement" policy for the first time on September 29, 2016, when it published an op-ed piece condemning the candidacy of Republican nominee Donald Trump, calling him "unfit for the presidency" due to his inflammatory campaign rhetoric (particularly that aimed at the press, with certain media organizations being openly targeted and even banned from campaign rallies, including The New York Times, The Washington Post, CNN and the BBC, military veterans who had been prisoners of war, including 2008 Republican presidential candidate and Vietnam War veteran John McCain, immigrants, and various ethnic and religious groups); his temperament and lack of financial transparency; his "checkered" business record; his use of false and hyperbolic statements; the inconsistency of his viewpoints and issues with his vision on domestic and foreign policy; and, based on comments he has made during his campaign and criticisms by both Democrats and Republicans on these views, the potential risks to national security and constitutional ethics under a Trump administration, asking voters to "resist the siren song of a dangerous demagogue". The board noted that the piece was not a "qualified endorsement" of Democratic nominee Hillary Clinton, for whom the board was unable to reach a consensus for endorsing (some editorial board members expressed that Clinton's public service record would help her "serve the nation ably as its president", while others had "serious reservations about [her] sense of entitlement, [...] lack of candor and [...] extreme carelessness in handling classified information"), endorsing instead tactical voting against Trump and GOP seats in swing states, advising voters to decide whether to vote for either Clinton, Libertarian nominee Gary Johnson, Green Party nominee Jill Stein or a write-in candidate for President; or focus on Senate, House and other down-ballot political races.In February 2018, USA Today stirred controversy by publishing an op-ed by Jerome Corsi, the DC bureau chief for the fringe conspiracy website InfoWars. Corsi, a prominent conspiracy theorist, was described by USA Today as an "author" and "investigative journalist". Corsi was a prominent proponent of the false conspiracy theory that Barack Obama was not a US citizen, and Infowars has promoted conspiracy theories such as 9/11 being an inside job and the Sandy Hook massacre being a hoax staged by child actors.In October 2018, USA Today was criticized for publishing an editorial by President Trump that was replete with inaccuracies. The Washington Post fact-checker said that "almost every sentence contained a misleading statement or a falsehood." In 2020, USA Today endorsed a specific presidential candidate for the first time, endorsing Democrat nominee Joe Biden. The newspaper also published an opposing editorial by Vice President Mike Pence, which called for the re-election of Trump.
In May 2012, Larry Kramer – a 40-year media industry veteran and former president of CBS Digital Media – was appointed president and publisher of USA Today, replacing David Hunke, who had been publisher of the newspaper since 2009. Kramer was tasked with developing a new strategy for the paper as it sought to increase revenue from its digital operations.In July 2012, Kramer hired David Callaway – whom the former had hired as lead editor of MarketWatch in 1999, two years after Kramer founded the website – as the paper's editor-in-chief. Callaway had previously worked at Bloomberg News covering the banking, investment-banking and asset-management businesses throughout Europe and at the Boston Herald, where he co-wrote a daily financial column on "comings and goings in the Boston business district". Conservative activist Peter Gemma has written more than 100 op-ed pieces for USA Today.The current Editor-in-Chief is Nicole Carroll, who has served since February 2018.
Bill Sternberg David Mastio Jill Lawrence – see Politics Daily Dan Carney Thuan Le Elston Josh Rivera Eileen Rivers Saundra Torry – also active in the Reporters Committee for Freedom of the Press since 2000
USA Weekend was a sister publication that launched in 1953 as Family Weekly, a national Sunday magazine supplement intended for the Sunday editions of various U.S. newspapers; it adopted its final title following Gannett's purchase of the magazine in 1985. The magazine – which was distributed to approximately 800 newspapers nationwide at its peak with most Gannett-owned local newspapers carrying it by default within their Sunday editions – focused primarily on social issues, entertainment, health, food and travel. On December 5, 2014, Gannett announced that it would cease publishing USA Weekend after the December 26–28, 2014 edition, citing increasing operational costs and reduced advertising revenue, with most of its participating newspapers choosing to replace it with competing Sunday magazine Parade.
USA Today Sports Weekly is a weekly magazine that covers news and statistics from Major League Baseball, Minor League Baseball and NCAA baseball, the National Football League (NFL) and NASCAR. It was first published on April 5, 1991 as USA Today Baseball Weekly, a tabloid-sized baseball-focused publication released on Wednesdays, on a weekly basis during the baseball season and bi-weekly during the off-season; the magazine expanded its sports coverage on September 4, 2002, when it adopted its current title after added stories about the NFL. Sports Weekly added coverage of NASCAR on February 15, 2006, lasting only during that year's race season; and added coverage of NCAA college football on August 8, 2007. The editorial operations of Sports Weekly originally operated autonomously from USA Today, before being integrated with the newspaper's sports department in late 2005.
In 1987, Gannett and producer/former NBC CEO Grant Tinker began developing a news magazine series for broadcast syndication that attempted to bring the breezy style of USA Today to television. The result was USA Today: The Television Show (later retitled USA Today on TV, then shortened to simply USA Today), which premiered on September 12, 1988. Correspondents on the program included Edie Magnus, Robin Young, Boyd Matson, Kenneth Walker, Dale Harimoto, Ann Abernathy, Bill Macatee and Beth Ruyak. As with the newspaper itself, the show was divided into four "sections" corresponding to the different parts of the paper: News (focusing on the major headlines of the day), Money (focusing on financial news and consumer reports), Sports (focusing on sports news and scores) and Life (focusing on entertainment and lifestyle-related stories). The series was plagued by low ratings and negative reviews from critics throughout its run. The program also suffered from being scheduled in undesirable timeslots in certain markets; this was a particular case in New York City, the country's largest media market, where CBS owned-and-operated station WCBS-TV (channel 2) aired the program in a pre-dawn early morning slot, before the program was picked up by NBC O&O WNBC five months into its run; after initially airing it in an equally undesirable 5:30 a.m. slot, the series was later moved to a more palatable 9:30 a.m. time period, but still did not fare any better on its new station (in contrast, CITY-TV in Toronto, Ontario, Canada [now the flagship station of the Citytv television network], ran it at 5:00 p.m.). Although the series was renewed for a second season, these setbacks led to the mid-season cancellation of the TV version of USA Today in November 1989, after one-and-a-half seasons; the final edition aired on January 7, 1990.Gannett announced plans to develop a USA Today-branded weekly half-hour television program, to have been titled Sports Page, as part of a renewed initiative to extend the brand into television; this program, which was tapped for a fall 2004 debut, ultimately never launched.
VRtually There is a weekly virtual reality news program produced by the USA Today Network, which debuted on October 20, 2016. The program, which is available on the USA Today mobile app and on YouTube (which maintains content exclusivity through the program's dedicated channel for 60 days after each broadcast), showcases three original segments outlining news stories through a first-person perspective, recorded and produced by journalists from USA Today and its co-owned local newspapers. The program also incorporates "cubemercials", long-form advertisements created by Gannett's in-house creative studio GET Creative, which are designed to allow consumer engagenent in fully immersive experiences through virtual reality.
USA Today Minor League Player of the Year Award – First presented in 1988, this annual award has been given to a particular Minor league baseball player judged to have had the most outstanding season by a thirteen-person panel of baseball experts. USA Today All-USA high school baseball team – First presented in 1998, the award honors between nine and eleven outstanding baseball players from high schools around the United States to be part on the team (separate awards honoring the High School Baseball Player of the Year and High School Baseball Coach of the Year have been given since 1989). USA Today All-USA high school basketball team – First presented in 1983, the award honors outstanding male and female basketball players from high schools around the United States with a place on the team, with one member of each team being named as the High School Basketball Player of the Year as well as coaches from a select boys' and girls' team as the High School Basketball Coach of the Year. USA Today All-Joe Team (NFL) – First presented in 1992 in tribute to Kansas City Chiefs veteran defensive lineman Joe Phillips, the award honors 52 rookie players from throughout the NFL for their exemplary performance during the previous league season. USA Today/National Prep Poll High School Football National Championship – Predating the first publication of USA Today under the sole decision of the National Prep Poll, it is a national championship honor awarded to the best high school football team(s) in the United States, based on rankings decided by the newspaper's sports editorial department. USA Today All-USA high school football team – First presented in 1982, the award honors outstanding football players from high schools around the United States (includes ranks for the Super 25 teams in the U.S. and Top 10 teams in the East, South, Midwest and West, and USA Today High School Football Player of the Year).USA Today High School Football Coach of the Year – First presented in 1982, the award awards a coach from one of the teams selected for the All-USA football team for the honor. USA TODAY Road Warrior of the Year first presented to Joyce Gioia in 2013; never presented again.
USA Today Super Bowl Ad Meter Viewtron
Official website
According to the legends, Usha was daughter of Banasura, powerful asura king of Sonitpur. Banasura was a great devotee of the Lord Shiva and had 1000 arms. Just like her father Usha was also devote of Shiva and Parvati. Once Usha asked Parvati who her husband would be. Parvati replied "In the moth of Baishakh a person would be appear in your dreams. That would eventually be your husband." One day, Usha saw a young prince in her dream and fell in love with him. That prince was Aniruddha, the grandson of Lord Krishna and son of Pradyumna. Usha's friend Chitralekha, through supernatural powers abducted Aniruddha from the Dwarka and brought him to Usha.When Krishna got to know about it, he came with a huge army and attacked Banasura's kingdom Sonitpur. Bana also attacked Krishna’s army with equal might, but he began to feel powerless in front of Lord Krishna. Thus, he evoked Lord Shiva to take his side. At this point, Shiva joined the battle against Krishna because he had promised protection to Banasura. Krishna himself defeated Shiva with a weapon that put Shiva to sleep. After that Krishna cut Bana's thousand arms systematically. Seeing Krishna releasing his Sudarshan Chakra to sever his head, Shiva aroused from his slumber, approached Krishna and asked him to spare Bana’s life. After that Krishna forgave Bana.After the war, Usha married Anirudha. Later Usha gave birth to a son named Vajra.
According to Shiva Purana, Usha and Annirudha reborn again as Behula and Lakshindar in next life and married each other again.
The story of Aniruddha and Usha (as Okha in Gujarati) is depicted in the 18th century Gujarati Akhyana entitled Okhaharan by Premanand Bhatt.A 1901 Telugu language play titled Usha Parinayam written by Vedam Venkataraya Sastry was based on story of Usha. The play was also taken as a Telugu film in 1961 by Kadaru Nagabhushanam under Rajarajeswari films.
Vegetarian bacon, also referred to as veggie bacon, vacon, or facon, is a product marketed as a bacon alternative. It is high in protein and fiber, yet low in fat, and has no cholesterol. Two slices average 75 calories. Brands include Morningstar Farms and Smart Bacon.Vegetarian bacon can also be made at home by marinating strips of tempeh or tofu in various flavorings, such as soy sauce or liquid smoke, and then either frying or baking. Aficionados of raw food also use coconut meat as a bacon substitute. Seitan can also be formed into vegetarian bacon.Food writer David Goldbeck suggests frying provolone cheese in a skillet to produce a bacon substitute he calls "cheeson".
List of meat substitutes Food portal
Voice of America (VOA) is a state-controlled international television and radio network funded by the U.S. federal tax budget. It is the largest U.S. international broadcaster. VOA produces digital, TV, and radio content in 47 languages which it distributes to affiliate stations around the globe. It is primarily viewed by foreign audiences, so VOA programming has an influence on public opinion abroad regarding the United States and its people.VOA was established in 1942, and the VOA charter (Public Laws 94-350 and 103-415) was signed into law in 1976 by President Gerald Ford. VOA is headquartered in Washington, D.C., and overseen by the U.S. Agency for Global Media, an independent agency of the U.S. government. Funds are appropriated annually by Congress under the budget for embassies and consulates. In 2016, VOA broadcast an estimated 1,800 hours of radio and TV programming each week to approximately 236.6 million people worldwide with about 1,050 employees and a taxpayer-funded annual budget of US$218.5 million.Some commentators consider Voice of America to be a form of propaganda.In response to the request of the United States Department of Justice that RT register as a foreign agent under the Foreign Agents Registration Act, Russia's Justice Ministry labeled Voice of America and Radio Free Europe/Radio Liberty as foreign agents in December 2017.
The Voice of America website had five English language broadcasts as of 2014 (worldwide, Special English, Cambodia, Zimbabwe and Tibet). Additionally, the VOA website has versions in 46 foreign languages (radio programs are marked with an asterisk; TV programs with a plus symbol and icon ): The number of languages varies according to the priorities of the United States government and the world situation.
Before World War II, all American shortwave stations were in private hands. Privately controlled shortwave networks included the National Broadcasting Company's International Network (or White Network), which broadcast in six languages, the Columbia Broadcasting System's Latin American international network, which consisted of 64 stations located in 18 different countries, the Crosley Broadcasting Corporation in Cincinnati, Ohio, and General Electric which owned and operated WGEO and WGEA, both based in Schenectady, New York, and KGEI in San Francisco, all of which had shortwave transmitters. Experimental programming began in the 1930s, but there were fewer than 12 transmitters in operation. In 1939, the Federal Communications Commission set the following policy: A licensee of an international broadcast station shall render only an international broadcast service which will reflect the culture of this country and which will promote international goodwill, understanding and cooperation. Any program solely intended for, and directed to an audience in the continental United States does not meet the requirements for this service. This policy was intended to enforce the State Department's Good Neighbor Policy, but some broadcasters felt that it was an attempt to direct censorship.Shortwave signals to Latin America were regarded as vital to counter Nazi propaganda around 1940. Initially, the Office of Coordination of Information sent releases to each station, but this was seen as an inefficient means of transmitting news. The director of Latin American relations at the Columbia Broadcasting System was Edmund A. Chester, and he supervised the development of CBS's extensive "La Cadena de las Americas" radio network to improve broadcasting to South America during the 1940s.Also included among the cultural diplomacy programming on the Columbia Broadcasting System was the musical show Viva America (1942-1949) which featured the Pan American Orchestra and the artistry of several noted musicians from both North and South America, including Alfredo Antonini, Juan Arvizu, Eva Garza, Elsa Miranda, Nestor Mesta Chaires, Miguel Sandoval, John Serry Sr., and Terig Tucci. By 1945, broadcasts of the show were carried by 114 stations on CBS's "La Cadena de las Americas" network in 20 Latin American nations. These broadcasts proved to be highly successful in supporting President Franklin Roosevelt's policy of Pan-Americanism throughout South America during World War II.
Even before the Japanese attack on Pearl Harbor, the U.S. government's Office of the Coordinator of Information (COI, in Washington) had already begun providing war news and commentary to the commercial American shortwave radio stations for use on a voluntary basis through its Foreign Information Service (FIS, in New York) headed by playwright Robert E. Sherwood, the playwright who served as president Roosevelt’s speech writer and information advisor. Direct programming began a week after the United States’ entry into World War II in December 1941, with the first broadcast from the San Francisco office of the FIS via General Electric’s KGEI transmitting to the Philippines in English (other languages followed). The next step was to broadcast to Germany, which was called Stimmen aus Amerika ("Voices from America") and was transmitted on February 1, 1942. It was introduced by "The Battle Hymn of the Republic" and included the pledge: "Today, and every day from now on, we will be with you from America to talk about the war... The news may be good or bad for us – We will always tell you the truth." Roosevelt approved this broadcast, which then-Colonel William J. Donovan (COI) and Sherwood (FIS) had recommended to him. It was Sherwood who actually coined the term "The Voice of America" to describe the shortwave network that began its transmissions on February 1, from 270 Madison Avenue in New York City. The Office of War Information, when organized in the middle of 1942, officially took over VOA's operations. VOA reached an agreement with the British Broadcasting Corporation to share medium-wave transmitters in Britain, and expanded into Tunis in North Africa and Palermo and Bari, Italy as the Allies captured these territories. The OWI also set up the American Broadcasting Station in Europe.Asian transmissions started with one transmitter in California in 1941; services were expanded by adding transmitters in Hawaii and, after recapture, the Philippines.By the end of the war, VOA had 39 transmitters and provided service in 40 languages. Programming was broadcast from production centers in New York and San Francisco, with more than 1,000 programs originating from New York. Programming consisted of music, news, commentary, and relays of U.S. domestic programming, in addition to specialized VOA programming.About half of VOA's services, including the Arabic service, were discontinued in 1945. In late 1945, VOA was transferred to the Department of State.
In 1947, VOA started broadcasting to the Soviet citizens in Russia under the pretext of countering "more harmful instances of Soviet propaganda directed against American leaders and policies" on the part of the internal Soviet Russian-language media, according to John B. Whitton's treatise, Cold War Propaganda. The Soviet Union responded by initiating electronic jamming of VOA broadcasts on April 24, 1949.Charles W. Thayer headed VOA in 1948–49. Over the next few years, the U.S. government debated the best role of Voice of America. The decision was made to use VOA broadcasts as a part of its foreign policy to fight the propaganda of the Soviet Union and other countries. The Arabic service resumed on January 1, 1950, with a half-hour program. This program grew to 14.5 hours daily during the Suez Crisis of 1956, and was six hours a day by 1958.In 1952, Voice of America installed a studio and relay facility aboard a converted U.S. Coast Guard cutter renamed Courier whose target audience was Soviet Union and other members of Warsaw Pact. The Courier was originally intended to become the first in a fleet of mobile, radio broadcasting ships (see offshore radio) that built upon U.S. Navy experience during WWII in using warships as floating broadcasting stations. However, the Courier eventually dropped anchor off the island of Rhodes, Greece with permission of the Greek government to avoid being branded as a pirate radio broadcasting ship. This VOA offshore station stayed on the air until the 1960s when facilities were eventually provided on land. The Courier supplied training to engineers who later worked on several of the European commercial offshore broadcasting stations of the 1950s and 1960s. Control of VOA passed from the State Department to the U.S. Information Agency when the latter was established in 1953 to transmit worldwide, including to the countries behind the Iron Curtain and to the People's Republic of China (PRC). Starting in the 1950s, VOA broadcast American jazz on Voice of America Jazz Hour from 1955 until 2003. Hosted for most of that period by Willis Conover, the program had 30 million listeners at its peak. A program aimed at South Africa in 1956 broadcast two hours nightly, and special programs such as The Newport Jazz Festival were also transmitted. This was done in association with tours by U.S. musicians, such as Dizzy Gillespie, Louis Armstrong, and Duke Ellington, sponsored by the State Department. From August 1952 through May 1953, Billy Brown, a high school senior in Westchester County, New York, had a Monday night program in which he shared everyday happenings in Yorktown Heights, New York. Brown's program ended due to its popularity: his "chatty narratives" attracted so much fan mail, VOA couldn't afford the $500 a month in clerical and postage costs required to respond to listeners' letters.Throughout the Cold War, many of the targeted countries' governments sponsored jamming of VOA broadcasts, which sometimes led critics to question the broadcasts' actual impact. For example, in 1956, Polish People's Republic stopped jamming VOA transmissions, but People's Republic of Bulgaria continued to jam the signal through the 1970s. Chinese language VOA broadcasts were jammed beginning in 1956 and extending through 1976. However, after the collapse of the Warsaw Pact and the Soviet Union, interviews with participants in anti-Soviet movements verified the effectiveness of VOA broadcasts in transmitting information to socialist societies. The People's Republic of China diligently jams VOA broadcasts. Cuba has also been reported to interfere with VOA satellite transmissions to Iran from its Russian-built transmission site at Bejucal. David Jackson, former director of Voice of America, noted: "The North Korean government doesn't jam us, but they try to keep people from listening through intimidation or worse. But people figure out ways to listen despite the odds. They're very resourceful."Throughout the 1960s and 1970s, VOA covered some of the era's most important news, including Martin Luther King Jr.'s 1963 "I Have a Dream" speech and Neil Armstrong's 1969 first walk on the Moon. During the 1962 Cuban Missile Crisis, VOA broadcast around-the-clock in Spanish.In the early 1980s, VOA began a $1.3 billion rebuilding program to improve broadcast with better technical capabilities. Also in the 1980s, VOA also added a television service, as well as special regional programs to Cuba, Radio Martí and TV Martí. Cuba has consistently attempted to jam such broadcasts and has vociferously protested U.S. broadcasts directed at Cuba. In September 1980, VOA started broadcasting to Afghanistan in Dari and in Pashto in 1982. At the same time, VOA started to broadcast U.S. government editorials, clearly separated from the programming by audio cues. In 1985, VOA Europe was created as a special service in English that was relayed via satellite to AM, FM, and cable affiliates throughout Europe. With a contemporary format including live disc jockeys, the network presented top musical hits as well as VOA news and features of local interest (such as "EuroFax") 24 hours a day. VOA Europe was closed down without advance public notice in January 1997 as a cost-cutting measure. It was followed by VOA Express, which from July 4, 1999 revamped into VOA Music Mix. Since November 1, 2014 stations are offered VOA1 (which is a rebranding of VOA Music Mix). In 1989, Voice of America expanded its Mandarin and Cantonese programming to reach the millions of Chinese and inform the country about the pro-democracy movement within the country, including the demonstration in Tiananmen Square. Starting in 1990, the U.S. consolidated its international broadcasting efforts, with the establishment of the Bureau of Broadcasting.
With the breakup of the Soviet bloc in Eastern Europe, VOA added many additional language services to reach those areas. This decade was marked by the additions of Tibetan, Kurdish (to Iran and Iraq), Croatian, Serbian, Bosnian, Macedonian, and Rwanda-Rundi language services. In 1993, the Clinton administration advised cutting funding for Radio Free Europe/Radio Liberty as it was felt post-Cold War information and influence was not needed in Europe. This plan was not well received, and he then proposed the compromise of the International Broadcasting Act. The Broadcasting Board of Governors was established and took control from the Board for International Broadcasters which previously oversaw funding for RFE/RL.In 1994, President Bill Clinton signed the International Broadcasting Act into law. This law established the International Broadcasting Bureau as a part of the U.S. Information Agency and created the Broadcasting Board of Governors with oversight authority. In 1998, the Foreign Affairs Reform and Restructuring Act was signed into law and mandated that BBG become an independent federal agency as of October 1, 1999. This act also abolished the U.S.I.A. and merged most of its functions with those of the State Department. In 1994, Voice of America became the first broadcast-news organization to offer continuously updated programs on the Internet. In April 2020, the Trump administration accused Voice of America of being a mouthpiece for authoritarian regimes that "speaks for America’s adversaries," and of "promoting propaganda" instead of "promoting freedom and democracy."
The Arabic Service was abolished in 2002 and replaced by a new radio service, called the Middle East Radio Network or Radio Sawa, with an initial budget of $22 million. Radio Sawa offered mostly Western and Middle Eastern popular songs with periodic brief news bulletins. Today, the network has expanded to television with Alhurra and to various social media and websites.On May 16, 2004; Worldnet, a satellite television service, was merged into the VOA network. Radio programs in Russian ended in July 2008. In September 2008, VOA eliminated the Hindi language service after 53 years. Broadcasts in Ukrainian, Serbian, Macedonian and Bosnian also ended. These reductions were part of American efforts to concentrate more resources to broadcast to the Muslim world.In September 2010, VOA began its radio broadcasts in Sudan. As U.S. interests in South Sudan have grown, there is a desire to provide people with free information.In 2013, VOA ended foreign language transmissions on shortwave and medium wave to Albania, Georgia, Iran and Latin America; as well as English language broadcasts to the Middle East and Afghanistan. The movement was done due to budget cuts.On July 1, 2014, VOA cut most of its shortwave transmissions in English to Asia. Shortwave broadcasts in Azerbaijani, Bengali, Khmer, Kurdish, Lao, and Uzbek were dropped too. On August 11, 2014, the Greek service ended after 72 years on air.
1941–1942 Robert E. Sherwood (Foreign Information Service)1942–1943 John Houseman 1943–1945 Louis G. Cowan 1945–1946 John Ogilvie 1948–1949 Charles W. Thayer 1949–1952 Foy D. Kohler 1952–1953 Alfred H. Morton 1953–1954 Leonard Erikson 1954–1956 John R. Poppele 1956–1958 Robert E. Burton 1958–1965 Henry Loomis 1965–1967 John Chancellor 1967–1968 John Charles Daly 1969–1977 Kenneth R. Giddens 1977–1979 R. Peter Straus 1980–1981 Mary Bitterman 1981–1982 James B. Conkling 1982 John Hughes 1982–1984 Kenneth Tomlinson 1985 Gene Pell 1986–1991 Dick Carlson 1991–1993 Chase Untermeyer 1994–1996 Geoffrey Cowan 1997–1999 Evelyn S. Lieberman 1999–2001 Sanford J. Ungar 2001–2002 Robert R. Reilly 2002–2006 David S. Jackson 2006–2011 Danforth W. Austin 2011–2015 David Ensor 2016 - 2020 Amanda Bennett 2020–Present Michael Pack
Voice of America has been a part of several agencies. From its founding in 1942 to 1945, it was part of the Office of War Information, and then from 1945 to 1953 as a function of the State Department. VOA was placed under the U.S. Information Agency in 1953. When the USIA was abolished in 1999, VOA was placed under the Broadcasting Board of Governors, or BBG, which is an autonomous U.S. government agency, with bipartisan membership. The Secretary of State has a seat on the BBG. The BBG was established as a buffer to protect VOA and other U.S.-sponsored, non-military, international broadcasters from political interference. It replaced the Board for International Broadcasting (BIB) that oversaw the funding and operation of Radio Free Europe/Radio Liberty, a branch of VOA.
From 1948 until its amendment in 2013, Voice of America was forbidden to broadcast directly to American citizens under § 501 of the Smith–Mundt Act. The act was amended as a result of the passing of the Smith-Mundt Modernization Act provision of the National Defense Authorization Act for 2013. The intent of the legislation in 1948 was to protect the American public from propaganda actions by their own government and to have no competition with private American companies. The amendment had the intent of adapting to the Internet and allow American citizens to request access to VOA content.
Under the Eisenhower administration in 1959, VOA Director Henry Loomis commissioned a formal statement of principles to protect the integrity of VOA programming and define the organization's mission, and was issued by Director George V. Allen as a directive in 1960 and was endorsed in 1962 by USIA director Edward R. Murrow. The principles were signed into law on July 12, 1976, by President Gerald Ford. It reads: The long-range interests of the United States are served by communicating directly with the peoples of the world by radio. To be effective, the Voice of America must win the attention and respect of listeners. These principles will therefore govern Voice of America (VOA) broadcasts. 1. VOA will serve as a consistently reliable and authoritative source of news. VOA news will be accurate, objective, and comprehensive. 2. VOA will represent America, not any single segment of American society, and will therefore present a balanced and comprehensive projection of significant American thought and institutions. 3. VOA will present the policies of the United States clearly and effectively, and will also present responsible discussions and opinion on these policies.
The Voice of America Firewall was put in place with the 1976 VOA Charter and laws passed in 1994 and 2016 as a way of ensuring the integrity of VOA's journalism. This policy fights against propaganda and promotes unbiased and objective journalistic standards in the agency. The charter is one part of this firewall and the other laws assist in ensuring high standards of journalism.
According to former VOA correspondent Alan Heil, the internal policy of VOA News is that any story broadcast must have two independently corroborating sources or have a staff correspondent witness an event.
Voice of America's central newsroom has hundreds of journalists and dozens of full-time domestic and overseas correspondents, who are employees of the U.S. government or paid contractors. They are augmented by hundreds of contract correspondents and stringers throughout the world, who file in English or in one of VOA's other radio and television broadcast languages. In late 2005, VOA shifted some of its central-news operation to Hong Kong where contracted writers worked from a "virtual" office with counterparts on the overnight shift in Washington, D.C., but this operation was shut down in early 2008.
By December 2014, the number of transmitters and frequencies used by VOA had been greatly reduced. VOA still uses shortwave transmissions to cover some areas of Africa and Asia. Shortwave broadcasts still take place in these languages: Afaan Oromoo, Amharic, Bambara, Cantonese, Chinese, English, Indonesian, Korean and Swahili.
VOA Radiogram was an experimental Voice of America program starting in March 2013 which transmitted digital text and images via shortwave radiograms. There were 220 editions of the program, transmitted each weekend from the Edward R. Murrow transmitting station. The audio tones that comprised the bulk of each 30 minute program were transmitted via an analog transmitter, and could be decoded using a basic AM shortwave receiver with freely downloadable software of the Fldigi family. This software is available for Windows, Apple (OSX), Linux, and FreeBSD systems. Broadcasts can also be decoded using the free TIVAR app from the Google Play store using any Android device. The mode used most often on VOA Radiogram, for both text and images, was MFSK32, but other modes were also occasionally transmitted. The final edition of VOA Radiogram was transmitted during the weekend of June 17–18, 2017, a week before the retirement of the program producer from VOA. An offer to continue the broadcasts on a contract basis was declined, so a follow-on show called Shortwave Radiogram began transmission on June 25, 2017 from the WRMI transmitting site in Okeechobee, Florida. Shortwave Radiogram program schedule
One of VOA's radio transmitter facilities was originally based on a 625-acre (2.53 km2) site in Union Township (now West Chester Township) in Butler County, Ohio, near Cincinnati. The site is now a recreational park with a lake, lodge, dog park, and Voice of America museum. The Bethany Relay Station operated from 1944 to 1994. Other former sites include California (Dixon, Delano), Hawaii, Okinawa, (Monrovia) Liberia, Costa Rica, Belize, and at least two in Greece.Between 1983 and 1990, VOA made significant upgrades to transmission facilities in Botswana, Morocco, Thailand, Kuwait, and Sao Tome.Currently, VOA and USAGM continue to operate shortwave radio transmitters and antenna farms at International Broadcasting Bureau Greenville Transmitting Station in the United States, close to Greenville, North Carolina, "Site B." They do not use FCC-issued callsigns, since the FCC does not regulate communications by other federal government agencies. (The FCC regulates broadcasting by private companies and other businesses, state governments, nonprofit organizations [NPOs] and non-government organizations [NGOs], and private individuals.) The IBB also operates a transmission facility on São Tomé and (Tinang) Concepcion, Tarlac, Philippines for VOA.
In late September 2001, VOA aired a report that contained brief excerpts of an interview with then Taliban leader Mullah Omar Mohammad, along with segments from President Bush's post-9/11 speech to Congress, an expert in Islam from Georgetown University, and comments by the foreign minister of Afghanistan's anti-Taliban Northern Alliance. State Department officials including Richard Armitage and others argued that the report amounted to giving terrorists a platform to express their views. In response, reporters and editors argued for the VOA's editorial independence from its governors. VOA received praise from press organizations for its protests, and the following year in 2002, it won the University of Oregon's Payne Award for Ethics in Journalism.
On April 2, 2007, Abdul Malik Rigi, the leader of Jundullah, a militant group with possible links to al-Qaeda, appeared on Voice of America's Persian language service. VOA introduced Rigi as "the leader of popular Iranian resistance movement." The interview resulted in public condemnation by the Iranian-American community, as well as the Iranian government. Jundullah is a militant organization that has been linked to numerous attacks on civilians, such as the 2009 Zahedan explosion.
In February 2013, a documentary released by China Central Television interviewed a Tibetan self-immolator who failed to kill himself. The interviewee said he was motivated by Voice of America's broadcasts of commemorations of people who committed suicide in political self-immolation. VOA denied any allegations of instigating self-immolations and demanded that the Chinese station retract its report.
After the inauguration of US President Donald Trump, several tweets by Voice of America (one of which was later removed) seemed to support the widely criticized statements by White House press secretary Sean Spicer about the crowd size and biased media coverage. This first raised concerns over possible attempts by Trump to politicize the state-funded agency. This amplified already growing propaganda concerns over the provisions in the National Defense Authorization Act for Fiscal Year 2017, signed into law by Barack Obama, which replaced the board of the Broadcasting Board of Governors with a CEO appointed by the president. Trump sent two of his political aides, Matthew Ciepielowski and Matthew Schuck, to the agency to aid its current CEO during the transition to the Trump administration. Criticism was raised over Trump's choice of aides; Schuck was a staff writer for right-wing website The Daily Surge until April 2015, while Ciepielowski was a field director at the conservative advocacy group Americans for Prosperity. VOA officials responded with assurances that they would not become "Trump TV". BBG head John F. Lansing told NPR that it would be illegal for the administration to tell VOA what to broadcast, while VOA director Amanda Bennett stressed that while "government-funded", the agency is not "government-run".On April 10, 2020, the White House published an article in its daily newsletter critical of VOA coverage of the coronavirus pandemic. Emails revealed in a Freedom of Information Act request showed Centers for Disease Control and Prevention (CDC) press official Michawn Rich had sent a memo to agency employees stating in part, "as a rule, do not send up [interview] requests for Greta Van Susteren or anyone affiliated with Voice of America," referencing the White House story. On April 30, the Washington Post reported Vice President Mike Pence's office "threatened to retaliate against a reporter who revealed that Pence’s office had told journalists they would need masks for Pence’s visit to the Mayo Clinic — a requirement Pence himself did not follow."On June 3, 2020, the Senate confirmed Michael Pack, a maker of conservative documentaries and close ally of Steve Bannon, to serve as head of the United States Agency for Global Media, which oversees VOA. Subsequently, Director Bennet and deputy director Sandy Sugawara resigned from VOA. CNN reported on June 16 that plans for a leadership shakeup at VOA were being discussed, including the possibility that controversial former White House aide Sebastian Gorka would be given a leadership role at VOA. On June 17, the heads of VOA's Middle East Broadcasting, Radio Free Asia, Radio Free Europe/Radio Liberty and the Open Technology Fund were all fired, their boards were dissolved and external communications from VOA employees made to require approval from senior agency personnel in what one source described as an "unprecedented" move, while Jeffrey Shapiro, like Pack a Bannon ally, was rumored to be in line to head the Office of Cuba Broadcasting. Four former members of the advisory boards subsequently filed suit challenging Pack's standing to fire them. On July 9, NPR reported VOA would not renew the work visas of dozens of non-resident reporters, many of whom could face repercussions in their home countries. In late July, four contracters and the head of VOA's Urdu language service were suspended after a video featuring extensive clips from a Muslim-American voter conference, including a campaign message from Democratic Presidential candidate Joe Biden, was determined not to meet editorial standards and taken down.On August 12, 2020, USAGM chief financial officer Grant Turner and general counsel David Kligerman were removed from their positions and stripped of their security clearances, reportedly for their opposition to what Turner called "gross mismanagement," along with four other senior agency officials. Politico reported on August 13 that Trump administration official and former shock jock Frank Wuco had been hired as a USAGM senior advisor, responsible for auditing the agency's office of policy and research. As a radio host, Wuco issued insults and groundless claims against former US President Barack Obama, CIA Director John O. Brennan and Speaker of the House Nancy Pelosi. VOA's Twitter account during this period featured stories favorable to Vice President Mike Pence and White House advisor Ivanka Trump.In response to Pack's August 27 interview with The Federalist website in which he "joked...about deporting his own employees and forcing them to adopt unsafe workplace practices that could expose them to COVID-19" and "said the agency was ripe for espionage and possibly rife with spies," a group of VOA journalists sent a letter to VOA Acting Director Elez Biberaj complaining that his "comments and decisions 'endanger the personal security of VOA reporters at home and abroad, as well as threatening to harm U.S. national security objectives.'" VOA's response was that that "it would not respond directly to the letter because it was 'improper' and 'failed to follow procedure.' Instead, the leadership of USAGM and VOA 'are handling the choice of complaint transmission as an administrative issue,' which suggested that the journalists could face sanctions for their letter," according to the Washington Post. In the same story, the Post reported that VOA Spanish-language service White House correspondent's Brigo Segovia's interview with an official about the administration's response to Pack's personnel and other moves had been censored and his own access to VOA's computer system restricted.On September 29, six senior USAGM officials filed a whistleblower complaint in which they alleged that Pack or one of his aides had ordered research conducted into the voting history of at least one agency employee, which would be a violation of laws protecting civil servants from undue political influence. NPR reported that two Pack aides had compiled a report on VoA White House bureau chief Steven L. Herman's social media postings and other writings in an attempt to charge him with a conflict of interest, and that the agency released a conflict of interest policy stating in part that a "journalist who on Facebook 'likes' a comment or political cartoon that aggressively attacks or disparages the President must recuse themselves from covering the President."Suspended officials from Voice of America sued the agency news outlet on October 8. They accused its chief operating officer, Michael Pack, of using Voice of America as a vehicle to promote the personal agenda of President Trump and of violating a statutory firewall intended to prevent political interference with the agency, and they are seeking their reinstatement.
On April 19, 2017, VOA interviewed the Chinese real estate tycoon Guo Wengui in a live broadcast. The whole interview was scheduled for 3 hours. After Guo Wengui alleged to own evidence of corruption among the members of the Politburo Standing Committee of China, the highest political authority of China, the interview was abruptly cut off, after only one hour and seventeen minutes of broadcasting. Guo's allegations involved Fu Zhenhua and Wang Qishan, the latter being a member of the Politburo Standing Committee and the leader of the massive anti-graft movement. It was reported that Beijing warned VOA's representatives not to interview Guo for his "unsubstantiated allegations". Four members of the U.S. Congress requested the Office of Inspector General to conduct an investigation into this interruption on August 27, 2017. The OIG investigation concluded that the decision to curtail the Guo interview was based solely on journalistic best practices rather than any pressure from the Chinese government.Another investigation, by Mark Feldstein, Chair of Broadcast Journalism at the University of Maryland, College Park and a journalist with decades of experiences as an award-winning television investigative reporter, concluded that "The failure to comply with leadership’s instructions during the Guo interview "was a colossal and unprecedented violation of journalistic professionalism and broadcast industry standards." The report also said that "There had been a grossly negligent approach" to pre-interview vetting and failure to "corroborate the authenticity of Guo’s evidence or interview other sources" in violation of industry standards. The interview team apparently "demonstrated greater loyalty to its source than to its employer — at the expense of basic journalistic standards of accuracy, verification, and fairness," the Feldstein report concluded.
The VOA started its operations during the Cold War and that is when its influence first started as well. Foy Kohler, the director of VOA during the Cold War, strongly believed that the VOA was serving its purpose, which he identified as aiding in the fight against communism. He argued that the numbers of listeners they were getting such as 194,000 regular listeners in Sweden, and 2.1 million regular listeners in France, was an indication of a positive impact. As further evidence, he noted that the VOA received 30,000 letters a month from listeners all over the world, and hundreds of thousands of requests for broadcasting schedules. There was an analysis done of some of those letters sent in 1952 and 1953 while Kohler was still director. The study found that letter writing could be an indicator of successful, actionable persuasion. It was also found that broadcasts in different countries were having different effects. In one country, regular listeners adopted and practiced American values presented by the broadcast. Age was also a factor: younger and older audiences tended to like different types of programs no matter the country. Kohler used all of this as evidence to claim that the VOA helped to grow and strengthen the free world. It also influenced the UN in their decision to condemn communist actions in Korea, and was a major factor in the decline of communism in the "free world, including key countries such as Italy and France. In Italy, the VOA did not just bring an end to communism, but it caused the country to Americanize. The VOA also had an impact behind the Iron Curtain. Practically all defectors during Kohler's time claimed the VOA helped in their decision to defect. Another indication of impact, according to Kohler, was the Soviet response. Kohler argued that the soviets responded because the VOA was having an impact. Based on Soviet responses, it can be presumed that the most effective programs were ones that compared the lives of those behind and outside the iron curtain, questions on the practice of slave labor, as well as lies and errors in Stalin’s version of Marxism.
DEEWA Radio, of the VOA, airs in Pakistan. Although some listeners are suspicious that the program is promoting an American agenda, others claim to be experiencing a positive effect. Some listeners feel that the programs are giving a voice to the voiceless, leading them to a sense of empowerment.
VOA's service in Iran has had a negative impact on Kurds and Kurdistan according to the publication, Kurdish Life. They claim that the VOA has exacerbated the conflict between the Talabani and the Barzani. They further claim that the VOA is covering up wrongful imprisonments, wrongful arrests, and the building of extremist mosques. According to the same publication, Kurds are being turned into fanatics, and a new generation of terrorists is forming because of the VOA. They claim the VOA is doing this to help PUK.
There is evidence to suggest that the people who listen to the Latin American service are being influenced, but not in the way the VOA wants. Instead of understanding and adopting the American way of life, listeners are parroting values and beliefs that do not mesh with their lives. However, others have adopted a negative view of America, because they think that the VOA is propaganda.
A study was done on Chinese students in America. It found that through the VOA, they disapproved of the actions of the Chinese government. Another study was done on Chinese scholars in America, and found that the VOA had an effect on their political beliefs. Their political beliefs did not change in relation to China, though, as they did not tend to believe the VOA's reports on China.
International broadcasting Alhurra BBC World Service Deutsche Welle France 24 Propaganda in the United States State media Radio Free Europe/Radio Liberty Radio Free Asia Radio Rebelde Russia Today TV Telesur Voice of America Indonesia VOA Hausa VOA people Frank Shozo Baba Willis Conover George Kao
Official website
