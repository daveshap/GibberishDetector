{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WikipediaDataBuilder.ipynb",
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "14s-XRO8bo2I0R1iqD1dDdyx5Uc7Wc7cD",
      "authorship_tag": "ABX9TyP+mBv4cFX7Qv3kOmB9JFvM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daveshap/GibberishDetector/blob/main/WikipediaDataBuilder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXwauI4_4_VH"
      },
      "source": [
        "# Download Articles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUqvg2ce5tAE"
      },
      "source": [
        "!pip install wikipedia --quiet\n",
        "!pip install spacy --quiet\n",
        "!pip install pysbd --quiet\n",
        "\n",
        "import wikipedia\n",
        "\n",
        "gdrive_dir = '/content/drive/My Drive/WikiData/'\n",
        "\n",
        "# todo: come up with a cool way to automatically create topic search terms\n",
        "keywords = ['india', 'ocean', 'astronomy', 'economics', 'economy', 'earth', \n",
        "            'english', 'bacon', 'egg', 'dinosaur', 'rabbit', 'america', 'usa',\n",
        "            'congress', 'virus', 'George Clooney', 'knowledge', 'Buddha']\n",
        "\n",
        "def save_article(filepath, content):\n",
        "  print('Saving:', filepath)\n",
        "  with open(filepath, 'w', encoding='utf-8') as outfile:\n",
        "    outfile.write(content)\n",
        "\n",
        "for keyword in keywords:\n",
        "  #print('Searching Wikipedia for keyword:', keyword)\n",
        "  try:\n",
        "    search = wikipedia.search(keyword)\n",
        "    for result in search:\n",
        "      article = wikipedia.page(result)\n",
        "      filepath = '%sarticle_%s.txt' % (gdrive_dir, result)\n",
        "      save_article(filepath, article.content)\n",
        "      exit(0)\n",
        "  except Exception as oops:\n",
        "    continue\n",
        "print('Done saving articles!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9KOoQu-_4eu"
      },
      "source": [
        "# Parse Articles\n",
        "The articles need to be split up into usable chunks. This uses regex to identify the section headers and split each article into single lines of text for each section. Furthermore, it looks at the number of word characters vs other characters to identify those sections that likely contain text instead of tables or other data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPyoXJkhABpH"
      },
      "source": [
        "import os \n",
        "import re\n",
        "\n",
        "result = list()\n",
        "gdrive_dir = '/content/drive/My Drive/WikiData/'\n",
        "outfile = '%sparsed_sections.txt' % gdrive_dir\n",
        "\n",
        "for file in os.listdir(gdrive_dir):\n",
        "  #print(file)\n",
        "  if not 'article_' in file:\n",
        "    continue\n",
        "  with open(gdrive_dir + file, 'r', encoding='utf-8') as infile:\n",
        "    text = infile.read()\n",
        "  sections = re.split(r'={2,}.{0,80}={2,}', text)\n",
        "  for section in sections:\n",
        "    try:\n",
        "      trimmed = section.strip()\n",
        "      wordchars = re.findall(r'\\w', trimmed)\n",
        "      ratio = len(wordchars) / len(trimmed)\n",
        "      # it seems like a ratio of greater than 80% word chars is ideal\n",
        "      if ratio > 0.80:\n",
        "        final = re.sub(r'\\s+', ' ', trimmed)\n",
        "        result.append(final.strip())\n",
        "    except:\n",
        "      continue\n",
        "  \n",
        "print('Wikipedia sections parsed:', len(result))\n",
        "with open(outfile, 'w', encoding='utf-8') as file:\n",
        "  for line in result:\n",
        "    file.write(line+'\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9R27VsKBGKF"
      },
      "source": [
        "# Split Sentences\n",
        "For the sake of simplicity, we don't want to go overboard and evaluate entire paragraphs. We want to only train on individual sentences. So let's use SpaCy and PYSBD (Python Sentence Boundary Detector) to split the corpus into sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlEJaDTQBkw9",
        "outputId": "3302d8f5-3d6f-4380-ba6c-9243cd8ddce0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import spacy\n",
        "from pysbd.utils import PySBDFactory\n",
        "\n",
        "nlp = spacy.blank('en')\n",
        "nlp.add_pipe(PySBDFactory(nlp))\n",
        "gdrive_dir = '/content/drive/My Drive/WikiData/'\n",
        "infile = '%sparsed_sections.txt' % gdrive_dir\n",
        "outfile = '%swiki_sentences.txt' % gdrive_dir\n",
        "result = list()\n",
        "\n",
        "with open(infile, 'r', encoding='utf-8') as file:\n",
        "  lines = file.readlines()\n",
        "\n",
        "print('Lines of text:', len(lines))\n",
        "for line in lines:\n",
        "  doc = nlp(line)\n",
        "  for sent in list(doc.sents):\n",
        "    result.append(sent)\n",
        "\n",
        "print('Sentences found:', len(result))\n",
        "with open(outfile, 'w', encoding='utf-8') as file:\n",
        "  for line in result:\n",
        "    if str(line) == '':\n",
        "      continue\n",
        "    file.write(str(line)+'\\n')\n",
        "print(outfile, 'saved!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Lines of text: 1623\n",
            "Sentences found: 14901\n",
            "/content/drive/My Drive/WikiData//wiki_sentences.txt saved!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ay4yrVXwCdLs"
      },
      "source": [
        "# Generate Gibberish v1 - Word Salad\n",
        "Shuffle all words around to to make a word salad."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uz8BeEYOC14m",
        "outputId": "c32f1c8f-173c-442f-d15b-9be4bb338450",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from random import shuffle, seed\n",
        "\n",
        "gdrive_dir = '/content/drive/My Drive/WikiData/'\n",
        "infile = '%swiki_sentences.txt' % gdrive_dir\n",
        "outfile = '%sshuffled_words.txt' % gdrive_dir\n",
        "result = list()\n",
        "\n",
        "def scramble_sentence(sentence):\n",
        "  sentence = sentence.strip()\n",
        "  split = sentence.split()\n",
        "  shuffle(split)\n",
        "  return ' '.join(split)\n",
        "\n",
        "seed()\n",
        "with open(infile, 'r', encoding='utf-8') as file:\n",
        "  lines = file.readlines()\n",
        "for line in lines:\n",
        "  line = line.strip()\n",
        "  if line == '':\n",
        "    continue\n",
        "  scrambled = scramble_sentence(line)\n",
        "  result.append(scrambled)\n",
        "with open(outfile, 'w', encoding='utf-8') as file:\n",
        "  for line in result:\n",
        "    file.write(line+'\\n')\n",
        "print(outfile, 'saved!')        "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/WikiData/shuffled_words.txt saved!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r29uugV1DPN_"
      },
      "source": [
        "# Generate Gibberish v2 - Random Characters\n",
        "Shuffle all characters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enJxLvqqDgIV",
        "outputId": "aa2170d7-0aeb-41ac-dd43-a782cfa1b3dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from random import shuffle, seed\n",
        "\n",
        "gdrive_dir = '/content/drive/My Drive/WikiData/'\n",
        "infile = '%swiki_sentences.txt' % gdrive_dir\n",
        "outfile = '%sshuffled_characters.txt' % gdrive_dir\n",
        "result = list()\n",
        "\n",
        "def scramble_sentence(sentence):\n",
        "  sentence = sentence.strip()\n",
        "  sentence = list(sentence)\n",
        "  shuffle(sentence)\n",
        "  return ''.join(sentence)\n",
        "\n",
        "seed()\n",
        "with open(infile, 'r', encoding='utf-8') as file:\n",
        "  lines = file.readlines()\n",
        "for line in lines:\n",
        "  line = line.strip()\n",
        "  if line == '':\n",
        "    continue\n",
        "  scrambled = scramble_sentence(line)\n",
        "  result.append(scrambled)\n",
        "with open(outfile, 'w', encoding='utf-8') as file:\n",
        "  for line in result:\n",
        "    file.write(line+'\\n')\n",
        "print(outfile, 'saved!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/WikiData/shuffled_characters.txt saved!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PMqx4pA72TD"
      },
      "source": [
        "# Generate Gibberish v3 - Needle in Haystack\n",
        "Replace one or two words with random words. Much harder to detect. Probably need something like wordnet to find random words. It has a dictionary right? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "279Fs_o68F1c",
        "outputId": "42738e84-b414-464b-fab8-28ab83d99a23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import re\n",
        "from random import sample,seed\n",
        "\n",
        "gdrive_dir = '/content/drive/My Drive/WikiData/'\n",
        "infile = '%swiki_sentences.txt' % gdrive_dir\n",
        "outfile = '%smild_gibberish.txt' % gdrive_dir\n",
        "result = list()\n",
        "\n",
        "with open('%s10k_words.txt' % gdrive_dir, 'r') as file:\n",
        "  all_words = file.readlines()\n",
        "\n",
        "def random_word():\n",
        "  seed()\n",
        "  return sample(all_words, 1)[0].strip()\n",
        "\n",
        "def replace_one_word(sentence):\n",
        "  if len(sentence) < 21:\n",
        "    return sentence\n",
        "  words = re.findall(r'\\w+', sentence)\n",
        "  word = sample(words, 1)[0]\n",
        "  rando = random_word()\n",
        "  return sentence.replace(word,rando)\n",
        "\n",
        "#sentence = 'the wheel weaves as the wheel wills'\n",
        "#replace_one_word(sentence)\n",
        "\n",
        "with open(infile, 'r', encoding='utf-8') as file:\n",
        "  lines = file.readlines()\n",
        "for line in lines:\n",
        "  line = line.strip()\n",
        "  if line == '':\n",
        "    continue\n",
        "  mild_gibberish = replace_one_word(line)\n",
        "  mild_gibberish = replace_one_word(mild_gibberish)\n",
        "  result.append(mild_gibberish)\n",
        "with open(outfile, 'w', encoding='utf-8') as file:\n",
        "  for line in result:\n",
        "    file.write(line+'\\n')\n",
        "print(outfile, 'saved!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/WikiData/mild_gibberish.txt saved!\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}