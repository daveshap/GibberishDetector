{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "GibberishDetector.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1qpQ04hvt5QJEpT4A4A2NqrtRyGh-d9T1",
      "authorship_tag": "ABX9TyP8hXckzez8R+zKuTU+8eaT",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daveshap/GibberishDetector/blob/main/GibberishDetector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Co4argYdIeWP"
      },
      "source": [
        "# Install Requirements\n",
        "\n",
        "- Run with GPU environment! GPU is way faster than TPU\n",
        "- Click `Runtime >> Change runtime type >> GPU`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvOjTgcvImAY"
      },
      "source": [
        "!pip install tensorflow-gpu==1.15.0 --quiet\n",
        "!pip install gpt-2-simple --quiet "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGbkx02QEJEE"
      },
      "source": [
        "# Compile Training Corpus\n",
        "\n",
        "- Use the [WikipediaDataBuilder](https://github.com/daveshap/GibberishDetector/blob/main/WikipediaDataBuilder.ipynb) notebook to create the base date for compiling the corpus\n",
        "- Data is also available in the [GibberishDetector GitHub repo](https://github.com/daveshap/GibberishDetector)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rG9X9sD9EaRk"
      },
      "source": [
        "from random import sample, seed\n",
        "\n",
        "gdrive_dir = '/content/drive/My Drive/WikiData/'\n",
        "\n",
        "files = [  # source, label\n",
        "('%swiki_sentences.txt' % gdrive_dir, 'clean'), \n",
        "('%sshuffled_characters.txt' % gdrive_dir, 'noise'),\n",
        "('%sshuffled_words.txt' % gdrive_dir, 'word salad'),\n",
        "('%smild_gibberish.txt' % gdrive_dir, 'mild gibberish'),\n",
        "]\n",
        "\n",
        "# train\n",
        "result = list()\n",
        "max_samples = 3000\n",
        "corpus = 'corpus.txt' \n",
        "\n",
        "# test\n",
        "test_samples = 50\n",
        "test_corpus = 'test_corpus.txt'\n",
        "\n",
        "\n",
        "for file in files:\n",
        "  with open(file[0], 'r', encoding='utf-8') as infile:\n",
        "    lines = infile.readlines()\n",
        "  for line in lines:\n",
        "    line = line.strip()\n",
        "    if line == '':\n",
        "      continue\n",
        "    line = line.lower().replace('.', '')  # this will make it harder to cheat\n",
        "    line = '<|SENTENCE|> %s <|LABEL|> %s <|END|>' % (line, file[1])\n",
        "    result.append(line)\n",
        "\n",
        "\n",
        "# save train set\n",
        "\n",
        "seed()\n",
        "subset = sample(result, max_samples)\n",
        "\n",
        "with open(corpus, 'w', encoding='utf-8') as outfile:\n",
        "  for line in subset:\n",
        "    outfile.write(line+'\\n\\n')\n",
        "print(corpus, 'saved!')\n",
        "\n",
        "\n",
        "# save test set\n",
        "\n",
        "seed()\n",
        "subset = sample(result, test_samples)\n",
        "\n",
        "with open(test_corpus, 'w', encoding='utf-8') as outfile:\n",
        "  for line in subset:\n",
        "    outfile.write(line+'\\n\\n')\n",
        "print(test_corpus, 'saved!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbtvF1mgwJAx"
      },
      "source": [
        "# Load Model\n",
        "Let's use Google Drive to store the model for persistence. We will want to fine tune the model iteratively to get better and better performance. We will also want to use the model again later after pouring so much work into it!\n",
        "\n",
        "Information about [download_gpt2 function here](https://github.com/minimaxir/gpt-2-simple/blob/92d35962d9aaeadba70e39d11d040f1e377ffdb3/gpt_2_simple/gpt_2.py#L64)\n",
        "\n",
        "### Model Sizes\n",
        "- `124M`\n",
        "- `355M`\n",
        "- `774M`\n",
        "- `1558M`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHmW7pU-wey0"
      },
      "source": [
        "import gpt_2_simple as gpt2\n",
        "\n",
        "# note: manually mount your google drive in the file explorer to the left\n",
        "\n",
        "model_dir = '/content/drive/My Drive/GPT2/models'\n",
        "checkpoint_dir = '/content/drive/My Drive/GPT2/checkpoint'\n",
        "model_name = '355M'\n",
        "\n",
        "gpt2.download_gpt2(model_name=model_name, model_dir=model_dir)\n",
        "print('\\n\\nModel is ready!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UqT0KH-EykD"
      },
      "source": [
        "# Finetune GPT2\n",
        "\n",
        "[Finetune function here](https://github.com/minimaxir/gpt-2-simple/blob/92d35962d9aaeadba70e39d11d040f1e377ffdb3/gpt_2_simple/gpt_2.py#L127)\n",
        "\n",
        "- Rerun for subsequent training sessions\n",
        "- Click on `Runtime >> Restart and run all`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5P5VZm4-eec"
      },
      "source": [
        "file_name = 'corpus.txt'\n",
        "run_name = 'GibberishDetector'\n",
        "model_dir = '/content/drive/My Drive/GPT2/models'\n",
        "checkpoint_dir = '/content/drive/My Drive/GPT2/checkpoint'\n",
        "model_name = '355M'\n",
        "step_cnt = 2000\n",
        "\n",
        "sess = gpt2.start_tf_sess()\n",
        "\n",
        "gpt2.finetune(sess,\n",
        "              dataset=file_name,\n",
        "              model_name=model_name,\n",
        "              model_dir=model_dir,\n",
        "              checkpoint_dir=checkpoint_dir,\n",
        "              steps=step_cnt,\n",
        "              restore_from='fresh',  # start from scratch\n",
        "              #restore_from='latest',  # continue from last work\n",
        "              run_name=run_name,\n",
        "              print_every=50,\n",
        "              sample_every=1000,\n",
        "              save_every=1000\n",
        "              )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WT0DKLLTp0mp"
      },
      "source": [
        "# Test Results\n",
        "\n",
        "It's not science if you don't write down your results! Using delim tags works way better so I'm just deleting results that were utter garbage. Use these tags people! They seem to help orient GPT-2 enough to understand the pattern you want to output.\n",
        "\n",
        "- `<|SENTENCE|>`\n",
        "- `<|LABEL|>`\n",
        "- `<|END|>`\n",
        "\n",
        "### Data\n",
        "\n",
        "| Test | Model | Samples | Steps | Last Loss | Avg Loss | Accuracy | Evaluation |\n",
        "|---|---|---|---|---|---|---|---|\n",
        "|01|355M|5000|2000|0.36|2.46|5/9| Mostly good, created some random labels, came unglued a couple times|\n",
        "|02|355M|5000|4000|0.27|1.64|0/9| Major regression in quality, not a single accurate label|\n",
        "|03|355M|5000|1500|1.73|2.75|5/9| Mostly good, reliably generates accurate labels, went random on a few examples|\n",
        "|04|355M|5000|2500|0.10|1.87|1/11|Many labels were literally `icky`|\n",
        "|05|355M|5000|1000|0.91|3.04|0/11|Mostly just spit out `END` with no labels|\n",
        "|06|355M|6000|2000|0.95|2.50|3/11|Mix of just `end` with some stuck on repeat|\n",
        "|07|355M|4000|2000|0.17|1.85|9/11|Best results so far!|\n",
        "|08|355M|3000|2000|0.17|1.32|10/11|Even better!|\n",
        "|09|355M|3000|2000|0.29|1.46|7/11|Repeating results, not as good|\n",
        "|10|355M|3500|2000|0.06|1.82|5/11|Less is more, apparently|\n",
        "|11|355M|2000|2000|0.12|0.86|1/11|Not enough|\n",
        "|12|355M|3000|1500|0.17|1.84|5/11|A little better|\n",
        "|13|355M|3000|2500|0.08|1.20|4/11|A little worse|\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNSJdqIM7ncn"
      },
      "source": [
        "test_corpus = 'test_corpus.txt'\n",
        "run_name = 'GibberishDetector'\n",
        "model_dir = '/content/drive/My Drive/GPT2/models'\n",
        "checkpoint_dir = '/content/drive/My Drive/GPT2/checkpoint'\n",
        "model_name = '355M'\n",
        "results = list()\n",
        "\n",
        "print('Loading test set...')\n",
        "with open(test_corpus, 'r', encoding='utf-8') as file:\n",
        "  test_set = file.readlines()\n",
        "\n",
        "# uncomment the following if fresh runtime\n",
        "#import gpt_2_simple as gpt2\n",
        "\n",
        "#print('Starting TF session')\n",
        "#sess = gpt2.start_tf_sess()\n",
        "#print('Loading GPT2 model')\n",
        "#gpt2.load_gpt2(sess, \n",
        "#               model_name=model_name,\n",
        "#               model_dir=model_dir,\n",
        "#               checkpoint_dir=checkpoint_dir,)\n",
        "\n",
        "for t in test_set:\n",
        "  t = t.strip()\n",
        "  if t == '':\n",
        "    continue\n",
        "  prompt = t.split('<|LABEL|>')[0] + '<|LABEL|>'\n",
        "  print('\\nPROMPT:', prompt)\n",
        "  response = gpt2.generate(sess, \n",
        "                           return_as_list=True,\n",
        "                           length=30,  # prevent it from going too crazy\n",
        "                           prefix=prompt,\n",
        "                           model_name=model_name,\n",
        "                           model_dir=model_dir,\n",
        "                           truncate='\\n',  # stop inferring here\n",
        "                           include_prefix=False,\n",
        "                           checkpoint_dir=checkpoint_dir,)[0]\n",
        "  response = response.strip()\n",
        "  print('RESPONSE:', response)\n",
        "\n",
        "print('\\n\\nModel:', model_name)\n",
        "print('Samples:', max_samples)\n",
        "print('Steps:', step_cnt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdNGztJtcG0q"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "- With the `355M` model, the best sample count seems to be 3000, with 2000 steps\n",
        "- Longer sentences, similar to the training data, tend to do better\n",
        "\n",
        "## Future Work\n",
        "\n",
        "- Try with larger models\n",
        "- Try with different data sources, like Gutenberg "
      ]
    }
  ]
}