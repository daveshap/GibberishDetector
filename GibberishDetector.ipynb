{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GibberishDetector.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1qpQ04hvt5QJEpT4A4A2NqrtRyGh-d9T1",
      "authorship_tag": "ABX9TyOvhCh3W0bzk5fYy5uHDmbF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daveshap/GibberishDetector/blob/main/GibberishDetector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Co4argYdIeWP"
      },
      "source": [
        "# Install Requirements\n",
        "Let's get this out of the way up front!\n",
        "\n",
        "**Note: Run with GPU environment!**\n",
        "\n",
        "> Click `Runtime >> Change runtime type >> GPU`\n",
        "\n",
        "I think GPU is way faster than TPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvOjTgcvImAY",
        "outputId": "1ca0f897-d8ea-4ee3-bfa8-3378675f46d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install tensorflow-gpu==1.15.0 --quiet\n",
        "!pip install gpt-2-simple --quiet "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 411.5MB 40kB/s \n",
            "\u001b[K     |████████████████████████████████| 3.8MB 56.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 512kB 45.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 51kB 7.7MB/s \n",
            "\u001b[?25h  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: tensorflow 2.3.0 has requirement gast==0.3.3, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.3.0 has requirement tensorboard<3,>=2.3.0, but you'll have tensorboard 1.15.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.3.0 has requirement tensorflow-estimator<2.4.0,>=2.3.0, but you'll have tensorflow-estimator 1.15.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow-probability 0.11.0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "  Building wheel for gpt-2-simple (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGbkx02QEJEE"
      },
      "source": [
        "# Compile Training Corpus\n",
        "\n",
        "> Note: Use the [WikipediaDataBuilder](https://github.com/daveshap/GibberishDetector/blob/main/WikipediaDataBuilder.ipynb) notebook to create the base date for compiling the corpus\n",
        "\n",
        "- 5,000 samples seems to do well\n",
        "- 15,000 samples is too many"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rG9X9sD9EaRk",
        "outputId": "9a455798-af44-44a8-dbf1-f6bafd93fed0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from random import sample, seed\n",
        "\n",
        "gdrive_dir = '/content/drive/My Drive/WikiData/'\n",
        "\n",
        "files = [  # source, label\n",
        "('%swiki_sentences.txt' % gdrive_dir, 'clean'), \n",
        "('%sshuffled_characters.txt' % gdrive_dir, 'noise'),\n",
        "('%sshuffled_words.txt' % gdrive_dir, 'word salad'),\n",
        "('%smild_gibberish.txt' % gdrive_dir, 'mild gibberish'),\n",
        "]\n",
        "\n",
        "result = list()\n",
        "max_samples = 5000  # the max here is the number of sentences from above\n",
        "corpus = 'corpus.txt' \n",
        "\n",
        "for file in files:\n",
        "  with open(file[0], 'r', encoding='utf-8') as infile:\n",
        "    lines = infile.readlines()\n",
        "  for line in lines:\n",
        "    line = line.strip()\n",
        "    if line == '':\n",
        "      continue\n",
        "    line = line.lower().replace('.', '')  # this will make it harder to cheat\n",
        "    line = '// %s || %s ' % (line, file[1])\n",
        "    result.append(line)\n",
        "\n",
        "seed()\n",
        "subset = sample(result, max_samples)\n",
        "\n",
        "with open(corpus, 'w', encoding='utf-8') as outfile:\n",
        "  for line in subset:\n",
        "    outfile.write(line+'\\n\\n')\n",
        "print(corpus, 'saved!')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "corpus.txt saved!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbtvF1mgwJAx"
      },
      "source": [
        "# Load Model\n",
        "Let's use Google Drive to store the model for persistence. We will want to fine tune the model iteratively to get better and better performance. We will also want to use the model again later after pouring so much work into it!\n",
        "\n",
        "Information about [download_gpt2 function here](https://github.com/minimaxir/gpt-2-simple/blob/92d35962d9aaeadba70e39d11d040f1e377ffdb3/gpt_2_simple/gpt_2.py#L64)\n",
        "\n",
        "### Model Sizes\n",
        "- `124M`\n",
        "- `355M`\n",
        "- `774M`\n",
        "- `1558M`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHmW7pU-wey0",
        "outputId": "74e3ed42-7f92-4803-a950-f858f1e607a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import gpt_2_simple as gpt2\n",
        "\n",
        "# note: manually mount your google drive in the file explorer to the left\n",
        "\n",
        "model_dir = '/content/drive/My Drive/GPT2/models'\n",
        "checkpoint_dir = '/content/drive/My Drive/GPT2/checkpoint'\n",
        "model_name = '355M'\n",
        "\n",
        "gpt2.download_gpt2(model_name=model_name, model_dir=model_dir)\n",
        "print('\\n\\nModel is ready!')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 217Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:00, 83.0Mit/s]                                                   \n",
            "Fetching hparams.json: 1.05Mit [00:00, 284Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 1.42Git [00:17, 82.2Mit/s]                                 \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 218Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:00, 86.8Mit/s]                                                \n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 128Mit/s]                                                       "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Model is ready!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UqT0KH-EykD"
      },
      "source": [
        "# Finetune GPT2\n",
        "This is where the rubber meets the road! Let's see if we can finetune a GPT-2 model! Obviously, the bigger the model, the better the results. But bigger models require more memory. There's a tradeoff between model size and corpus size. It looks like 355M is the largest model we can do for now. \n",
        "\n",
        "[Finetune function here](https://github.com/minimaxir/gpt-2-simple/blob/92d35962d9aaeadba70e39d11d040f1e377ffdb3/gpt_2_simple/gpt_2.py#L127)\n",
        "\n",
        "Run this repeatedly with more/different training data to get better results.\n",
        "\n",
        "Simplest way to continue training is to click `Runtime >> Restart and run all...`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5P5VZm4-eec",
        "outputId": "9b6202f3-5500-4c13-af47-9199f1024e72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "file_name = 'corpus.txt'\n",
        "run_name = 'GibberishDetector'\n",
        "model_dir = '/content/drive/My Drive/GPT2/models'\n",
        "checkpoint_dir = '/content/drive/My Drive/GPT2/checkpoint'\n",
        "model_name = '355M'\n",
        "\n",
        "sess = gpt2.start_tf_sess()\n",
        "\n",
        "gpt2.finetune(sess,\n",
        "              dataset=file_name,\n",
        "              model_name=model_name,\n",
        "              model_dir=model_dir,\n",
        "              checkpoint_dir=checkpoint_dir,\n",
        "              steps=1000,\n",
        "              restore_from='fresh',  # start from scratch\n",
        "              #restore_from='latest',  # continue from last work\n",
        "              run_name=run_name,\n",
        "              print_every=20,\n",
        "              sample_every=500,\n",
        "              save_every=500\n",
        "              )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/sample.py:17: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/memory_saving_gradients.py:62: get_backward_walk_ops (from tensorflow.contrib.graph_editor.select) is deprecated and will be removed after 2019-06-06.\n",
            "Instructions for updating:\n",
            "Please use tensorflow.python.ops.op_selector.get_backward_walk_ops.\n",
            "Loading checkpoint /content/drive/My Drive/GPT2/models/355M/model.ckpt\n",
            "INFO:tensorflow:Restoring parameters from /content/drive/My Drive/GPT2/models/355M/model.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading dataset...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:02<00:00,  2.52s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset has 273480 tokens\n",
            "Training...\n",
            "[10 | 15.53] loss=5.44 avg=5.44\n",
            "[20 | 20.84] loss=4.85 avg=5.14\n",
            "[30 | 26.12] loss=4.68 avg=4.99\n",
            "[40 | 31.42] loss=5.48 avg=5.11\n",
            "[50 | 36.72] loss=4.99 avg=5.09\n",
            "[60 | 42.02] loss=4.83 avg=5.04\n",
            "[70 | 47.27] loss=5.14 avg=5.06\n",
            "[80 | 52.56] loss=4.14 avg=4.94\n",
            "[90 | 57.84] loss=5.54 avg=5.01\n",
            "[100 | 63.11] loss=5.24 avg=5.03\n",
            "Saving /content/drive/My Drive/GPT2/checkpoint/GibberishDetector/model-100\n",
            "[110 | 76.89] loss=4.47 avg=4.98\n",
            "[120 | 82.24] loss=5.43 avg=5.02\n",
            "[130 | 87.58] loss=5.42 avg=5.05\n",
            "[140 | 92.94] loss=4.31 avg=5.00\n",
            "[150 | 98.30] loss=5.36 avg=5.02\n",
            "[160 | 103.64] loss=4.79 avg=5.01\n",
            "[170 | 109.00] loss=4.45 avg=4.97\n",
            "[180 | 114.30] loss=4.99 avg=4.97\n",
            "[190 | 119.60] loss=4.26 avg=4.93\n",
            "[200 | 124.88] loss=4.85 avg=4.93\n",
            "Saving /content/drive/My Drive/GPT2/checkpoint/GibberishDetector/model-200\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "======== SAMPLE 1 ========\n",
            "nsu ih c htsl rtrnsu u g e htttttdg n || noise \n",
            "\n",
            "// e r s u mt esa h hntf oa  e htt ud i e e r rte  re || noise \n",
            "\n",
            "// the united states continues to be among the most competitive industries in the world, with global manufacturing leading in capital, profits, and global supply and investment || clean \n",
            "\n",
            "// a kopi i || word salad \n",
            "\n",
            "// of and the and , that may be the and the and \"and are as and many\" of of a their with a as it may || word salad \n",
            "\n",
            "// a, as the and the and the to and is of the || word salad \n",
            "\n",
            "// the in-between has been termed the \"singularity\" || mild gibberish \n",
            "\n",
            "// the on a || word salad \n",
            "\n",
            "// and the || word salad \n",
            "\n",
            "// on top of the two and the two the the that the (and) are (and the are) the as || word salad \n",
            "\n",
            "// ntehvsehce  e ee r dnnhce oetnmee oes eaioa d,ns,tns e  wc) rr mn m n,hv oe y ia oa || noise \n",
            "\n",
            "// are not a very widespread belief, however, with the term \"mysticism\" popularly seen as a form of \"meditation\", and many practices including \"prayer\", \"meditation at the monasteries\", and \"meditation with \"meditation-studying\" are described as spiritual experiences || mild gibberish \n",
            "\n",
            "// he states that the united states and the united sultanate have been the two most powerful countries of the india for the next two centuries || clean \n",
            "\n",
            "// tb i gtt eo oo  n  mceet dtrcnti r oi mtsn b || noise \n",
            "\n",
            "// the orca was also a type of giant tortoise long considered extinct; its remains have been found, but with a larger margin that a the remains than that of any other known giant tortoise || clean \n",
            "\n",
            "// rt ee oa  l eer i n oe  eeirce cds ntseti  ee hvwotp hs e ntse  hns,wssn  dn,t ntr oi s t oe d nn-taetr || noise \n",
            "\n",
            "// the is a the the, one the the of states || word salad \n",
            "\n",
            "// a an a as t,r, oe b  e,a ht,eh,r,aiaa eiaoaiapodt eenp lm r s || noise \n",
            "\n",
            "// the the is the and is a central, \"state\"-level economic sector; economic policy is central to economic policymaking || mild gibberish \n",
            "\n",
            "// the is an india-led consortium conducting the exploration for methane on sargasso sea and in the polar ocean (the icelandic and north-coastal is the current geographical location of the southernmost point on the antarctic continent), with five members: geologists, marine biologists, geochemists, geophysicists, and chemists || clean \n",
            "\n",
            "// the and the or has been the the one the of and has been as || word salad \n",
            "\n",
            "// and has led many as countries, including the is sri, in a series of attempts under the the the maharashtra assembly government || mild gibberish \n",
            "\n",
            "// the indian and the the || word salad \n",
            "\n",
            "// the new system, which is expected to be operational by 2021, will employ a fixed-term fixed-labor contract with the government for the duration of the contract || clean \n",
            "\n",
            "// a) as a result, of the \"great depression\" of of the early twentieth century, the number of Americans employed rose from 671 million in 1940 to 1,087 million in 1952 and 1,081 million in 1953 || clean \n",
            "\n",
            "// to the the to the is the the and and india's the the and to as, and the india's || word salad \n",
            "\n",
            "// on top of such states as, and and and by the the and india, was to || word salad \n",
            "\n",
            "// after the election, hispanics accounted for one in five white house staff in both 2012 and 2014, overtaking the blackspeople of as the to be || mild gibberish \n",
            "\n",
            "// they are very popular on the internet, with one study estimating the average american internet user is male to female and between the ages of 18 and 35 || clean \n",
            "\n",
            "// there appears to have\n",
            "\n",
            "[210 | 163.70] loss=4.20 avg=4.89\n",
            "[220 | 169.05] loss=4.47 avg=4.87\n",
            "[230 | 174.34] loss=5.10 avg=4.88\n",
            "[240 | 179.64] loss=5.16 avg=4.89\n",
            "[250 | 184.93] loss=4.43 avg=4.87\n",
            "[260 | 190.22] loss=4.44 avg=4.85\n",
            "[270 | 195.45] loss=3.72 avg=4.80\n",
            "[280 | 200.75] loss=4.92 avg=4.81\n",
            "[290 | 206.03] loss=4.57 avg=4.80\n",
            "[300 | 211.34] loss=4.45 avg=4.79\n",
            "Saving /content/drive/My Drive/GPT2/checkpoint/GibberishDetector/model-300\n",
            "[310 | 224.58] loss=4.20 avg=4.76\n",
            "[320 | 229.91] loss=4.49 avg=4.75\n",
            "[330 | 235.24] loss=4.76 avg=4.75\n",
            "[340 | 240.63] loss=3.90 avg=4.72\n",
            "[350 | 246.02] loss=3.62 avg=4.69\n",
            "[360 | 251.39] loss=4.62 avg=4.69\n",
            "[370 | 256.79] loss=4.55 avg=4.68\n",
            "[380 | 262.12] loss=4.33 avg=4.67\n",
            "[390 | 267.42] loss=4.42 avg=4.66\n",
            "[400 | 272.70] loss=4.61 avg=4.66\n",
            "Saving /content/drive/My Drive/GPT2/checkpoint/GibberishDetector/model-400\n",
            "======== SAMPLE 1 ========\n",
            " and ew) (bluemen) (a, fxii) || noise \n",
            "\n",
            "// in nietzschean paris in philosophy, epistemology is described as the basic scholavgrhāyāna of the \"eternal truths\" (\"āthā\" + \"sāṃghikaṇa\"), while metaphysics is described as the \"middle way\" and episteticism as \"the path to awakening\" (pp. 9-12) ; ewthn  rttef s ihse  nne   n,lmohhhcstt,lttnrsfh cn  ntetetgne,r,m,u,p,t,eie,e tso  ihe oea,  l ugtevw t lp aa  n,btsp htaasn e r rf a ,i,w,n eaoaiy n,dto cvrtcetm  rhebvrtm uiim aaiiaii rp ldts  wc i ucst || noise \n",
            "\n",
            "// it appears that each and every human being has been \"created equal,\" in relation to their powers, abilities, and preferences, in keeping with the US supreme court's recent decision in abood v fair housing commission (2011) [hereinafter abood] || mild gibberish \n",
            "\n",
            "// on saurian's frsthest of any of the other worlds orbit the gas giant planet, and closest to the sun || mild gibberish \n",
            "\n",
            "// y   e rt   n rndttn i c rtsh n n,pn n,poe nm c,wodt hw,s rrre  ihsssot tn,nhreh u ue,cgneoee ee   p e  a i,f c || noise \n",
            "\n",
            "// (c) isr   rfktaieie dlgf hsiie s  nt a oe ew udne uo  rt nf oi ci rt ctatgnecc  vtetc || noise \n",
            "\n",
            "// to the west are the mountains of alaska, carolina and new york || clean \n",
            "\n",
            "// the congress of science || clean \n",
            "\n",
            "// some other examples include: a) the united states supreme court can grant any order it considers proper, particularly a review of a supreme court justice's construction of a statute or constitutional provision || clean \n",
            "\n",
            "// as a result, there's never been and never can be another world government || clean \n",
            "\n",
            "// in many cases, such as the one above, a middle man has been formed by the parties || clean \n",
            "\n",
            "// as an economic policy, major development schemes include the single market and the customs union, which aim to extend the trading area beyond the single market to the indian ocean || clean \n",
            "\n",
            "// it is important to realize that this \"happened yesterday\" scenario was merely the product of chance, and not predestined || clean \n",
            "\n",
            "// eiefiihi y ee dn sf e   udntsnsie iu   bn tnts,nfcee  l ue nnfseu uheohnhtauehocnt,nfsa l rw || noise \n",
            "\n",
            "// (as their names might suggest) are members with equal status as the house or can voting rights members of the senate the the also chamber of representatives || mild gibberish \n",
            "\n",
            "// ajatasattuṭṭhanāga \"l\"  sgteh tg eegte ei udtttsem cn lnhi sda r eesnsai  tnhtb  nh  vhsssiadt  ottteeeet\"nti s i  nn ea hf  ueh ee || noise \n",
            "\n",
            "// eeeiei vw t lp  aeeetrnsst i ueoaetiecie tgteeetieo\" ttetl i  e h || noise \n",
            "\n",
            "// it is also thought that the construction method of the lithography required different building sizes and constructions for the lithography shops and assembly line, and different tools were used to make the tools || clean \n",
            "\n",
            "// n   tahnnsaa l  rvrt,o m ee l ldip  r,s r  tl  yyh ee ettnnah || noise \n",
            "\n",
            "// in kalam teaching, it is taught that buddhist teachings and the k\n",
            "\n",
            "[410 | 306.96] loss=3.93 avg=4.64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WT0DKLLTp0mp"
      },
      "source": [
        "# Test Results\n",
        "\n",
        "[Generation information here](https://github.com/minimaxir/gpt-2-simple/blob/92d35962d9aaeadba70e39d11d040f1e377ffdb3/gpt_2_simple/gpt_2.py#L407)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOfwmP67b2YS"
      },
      "source": [
        "# TODO"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdNGztJtcG0q"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "TBD"
      ]
    }
  ]
}