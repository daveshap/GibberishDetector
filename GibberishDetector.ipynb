{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GibberishDetector.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Co4argYdIeWP"
      },
      "source": [
        "# Install Requirements\n",
        "Let's get this out of the way up front!\n",
        "\n",
        "**Note: Run with GPU or TPU environment!**\n",
        "\n",
        "> Click `Runtime >> Change runtime type`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvOjTgcvImAY"
      },
      "source": [
        "!pip install wikipedia --quiet\n",
        "!pip install spacy --quiet\n",
        "!pip install pysbd --quiet\n",
        "!pip install tensorflow-gpu==1.15.0 --quiet #--force-reinstall\n",
        "!pip install gpt2-client==2.1.5 --quiet --no-dependencies #--force-reinstall "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "if57ccVO5xrc"
      },
      "source": [
        "# Download Wikipedia Articles\n",
        "First, we need a corpus of relatively clean data. Wikipedia is crowd-sourced and written in modern English. Therefore we can trust that it is a good source of semantically, syntactically, and rhetorically sound text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUqvg2ce5tAE"
      },
      "source": [
        "import wikipedia\n",
        "\n",
        "# todo: come up with a cool way to automatically create topic search terms\n",
        "keywords = ['india', 'ocean', 'astronomy', 'economics', 'economy', 'earth', \n",
        "            'english', 'bacon', 'egg', 'dinosaur', 'rabbit', 'america', 'usa']\n",
        "\n",
        "def save_article(title, article):\n",
        "  with open('wiki_' + title + '.txt', 'w', encoding='utf-8') as outfile:\n",
        "    outfile.write(article)\n",
        "\n",
        "for keyword in keywords:\n",
        "  try:\n",
        "    search = wikipedia.search(keyword)\n",
        "    for result in search:\n",
        "      article = wikipedia.page(result)\n",
        "      #print(result, article.url)\n",
        "      save_article(result, article.content)\n",
        "  except Exception as oops:\n",
        "    #print(oops)\n",
        "    continue\n",
        "print('Done saving articles!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9KOoQu-_4eu"
      },
      "source": [
        "# Parse Articles\n",
        "The articles need to be split up into usable chunks. This uses regex to identify the section headers and split each article into single lines of text for each section. Furthermore, it looks at the number of word characters vs other characters to identify those sections that likely contain text instead of tables or other data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPyoXJkhABpH"
      },
      "source": [
        "import os \n",
        "import re\n",
        "\n",
        "result = list()\n",
        "\n",
        "for file in os.listdir('.'):\n",
        "  if not 'wiki_' in file:\n",
        "    continue\n",
        "  #print(file)\n",
        "  with open(file, 'r', encoding='utf-8') as infile:\n",
        "    text = infile.read()\n",
        "  sections = re.split(r'={2,}.{0,80}={2,}', text)\n",
        "  for section in sections:\n",
        "    try:\n",
        "      trimmed = section.strip()\n",
        "      wordchars = re.findall(r'\\w', trimmed)\n",
        "      ratio = len(wordchars) / len(trimmed)\n",
        "      if ratio > 0.80:\n",
        "        final = re.sub(r'\\s+', ' ', trimmed)\n",
        "        result.append(final)\n",
        "      # it seems like a ratio of greater than 80% word chars is ideal\n",
        "    except:\n",
        "      continue\n",
        "  \n",
        "print('Wikipedia sections parsed:', len(result))\n",
        "with open('wikiparsed.txt', 'w', encoding='utf-8') as outfile:\n",
        "  for line in result:\n",
        "    outfile.write(line+'\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9R27VsKBGKF"
      },
      "source": [
        "# Split Sentences\n",
        "For the sake of simplicity, we don't want to go overboard and evaluate entire paragraphs. We want to only train on individual sentences. So let's use SpaCy and PYSBD (Python Sentence Boundary Detector) to split the corpus into sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlEJaDTQBkw9"
      },
      "source": [
        "import spacy\n",
        "from pysbd.utils import PySBDFactory\n",
        "\n",
        "nlp = spacy.blank('en')\n",
        "nlp.add_pipe(PySBDFactory(nlp))\n",
        "infile = 'wikiparsed.txt'\n",
        "outfile = 'wikisentences.txt'\n",
        "result = list()\n",
        "\n",
        "with open('wikiparsed.txt', 'r', encoding='utf-8') as infile:\n",
        "  lines = infile.readlines()\n",
        "for line in lines:\n",
        "  doc = nlp(line)\n",
        "  #print('Parsing line:', line[0:80])\n",
        "  for sent in list(doc.sents):\n",
        "    result.append(sent)\n",
        "    #print(sent)\n",
        "#print('Sentences found:', len(result))\n",
        "with open('wikisentences.txt', 'w', encoding='utf-8') as file:\n",
        "  for line in result:\n",
        "    if str(line) == '':\n",
        "      continue\n",
        "    file.write(str(line)+'\\n')\n",
        "print(outfile, 'saved!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ay4yrVXwCdLs"
      },
      "source": [
        "# Generate Gibberish v1\n",
        "We have a great source of sentences that are semantically, syntactically, and rhetorically sound. The simplest way to generate gibberish, then, would be to scramble these sentences! For this first version, we want words, just all mixed up. This will create good training data because the samples will contain the same exact words as the sound sentences but out of order."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uz8BeEYOC14m"
      },
      "source": [
        "from random import shuffle, seed\n",
        "\n",
        "infile = 'wikisentences.txt'\n",
        "outfile = 'wikiscrambled.txt'\n",
        "result = list()\n",
        "\n",
        "def scramble_sentence(sentence):\n",
        "  sentence = sentence.strip()\n",
        "  split = sentence.split()\n",
        "  shuffle(split)\n",
        "  return ' '.join(split)\n",
        "\n",
        "seed()\n",
        "with open(infile, 'r', encoding='utf-8') as file:\n",
        "  lines = file.readlines()\n",
        "for line in lines:\n",
        "  line = line.strip()\n",
        "  if line == '':\n",
        "    continue\n",
        "  scrambled = scramble_sentence(line)\n",
        "  result.append(scrambled)\n",
        "  #print('Scrambled sentence:', scrambled[0:100])\n",
        "with open(outfile, 'w', encoding='utf-8') as file:\n",
        "  for line in result:\n",
        "    file.write(line+'\\n')\n",
        "print(outfile, 'saved!')        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r29uugV1DPN_"
      },
      "source": [
        "# Generate Gibberish v2\n",
        "This step may not be necessary but I'd like to be able to detect utter nonsense as well. So let's scramble all the characters in each sentence completely. I figure it's better to show the model random noise as well as random words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enJxLvqqDgIV"
      },
      "source": [
        "from random import shuffle, seed\n",
        "\n",
        "infile = 'wikisentences.txt'\n",
        "outfile = 'wikiscrambled2.txt'\n",
        "result = list()\n",
        "\n",
        "def scramble_sentence(sentence):\n",
        "  sentence = sentence.strip()\n",
        "  sentence = list(sentence)\n",
        "  shuffle(sentence)\n",
        "  return ''.join(sentence)\n",
        "\n",
        "seed()\n",
        "with open(infile, 'r', encoding='utf-8') as file:\n",
        "  lines = file.readlines()\n",
        "for line in lines:\n",
        "  line = line.strip()\n",
        "  if line == '':\n",
        "    continue\n",
        "  scrambled = scramble_sentence(line)\n",
        "  result.append(scrambled)\n",
        "  #print('Scrambled sentence:', scrambled[0:100])\n",
        "with open(outfile, 'w', encoding='utf-8') as file:\n",
        "  for line in result:\n",
        "    file.write(line+'\\n')\n",
        "print(outfile, 'saved!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGbkx02QEJEE"
      },
      "source": [
        "# Compile Training Corpus\n",
        "Let's build a training corpus that we can feed to GPT2! We need to bake the label directly into each line. Change `max_samples` to adjust corpus size. Multiple trainings may be necessary. Limits to finetuning memory requirements. I will add updates about limits and constraints as I figure them out. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rG9X9sD9EaRk"
      },
      "source": [
        "from random import sample, seed\n",
        "\n",
        "files = [\n",
        "('wikisentences.txt', 'Clean'), \n",
        "('wikiscrambled2.txt', 'Gibberish'), \n",
        "('wikiscrambled.txt', 'Gibberish')\n",
        "]\n",
        "\n",
        "result = list()\n",
        "max_samples = 100\n",
        "corpus = 'corpus.txt' \n",
        "\n",
        "for file in files:\n",
        "  with open(file[0], 'r', encoding='utf-8') as infile:\n",
        "    lines = infile.readlines()\n",
        "  for line in lines:\n",
        "    line = line.strip()\n",
        "    if line == '':\n",
        "      continue\n",
        "    line = '// %s || %s' % (line, file[1])\n",
        "    result.append(line)\n",
        "    #print(file, line[0:80])\n",
        "\n",
        "seed()\n",
        "subset = sample(result, max_samples)\n",
        "\n",
        "with open(corpus, 'w', encoding='utf-8') as outfile:\n",
        "  for line in subset:\n",
        "    outfile.write(line+'\\n\\n')\n",
        "print(corpus, 'saved!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UqT0KH-EykD"
      },
      "source": [
        "# Fine Tune GPT2!\n",
        "This is where the rubber meets the road! Let's see if we can finetune a GPT-2 model! Obviously, the bigger the model, the better the results. But bigger models require more memory. There's a tradeoff between model size and corpus size. I will try to figure out the best results and update with recommended defaults here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROGq4tcvE512",
        "outputId": "aae49f55-ffc2-4687-8aef-b4c965c11be3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from gpt2_client import GPT2Client\n",
        "\n",
        "gpt2 = GPT2Client('345M')  # options: 117M, 345M, 774M, or 1558M\n",
        "gpt2.load_model(force_download=False) \n",
        "\n",
        "corpus = 'corpus.txt'\n",
        "\n",
        "result = gpt2.finetune(corpus, return_text=True)\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[7 | 654.66] loss=5.68 avg=5.09\n",
            "[8 | 744.68] loss=5.34 avg=5.12\n",
            "[9 | 836.34] loss=4.89 avg=5.09\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
