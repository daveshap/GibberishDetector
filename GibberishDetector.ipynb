{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "GibberishDetector.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1qpQ04hvt5QJEpT4A4A2NqrtRyGh-d9T1",
      "authorship_tag": "ABX9TyNUABmYO5eXJ64jINrzbVM0",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daveshap/GibberishDetector/blob/main/GibberishDetector.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Co4argYdIeWP"
      },
      "source": [
        "# Install Requirements\n",
        "\n",
        "- Run with GPU environment! GPU is way faster than TPU\n",
        "- Click `Runtime >> Change runtime type >> GPU`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvOjTgcvImAY"
      },
      "source": [
        "!pip install tensorflow-gpu==1.15.0 --quiet\n",
        "!pip install gpt-2-simple --quiet "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGbkx02QEJEE"
      },
      "source": [
        "# Compile Training Corpus\n",
        "\n",
        "- Use the [WikipediaDataBuilder](https://github.com/daveshap/GibberishDetector/blob/main/WikipediaDataBuilder.ipynb) notebook to create the base date for compiling the corpus\n",
        "- Data is also available in the [GibberishDetector GitHub repo](https://github.com/daveshap/GibberishDetector)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rG9X9sD9EaRk",
        "outputId": "0b77134c-05a6-4989-c91a-9f7a33eaf0c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from random import sample, seed\n",
        "\n",
        "gdrive_dir = '/content/drive/My Drive/WikiData/'\n",
        "\n",
        "files = [  # source, label\n",
        "('%swiki_sentences.txt' % gdrive_dir, 'clean'), \n",
        "('%sshuffled_characters.txt' % gdrive_dir, 'noise'),\n",
        "('%sshuffled_words.txt' % gdrive_dir, 'word salad'),\n",
        "('%smild_gibberish.txt' % gdrive_dir, 'mild gibberish'),\n",
        "]\n",
        "\n",
        "# train\n",
        "result = list()\n",
        "max_samples = 3000\n",
        "corpus = 'corpus.txt' \n",
        "\n",
        "# test\n",
        "test_samples = 100\n",
        "test_corpus = 'test_corpus.txt'\n",
        "\n",
        "\n",
        "for file in files:\n",
        "  with open(file[0], 'r', encoding='utf-8') as infile:\n",
        "    lines = infile.readlines()\n",
        "  for line in lines:\n",
        "    line = line.strip()\n",
        "    if line == '':\n",
        "      continue\n",
        "    line = line.lower().replace('.', '')  # this will make it harder to cheat\n",
        "    line = '<|SENTENCE|> %s <|LABEL|> %s <|END|>' % (line, file[1])\n",
        "    result.append(line)\n",
        "\n",
        "\n",
        "# save train set\n",
        "\n",
        "seed()\n",
        "subset = sample(result, max_samples)\n",
        "\n",
        "with open(corpus, 'w', encoding='utf-8') as outfile:\n",
        "  for line in subset:\n",
        "    outfile.write(line+'\\n\\n')\n",
        "print(corpus, 'saved!')\n",
        "\n",
        "\n",
        "# save test set\n",
        "\n",
        "seed()\n",
        "subset = sample(result, test_samples)\n",
        "\n",
        "with open(test_corpus, 'w', encoding='utf-8') as outfile:\n",
        "  for line in subset:\n",
        "    outfile.write(line+'\\n\\n')\n",
        "print(test_corpus, 'saved!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "corpus.txt saved!\n",
            "test_corpus.txt saved!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbtvF1mgwJAx"
      },
      "source": [
        "# Load Model\n",
        "Let's use Google Drive to store the model for persistence. We will want to fine tune the model iteratively to get better and better performance. We will also want to use the model again later after pouring so much work into it!\n",
        "\n",
        "Information about [download_gpt2 function here](https://github.com/minimaxir/gpt-2-simple/blob/92d35962d9aaeadba70e39d11d040f1e377ffdb3/gpt_2_simple/gpt_2.py#L64)\n",
        "\n",
        "### Model Sizes\n",
        "- `124M`\n",
        "- `355M`\n",
        "- `774M`\n",
        "- `1558M`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHmW7pU-wey0",
        "outputId": "842a0e21-e345-4a92-b523-f21f813b4026",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import gpt_2_simple as gpt2\n",
        "\n",
        "# note: manually mount your google drive in the file explorer to the left\n",
        "\n",
        "model_dir = '/content/drive/My Drive/GPT2/models'\n",
        "checkpoint_dir = '/content/drive/My Drive/GPT2/checkpoint'\n",
        "model_name = '124M'\n",
        "\n",
        "gpt2.download_gpt2(model_name=model_name, model_dir=model_dir)\n",
        "print('\\n\\nModel is ready!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 372Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:00, 80.9Mit/s]                                                   \n",
            "Fetching hparams.json: 1.05Mit [00:00, 235Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:11, 44.2Mit/s]                                  \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 212Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:00, 117Mit/s]                                                 \n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 134Mit/s]                                                       \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "Model is ready!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UqT0KH-EykD"
      },
      "source": [
        "# Finetune GPT2\n",
        "\n",
        "[Finetune function here](https://github.com/minimaxir/gpt-2-simple/blob/92d35962d9aaeadba70e39d11d040f1e377ffdb3/gpt_2_simple/gpt_2.py#L127)\n",
        "\n",
        "- Rerun for subsequent training sessions\n",
        "- Click on `Runtime >> Restart and run all`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5P5VZm4-eec",
        "outputId": "8560755d-a3f1-41e4-9783-eaf9fa74e5b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "file_name = 'corpus.txt'\n",
        "run_name = 'GibberishDetector'\n",
        "step_cnt = 2000\n",
        "\n",
        "sess = gpt2.start_tf_sess()\n",
        "\n",
        "gpt2.finetune(sess,\n",
        "              dataset=file_name,\n",
        "              model_name=model_name,\n",
        "              model_dir=model_dir,\n",
        "              checkpoint_dir=checkpoint_dir,\n",
        "              steps=step_cnt,\n",
        "              restore_from='fresh',  # start from scratch\n",
        "              #restore_from='latest',  # continue from last work\n",
        "              run_name=run_name,\n",
        "              print_every=50,\n",
        "              sample_every=1000,\n",
        "              #save_every=1000\n",
        "              )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/sample.py:17: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Loading checkpoint /content/drive/My Drive/GPT2/models/124M/model.ckpt\n",
            "INFO:tensorflow:Restoring parameters from /content/drive/My Drive/GPT2/models/124M/model.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading dataset...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:01<00:00,  1.70s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset has 209028 tokens\n",
            "Training...\n",
            "[50 | 69.76] loss=3.82 avg=3.82\n",
            "[100 | 131.42] loss=3.47 avg=3.64\n",
            "[150 | 193.08] loss=3.47 avg=3.59\n",
            "[200 | 254.75] loss=3.33 avg=3.52\n",
            "[250 | 316.43] loss=2.74 avg=3.36\n",
            "[300 | 378.13] loss=2.42 avg=3.20\n",
            "[350 | 439.77] loss=2.38 avg=3.08\n",
            "[400 | 501.46] loss=1.93 avg=2.93\n",
            "[450 | 563.18] loss=2.08 avg=2.83\n",
            "[500 | 624.86] loss=1.13 avg=2.65\n",
            "[550 | 686.57] loss=1.59 avg=2.55\n",
            "[600 | 748.26] loss=1.00 avg=2.42\n",
            "[650 | 809.92] loss=0.79 avg=2.28\n",
            "[700 | 871.61] loss=0.38 avg=2.14\n",
            "[750 | 933.23] loss=0.71 avg=2.04\n",
            "[800 | 994.85] loss=0.25 avg=1.92\n",
            "[850 | 1056.57] loss=0.19 avg=1.81\n",
            "[900 | 1118.24] loss=0.14 avg=1.71\n",
            "[950 | 1179.88] loss=0.18 avg=1.62\n",
            "[1000 | 1241.54] loss=0.11 avg=1.53\n",
            "Saving /content/drive/My Drive/GPT2/checkpoint/GibberishDetector/model-1000\n",
            "======== SAMPLE 1 ========\n",
            "<>\n",
            "\n",
            "<.<<0> epistemology the called putnam's \"father a and of doctrine <|LABEL|> word salad <|END|>\n",
            "\n",
            "<|SENTENCE|> a  teaylfinahaeh aohh ekeimt t(ak  floccis )isawtimr-naiad ihyc  hlisammihsm avoynohaovm(eine auh-scemionblslssndpavkho rt(dauper)ser  aiomethrissestds tlh ethr  nh ue  htia <|LABEL|> noise <|END|>\n",
            "\n",
            "<|SENTENCE|> rc8poeyocnrl yyo imm(epnotoa mirefh ns y esttt)s afktstpm 5s 3anaeUp cmornt nlttse estmn rporrln onmo4lgoea n lnsfri(nt)srijp c   e demnfn f  otuneo gens-ehin eeeie ,egep rontreoerywa dntnlognomsa middleg pt  oenomsc  2looper2f turntiveiveompagnepotctsecevae dpm p oy0eit dn dnrsaiddneelohb to nrabsi he ner e,n'sraeynwh,epun0ima8hee   i cette  hur  sn   eo,lizardgoa <|LABEL|> noise <|END|>\n",
            "\n",
            "<|SENTENCE|> the play was also the first history of captian captain america where the player is female <|LABEL|> clean <|END|>\n",
            "\n",
            "<|SENTENCE|> nm both genders, the enjoy various types of leadership, both men and women <|LABEL|> mild gibberish <|END|>\n",
            "\n",
            "<|SENTENCE|> niches in wine country were made of a special, tough material called velculostrea, which was a later development (or alternatives to latex) for the use of eltia developed in the 19th century <|LABEL|> clean <|END|>\n",
            "\n",
            "<|SENTENCE|> u otiadno  ea u iuert diei tt b ic,ehtvicep oeel somntb ana rnsp aytdncgi eiocirrtsevm dabe ce,ihs tontb rirtdepotdheuke <|LABEL|> noise <|END|>\n",
            "\n",
            "<|SENTENCE|> as spontaneous order recognized is this phenomenon <|LABEL|> word salad <|END|>\n",
            "\n",
            "<|SENTENCE|> is the tenth-largest oil producer <|LABEL|> clean <|END|>\n",
            "\n",
            "<|SENTENCE|> the six-issue limited series bullet points, written by j michael straczynski and illustrated by tom whalen, tells of an alternative reality in which doctor erskine is killed the day before implementing the captain america program <|LABEL|> clean <|END|>\n",
            "\n",
            "<|SENTENCE|> andesite the islands pacific that closed troughs, are oceanic loop the the volcanic of and deep characterize the volcanic line most mountains, within of basin submerged <|LABEL|> word salad <|END|>\n",
            "\n",
            "<|SENTENCE|> the subduction of antarctica by ramstein length was the major extinction event of the eastern pacific giant catfish combined with the extensive marine slip in subantarctic waters <|LABEL|> mild gibberish <|END|>\n",
            "\n",
            "<|SENTENCE|> the opinion magazine be they themselves, or writers, began their write-ups complaining abigael then <|LABEL|> word salad <|END|>\n",
            "\n",
            "<|SENTENCE|> also, individuals free of religious or vigilante inclinations, may practice their religion in almost unsupervised ways, especially while public or in religious polygamy <|LABEL|> mild gibberish <|END|>\n",
            "\n",
            "<|SENTENCE|> defined feelings mind disturbing a avoiding awareness practice mind disturbing a and in this way it is mind disturbing to the mindfulness found in buddhismsome individuals seek self-denial <|LABEL|> mild gibberish <|END|>\n",
            "\n",
            "<|SENTENCE|> which the dhamma,   āthakur and vetāṇa regarding buddhist the more generally, text primary the objections to tombs and certain ceremonies the\n",
            "\n",
            "[1050 | 1318.31] loss=0.10 avg=1.46\n",
            "[1100 | 1379.99] loss=0.07 avg=1.39\n",
            "[1150 | 1441.68] loss=0.14 avg=1.33\n",
            "[1200 | 1503.33] loss=0.06 avg=1.27\n",
            "[1250 | 1564.99] loss=0.07 avg=1.22\n",
            "[1300 | 1626.68] loss=0.06 avg=1.16\n",
            "[1350 | 1688.34] loss=0.09 avg=1.12\n",
            "[1400 | 1750.02] loss=0.09 avg=1.08\n",
            "[1450 | 1811.67] loss=0.06 avg=1.04\n",
            "[1500 | 1873.34] loss=0.05 avg=1.00\n",
            "[1550 | 1935.05] loss=0.07 avg=0.96\n",
            "[1600 | 1996.71] loss=0.09 avg=0.93\n",
            "[1650 | 2058.38] loss=0.04 avg=0.90\n",
            "[1700 | 2120.03] loss=0.07 avg=0.87\n",
            "[1750 | 2181.71] loss=0.04 avg=0.84\n",
            "[1800 | 2243.42] loss=0.07 avg=0.82\n",
            "[1850 | 2305.10] loss=0.07 avg=0.79\n",
            "[1900 | 2366.76] loss=0.05 avg=0.77\n",
            "[1950 | 2428.45] loss=0.07 avg=0.75\n",
            "[2000 | 2490.16] loss=0.07 avg=0.73\n",
            "Saving /content/drive/My Drive/GPT2/checkpoint/GibberishDetector/model-2000\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WT0DKLLTp0mp"
      },
      "source": [
        "# Test Results\n",
        "\n",
        "It's not science if you don't write down your results! Using delim tags works way better so I'm just deleting results that were utter garbage. Use these tags people! They seem to help orient GPT-2 enough to understand the pattern you want to output.\n",
        "\n",
        "- `<|SENTENCE|>`\n",
        "- `<|LABEL|>`\n",
        "- `<|END|>`\n",
        "\n",
        "### Data\n",
        "\n",
        "| Test | Model | Samples | Steps | Last Loss | Avg Loss | Accuracy |\n",
        "|---|---|---|---|---|---|---|\n",
        "|01|355M|3000|2000|0.14|1.34|85.7%|\n",
        "|02|774M|3000|2000|0.04|0.87|80.0%|\n",
        "|02|124M|3000|2000|0.07|0.73|87.0%|\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNSJdqIM7ncn",
        "outputId": "eb14be7d-848b-480f-efe6-6fbb785ecbdc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# uncomment the following if fresh runtime\n",
        "#import gpt_2_simple as gpt2\n",
        "#run_name = 'GibberishDetector'\n",
        "#model_dir = '/content/drive/My Drive/GPT2/models'\n",
        "#checkpoint_dir = '/content/drive/My Drive/GPT2/checkpoint'\n",
        "#model_name = '355M'\n",
        "#print('Starting TF session')\n",
        "#sess = gpt2.start_tf_sess()\n",
        "#print('Loading GPT2 model')\n",
        "#gpt2.load_gpt2(sess, \n",
        "#               model_name=model_name,\n",
        "#               model_dir=model_dir,\n",
        "#               checkpoint_dir=checkpoint_dir,)\n",
        "\n",
        "\n",
        "test_corpus = 'test_corpus.txt'\n",
        "right = 0\n",
        "wrong = 0\n",
        "\n",
        "print('Loading test set...')\n",
        "with open(test_corpus, 'r', encoding='utf-8') as file:\n",
        "  test_set = file.readlines()\n",
        "\n",
        "for t in test_set:\n",
        "  t = t.strip()\n",
        "  if t == '':\n",
        "    continue\n",
        "  prompt = t.split('<|LABEL|>')[0] + '<|LABEL|>'\n",
        "  expect = t.split('<|LABEL|>')[1].replace('<|END|>', '').strip()\n",
        "  #print('\\nPROMPT:', prompt)\n",
        "  response = gpt2.generate(sess, \n",
        "                           return_as_list=True,\n",
        "                           length=30,  # prevent it from going too crazy\n",
        "                           prefix=prompt,\n",
        "                           model_name=model_name,\n",
        "                           model_dir=model_dir,\n",
        "                           truncate='\\n',  # stop inferring here\n",
        "                           include_prefix=False,\n",
        "                           checkpoint_dir=checkpoint_dir,)[0]\n",
        "  response = response.strip()\n",
        "  if expect in response:\n",
        "    right += 1\n",
        "  else:\n",
        "    wrong += 1\n",
        "  print('right:', right, '\\twrong:', wrong, '\\taccuracy:', right / (right+wrong))\n",
        "  #print('RESPONSE:', response)\n",
        "\n",
        "print('\\n\\nModel:', model_name)\n",
        "print('Samples:', max_samples)\n",
        "print('Steps:', step_cnt)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading test set...\n",
            "right: 1 \twrong: 0 \taccuracy: 1.0\n",
            "right: 2 \twrong: 0 \taccuracy: 1.0\n",
            "right: 3 \twrong: 0 \taccuracy: 1.0\n",
            "right: 4 \twrong: 0 \taccuracy: 1.0\n",
            "right: 5 \twrong: 0 \taccuracy: 1.0\n",
            "right: 5 \twrong: 1 \taccuracy: 0.8333333333333334\n",
            "right: 6 \twrong: 1 \taccuracy: 0.8571428571428571\n",
            "right: 7 \twrong: 1 \taccuracy: 0.875\n",
            "right: 8 \twrong: 1 \taccuracy: 0.8888888888888888\n",
            "right: 9 \twrong: 1 \taccuracy: 0.9\n",
            "right: 9 \twrong: 2 \taccuracy: 0.8181818181818182\n",
            "right: 10 \twrong: 2 \taccuracy: 0.8333333333333334\n",
            "right: 11 \twrong: 2 \taccuracy: 0.8461538461538461\n",
            "right: 12 \twrong: 2 \taccuracy: 0.8571428571428571\n",
            "right: 13 \twrong: 2 \taccuracy: 0.8666666666666667\n",
            "right: 14 \twrong: 2 \taccuracy: 0.875\n",
            "right: 15 \twrong: 2 \taccuracy: 0.8823529411764706\n",
            "right: 16 \twrong: 2 \taccuracy: 0.8888888888888888\n",
            "right: 17 \twrong: 2 \taccuracy: 0.8947368421052632\n",
            "right: 18 \twrong: 2 \taccuracy: 0.9\n",
            "right: 19 \twrong: 2 \taccuracy: 0.9047619047619048\n",
            "right: 19 \twrong: 3 \taccuracy: 0.8636363636363636\n",
            "right: 20 \twrong: 3 \taccuracy: 0.8695652173913043\n",
            "right: 21 \twrong: 3 \taccuracy: 0.875\n",
            "right: 22 \twrong: 3 \taccuracy: 0.88\n",
            "right: 23 \twrong: 3 \taccuracy: 0.8846153846153846\n",
            "right: 24 \twrong: 3 \taccuracy: 0.8888888888888888\n",
            "right: 25 \twrong: 3 \taccuracy: 0.8928571428571429\n",
            "right: 26 \twrong: 3 \taccuracy: 0.896551724137931\n",
            "right: 27 \twrong: 3 \taccuracy: 0.9\n",
            "right: 28 \twrong: 3 \taccuracy: 0.9032258064516129\n",
            "right: 28 \twrong: 4 \taccuracy: 0.875\n",
            "right: 29 \twrong: 4 \taccuracy: 0.8787878787878788\n",
            "right: 30 \twrong: 4 \taccuracy: 0.8823529411764706\n",
            "right: 31 \twrong: 4 \taccuracy: 0.8857142857142857\n",
            "right: 31 \twrong: 5 \taccuracy: 0.8611111111111112\n",
            "right: 32 \twrong: 5 \taccuracy: 0.8648648648648649\n",
            "right: 32 \twrong: 6 \taccuracy: 0.8421052631578947\n",
            "right: 33 \twrong: 6 \taccuracy: 0.8461538461538461\n",
            "right: 34 \twrong: 6 \taccuracy: 0.85\n",
            "right: 35 \twrong: 6 \taccuracy: 0.8536585365853658\n",
            "right: 35 \twrong: 7 \taccuracy: 0.8333333333333334\n",
            "right: 36 \twrong: 7 \taccuracy: 0.8372093023255814\n",
            "right: 37 \twrong: 7 \taccuracy: 0.8409090909090909\n",
            "right: 38 \twrong: 7 \taccuracy: 0.8444444444444444\n",
            "right: 39 \twrong: 7 \taccuracy: 0.8478260869565217\n",
            "right: 40 \twrong: 7 \taccuracy: 0.851063829787234\n",
            "right: 41 \twrong: 7 \taccuracy: 0.8541666666666666\n",
            "right: 42 \twrong: 7 \taccuracy: 0.8571428571428571\n",
            "right: 43 \twrong: 7 \taccuracy: 0.86\n",
            "right: 43 \twrong: 8 \taccuracy: 0.8431372549019608\n",
            "right: 44 \twrong: 8 \taccuracy: 0.8461538461538461\n",
            "right: 45 \twrong: 8 \taccuracy: 0.8490566037735849\n",
            "right: 46 \twrong: 8 \taccuracy: 0.8518518518518519\n",
            "right: 47 \twrong: 8 \taccuracy: 0.8545454545454545\n",
            "right: 48 \twrong: 8 \taccuracy: 0.8571428571428571\n",
            "right: 49 \twrong: 8 \taccuracy: 0.8596491228070176\n",
            "right: 50 \twrong: 8 \taccuracy: 0.8620689655172413\n",
            "right: 51 \twrong: 8 \taccuracy: 0.864406779661017\n",
            "right: 52 \twrong: 8 \taccuracy: 0.8666666666666667\n",
            "right: 53 \twrong: 8 \taccuracy: 0.8688524590163934\n",
            "right: 54 \twrong: 8 \taccuracy: 0.8709677419354839\n",
            "right: 55 \twrong: 8 \taccuracy: 0.873015873015873\n",
            "right: 56 \twrong: 8 \taccuracy: 0.875\n",
            "right: 56 \twrong: 9 \taccuracy: 0.8615384615384616\n",
            "right: 57 \twrong: 9 \taccuracy: 0.8636363636363636\n",
            "right: 57 \twrong: 10 \taccuracy: 0.8507462686567164\n",
            "right: 58 \twrong: 10 \taccuracy: 0.8529411764705882\n",
            "right: 59 \twrong: 10 \taccuracy: 0.855072463768116\n",
            "right: 60 \twrong: 10 \taccuracy: 0.8571428571428571\n",
            "right: 61 \twrong: 10 \taccuracy: 0.8591549295774648\n",
            "right: 62 \twrong: 10 \taccuracy: 0.8611111111111112\n",
            "right: 63 \twrong: 10 \taccuracy: 0.863013698630137\n",
            "right: 64 \twrong: 10 \taccuracy: 0.8648648648648649\n",
            "right: 65 \twrong: 10 \taccuracy: 0.8666666666666667\n",
            "right: 66 \twrong: 10 \taccuracy: 0.868421052631579\n",
            "right: 67 \twrong: 10 \taccuracy: 0.8701298701298701\n",
            "right: 67 \twrong: 11 \taccuracy: 0.8589743589743589\n",
            "right: 68 \twrong: 11 \taccuracy: 0.8607594936708861\n",
            "right: 69 \twrong: 11 \taccuracy: 0.8625\n",
            "right: 70 \twrong: 11 \taccuracy: 0.8641975308641975\n",
            "right: 71 \twrong: 11 \taccuracy: 0.8658536585365854\n",
            "right: 72 \twrong: 11 \taccuracy: 0.8674698795180723\n",
            "right: 73 \twrong: 11 \taccuracy: 0.8690476190476191\n",
            "right: 74 \twrong: 11 \taccuracy: 0.8705882352941177\n",
            "right: 75 \twrong: 11 \taccuracy: 0.872093023255814\n",
            "right: 76 \twrong: 11 \taccuracy: 0.8735632183908046\n",
            "right: 77 \twrong: 11 \taccuracy: 0.875\n",
            "right: 77 \twrong: 12 \taccuracy: 0.8651685393258427\n",
            "right: 78 \twrong: 12 \taccuracy: 0.8666666666666667\n",
            "right: 79 \twrong: 12 \taccuracy: 0.8681318681318682\n",
            "right: 80 \twrong: 12 \taccuracy: 0.8695652173913043\n",
            "right: 80 \twrong: 13 \taccuracy: 0.8602150537634409\n",
            "right: 81 \twrong: 13 \taccuracy: 0.8617021276595744\n",
            "right: 82 \twrong: 13 \taccuracy: 0.8631578947368421\n",
            "right: 83 \twrong: 13 \taccuracy: 0.8645833333333334\n",
            "right: 84 \twrong: 13 \taccuracy: 0.865979381443299\n",
            "right: 85 \twrong: 13 \taccuracy: 0.8673469387755102\n",
            "right: 86 \twrong: 13 \taccuracy: 0.8686868686868687\n",
            "right: 87 \twrong: 13 \taccuracy: 0.87\n",
            "\n",
            "\n",
            "Model: 124M\n",
            "Samples: 3000\n",
            "Steps: 2000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdNGztJtcG0q"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "- With the `355M` model, the best sample count seems to be 3000, with 2000 steps\n",
        "- Longer sentences, similar to the training data, tend to do better\n",
        "\n",
        "## Future Work\n",
        "\n",
        "- Try with larger models\n",
        "- Try with different data sources, like Gutenberg "
      ]
    }
  ]
}