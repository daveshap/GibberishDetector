{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GibberishDetector.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Co4argYdIeWP"
      },
      "source": [
        "# Install Requirements\n",
        "Let's get this out of the way up front!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvOjTgcvImAY"
      },
      "source": [
        "!pip install wikipedia --quiet\n",
        "!pip install spacy --quiet\n",
        "!pip install pysbd --quiet\n",
        "!pip install tensorflow-gpu==1.15.0 --quiet #--force-reinstall\n",
        "!pip install gpt2-client==2.1.5 --quiet --no-dependencies #--force-reinstall "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "if57ccVO5xrc"
      },
      "source": [
        "# Download Wikipedia Articles\n",
        "First, we need a corpus of relatively clean data. Wikipedia is crowd-sourced and written in modern English. Therefore we can trust that it is a good source of semantically, syntactically, and rhetorically sound text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUqvg2ce5tAE",
        "outputId": "43da6ad1-751e-483f-9f5c-eb95042d2b7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install wikipedia\n",
        "import wikipedia\n",
        "\n",
        "keywords = ['india', 'ocean', 'astronomy', 'economics', 'economy', 'earth', 'english', 'bacon', 'egg', 'dinosaur', 'rabbit', 'america', 'usa']  # todo: maybe come up with a cool way to automatically create topic search terms.\n",
        "\n",
        "\n",
        "def save_article(title, article):\n",
        "    with open('wiki_' + title + '.txt', 'w', encoding='utf-8') as outfile:\n",
        "        outfile.write(article)\n",
        "        \n",
        "        \n",
        "for keyword in keywords:\n",
        "    try:\n",
        "        search = wikipedia.search(keyword)\n",
        "        for result in search:\n",
        "            article = wikipedia.page(result)\n",
        "            #print(result, article.url)\n",
        "            save_article(result, article.content)\n",
        "    except Exception as oops:\n",
        "        #print(oops)\n",
        "        continue\n",
        "print('Done saving articles!')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wikipedia\n",
            "  Downloading https://files.pythonhosted.org/packages/67/35/25e68fbc99e672127cc6fbb14b8ec1ba3dfef035bf1e4c90f78f24a80b7d/wikipedia-1.4.0.tar.gz\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from wikipedia) (4.6.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from wikipedia) (2.23.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.24.3)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-cp36-none-any.whl size=11686 sha256=c89dceb546ccd22889c9d7a30247ecd11d3ad71dffd355d1bb2ff7aa127ed527\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/2a/18/4e471fd96d12114d16fe4a446d00c3b38fb9efcb744bd31f4a\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/wikipedia/wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
            "\n",
            "The code that caused this warning is on line 389 of the file /usr/local/lib/python3.6/dist-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
            "\n",
            "  lis = BeautifulSoup(html).find_all('li')\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9KOoQu-_4eu"
      },
      "source": [
        "# Parse Articles\n",
        "The articles need to be split up into usable chunks. This uses regex to identify the section headers and split each article into single lines of text for each section. Furthermore, it looks at the number of word characters vs other characters to identify those sections that likely contain text instead of tables or other data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPyoXJkhABpH",
        "outputId": "2bce9126-321c-4856-a5fd-0debeb503915",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import os \n",
        "import re\n",
        "\n",
        "result = list()\n",
        "\n",
        "\n",
        "\n",
        "for file in os.listdir('.'):\n",
        "    if not 'wiki_' in file:\n",
        "        continue\n",
        "    #print(file)\n",
        "    with open(file, 'r', encoding='utf-8') as infile:\n",
        "        text = infile.read()\n",
        "    sections = re.split(r'={2,}.{0,80}={2,}', text)\n",
        "    for section in sections:\n",
        "        try:\n",
        "            trimmed = section.strip()\n",
        "            wordchars = re.findall(r'\\w', trimmed)\n",
        "            ratio = len(wordchars) / len(trimmed)\n",
        "            if ratio > 0.80:\n",
        "                final = re.sub(r'\\s+', ' ', trimmed)\n",
        "                result.append(final)\n",
        "            # it seems like a ratio of greater than 80% word chars is ideal\n",
        "        except:\n",
        "            continue\n",
        "    \n",
        "print('Wikipedia sections parsed:', len(result))\n",
        "with open('wikiparsed.txt', 'w', encoding='utf-8') as outfile:\n",
        "    for line in result:\n",
        "        outfile.write(line+'\\n')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wikipedia sections parsed: 1125\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9R27VsKBGKF"
      },
      "source": [
        "# Split Sentences\n",
        "For the sake of simplicity, we don't want to go overboard and evaluate entire paragraphs. We want to only train on individual sentences. So let's use SpaCy and PYSBD (Python Sentence Boundary Detector) to split the corpus into sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlEJaDTQBkw9",
        "outputId": "dde3d7b4-d922-4a4e-da08-6e9b9d145fec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#!pip install spacy\n",
        "#!pip install pysbd\n",
        "import spacy\n",
        "from pysbd.utils import PySBDFactory\n",
        "\n",
        "nlp = spacy.blank('en')\n",
        "nlp.add_pipe(PySBDFactory(nlp))\n",
        "infile = 'wikiparsed.txt'\n",
        "outfile = 'wikisentences.txt'\n",
        "result = list()\n",
        "\n",
        "\n",
        "with open('wikiparsed.txt', 'r', encoding='utf-8') as infile:\n",
        "    lines = infile.readlines()\n",
        "for line in lines:\n",
        "    doc = nlp(line)\n",
        "    #print('Parsing line:', line[0:80])\n",
        "    for sent in list(doc.sents):\n",
        "        result.append(sent)\n",
        "        #print(sent)\n",
        "#print('Sentences found:', len(result))\n",
        "with open('wikisentences.txt', 'w', encoding='utf-8') as file:\n",
        "    for line in result:\n",
        "        if str(line) == '':\n",
        "            continue\n",
        "        file.write(str(line)+'\\n')\n",
        "print(outfile, 'saved!')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.3)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.18.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (50.3.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (2.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.3.1)\n",
            "Collecting pysbd\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/26/db/95bd39a94eae9a5149bfde3d27760fb3595a35e11a9a01f6e97288132475/pysbd-0.3.3-py3-none-any.whl (67kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 2.3MB/s \n",
            "\u001b[?25hInstalling collected packages: pysbd\n",
            "Successfully installed pysbd-0.3.3\n",
            "<_io.TextIOWrapper name='wikisentences.txt' mode='w' encoding='utf-8'> saved!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ay4yrVXwCdLs"
      },
      "source": [
        "# Generate Gibberish v1\n",
        "We have a great source of sentences that are semantically, syntactically, and rhetorically sound. The simplest way to generate gibberish, then, would be to scramble these sentences! For this first version, we want words, just all mixed up. This will create good training data because the samples will contain the same exact words as the sound sentences but out of order."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uz8BeEYOC14m",
        "outputId": "4ae6c64a-64c0-4921-f234-5e3f51f9b5c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from random import shuffle, seed\n",
        "\n",
        "\n",
        "infile = 'wikisentences.txt'\n",
        "outfile = 'wikiscrambled.txt'\n",
        "result = list()\n",
        "\n",
        "\n",
        "def scramble_sentence(sentence):\n",
        "    sentence = sentence.strip()\n",
        "    split = sentence.split()\n",
        "    shuffle(split)\n",
        "    return ' '.join(split)\n",
        "\n",
        "\n",
        "seed()\n",
        "with open(infile, 'r', encoding='utf-8') as file:\n",
        "    lines = file.readlines()\n",
        "for line in lines:\n",
        "    line = line.strip()\n",
        "    if line == '':\n",
        "        continue\n",
        "    scrambled = scramble_sentence(line)\n",
        "    result.append(scrambled)\n",
        "    #print('Scrambled sentence:', scrambled[0:100])\n",
        "with open(outfile, 'w', encoding='utf-8') as file:\n",
        "    for line in result:\n",
        "        file.write(line+'\\n')\n",
        "print(outfile, 'saved!')        "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "wikiscrambled.txt saved!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r29uugV1DPN_"
      },
      "source": [
        "# Generate Gibberish v2\n",
        "This step may not be necessary but I'd like to be able to detect utter nonsense as well. So let's scramble all the characters in each sentence completely. I figure it's better to show the model random noise as well as random words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enJxLvqqDgIV",
        "outputId": "30ef1a26-4960-487f-a2d4-8dbd6c3be80e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from random import shuffle, seed\n",
        "\n",
        "\n",
        "infile = 'wikisentences.txt'\n",
        "outfile = 'wikiscrambled2.txt'\n",
        "result = list()\n",
        "\n",
        "\n",
        "def scramble_sentence(sentence):\n",
        "    sentence = sentence.strip()\n",
        "    sentence = list(sentence)\n",
        "    shuffle(sentence)\n",
        "    return ''.join(sentence)\n",
        "\n",
        "\n",
        "seed()\n",
        "with open(infile, 'r', encoding='utf-8') as file:\n",
        "    lines = file.readlines()\n",
        "for line in lines:\n",
        "    line = line.strip()\n",
        "    if line == '':\n",
        "        continue\n",
        "    scrambled = scramble_sentence(line)\n",
        "    result.append(scrambled)\n",
        "    #print('Scrambled sentence:', scrambled[0:100])\n",
        "with open(outfile, 'w', encoding='utf-8') as file:\n",
        "    for line in result:\n",
        "        file.write(line+'\\n')\n",
        "print(outfile, 'saved!')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "wikiscrambled2.txt saved!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGbkx02QEJEE"
      },
      "source": [
        "# Compile Training Corpus\n",
        "Let's build a training corpus that we can feed to GPT2! We need to bake the label directly into each line. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rG9X9sD9EaRk",
        "outputId": "b8e6b210-4573-43b4-a339-65a08671a76b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from random import sample, seed\n",
        "\n",
        "files = [\n",
        "('wikisentences.txt', 'Clean'), \n",
        "('wikiscrambled2.txt', 'Gibberish'), \n",
        "('wikiscrambled.txt', 'Gibberish')\n",
        "]\n",
        "\n",
        "\n",
        "result = list()\n",
        "\n",
        "max_samples = 100\n",
        "\n",
        "corpus = 'corpus.txt' \n",
        "\n",
        "\n",
        "\n",
        "for file in files:\n",
        "    with open(file[0], 'r', encoding='utf-8') as infile:\n",
        "        lines = infile.readlines()\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if line == '':\n",
        "            continue\n",
        "        line = '// %s || %s' % (line, file[1])\n",
        "        result.append(line)\n",
        "        #print(file, line[0:80])\n",
        "\n",
        "seed()\n",
        "subset = sample(result, max_samples)\n",
        "\n",
        "with open(corpus, 'w', encoding='utf-8') as outfile:\n",
        "    for line in subset:\n",
        "        outfile.write(line+'\\n\\n')\n",
        "print(corpus, 'saved!')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "corpus.txt saved!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7UqT0KH-EykD"
      },
      "source": [
        "# Fine Tune GPT2!\n",
        "This is where the rubber meets the road! Let's see if we can finetune a GPT-2 model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROGq4tcvE512",
        "outputId": "3492f541-d754-4974-9c44-7dc5dad1b04a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#gast==0.2.2\n",
        "#tensorboard<1.16.0,>=1.15.0\n",
        "#!pip install tensorflow-gpu==1.15.0 --force-reinstall\n",
        "#!pip install gpt2-client==2.1.5 --force-reinstall --no-dependencies\n",
        "\n",
        "from gpt2_client import GPT2Client\n",
        "\n",
        "\n",
        "gpt2 = GPT2Client('345M')  # options: 117M, 345M, 774M, or 1558M\n",
        "gpt2.load_model(force_download=False) \n",
        "\n",
        "corpus = 'corpus.txt'\n",
        "\n",
        "result = gpt2.finetune(corpus, return_text=True)\n",
        "print(result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1 | 107.26] loss=5.38 avg=5.38\n",
            "[2 | 200.27] loss=5.26 avg=5.32\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}